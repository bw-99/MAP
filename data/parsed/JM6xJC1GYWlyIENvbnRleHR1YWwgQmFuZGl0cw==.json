{"title": "\u03b1-Fair Contextual Bandits", "authors": "Siddhant Chaudhary; Abhishek Sinha", "pub_date": "", "abstract": "Contextual bandit algorithms are at the core of many applications, including recommender systems, clinical trials, and optimal portfolio selection. One of the most popular problems studied in the contextual bandit literature is to maximize the sum of the rewards in each round by ensuring a sublinear regret against the best-fixed context-dependent policy. However, in many applications, the cumulative reward is not the right objectivethe bandit algorithm must be fair in order to avoid the echo-chamber effect and comply with the regulatory requirements. In this paper, we consider the \u03b1-Fair Contextual Bandits problem, where the objective is to maximize the global \u03b1-fair utility functiona non-decreasing concave function of the cumulative rewards in the adversarial setting. The problem is challenging due to the nonseparability of the objective across rounds. We design an efficient algorithm that guarantees an approximately sublinear regret in the full-information and bandit feedback settings.", "sections": [{"heading": "Introduction and related work", "text": "In applications such as personalized recommendations, greedily optimizing for the most relevant content for each user profile tends to reduce the diversity of the recommended items as it induces an unhealthy echochamber effect and propagates systematic biases (Celis et al., 2019). Recall that standard contextual bandits with a separable cumulative utility function tend to maximize the click-through rates (CTR) by recommending the most popular item for each user profile (Semenov et al., 2022). However, an over-emphasis on Preprint the CTR metric invariably leads to polarization of opinions. A similar fairness issue arises with other popular recommender systems, such as movie or song recommendations by Netflix and Spotify and various online job recommendation portals. The main objective of this paper is to design a class of fair contextual bandit algorithms equipped with a quantifiable fairness guarantee that holds even in the adversarial setting. Towards this goal, we propose a contextual bandit algorithm that maximizes the non-linear \u03b1-fair utility function instead of the usual time-separable utility function. Due to the diminishing return property, the optimizer of the concave \u03b1-fair utility function strikes a trade-off between the fairness and the accuracy of the recommendations through a tunable hyperparameter \u03b1 \u2208 [0, 1). Lan et al. (2010) gave an axiomatic characterization of fair utility functions and showed that the \u03b1-fair utility function comes out naturally. Other standard utility functions, e.g., proportional fair and min-max utilities, can be shown to be a limiting form of the \u03b1-fair utility.\nFairness in bandit and online convex optimization have been extensively studied in the literature (Joseph et al., 2016;Chen et al., 2020;Agarwal et al., 2014;Patil et al., 2021;Si Salem et al., 2022;Even-Dar et al., 2009;Claure et al., 2020;Li et al., 2019). Chen et al. (2020) considered a fair contextual bandit problem with a finite number of contexts. Their online policy ensures that the probability of pulling each arm is lower-bounded by a pre-specified constant on every round. They establish a O( \u221a T M N log N ) regret bound for the usual separable cumulative loss metric. In the stochastic setting, the work by Patil et al. (2021); Claure et al. (2020), and Li et al. (2019) proposed constrained bandit policies that guarantee that the minimum fraction of pulls of each arm exceeds a given threshold. Our work complements this line of work where we consider an unconstrained maximization of the non-separable \u03b1fair utility function. A detailed numerical comparison between our policy and the constrained bandit policy of Chen et al. (2020) is presented in Section 4. Badanidiyuru et al. (2014) considered a similar contextual bandit problem in the stochastic setting, which was later extended to concave utility functions (Agrawal and Devanur, 2014;Agrawal et al., 2016). Agrawal et al. (2016) gave an efficient policy with O( \u221a T ) regret in the stochastic setting. However, because of the impossibility of attaining a sublinear regret bound in the full-information setting (Sinha et al., 2023, Theorem 2), their result can not be extended to the adversarial rewards, which is the main focus of this paper. Closest to this paper is the recent work by Sinha et al. (2023), which considers the problem of maximizing the \u03b1-fair utility function in the non-contextual full-information setting. In this paper, we extend their policy to the adversarial contextual bandit setting with finitely many contexts. This is accomplished by combining a recent scale-free bandit policy with non-separable rewards.\nOur contributions: In this paper, we make the following contributions.\n\u2022 We propose an approximately no-regret contextual bandit algorithm for the \u03b1-fair global utility function with an approximation factor at most 1.445. The non-additivity of the \u03b1-fair utility function across rounds makes this problem significantly more challenging than the classic contextual bandit problems. We combine recent advances in online convex optimization and scale-free bandits to propose an efficient policy for this problem.\n\u2022 As a by-product of our algorithm specialized to a single context, we give the first fair MAB algorithm with an approximately sublinear regret for the \u03b1fair utility function in the adversarial setting.\n\u2022 Because of the global non-separability of the utility function, we introduce a new analytical technique involving a novel bootstrapping method to bound the regret in both full-information and bandit settings.\n\u2022 We perform extensive numerical simulations of our policy and compare it with the state-of-the-art benchmarks with standard datasets.\nAll missing proofs can be found in the accompanying supplementary material.", "publication_ref": ["b6", "b21", "b15", "b14", "b7", "b0", "b19", "b22", "b11", "b8", "b16", "b7", "b19", "b8", "b16", "b7", "b5", "b1", "b2", "b2", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "The Full-information setting", "text": "We start our discourse with the simpler full-information setting where the entire reward vector for all arms is revealed to the policy at the end of every round. The more challenging bandit feedback setting, where only the reward component corresponding to the arm that was pulled is revealed on every round (where the event 3 \u2032 takes place), will be studied in Section 3. Specifically, we consider a fully adversarial setting with N armsfoot_0 and a small number of contexts M . For structured contexts, one must reduce the number of distinct contexts, e.g., by clustering using similarity information (Slivkins, 2011), before using our algorithm.\nThe following sequence of events takes place on every round t \u2208 [T ].\n1. The adversary first decides a context-reward pair (c t , r(t)), where\nc t \u2208 [M ] and \u03b4 \u2264 r i (t) \u2264 1, \u2200i \u2208 [N ].\nHere \u03b4 > 0 is a fixed positive constant.\n2. The context c t is revealed to the online policy, which then uses this information to choose an arm (possibly randomly)\nI t \u2208 [N ].", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "(Full-Information", "text": "Setting) The policy obtains a reward of r It (t) and the entire reward vector r(t) is revealed to the policy. Or,\n3 \u2032 . (Bandit-feedback Setting)\nThe policy obtains a reward of r It (t) and only the value of r It (t) is revealed to the policy.\nFor a given online algorithm, let the probability vector\nx j (t) \u2208 \u2206 N , j \u2208 [M ]\ndenote the probability of pulling the arms when the j th context is revealed to the policy on round t. An online policy is defined by the collection of (conditional) distributions x j (t), j \u2208 [M ] , where, upon observing the current context c t , the policy samples an arm I t \u223c x ct (t) for round t. The goal of the policy is to sequentially learn the best collection of distributions x j (t), j \u2208 [M ] , one for each context, to maximize the \u03b1-fair utility function described next.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Utility function and the regret metric", "text": "For each arm i \u2208 [N ], the (expected) cumulative reward accrued till round t for a given policy is defined as:\nR i (t) = R i (t -1) + x ct i (t)r i (t), R i (0) = 1. (1)\nIn this paper, we consider the problem of maximizing the sum of \u03b1-fair utility functions of the arms where the utility of the i th arm is defined as:\n\u03d5(R i (T )) := (R i (T )) 1-\u03b1 1 -\u03b1 , i \u2208 [N ],(2)\nwhere 0 \u2264 \u03b1 < 1 is some fixed constant. The parameter \u03b1 strikes a trade-off between fairness and efficiency.\nSetting \u03b1 = 0 corresponds to the usual linear reward function. On the other hand, larger \u03b1 induces fairness because of the diminishing return property, which encourages playing all arms evenly (Lan et al., 2010).\nFormally, our objective is to design an online policy that minimizes the c-approximate contextual regret, which competes with the best offline policy in hindsight (i.e., a fixed mapping from contexts to arms) instead of the best arm. Formally, the contextual regret is defined as:\nRegret T (c) := max x * N i=1 \u03d5(R * i (T )) -c N i=1 \u03d5(R i (T )),(3)\nwhere c \u2265 1 is some small constant, and, for each user i, R * i (T ) is the cumulative reward (1) accrued by any static policy using the fixed collection of distributions x * \u2261 (x 1 * , ..., x M * ) used in Eq. ( 1). A few words on the c-regret metric (3) are in order. Clearly, c = 1 corresponds to the usual static regret. However, it is known from Sinha et al. (2023, Theorem 2) that even in the full-information setting, no online policy can achieve a sublinear regret for c = 1. The concept of c-approximate regret has been useful in other online learning problems as well (Azar et al., 2022;Emamjomeh-Zadeh et al., 2021;Paria and Sinha, 2021).\nNote: 1. We initialize R i (0) to 1 so that the derivative \u03d5 \u2032 (R i (t)) remains well-defined for all t \u2208 [T ].\n2. In the full-information setting, we work exclusively with the expected cumulative rewards rather than the true rewards, which is stochastic due to the randomness of the policy. This allows us to carry out a simpler deterministic analysis. Using standard concentration inequalities, it can be shown that resulting bounds carry over for the true rewards as well (Sinha et al., 2023, Section 4). However, due to the limited feedback, this trick no longer works in the bandit setting, where we work with the stochastic true rewards.\nFigure 1: Diagram representing the web of reductions used in the paper. First, the contextual bandit problem with a global \u03b1fair objective is reduced to a standard online linear optimization (OLO) problem. The reduction works the same way in both the full-information and bandit-information feedback settings. Then, in either setting, we parallelly run M instances of a non-contextual policy, and all the M policies are coupled through the shared vector R(t) of cumulative rewards. On a high level, after the linearization step, the j th policy for j \u2208 [M ] controls the regret for the jth context.", "publication_ref": ["b15", "b4", "b10", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm design I: Linearization", "text": "Similar to Sinha et al. (2023), the algorithm design proceeds in two steps -(1) linearization with policydependent gradients and then (2) solving the linearized online optimization problem. See Figure 1 for a schematic. In the linearization step, we first reduce the problem to an instance of an online linear optimization (OLO) problem. Since the utility function \u03d5(\u2022) is concave, we have\n\u03d5(x) -\u03d5(y) \u2264 \u03d5 \u2032 (y)(x -y)(4)\nfor all x, y > 0. Now, let \u03b2 \u2265 1 be a constant, which will be fixed later. Taking x = R * i (T ) and y = \u03b2R i (T ) in the above inequality, we get\n\u03d5(R * i (T )) -\u03b2 1-\u03b1 \u03d5(R i (T )) (a) = \u03d5(R * i (T )) -\u03d5(\u03b2R i (T )) (b) \u2264 \u03d5 \u2032 (\u03b2R i (T ))[R * i (T ) -\u03b2R i (T )] (c) \u2264 \u03b2 -\u03b1 \u03d5 \u2032 (R i (T )) T t=1 r i (t)[x ct * ,i -\u03b2x ct i (t)],(5)\nwhere in (a), we have used the property that \u03d5(\u03b2x) = \u03b2 1-\u03b1 (x) which holds for (2); in (b), we have used inequality (4), and in (c), we have used the definition of the cumulative rewards given in (1), the fact that \u03b2 \u2265 1 and the property \u03d5 \u2032 (\u03b2x) = \u03b2 -\u03b1 \u03d5 \u2032 (x). Summing up the bound (5) over all the arms i \u2208 [N ], we obtain the following bound to the \u03b2 1-\u03b1 -approximate regret of any online policy:\nRegret T (\u03b2 1-\u03b1 ) \u2264 \u03b2 -\u03b1 T t=1 i\u2208[N ] \u03d5 \u2032 (R i (T ))r i (t)[x ct * ,i -\u03b2x ct i (t)].(6)\nNote that R i (T ) is the cumulative reward accrued in the entire horizon of length T , and hence, it depends on the entire sequence of rewards and the actions of the policy. Clearly, this non-causal information is not available to the online policy at any intermediate round t < T. This shows that directly minimizing the upper bound (6) using online convex optimization methods is not feasible as the reward function involves the variables \u03d5 \u2032 (R i (T ))'s. To get around this fundamental difficulty, we now define a surrogate online linear optimization problem by replacing the t th coefficient \u03d5 \u2032 (R i (T )) in the RHS of the upper bound (6) with its causal surrogate \u03d5 \u2032 (R i (t -1)). With this substitution, the problem of minimizing (6) becomes an instance of the online linear optimization (OLO) problem. However, in contrast with the standard OLO problem, here the reward functions are no longer oblivious as they depend on the policy through its past actions. By bounding the regret of the surrogate problem, we show that it is possible to derive an approximate regret bound to the original regret minimization problem (3). Hence, dropping the factor \u03b2 -\u03b1 , the surrogate regret that we minimize is:\nSurrogate Regret T = max x * T t=1 i\u2208[N ] \u03d5 \u2032 (R i (t -1))r i (t)[x ct * ,i -x ct i (t)] (7)\nIn particular, for the surrogate problem, the linear reward vector at time step t is given by \u03d5 \u2032 (R(t -1)) \u2299 r(t), which implicitly depends on the past actions of the policy (through the first term). Here,\n\u03d5 \u2032 (R(t - 1)) \u2261 (\u03d5 \u2032 (R 1 (t -1)), ..., \u03d5 \u2032 (R N (t -1))). Upon setting \u03b2 \u2261 (1 -\u03b1) -1\n, the following result relates the original regret (3) with the surrogate regret ( 7) for any policy.\nLemma 2.1. For any T \u2265 1 and for any policy, we have\nRegret T (c \u03b1 ) \u2264 (1 -\u03b1) \u03b1 Surrogate Regret T + c \u03b1 N (8)\nwhere\nc \u03b1 = (1 -\u03b1) -(1-\u03b1) \u2264 e 1/e < 1.445.\nAfter accounting for M different contexts with a common cumulative reward vector R(t), the proof generalizes the arguments in Sinha et al. (2023, Lemma 1). See Section 6.1 in the Appendix for the complete proof.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm design II: Solving the linearized problem with full information", "text": "In view of the regret bound (8), we now propose \u03b1-FairCB -an online policy to approximately minimize the surrogate regret (7). In brief, \u03b1-FairCB runs M instances of adaptive online gradient descent policy in parallel, where the j th instance is responsible for controlling the regret for the j th context. These parallel policies are coupled through the common state vector R(t) -the cumulative reward accrued up to time t, which is affected by all contexts. Technically, this strategy works because, after the linearization step above, using the Cauchy-Scwarz inequality, the regret can be upper-bounded by the sum of policy-dependent gradients over all M instances. Finally, the norm of these policy-dependent gradients are controlled using a novel bootstrapping technique. The following lemma gives a precise regret bound for the surrogate problem.\nLemma 2.2. The \u03b1-FairCB policy described in Algorithm 1 achieves the following static regret bound for the surrogate problem (7):\nSurrogate Regret T = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 O(N 3 M T 1/2-\u03b1 ), if 0 < \u03b1 < 1 2 O(N 3 M \u221a log T ), if \u03b1 = 1 2 O(1), if 1 2 < \u03b1 < 1. (9)\nSee Section 6.2 for the proof of the result. The proof of this lemma exploits a novel bootstrapping technique which repeatedly boosts the estimate of the gradients, which are controlled by the policy, to obtain a better adaptive regret bound. Combining Lemma 2.1 and Lemma (2.2), we establish our main result.\nTheorem 2.3. Algorithm 1 achieves the following approximate regret bound for the contextual bandit problem in the full information setting with the \u03b1-fair utility function:\nRegret T (c \u03b1 ) = (1 -\u03b1) \u03b1 \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 O(N 3 M T 1/2-\u03b1 ), if 0 < \u03b1 < 1 O(N 3 M \u221a log T ), if \u03b1 = 1 2 O(1), if 1 2 < \u03b1 < 1.\nwhere\nc \u03b1 = (1 -\u03b1) -(1-\u03b1) < 1.445.\nAlgorithm 1 \u03b1-FairCB (Full Information Setting) 1: Input: Fairness parameter 0 \u2264 \u03b1 < 1, Sequence of reward vectors r(1), ..., r(T ), Sequence of contexts c 1 , ..., c T , Euclidean projection oracle on the simplex \u03a0 \u2206 N , and an upper bound D = \u221a 2 to the Euclidean diameter of the simplex \u2206 N . 2: Output: Distributions x ct (t) for each round t. 3: Initialization:\nR i (0) \u2190 1, S j \u2190 0, x j \u2190 1 N , \u2200i, j.\n4: for t = 1 to T do 5:\nReceive the context c t for round t.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "6:", "text": "if Context c t is seen for the first time then 7:\nOutput x ct (t) = x ct (uniform distribution).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "8:", "text": "else 9:\nLet t \u2032 be the last time step when context c t was seen.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "10:", "text": "Compute gradient vector g as follows:\ng i = r i (t \u2032 ) R \u03b1 i \u2200i \u2208 [N ]\n11:\nUpdate the cumulative gradient norm:\nS ct \u2190 S ct + \u2225g\u2225 2 2 12:\nCarry out the online gradient ascent update:\nx ct \u2190 \u03a0 \u2206 N x ct + D 2S ct g 13:\nOutput x ct (t) = x ct .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "14:", "text": "end if 15:\nObserve reward vector r(t).\n16:\nUpdate R i (t) \u2190 R i (t -1) + x ct i (t)r i (t)\n. 17: end for We now study the same problem in the more challenging bandit feedback model. In this setup, only the reward of the arm selected by the policy, i.e., r ct It (t), is revealed on each round. Following standard practice, we assume that the reward vectors r(t) and the context sequence c t \u2208 [M ] for each time step t are generated by an oblivious adversary, i.e., the sequence of rewards and contexts is fixed a priori.\nBecause of the limited feedback, an online policy cannot observe the expected cumulative rewards defined in Eqn.\n(1) as one needs to know the entire reward vector r(t) to compute the expected reward. Hence, instead of using the distribution x ct (t), we directly use the random one-hot encoded vector X ct (t) to define the true cumulative rewardsfoot_2 . Here, the I t th component (which corresponds to the selected arm) of the vector X ct (t) is set to one, and the rest of the components are set to zero. Hence, the true cumulative reward vector, which the policy can observe under the bandit feedback setting, evolves as follows:\nR i (t) = R i (t -1) + X ct i (t)r i (t), R i (0) = 1.(10)\nAs before, we will use the notation x ct (t) \u2208 \u2206 N to denote the probability distribution of pulling the arms on step t. Hence, for all i \u2208 [N ] and t \u2208 [1, T ], we have\nP[X ct i (t) = 1] = x ct i (t). (11\n)\nOur objective is to design a policy which minimizes the expected c-approximate regret defined below:\nRegret T (c) := max x * \u2208(\u2206 N ) M E \uf8ee \uf8f0 i\u2208[N ] \u03d5(R * i (T )) -c i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb .(12)\nIn the above definition, c \u2265 1 is a small constant whose value will be specified later and R * (T ) is the cumulative reward vector obtained for a stationary contextual bandit policy which pulls arms according to the fixed collection of distributions x * \u2261 (x 1 * , ..., x M * ) depending on the current context. Let (x 1 * , ..., x M * ) \u2208 (\u2206 N ) M be the best-fixed collection of distributions which achieves the maximum in (12). We have\nRegret T (c) = E \uf8ee \uf8f0 i\u2208[N ] \u03d5(R * i (T )) -c i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb (a) = i\u2208[N ] E[\u03d5(R * i (T ))] -cE \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb (b) \u2264 i\u2208[N ] \u03d5(E[R * i (T )]) -cE \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb (c) = i\u2208[N ] \u03d5 1 + T t=1 r i (t)x ct * ,i -cE \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb (13)\nAbove, in (a), we have used the linearity of expectation.\nIn (b), we have used Jensen's Inequality on the concave function \u03d5. In (c), we have just expanded E[R * i (T )] using ( 10) and (11).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm design I: Linearization", "text": "Similar to the full-information setting, we handle the non-linearity by reducing the problem to a standard bandit problem with appropriately constructed linear reward functions. Following (5), we have\n\u03d5(ER * i (T )) -\u03b2 1-\u03b1 \u03d5(R i (T )) \u2264 \u03b2 -\u03b1 \u03d5 \u2032 (R i (T )) T t=1 r i (t)[x ct * ,i -\u03b2X ct i (t)](14)\nwhere above, \u03b2 \u2265 1 is some constant to be fixed later. Summing the above inequality for all i \u2208 [N ] and taking expectations w.r.t the actions of the policy, we have\ni\u2208[N ] \u03d5(ER * i (T )) -\u03b2 1-\u03b1 E \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb \u2264 \u03b2 -\u03b1 E \uf8ee \uf8f0 i\u2208[N ] T t=1 \u03d5 \u2032 (R i (T ))r i (t)[x ct * ,i -\u03b2X ct i (t)] \uf8f9 \uf8fb .(15)\nCombining the last inequality with (13), we get\nRegret T (\u03b2 1-\u03b1 ) \u2264 \u03b2 -\u03b1 E \uf8ee \uf8f0 i\u2208[N ] T t=1 \u03d5 \u2032 (R i (T ))r i (t)[x ct * ,i -\u03b2X ct i (t)] \uf8f9 \uf8fb .(16)\nMotivated by the above bound, we now consider a surrogate bandit problem by replacing the term \u03d5 \u2032 (R i (T ))\nwith its causal counterpart \u03d5 \u2032 (R i (t -1)). We now design an online policy to minimize the surrogate regret defined as follows:\nSurrogate Regret T \u2261 E \uf8ee \uf8f0 i\u2208[N ] T t=1 \u03d5 \u2032 (R i (t -1))r i (t)[x ct * ,i -X ct i (t)] \uf8f9 \uf8fb = E T t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct * -X ct (t)\u27e9 . (17) As before, \u03d5 \u2032 (R(t -1)) \u2261 (\u03d5 \u2032 (R 1 (t -1)), ..., \u03d5 \u2032 (R N (t - 1))).\nAnalogous to Lemma 2.1, we have the following result, which relates the regret defined in (12) to the surrogate regret defined in (17).\nLemma 3.1. For any T \u2265 1, we have\nRegret T (c \u03b1 ) \u2264 (1 -\u03b1) \u03b1 Surrogate Regret T + c \u03b1 N,(18)\nwhere\nc \u03b1 = (1 -\u03b1) -(1-\u03b1) \u2264 e 1/e < 1.445.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm design II: Solving the linearized problem with bandit feedback", "text": "Lemma 3.1 motivates us to design an online policy that minimizes the regret (17) for the surrogate bandit problem. However, unlike the standard adversarial bandit problem, where the reward functions are fixed a priori in an oblivious fashion, in this case, the rewards for each round t, defined as g t \u2261 \u03d5 \u2032 (R(t -1)) \u2299 r t , depends on the past actions of the policy. We can decompose the surrogate regret over the contexts as follows:\nE T t=1 \u27e8g t , x ct * -X ct (t)\u27e9 = E \uf8ee \uf8f0 j\u2208[M ] t:ct=j \u27e8g t , x j * -X j (t)\u27e9 \uf8f9 \uf8fb (a) = j\u2208[M ] E \uf8ee \uf8f0 t:ct=j \u27e8g t , x j * -X j (t)\u27e9 \uf8f9 \uf8fb (b) \u2264 j\u2208[M ] E \uf8ee \uf8f0 max y\u2208{e k } N k=1 t:ct=j \u27e8g t , y -X j (t)\u27e9 \uf8f9 \uf8fb regret for the j th context =: Regret T .(19)\nAbove, in (a), we have used the linearity of expectation, in (b), we have used the fact that for any fixed sequence of rewards in a bandit OLO problem, the best offline benchmark is the best-fixed arm in hindsight. The above inequality can be written as\nSurrogate Regret T \u2264 Regret T (20)\nTo minimize the surrogate regret, we now design a policy that minimizes Regret T , which is the sum of the regret for each context. Note that since the cumulative reward vector is common to all contexts, the regret bounds for different contexts are coupled with each other. To solve the per-context learning problem, we use the adaptive and scale-free multi-armed bandit policy, proposed by (Putta and Agrawal, 2022), as a black box. Specifically, we run M parallel instances of this policy, one for each context where they share the global cumulative reward vector R(t). For ease of reference, we quote regret bound achieved by the bandit policy of Putta and Agrawal (2022) in the following theorem.\nAlgorithm 2 \u03b1-FairCB (Bandit Information Setting) 1: Input: Fairness parameter 0 \u2264 \u03b1 < 1, Sequence of reward vectors r(1), ..., r(T ), Sequence of contexts c 1 , ..., c t . 2: Output: Observe context c t .\nArm I t \u2208 [N ] to be played at round t, for t \u2208 [1, T ]. 3: Initialize R i (0) \u2190 1 for all i \u2208 [N ].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "7:", "text": "Play an arm I t picked by policy A ct . Let X ct (t) denote the one-hot vector representing arm I t .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "8:", "text": "Feed the modified reward vector \u03d5 \u2032 (R(t -1)) \u2299 r(t) to policy A ct . 3 9:\nUpdate R i (t) \u2190 R i (t -1) + X ct i (t)r i (t) for all i \u2208 [N ].\n10: end for Theorem 3.2 (Theorem 1 of (Putta and Agrawal, 2022)). For any oblivious sequence of reward vectors l 1 , ..., l T \u2208 R N , the adaptive version of Algorithm 1 of Putta and Agrawal (2022) achieves the following regret bound:\nE max {e k } N k=1 T t=1 \u27e8l t , e k -X(t)\u27e9 = O(log T \u2022 [ N L 2 + L \u221e N L 1 ]).(21)\nIn the above, X(t) is the one-hot encoded vector denoting the arm pulled on round t,\nL \u221e = max t \u2225l t \u2225 \u221e , L 2 = T t=1 \u2225l t \u2225 2 2 , L 1 =\nT t=1 \u2225l t \u2225 1 and the expectation is taken w.r.t. the actions of the policy.\n3 Even though we pass the full vector \u03d5 \u2032 (R(t -1)) \u2299 r(t) to the bandit subroutine, it only \"sees\" the reward \u03d5 \u2032 (RI t (t -1))rI t (t) for the arm It it has just picked.\nRemarks: Technically, the regret bound in Theorem 3.2 was originally established for oblivious adversaries. However, in our case, the surrogate reward vector g t depends on the past actions of the policy up to round t -1. To see why we can still plug in the generic regret bound (21), note that the reward vector g t on round t does not depend on the action X(t) taken on round t. Hence, we can use the regret bound for an imaginary adversary that fixes the reward vector g t at the end of round t -1. Since the reward on round t does not affect the previous actions of the policy, the regret bound (21) holds. Adapting the above bound to our contextual setting, we have the following scale-free regret bound.\nLemma 3.3. For any t \u2208 [1, T ], let g t := \u03d5 \u2032 (R(t - 1)) \u2299 r(t).\nThe adaptive version of Algorithm 1 of (Putta and Agrawal, 2022) achieves the following bound for any j \u2208 [M ]:\nE \uf8ee \uf8f0 max y\u2208{e k } N k=1 t:ct=j \u27e8g t , y -X j (t)\u27e9 \uf8f9 \uf8fb \u2264 \u00d5 \uf8eb \uf8ed E \uf8ee \uf8f0 N t:ct=j \u2225g t \u2225 2 2 + max t:ct=j \u2225g t \u2225 \u221e N t:ct=j \u2225g t \u2225 1 \uf8f9 \uf8fb \uf8f6 \uf8f8 ,(22)\nwhere the \u00d5(\u2022) notation hides the logarithmic factors. Above, the expectation is taken w.r.t the policy actions.\nPlease refer to Section 6.5 for the proof. The following result bounds the surrogate regret (19).\nLemma 3.4. The \u03b1-FairCB policy described in Algorithm 2 achieves the following bound on the regret of the surrogate bandit OLO problem for the \u03b1-fair utility function:\nRegret T = \u00d5(M N 2 T 1-\u03b1 2 ) (23\n)\nwhere the \u00d5(\u2022) notation hides the log T factor.\nFinally, combining Lemma 3.1, (20) and Lemma 3.4, we establish our main result.\nTheorem 3.5. Algorithm 2 achieves the following approximate regret bound for the contextual bandit problem in the bandit information feedback setting with the \u03b1-fair utility function:\nRegret T (c \u03b1 ) = (1 -\u03b1) \u03b1 \u00d5(M N 2 T 1-\u03b1 2 ) (24\n)\nwhere c \u03b1 = (1\u03b1) -(1-\u03b1) < 1.445, and the \u00d5 notation hides factors logarithmic in T .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We evaluate the performance of the proposed algorithm on a movie genre recommendation problem using the MovieLens 25M dataset (Harper and Konstan, 2015). The dataset consists of 25 million data points, each consisting of a movie rating given by a user. For our experiments, we take a small sample comprising of the first 5, 000 data points. The underlying contextual bandit problem is formulated as follows: we interpret the users as contexts and movie genres as arms. In the selected sample, the number of contexts turns out to be M = 33, and the number of arms featured is N = 19. The dataset is sorted by the column containing the timestamps at which the ratings were reported, and this is taken to be the order of request arrivals.\nSince our policy requires a positive lower bound to the rewards, we take the minimum reward to be 0.2 if the recommended genre doesn't fit the current movie and 1 otherwise. In our experiments, we study both the full information and the bandit information settings.\nPerformance metrics: We define the \u03b1-performance of a policy at time stamp t \u2208 [1, T ] in these experiments as the total \u03b1-fair utility:\n\u03b1-Performance(t) := i\u2208[N ] \u03d5(R i (t)).(25)\nTo measure fairness, we use the popular Jain's Fairness Index (Jain et al., 1998), calculated for the vector of cumulative rewards at the end of the time horizon. For any round t \u2208 [1, T ], Jain's fairness index is defined as:\nJain's Fairness Index := (\nN i=1 R i (t)) 2 N N i=1 R 2 i (t) .(26)\nJain's fairness index assumes a value between 0 and 1, where a value of 1 is obtained when all components of the reward vector are the same (i.e., fully fair).\nIn particular, if each arm receives an equal share of cumulative rewards, this index will be 1. Throughout our experiments, we take \u03b1 = 0.9 (i.e. a high level of fairness). We also plot the approximate contextual regret as defined in equations ( 3) and ( 12) for the full information and bandit feedback settings, respectively.\nCalculating the offline baseline metrics: Note that the offline benchmarks in equations ( 3) and ( 12) required for computing the approximate regret involve computing the best offline collection of M distributions maximizing the cumulative \u03b1-fair utility function. Since \u03d5(\u2022) is a concave function, this is a standard concave maximization problem over the convex domain (\u2206 N ) M . In our experiments, we use the CVXPY package for solving this problem (Diamond and Boyd, 2016).", "publication_ref": ["b12", "b13", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments in the Full-information Setting", "text": "Baseline Policies: We consider two baselines (1) a context-agnostic Hedge policy (i.e. a policy that \u03b1-Fair Contextual Bandits ignores contexts) and ( 2) the FairCB policy from (Chen et al., 2020). Note that inherently Hedge is not a fair policy as its objective is to optimize the total reward. On the other hand, FairCB's fairness constraint is specified by a tunable parameter \u03bd \u2208 (0, 1 N ); in particular, the constraint is that the marginal probability of each arm being pulled at any given time step is at least \u03bd. For our experiments, we consider \u03bd = 1 2N (note that 1 N is the largest possible fairness level that is allowed by FairCB). Note that the FairCB policy assumes the context distribution to be known; we simply generate this distribution offline by observing the sequence of contexts (users) in the dataset (and generating a distribution based on the frequencies of each user) and feed it back to the FairCB policy.\nResults: Figure 2 shows that the proposed \u03b1-FairCB policy outperforms the Hedge and FairCB policies in terms of \u03b1-performance (25). As expected, the context-agnostic Hedge policy performs the worst among the three policies under consideration. Consequently, \u03b1-FairCB achieves the lowest approximate regret among all the policies (Figure 3). Finally, in terms of Jain's Fairness Index (26), we observe that the proposed \u03b1-FairCB outperforms both the noncontextual Hedge and FairCB policies even for a moderately large time horizon (Figure 4).", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments in the Bandit Setting", "text": "Baseline Policies: As a baseline policy, we run the context-agnostic adaptive multi-armed bandit policy proposed by Putta and Agrawal (2022), which is also used by our contextual bandit policy as a subroutine.\nResults: From Figure 5, it is observed that \u03b1-FairCB outperforms the policy by (Putta and Agrawal, 2022) in terms of \u03b1-performance, and consequently \u03b1-FairCB achieves a lower approximate regret as well (as seen in Figure 6). In terms of Jain's Fairness Index, it is observed from Figure 7 that although for the first few rounds, (Putta and Agrawal, 2022)'s policy outperforms \u03b1-FairCB, but over the entire time horizon, \u03b1-FairCB achieves a significantly better fairness index. The behaviour for the first few time steps can be explained by the fact that Putta and Agrawal (2022)'s policy has an exploration component, which makes the policy choose each arm with an approximately equal probability in the initial stages. However, since their policy maximizes the cumulative rewards, it achieves a worse fairness index over a longer horizon. See Section 7 in the Appendix for additional experimental results.", "publication_ref": ["b20", "b20"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "In this paper, we considered the problem of learning adversarial unstructured context-to-reward mapping and proposed an approximately regret-optimal policy in the full-information and bandit feedback setting. In the future, it will be interesting to design efficient algorithms for the case of structured contexts. Finally, similar to Chen et al. (2020), designing \u03b1-fair bandit algorithms that guarantee a fixed fraction of pulls to each arm would also be interesting to investigate.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Lemma 2.1", "text": "Before proving the claim, we establish an auxiliary result that will be useful later. Lemma 6.1. Under any policy which updates the cumulative rewards of the i th user R i (\u2022) as in (1) \u2200i \u2208 [N ], the following inequality holds:\n\u03d5 \u2032 (R i (t -1))[R i (t) -R i (t -1)] \u2264 Ri(t)-1 Ri(t-1)-1 \u03d5 \u2032 (R)dR. (27\n)\nProof. Since 0 \u2264 \u03b1 < 1, observe that the utility function \u03d5(\u2022) given by Eq. ( 2) is well-defined on [0, \u221e) and is differentiable in (0, \u221e). Also, because R i (\u2022) is monotonically non-decreasing and R i (0) = 1, we note that\nR i (t -1) -1 \u2265 0 for all t \u2208 [1, T ].\nBy the fundamental theorem of calculus combined with the mean value theorem, we have\nRi(t)-1 Ri(t-1)-1 \u03d5 \u2032 (R)dR = \u03d5 \u2032 (c 0 )[R i (t) -R i (t -1)](28)\nfor some\nc 0 \u2208 (R i (t -1) -1, R i (t) -1); in particular, we have c 0 < R i (t) -1. Now, from the defintion (1) observe that R i (t) -R i (t -1) = x ct i (t)r i (t) \u2264 1\n, where we have used the fact that\nx ct i (t), r i (t) \u2264 1. This implies that R i (t) -1 \u2264 R i (t -1), and hence, c 0 < R i (t -1). Finally, since \u03d5(\u2022) is concave, \u03d5 \u2032 (\u2022) is non-increasing; this implies that \u03d5 \u2032 (c 0 ) \u2265 \u03d5 \u2032 (R i (t -1)).\nCombining this with (28), the claim follows.\nWe now establish Lemma 2.1.\nProof. The upper bound for Regret T (\u03b2 1-\u03b1 ) from Eq. ( 6) can be split into the difference of two terms A and B as defined below:\nRegret T (\u03b2 1-\u03b1 ) \u2264 \u03b2 -\u03b1 [A -\u03b2B],(29)\nwhere\nA = i\u2208[N ] \u03d5 \u2032 (R i (T )) T t=1 r i (t)x ct * ,i ,(30)\nB = i\u2208[N ] \u03d5 \u2032 (R i (T )) T t=1 r i (t)x ct i (t).(31)\nAlso, let A \u2032 and B \u2032 denote the corresponding terms in the regret expression (7) for the surrogate OLO problem.\nWe will now bound the terms A and B in terms of A \u2032 and B \u2032 , respectively.\nProving A \u2264 A \u2032 : Note that the utility function \u03d5(\u2022) is concave, and hence its derivative is non-increasing. Also, from the recurrence equation for the cumulative rewards (1), it is clear that under any policy,\nR i (\u2022) is non-decreasing for any i \u2208 [N ]. Hence, we see that \u03d5 \u2032 (R i (t -1)) \u2265 \u03d5 \u2032 (R i (T )) for all t \u2208 [1, T ] and i \u2208 [N ]. This implies that A = i\u2208[N ] \u03d5 \u2032 (R i (T )) T t=1 r i (t)x ct * ,i \u2264 i\u2208[N ] \u03d5 \u2032 (R i (t -1)) T t=1 r i (t)x ct * ,i = A \u2032 (32) Proving B \u2032 \u2264 (1 -\u03b1) -1 (B + N ):\nWe now argue that the following set of inequalities holds:\nB \u2032 = i T t=1 \u03d5 \u2032 (R i (t -1))r i (t)x ct i (t) (a) = i T t=1 \u03d5 \u2032 (R i (t -1))[R i (t) -R i (t -1)]] (b) \u2264 i T t=1 Ri(t)-1 Ri(t-1)-1 \u03d5 \u2032 (R)dR (c) \u2264 i Ri(T ) 0 \u03d5 \u2032 (R)dR (d) = i \u03d5(R i (T )) (e) = (1 -\u03b1) -1 i \u03d5 \u2032 (R i (T ))R i (T ) (f ) = (1 -\u03b1) -1 i\u2208[N ] \u03d5 \u2032 (R i (T )) 1 + T t=1 x ct i (t)r i (t) (h) \u2264 (1 -\u03b1) -1 (B + N )(33)\nwhere in (a), we have used the recurrence for R i (\u2022) as given in (1). In (b), we have used ( 27). In (c), we have simply used the fact that R i (0) -1 = 0 and R i (T ) -1 \u2264 R i (T ). In (d), we have used the fundamental theorem of calculus and the fact that \u03d5(0) = 0. In (e), we have used the fact that x\u03d5 \u2032 (x) = (1\u03b1)\u03d5(x) which holds for the \u03b1-fair utility function \u03d5(\u2022). In (f ), we have used the definition of the cumulative rewards as in (1). In (h), we have used the definition of B and the fact that \u03d5 \u2032 (x) = x -\u03b1 \u2264 1 for all x \u2265 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Now, the inequality", "text": "B \u2032 \u2264 (1 -\u03b1) -1 (B + N ) implies that (1 -\u03b1)B \u2032 -N \u2264 B. Since \u03b2 > 0, we have \u03b2B \u2265 \u03b2(1 -\u03b1)B \u2032 -\u03b2N . Combining this with A \u2264 A \u2032 , we have that A -\u03b2B \u2264 A \u2032 -\u03b2(1 -\u03b1)B \u2032 + \u03b2N.(34)\nNow, pick \u03b2 = (1\u03b1) -1 (which ensures that \u03b2 \u2265 1), and hence we obtain\nA -\u03b2B \u2264 A \u2032 -B \u2032 + (1 -\u03b1) -1 N,(35)\nand from Eq. ( 29), we see that\nRegret T (c \u03b1 ) \u2264 (1 -\u03b1) \u03b1 (A \u2032 -B \u2032 ) + c \u03b1 N = (1 -\u03b1) \u03b1 Surrogate Regret T + c \u03b1 N,(36)\nwhich completes the proof of the lemma.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Lemma 2.2", "text": "For ease of notation, let (x 1 * , ..., x M * ) \u2208 (\u2206 N ) M be the collection of distributions achieving the maximum in equation ( 7). Now, observe that Surrogate Regret T defined in (7) for the surrogate problem can be split into the sum of regrets over each of the contexts as follows:\nSurrogate Regret T = T t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct * -x ct (t)\u27e9 = j\u2208[M ] t:ct=j \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x j * -x j (t)\u27e9 (a) \u2264 j\u2208[M ] max x j \u2022 \u2208\u2206 N t:ct=j \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x j \u2022 -x j (t)\u27e9 Regret for the j th context(37)\nwhere above in (a), we have simply used that the regret w.r.t x j * for context j is upper bounded by the regret associated to the best offline benchmark x j\n\u2022 for context j. Next, from the pseudocode of \u03b1-FairCB (Full Information Version, Algorithm 1), note that a Projected Online Gradient Ascent (OGA) policy with adaptive step sizes (Theorem 4.14 of (Orabona, 2019)) controls the regret for each context j \u2208 [M ]. For the sake of completeness, we mention the complete statement of the regret guarantee of the OGA policy. Theorem 6.2 (Theorem 4.14 of (Orabona, 2019)). Let \u2206 \u2282 R d be a convex set with diameter D. Let us consider a sequence of linear reward functions with gradients {g t } t\u22651 . Run the Online Gradient Ascent policy with step\nsizes \u03b7 t = D \u221a 2 T \u03c4 =1 \u2225g \u03c4 \u2225 2 , 1 \u2264 t \u2264 T .\nThen, the standard regret under the OGA policy can be upper bounded as follows:\nRegret T \u2264 D 2 T t=1 \u2225g t \u2225 2 . (38\n)\nNote that, for our case we have D = \u221a 2. So, by the regret bound of the OGA policy (38), for any j \u2208 [M ] we have max\nx j \u2022 \u2208\u2206 N t:ct=j \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x j \u2022 -x j (t)\u27e9 \u2264 D 2 t:ct=j \u2225\u03d5 \u2032 (R(t -1)) \u2299 r(t)\u2225 2 2 (a) \u2264 D 2 t:ct=j \u2225\u03d5 \u2032 (R(t -1))\u2225 2 2 (b) = D 2 t:ct=j i\u2208[N ] 1 R 2\u03b1 i (t -1)(39)\nwhere above in (a), we have used the fact that r(t) \u2264 1 for all t, and in (b) we have used the fact that \u03d5 \u2032 (x) = x -\u03b1 .\n\u03b1-Fair Contextual Bandits\nNow, summing (39) over all the contexts j \u2208 [M ] and combining this with (37), we see that\nSurrogate Regret T \u2264 j\u2208[M ] D 2 t:ct=j i\u2208[N ] 1 R 2\u03b1 i (t -1) = M j\u2208[M ] 1 M D 2 t:ct=j i\u2208[N ] 1 R 2\u03b1 i (t -1) (a) \u2264 DM 2 M j\u2208[M ] t:ct=j i\u2208[N ] 1 R 2\u03b1 i (t -1) = D \u221a M 2 T t=1 i\u2208[N ] 1 R 2\u03b1 i (t -1) , (40\n)\nwhere above in (a), we have used Jensen's Inequality for the square root function. Using the fact that R i (t -1) \u2265 1 for all t, bound (40\n) implies that Surrogate Regret T \u2264 O( \u221a M N T ).(41)\nIn the following, we show that the above O( \u221a T ) regret bound can be substantially improved using a novel bootstrapping technique described below.\nBootstrapping: Note that the adaptive regret bound depends on the sum of the norm of gradients of the reward vectors, which are controlled by the policy itself. This is in sharp contrast with the usual OCO setting where the policy does not explicitly control the gradients, and the final regret bound is given in terms of the sum of the norm of gradients as given in (38). The bootstrapping technique starts with a trivial upper bound on the gradient norms and then uses the regret bound itself to improve the upper bounds on the gradient norms. This, in turn, improves the regret bound through the adaptive regret bound (38). The process is repeated a few times to get the best possible bound.\nWe now apply the general bootstrapping method to our problem. Note that by the definition of Surrogate Regret T in (7), we have the following inequality for any fixed collection (x 1 0 , ..., x M 0 ) \u2208 (\u2206 N ) M of distributions:\nT t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct (t)\u27e9 \u2265 T t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct 0 \u27e9 -Surrogate Regret T .(42)\nAlso, using the fact that x ct i (t)r i (t) = R i (t) -R i (t -1) and following the same calculations up to step (d) of ( 33), we see that\nT t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct (t)\u27e9 \u2264 i\u2208[N ] \u03d5(R i (T )). (43\n)\nCombining the above inequality with (42), we have\ni\u2208[N ] \u03d5(R i (T )) \u2265 T t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct 0 \u27e9 -Surrogate Regret T .(44)\nNext, we lower bound \u03d5 \u2032 (R(t -1)) by \u03d5 \u2032 (R(T )) and pick x j 0 = 1 N 1 for all j \u2208 [M ] (i.e., we pick the uniform distribution as an offline benchmark for each context). Doing so, and using the fact that r(t) \u2265 \u03b41 for all t, we have\nT t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct 0 \u27e9 \u2265 T t=1 \u27e8\u03d5 \u2032 (R(T )) \u2299 r(t), x ct 0 \u27e9 = i\u2208[N ] T t=1 \u03d5 \u2032 (R i (T ))r i (t) 1 N \u2265 T i\u2208[N ] \u03d5 \u2032 (R i (T )) \u03b4 N . (45\n)\nPlugging the last inequality in (44), we conclude that\ni\u2208[N ] \u03d5(R i (T )) \u2265 T i\u2208[N ] \u03d5 \u2032 (R i (T )) \u03b4 N -Surrogate Regret T .(46)\nNow, noting that 0 < R i (T ) \u2264 T for all i, and that \u03d5(\u2022) is monotone non-decreasing, we see that for any i \u2208 [N ] the above inequality implies\nN T 1-\u03b1 1 -\u03b1 \u2265 T \u03d5 \u2032 (R i (T )) \u03b4 N -Surrogate Regret T ,(47)\nwhich implies the following inequality after dividing throughout by T and replacing \u03d5 \u2032 (R i (T )) by 1\nR \u03b1 i (T ) : N (1 -\u03b1)T \u03b1 \u2265 1 R \u03b1 i (T ) \u03b4 N - Surrogate Regret T T ,(48)\nwhich is equivalent to\n1 R \u03b1 i (T ) \u2264 N \u03b4 N (1 -\u03b1)T \u03b1 + Surrogate Regret T T .(49)\nNow, from Eq. ( 41), we have the following preliminary bound Surrogate Regret T \u2264 O( \u221a M N T ). We use the bootstrapping technique by plugging this in (49) to derive the following improved bound on the cumulative reward accrued by the i th arm.\n1 R \u03b1 i (T ) \u2264 O N 2 \u221a M N T min(1/2,\u03b1) , \u2200i \u2208 [N ].(50)\nNow, we consider the following two cases:\nCase 1: 0 \u2264 \u03b1 \u2264 1/2. In this case, from (50) we see that 1\nR \u03b1 i (T ) \u2264 O( N 2 \u221a M N T \u03b1\n), and hence\n1 R 2\u03b1 i (T ) \u2264 O( N 5 M T 2\u03b1\n). Note that this bound holds for all T . Hence, plugging this in (40), we get", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Surrogate Regret", "text": "T \u2264 O \uf8eb \uf8ed DN 5 2 M 2 T t=2 i\u2208[N ] 1 (t -1) 2\u03b1 \uf8f6 \uf8f8 (51) If 0 \u2264 \u03b1 < 1/2, the above bound becomes Surrogate Regret T \u2264 O DN 3 M T 1 2 -\u03b1 . If \u03b1 = 1 2 , the above bound becomes Surrogate Regret T \u2264 O DN 3 M \u221a log T .\nCase 2: 1/2 < \u03b1 < 1. In this case, bound (50) implies that 1\nR \u03b1 i (T ) \u2264 N 2 \u221a M N T 1/2\n, and hence\n1 R 2\u03b1 i (T ) \u2264 O N 5 M T\n. Again, this is true for all T . So, plugging this in (40), we get\nSurrogate Regret T \u2264 O \uf8eb \uf8ed DN 5 2 M 2 T t=2 i\u2208[N ] 1 (t -1) \uf8f6 \uf8f8 = O(DN 3 M log T ) (52)\n6.4 Proof of Lemma 3.4\nConsider some context j \u2208 [M ]. As before, for any t \u2208 [1, T ] let g t := \u03d5 \u2032 (R(t -1)) \u2299 r(t). Then, we have the following set of inequalities considering the adaptive regret bound of the MAB policy handling context j:\n\u00d5 \uf8eb \uf8ed E \uf8ee \uf8f0 N t:ct=j \u2225g t \u2225 2 2 + max t:ct=j \u2225g t \u2225 \u221e N t:ct=j \u2225g t \u2225 1 \uf8f9 \uf8fb \uf8f6 \uf8f8 (a) \u2264 \u00d5 \uf8eb \uf8ed E \uf8ee \uf8f0 N t:ct=j \u2225g t \u2225 2 2 + N t:ct=j \u2225g t \u2225 1 \uf8f9 \uf8fb \uf8f6 \uf8f8 (b) \u2264 \u00d5 \uf8eb \uf8ed E \uf8ee \uf8f0 N t:ct=j \u2225\u03d5 \u2032 (R(t -1))\u2225 2 2 + N t:ct=j \u2225\u03d5 \u2032 (R(t -1))\u2225 1 \uf8f9 \uf8fb \uf8f6 \uf8f8 (c) = \u00d5 \uf8eb \uf8ec \uf8edE \uf8ee \uf8ef \uf8f0 N t:ct=j i\u2208[N ] 1 R 2\u03b1 i (t -1) + N t:ct=j i\u2208[N ] 1 R \u03b1 i (t -1) \uf8f9 \uf8fa \uf8fb \uf8f6 \uf8f7 \uf8f8 (d) \u2264 \u00d5 \uf8eb \uf8ec \uf8edE \uf8ee \uf8ef \uf8f0 N t:ct=j i\u2208[N ] 1 R \u03b1 i (t -1) \uf8f9 \uf8fa \uf8fb \uf8f6 \uf8f7 \uf8f8 (e) \u2264 \u00d5 \uf8eb \uf8ec \uf8ed N t:ct=j i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f7 \uf8f8 .(61)\nAbove, in (a) we have used the fact that max t:ct=j \u2225g t \u2225 \u221e \u2264 1, which follows because r(t) \u2264 1 and \u03d5 \u2032 (R(t -1)) \u2264 1.\nIn (b), we have used the fact that r(t) \u2264 1. In (c), we have used \u03d5 \u2032 (x) = x -\u03b1 . In (d), we have used the fact that for each i \u2208 [N ], R i (t -1) \u2265 1. Finally, in (e), we have applied Jensen's Inequality to the concave square root function. So, from the last inequality and Lemma 3.3, we get\nE \uf8ee \uf8f0 max y\u2208{e k } N k=1 t:ct=j \u27e8\u03d5 \u2032 (R(t -1) \u2299 r(t)), y -X j (t)\u27e9 \uf8f9 \uf8fb \u2264 \u00d5 \uf8eb \uf8ec \uf8ed2 N t:ct=j i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f7 \uf8f8(62)\nSumming the above inquality over all contexts j \u2208 [M ], we get the following inequality on Regret T defined in (19):\nRegret T \u2264 j\u2208[M ] \u00d5 \uf8eb \uf8ec \uf8ed N t:ct=j i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f7 \uf8f8 = M j\u2208[M ] 1 M \u00d5 \uf8eb \uf8ec \uf8ed N t:ct=j i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f7 \uf8f8 (a) \u2264 M \u00d5 \uf8eb \uf8ec \uf8ed j\u2208[M ] N M t:ct=j i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f7 \uf8f8 = \u221a M N \u00d5 \uf8eb \uf8ed T t=1 i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f8 ,(63)\nwhere in (a) above, we have used Jensen's Inequality on the concave square root function. Note that this bound is similar to the bound in (40) for the full information feedback setting, with the only difference being in the exponent of the cumulative reward sequence (2\u03b1 versus \u03b1).\nNext, we will derive a bound similar to (49). Note that by the definition of Regret T in (19), we have the following inequality for any fixed collection of distributions (x 1 0 , ..., x M 0 ):\nj\u2208[M ] E \uf8ee \uf8f0 t:ct=j \u27e8g t , X j (t)\u27e9 \uf8f9 \uf8fb \u2265 j\u2208[M ] E \uf8ee \uf8f0 t:ct=j \u27e8g t , x j 0 \u27e9 \uf8f9 \uf8fb -Regret T .(64)\nUsing the linearity of expectation, the above inequality can be written as\nE T t=1 \u27e8g t , X ct (t)\u27e9 \u2265 E T t=1 \u27e8g t , x ct 0 \u27e9 -Regret T .(65)\nNext, observing that X ct i (t)r i (t) = R i (t) -R i (t -1) and following the same calculations up to step (d) of ( 33) and taking expectations w.r.t the policy actions, we get\nE T t=1 \u27e8g t , X ct (t)\u27e9 \u2264 E \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb (66) Lower bounding \u03d5 \u2032 (R i (t -1)) by \u03d5 \u2032 (R i (T )) yields T t=1 \u27e8g t , x ct 0 \u27e9 \u2265 T t=1 \u27e8\u03d5 \u2032 (R(T )) \u2299 r(t), x ct 0 \u27e9.(67)\nFinally, taking expectations w.r.t. the policy actions, we get\nE T t=1 \u27e8g t , x ct 0 \u27e9 \u2265 E T t=1 \u27e8\u03d5 \u2032 (R(T )) \u2299 r(t), x ct 0 \u27e9 .(68)\nNow, let us take the offline benchmark policy to be the uniform distribution for all contexts, i.e., x j 0 = 1 N 1 for all j \u2208 [M ], which will imply that T t=1 r i (t)x ct 0,i \u2265 \u03b4T N for all i \u2208 [N ]. So, the RHS in the last equation can be lower bounded by i\u2208\n[N ] E[\u03d5 \u2032 (R i (T ))] \u2022 \u03b4T N . Hence, we get E T t=1 \u27e8g t , x ct 0 \u27e9 \u2265 i\u2208[N ] E[\u03d5 \u2032 (R i (T ))] \u2022 \u03b4T N(69)\nSo, from the last equation and equations ( 65) and (66), we get\nE \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb \u2265 i\u2208[N ] E[\u03d5 \u2032 (R i (T ))] \u2022 \u03b4T N -Regret T(70)\nSo from here, following the same steps as in the full information feedback setting, we obtain\nE 1 R \u03b1 i (T ) \u2264 N \u03b4 N (1 -\u03b1)T \u03b1 + Regret T T (71)\nNote the similarity between the above inequality and inequality (49) for the full information setting. Now, we know that R \u03b1 i (t -1) \u2265 1 for all i \u2208 [N ] and t. Plugging this in (63), we get our first bound, which is Regret T \u2264 \u00d5(N \u221a M T ).\nAs before, we do a tighter analysis to get a better regret bound. So, let \u03b1 0 \u2208 [0, 1) be any number. As the result of Lemma 3.4 claims, we want to show that Regret T = \u00d5(T 1-\u03b1 0 2\n). Since \u03b1 0 \u2208 [0, 1), there is some positive integer N 0 \u2265 0 such that Since all the quantities on the RHS in the two equations above are positive, \u03f5 0 can be taken to be something smaller than the minimum of all the above quantities. Now, we have obtained Regret T \u2264 \u00d5(N \u221a M T ) = O(log T \u2022N \u221a M T ). Plugging this in (71), we get the following:\nE 1 R \u03b1 i (T ) \u2264 O N 2 1 T \u03b1 + Regret T T = O N 3 \u221a M 1 T \u03b1 + log T \u221a T (a) = O N 3 \u221a M 1 T \u03b1 + T \u03f50 \u221a T = O N 3 \u221a M T min(\u03b1, 1 2 -\u03f50)(77)\nwhere above in (a), we have used the simple fact that log T = O(T \u03f50 ). Plugging the above bound in (63), we get the following bound for any 0 \u2264 \u03b1 \u2264 1 2\u03f5 0 :\nRegret T \u2264 \u00d5 \uf8eb \uf8ed \u221a M N T t=1 i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f8 \u2264 \u00d5 \uf8eb \uf8ed M 1 2 + 1 4 N 1 2 + 3 2 N T t=1 1 (t -1) \u03b1 \uf8f6 \uf8f8 = \u00d5(M 1 2 + 1 4 N 1 2 + 3 2 + 1 2 T 1-\u03b1 2 ) = \u00d5(M 3 4 N 5 2 T 1-\u03b1 2 ) (78)\nBy the same inequalities as above, for any 1 2\u03f5 0 \u2264 \u03b1 < 1 we will have the bound:\nRegret T \u2264 \u00d5 M 3 4 N 5 2 T 1-( 1 2 -\u03f5 0 ) 2 = \u00d5(M 3 4 N 5 2 T 1 4 + \u03f5 0 2 ) (79)\noblivious adversary, i.e they are fixed beforehand. However, the cumulative reward vectors R(t -1) for t \u2208 [1, T ] are policy-dependent, i.e they are random. Also, note that at every time step t, our policy picks some arm I t \u2208 [N ] to be played; from equation ( 10) that defines how cumulative rewards are updated, we see that there are only finitely many sequences (g t ) t:ct=j that our policy can see over the time horizon T .\nSo, let S be the set of all sequences (g t ) t:ct=j that our policy can see. Let q \u2208 \u2206 S be the probability distribution that the policy induces over the set S of possible reward sequences. For a fixed reward sequence (l t ) t:ct=t \u2208 S, we have the following by Theorem 3.2:  where the expectation on the RHS above is taken w.r.t the policy actions. Combining the above with (88), the claim follows.\nE \uf8ee \uf8f0 max {e k } N\n7 Additional Experiments  Figures 8 and 9 show plots of the standard regret of all the policies in the full information and the bandit information settings respectively. As before, it is clearly seen that the \u03b1-FairCB policy beats all the other policies in terms of the standard regret in both the settings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Plugging this back in (49), we get that 1 R \u03b1 i (T ) \u2264 O( N 5 M T \u03b1 ), and hence\n). Again, note that this holds for all T . Hence, plugging this in (40), we see that\nwhere above, we have used the fact that 2\u03b1 > 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Lemma 3.1", "text": "The proof of Lemma 2.1 works here with minor modifications. Again, the upper bound in ( 16) for Regret T (\u03b2 1-\u03b1 ) can be split into the difference of two terms A and B as follows:\nwhere\nAlso, let A \u2032 and B \u2032 denote the corresponding terms in the surrogate regret for the OLO problem defined in (17). Following the same argument as in the proof of Lemma 2.1, we can obtain\nAs before, the inequality\nNow, pick \u03b2 = (1\u03b1) -1 (ensuring that \u03b2 \u2265 1), and hence, we obtain\nTaking expectations w.r.t the policy actions, we get\nFinally, from (54), we get\ncompleting the proof of the lemma.\nMore generally, suppose for some 0 \u2264 n < N 0 , we have\nwhere ( 80) holds for all \u03b1 \u2208 2 n -1 and(81) holds for all \u03b1 \u2208 2 n+1 -1  2 n+1 -2 n+1 -1 2 n \u03f5 0 , 1 . Note that, by our choice of \u03f5 0 , both these intervals have non-negative measure (recall (73)). Also, note that we have shown the base case for n = 0 via inequalities ( 78) and ( 79). We will now show that ( 80) and ( 81) continue to hold for n + 1. Now, since we know that (81) holds for all \u03b1 \u2208 2 n+1 -1 2 n+1 -2 n+1 -1 2 n \u03f5 0 , 1 , we plug the bound ( 81) in ( 71) and get the following for such \u03b1:\nwhere in (a) above, we have simply used the fact that log T = O(T \u03f50 ). Note that by our choice of \u03f5 0 (recall inequality ( 73)), we have\nSo, plugging the bound of ( 82) in (63), we can obtain\nwhere ( 84) holds for all \u03b1 \u2208 2 n+1 -1\n\u03f5 0 , 1 . Hence, by induction, we see that ( 80) and ( 81) hold for all 0 \u2264 n \u2264 N 0 in the respective intervals.\nFinally, ( 72) and ( 74\n, by what we've shown above, we conclude that\nand this completes the proof of the claim.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Lemma 3.3", "text": "Fix some context j \u2208 [M ] and a time horizon T . Consider the sequence (g t ) t:ct=j of all reward vector that context j sees. Recall that g t = \u03d5 \u2032 (R(t -1)) \u2299 r(t). By our assumption, the reward vectors r(t) are generated by an We also run a few experiments to see how varying values of \u03b1 in the interval [0, 1) affects fairness levels in terms of Jain's Fairness Index (26). Note that the offline benchmark defined by equations ( 3) and ( 12) becomes the usual sum of rewards benchmark in the case of \u03b1 = 0, i.e it corresponds to an unfair objective. Thus, increasing values of \u03b1 in the range [0, 1) should increase fairness levels. Consequently, the average of cumulative rewards, defined by the equation\nshould also decrease as \u03b1 increases.\nFor these experiments, we only consider those contexts (users) in the dataset that occur with a high frequency (at least 5000). In the dataset, there are M = 18 such contexts, and as before there are N = 19 arms (genres). In this case, the time horizon was T = 136267. We again sort the rows of the filtered dataset by timestamps. We took 100 distinct values of \u03b1 in the interval [0, 1), and trained the \u03b1-FairCB policy for each of these values of \u03b1 in the full information setting. Figure 10 shows a plot of Jain's Fairness Index achieved by the \u03b1-FairCB policy for varying values of \u03b1 (the index was calculated for the cumulative rewards at the final time step T ). It is clearly seen that as \u03b1 increases, the fairness index also increases. Figure 11 shows the average cumulative reward (90, computed at the final time step T ) and it is observed that as \u03b1 increases, the average cumulative reward decreases.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Taming the monster: A fast and simple algorithm for contextual bandits", "journal": "PMLR", "year": "2014", "authors": "Alekh Agarwal; Daniel Hsu; Satyen Kale; John Langford; Lihong Li; Robert Schapire"}, {"ref_id": "b1", "title": "Bandits with concave rewards and convex knapsacks", "journal": "", "year": "2014", "authors": "Shipra Agrawal;  Nikhil R Devanur"}, {"ref_id": "b2", "title": "An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives", "journal": "PMLR", "year": "2016", "authors": "Shipra Agrawal; Lihong Nikhil R Devanur;  Li"}, {"ref_id": "b3", "title": "The nonstochastic multiarmed bandit problem", "journal": "SIAM journal on computing", "year": "2002", "authors": "Peter Auer; Nicolo Cesa-Bianchi; Yoav Freund; Robert E Schapire"}, {"ref_id": "b4", "title": "An alpharegret analysis of adversarial bilateral trade", "journal": "Advances in Neural Information Processing Systems", "year": "2022", "authors": "Yossi Azar; Amos Fiat; Federico Fusco"}, {"ref_id": "b5", "title": "Resourceful contextual bandits", "journal": "PMLR", "year": "2014", "authors": "Ashwinkumar Badanidiyuru; John Langford; Aleksandrs Slivkins"}, {"ref_id": "b6", "title": "Controlling polarization in personalization: An algorithmic framework", "journal": "", "year": "2019", "authors": "Elisa Celis; Sayash Kapoor; Farnood Salehi; Nisheeth Vishnoi"}, {"ref_id": "b7", "title": "Fair contextual multi-armed bandits: Theory and experiments", "journal": "PMLR", "year": "2020", "authors": "Yifang Chen; Alex Cuellar; Haipeng Luo; Jignesh Modi; Heramb Nemlekar; Stefanos Nikolaidis"}, {"ref_id": "b8", "title": "Multi-armed bandits with fairness constraints for distributing resources to human teammates", "journal": "", "year": "2020", "authors": "Houston Claure; Yifang Chen; Jignesh Modi; Malte Jung; Stefanos Nikolaidis"}, {"ref_id": "b9", "title": "Cvxpy: A pythonembedded modeling language for convex optimization", "journal": "The Journal of Machine Learning Research", "year": "2016", "authors": "Steven Diamond; Stephen Boyd"}, {"ref_id": "b10", "title": "Adversarial online learning with changing action sets: Efficient algorithms with approximate regret bounds", "journal": "PMLR", "year": "2021", "authors": "Ehsan Emamjomeh-Zadeh; Chen-Yu Wei; Haipeng Luo; David Kempe"}, {"ref_id": "b11", "title": "Online learning with global cost functions", "journal": "COLT", "year": "2009", "authors": "Eyal Even-Dar; Robert Kleinberg; Shie Mannor; Yishay Mansour"}, {"ref_id": "b12", "title": "The movielens datasets: History and context", "journal": "ACM Trans. Interact. Intell. Syst", "year": "2015-12", "authors": "F ; Maxwell Harper; Joseph A Konstan"}, {"ref_id": "b13", "title": "A quantitative measure of fairness and discrimination for resource allocation in shared computer systems", "journal": "", "year": "1998", "authors": "R Jain; D Chiu; W Hawe"}, {"ref_id": "b14", "title": "Fairness in learning: Classic and contextual bandits", "journal": "Advances in neural information processing systems", "year": "2016", "authors": "Matthew Joseph; Michael Kearns; Jamie H Morgenstern; Aaron Roth"}, {"ref_id": "b15", "title": "An axiomatic theory of fairness in network resource allocation", "journal": "IEEE", "year": "2010", "authors": "Tian Lan; David Kao; Mung Chiang; Ashutosh Sabharwal"}, {"ref_id": "b16", "title": "Combinatorial sleeping bandits with fairness constraints", "journal": "IEEE Transactions on Network Science and Engineering", "year": "2019", "authors": "Fengjiao Li; Jia Liu; Bo Ji"}, {"ref_id": "b17", "title": "A modern introduction to online learning", "journal": "", "year": "2019", "authors": "Francesco Orabona"}, {"ref_id": "b18", "title": "LeadCache : Regretoptimal caching in networks", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Debjit Paria; Abhishek Sinha"}, {"ref_id": "b19", "title": "Achieving fairness in the stochastic multi-armed bandit problem", "journal": "The Journal of Machine Learning Research", "year": "2021", "authors": "Vishakha Patil; Ganesh Ghalme; Vineet Nair; Yadati Narahari"}, {"ref_id": "b20", "title": "Scale-free adversarial multi armed bandits", "journal": "PMLR", "year": "2022", "authors": "Raja Sudeep; Shipra Putta;  Agrawal"}, {"ref_id": "b21", "title": "Diversity in news recommendations using contextual bandits", "journal": "Expert Systems with Applications", "year": "2022", "authors": "Alexander Semenov; Maciej Rysz; Gaurav Pandey; Guanglin Xu"}, {"ref_id": "b22", "title": "Enabling long-term fairness in dynamic resource allocation", "journal": "", "year": "2022", "authors": "Si Tareq; Georgios Salem; Giovanni Iosifidis;  Neglia"}, {"ref_id": "b23", "title": "Noregret algorithms for fair resource allocation", "journal": "", "year": "2023", "authors": "Abhishek Sinha; Ativ Joshi; Rajarshi Bhattacharjee; Cameron Musco; Mohammad Hajiesmaili"}, {"ref_id": "b24", "title": "Contextual bandits with similarity information", "journal": "", "year": "2011", "authors": "Aleksandrs Slivkins"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "4: Initialize M adaptive, scale-free MAB policies from Putta andAgrawal (2022). Let A j denote the jth instance of the policy, for j \u2208 [M ]. 5: for t = 1 to T do 6:", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 7 :7Figure 2: \u03b1-performance for the full information setting.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "property of conditional expectations, the first term on the LHS in the above inequality is just", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 8 :8Figure 8: Standard regret for the full information setting.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 9 :9Figure 9: Standard regret for the bandit information setting.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "k=1 t:ct=j \u27e8l t , e k -X j (t)\u27e9 Above, the expectation in the first time is taken w.r.t the policy actions. Now, taking expectations in the above inequality w.r.t the distribution q over S, we get", "figure_data": "\uf8f9\uf8fb\u2264\u00d5 \uf8eb \uf8ed Nt:ct=j\u2225l t \u2225 2 2 + max t:ct=j\u2225l t \u2225 \u221e Nt:ct=j\u2225l t \u2225 1\uf8f6 \uf8f8(87)\uf8ee\uf8ee\uf8f9 \uf8f9E\uf8f0 E\uf8f0 max {e k } N k=1 t:ct=j\u27e8l t , e k -X j (t)\u27e9\uf8fb \uf8fb\u2264 E\uf8ee \uf8f0\u00d5 \uf8eb \uf8ed Nt:ct=j\u2225l t \u2225 2 2 + max"}], "formulas": [{"formula_id": "formula_0", "formula_text": "c t \u2208 [M ] and \u03b4 \u2264 r i (t) \u2264 1, \u2200i \u2208 [N ].", "formula_coordinates": [2.0, 334.93, 172.22, 214.08, 22.17]}, {"formula_id": "formula_1", "formula_text": "I t \u2208 [N ].", "formula_coordinates": [2.0, 424.37, 228.69, 37.46, 17.29]}, {"formula_id": "formula_2", "formula_text": "3 \u2032 . (Bandit-feedback Setting)", "formula_coordinates": [2.0, 319.4, 293.15, 144.14, 10.87]}, {"formula_id": "formula_3", "formula_text": "x j (t) \u2208 \u2206 N , j \u2208 [M ]", "formula_coordinates": [2.0, 315.0, 355.26, 87.2, 17.94]}, {"formula_id": "formula_4", "formula_text": "R i (t) = R i (t -1) + x ct i (t)r i (t), R i (0) = 1. (1)", "formula_coordinates": [2.0, 338.83, 529.25, 211.34, 18.76]}, {"formula_id": "formula_5", "formula_text": "\u03d5(R i (T )) := (R i (T )) 1-\u03b1 1 -\u03b1 , i \u2208 [N ],(2)", "formula_coordinates": [2.0, 357.91, 597.58, 192.25, 31.52]}, {"formula_id": "formula_6", "formula_text": "Regret T (c) := max x * N i=1 \u03d5(R * i (T )) -c N i=1 \u03d5(R i (T )),(3)", "formula_coordinates": [3.0, 63.0, 144.76, 238.02, 30.32]}, {"formula_id": "formula_7", "formula_text": "\u03d5(x) -\u03d5(y) \u2264 \u03d5 \u2032 (y)(x -y)(4)", "formula_coordinates": [3.0, 373.83, 198.89, 176.34, 18.44]}, {"formula_id": "formula_8", "formula_text": "\u03d5(R * i (T )) -\u03b2 1-\u03b1 \u03d5(R i (T )) (a) = \u03d5(R * i (T )) -\u03d5(\u03b2R i (T )) (b) \u2264 \u03d5 \u2032 (\u03b2R i (T ))[R * i (T ) -\u03b2R i (T )] (c) \u2264 \u03b2 -\u03b1 \u03d5 \u2032 (R i (T )) T t=1 r i (t)[x ct * ,i -\u03b2x ct i (t)],(5)", "formula_coordinates": [3.0, 342.46, 264.31, 207.7, 90.94]}, {"formula_id": "formula_9", "formula_text": "Regret T (\u03b2 1-\u03b1 ) \u2264 \u03b2 -\u03b1 T t=1 i\u2208[N ] \u03d5 \u2032 (R i (T ))r i (t)[x ct * ,i -\u03b2x ct i (t)].(6)", "formula_coordinates": [3.0, 327.58, 467.72, 222.59, 48.39]}, {"formula_id": "formula_10", "formula_text": "Surrogate Regret T = max x * T t=1 i\u2208[N ] \u03d5 \u2032 (R i (t -1))r i (t)[x ct * ,i -x ct i (t)] (7)", "formula_coordinates": [4.0, 73.03, 129.49, 225.14, 46.98]}, {"formula_id": "formula_11", "formula_text": "\u03d5 \u2032 (R(t - 1)) \u2261 (\u03d5 \u2032 (R 1 (t -1)), ..., \u03d5 \u2032 (R N (t -1))). Upon setting \u03b2 \u2261 (1 -\u03b1) -1", "formula_coordinates": [4.0, 62.5, 220.38, 234.5, 41.85]}, {"formula_id": "formula_12", "formula_text": "Regret T (c \u03b1 ) \u2264 (1 -\u03b1) \u03b1 Surrogate Regret T + c \u03b1 N (8)", "formula_coordinates": [4.0, 72.43, 301.66, 225.73, 23.32]}, {"formula_id": "formula_13", "formula_text": "c \u03b1 = (1 -\u03b1) -(1-\u03b1) \u2264 e 1/e < 1.445.", "formula_coordinates": [4.0, 90.48, 334.72, 152.2, 17.94]}, {"formula_id": "formula_14", "formula_text": "Surrogate Regret T = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 O(N 3 M T 1/2-\u03b1 ), if 0 < \u03b1 < 1 2 O(N 3 M \u221a log T ), if \u03b1 = 1 2 O(1), if 1 2 < \u03b1 < 1. (9)", "formula_coordinates": [4.0, 63.0, 687.19, 243.56, 54.7]}, {"formula_id": "formula_15", "formula_text": "Regret T (c \u03b1 ) = (1 -\u03b1) \u03b1 \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 O(N 3 M T 1/2-\u03b1 ), if 0 < \u03b1 < 1 O(N 3 M \u221a log T ), if \u03b1 = 1 2 O(1), if 1 2 < \u03b1 < 1.", "formula_coordinates": [4.0, 315.0, 207.92, 242.04, 43.32]}, {"formula_id": "formula_16", "formula_text": "c \u03b1 = (1 -\u03b1) -(1-\u03b1) < 1.445.", "formula_coordinates": [4.0, 342.48, 263.05, 121.95, 17.94]}, {"formula_id": "formula_17", "formula_text": "R i (0) \u2190 1, S j \u2190 0, x j \u2190 1 N , \u2200i, j.", "formula_coordinates": [4.0, 365.71, 397.43, 149.5, 23.14]}, {"formula_id": "formula_18", "formula_text": "g i = r i (t \u2032 ) R \u03b1 i \u2200i \u2208 [N ]", "formula_coordinates": [4.0, 394.75, 531.36, 91.43, 26.08]}, {"formula_id": "formula_19", "formula_text": "S ct \u2190 S ct + \u2225g\u2225 2 2 12:", "formula_coordinates": [4.0, 315.18, 586.95, 162.21, 32.53]}, {"formula_id": "formula_20", "formula_text": "x ct \u2190 \u03a0 \u2206 N x ct + D 2S ct g 13:", "formula_coordinates": [4.0, 315.18, 633.87, 182.27, 46.84]}, {"formula_id": "formula_21", "formula_text": "Update R i (t) \u2190 R i (t -1) + x ct i (t)r i (t)", "formula_coordinates": [4.0, 341.89, 705.64, 167.5, 18.76]}, {"formula_id": "formula_22", "formula_text": "R i (t) = R i (t -1) + X ct i (t)r i (t), R i (0) = 1.(10)", "formula_coordinates": [5.0, 79.13, 362.59, 219.04, 18.76]}, {"formula_id": "formula_23", "formula_text": "P[X ct i (t) = 1] = x ct i (t). (11", "formula_coordinates": [5.0, 130.96, 442.98, 162.78, 13.32]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [5.0, 293.74, 444.7, 4.43, 9.96]}, {"formula_id": "formula_25", "formula_text": "Regret T (c) := max x * \u2208(\u2206 N ) M E \uf8ee \uf8f0 i\u2208[N ] \u03d5(R * i (T )) -c i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb .(12)", "formula_coordinates": [5.0, 66.81, 519.12, 231.35, 62.76]}, {"formula_id": "formula_26", "formula_text": "Regret T (c) = E \uf8ee \uf8f0 i\u2208[N ] \u03d5(R * i (T )) -c i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb (a) = i\u2208[N ] E[\u03d5(R * i (T ))] -cE \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb (b) \u2264 i\u2208[N ] \u03d5(E[R * i (T )]) -cE \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb (c) = i\u2208[N ] \u03d5 1 + T t=1 r i (t)x ct * ,i -cE \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb (13)", "formula_coordinates": [5.0, 318.53, 95.28, 231.64, 182.32]}, {"formula_id": "formula_27", "formula_text": "\u03d5(ER * i (T )) -\u03b2 1-\u03b1 \u03d5(R i (T )) \u2264 \u03b2 -\u03b1 \u03d5 \u2032 (R i (T )) T t=1 r i (t)[x ct * ,i -\u03b2X ct i (t)](14)", "formula_coordinates": [5.0, 335.51, 426.47, 214.66, 47.65]}, {"formula_id": "formula_28", "formula_text": "i\u2208[N ] \u03d5(ER * i (T )) -\u03b2 1-\u03b1 E \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb \u2264 \u03b2 -\u03b1 E \uf8ee \uf8f0 i\u2208[N ] T t=1 \u03d5 \u2032 (R i (T ))r i (t)[x ct * ,i -\u03b2X ct i (t)] \uf8f9 \uf8fb .(15)", "formula_coordinates": [5.0, 321.97, 526.2, 228.2, 87.57]}, {"formula_id": "formula_29", "formula_text": "Regret T (\u03b2 1-\u03b1 ) \u2264 \u03b2 -\u03b1 E \uf8ee \uf8f0 i\u2208[N ] T t=1 \u03d5 \u2032 (R i (T ))r i (t)[x ct * ,i -\u03b2X ct i (t)] \uf8f9 \uf8fb .(16)", "formula_coordinates": [5.0, 320.31, 644.08, 229.86, 64.17]}, {"formula_id": "formula_30", "formula_text": "Surrogate Regret T \u2261 E \uf8ee \uf8f0 i\u2208[N ] T t=1 \u03d5 \u2032 (R i (t -1))r i (t)[x ct * ,i -X ct i (t)] \uf8f9 \uf8fb = E T t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct * -X ct (t)\u27e9 . (17) As before, \u03d5 \u2032 (R(t -1)) \u2261 (\u03d5 \u2032 (R 1 (t -1)), ..., \u03d5 \u2032 (R N (t - 1))).", "formula_coordinates": [6.0, 62.5, 121.71, 235.66, 120.03]}, {"formula_id": "formula_31", "formula_text": "Regret T (c \u03b1 ) \u2264 (1 -\u03b1) \u03b1 Surrogate Regret T + c \u03b1 N,(18)", "formula_coordinates": [6.0, 71.33, 294.24, 226.84, 23.32]}, {"formula_id": "formula_32", "formula_text": "c \u03b1 = (1 -\u03b1) -(1-\u03b1) \u2264 e 1/e < 1.445.", "formula_coordinates": [6.0, 90.48, 331.48, 152.2, 17.94]}, {"formula_id": "formula_33", "formula_text": "E T t=1 \u27e8g t , x ct * -X ct (t)\u27e9 = E \uf8ee \uf8f0 j\u2208[M ] t:ct=j \u27e8g t , x j * -X j (t)\u27e9 \uf8f9 \uf8fb (a) = j\u2208[M ] E \uf8ee \uf8f0 t:ct=j \u27e8g t , x j * -X j (t)\u27e9 \uf8f9 \uf8fb (b) \u2264 j\u2208[M ] E \uf8ee \uf8f0 max y\u2208{e k } N k=1 t:ct=j \u27e8g t , y -X j (t)\u27e9 \uf8f9 \uf8fb regret for the j th context =: Regret T .(19)", "formula_coordinates": [6.0, 85.63, 509.92, 212.53, 184.64]}, {"formula_id": "formula_34", "formula_text": "Surrogate Regret T \u2264 Regret T (20)", "formula_coordinates": [6.0, 367.43, 101.9, 182.74, 19.61]}, {"formula_id": "formula_35", "formula_text": "Arm I t \u2208 [N ] to be played at round t, for t \u2208 [1, T ]. 3: Initialize R i (0) \u2190 1 for all i \u2208 [N ].", "formula_coordinates": [6.0, 319.79, 347.93, 230.6, 41.2]}, {"formula_id": "formula_36", "formula_text": "Update R i (t) \u2190 R i (t -1) + X ct i (t)r i (t) for all i \u2208 [N ].", "formula_coordinates": [6.0, 331.93, 489.92, 217.06, 30.71]}, {"formula_id": "formula_37", "formula_text": "E max {e k } N k=1 T t=1 \u27e8l t , e k -X(t)\u27e9 = O(log T \u2022 [ N L 2 + L \u221e N L 1 ]).(21)", "formula_coordinates": [6.0, 356.31, 604.03, 193.85, 53.36]}, {"formula_id": "formula_38", "formula_text": "L \u221e = max t \u2225l t \u2225 \u221e , L 2 = T t=1 \u2225l t \u2225 2 2 , L 1 =", "formula_coordinates": [6.0, 315.0, 670.09, 235.37, 30.55]}, {"formula_id": "formula_39", "formula_text": "Lemma 3.3. For any t \u2208 [1, T ], let g t := \u03d5 \u2032 (R(t - 1)) \u2299 r(t).", "formula_coordinates": [7.0, 62.5, 232.21, 234.5, 29.9]}, {"formula_id": "formula_40", "formula_text": "E \uf8ee \uf8f0 max y\u2208{e k } N k=1 t:ct=j \u27e8g t , y -X j (t)\u27e9 \uf8f9 \uf8fb \u2264 \u00d5 \uf8eb \uf8ed E \uf8ee \uf8f0 N t:ct=j \u2225g t \u2225 2 2 + max t:ct=j \u2225g t \u2225 \u221e N t:ct=j \u2225g t \u2225 1 \uf8f9 \uf8fb \uf8f6 \uf8f8 ,(22)", "formula_coordinates": [7.0, 63.0, 287.96, 246.51, 87.96]}, {"formula_id": "formula_41", "formula_text": "Regret T = \u00d5(M N 2 T 1-\u03b1 2 ) (23", "formula_coordinates": [7.0, 136.05, 502.17, 157.69, 14.37]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [7.0, 293.74, 505.28, 4.43, 9.96]}, {"formula_id": "formula_43", "formula_text": "Regret T (c \u03b1 ) = (1 -\u03b1) \u03b1 \u00d5(M N 2 T 1-\u03b1 2 ) (24", "formula_coordinates": [7.0, 96.7, 629.53, 197.04, 20.15]}, {"formula_id": "formula_44", "formula_text": ")", "formula_coordinates": [7.0, 293.74, 632.65, 4.43, 9.96]}, {"formula_id": "formula_45", "formula_text": "\u03b1-Performance(t) := i\u2208[N ] \u03d5(R i (t)).(25)", "formula_coordinates": [7.0, 356.18, 330.02, 193.99, 21.19]}, {"formula_id": "formula_46", "formula_text": "N i=1 R i (t)) 2 N N i=1 R 2 i (t) .(26)", "formula_coordinates": [7.0, 454.36, 417.83, 95.81, 29.67]}, {"formula_id": "formula_47", "formula_text": "\u03d5 \u2032 (R i (t -1))[R i (t) -R i (t -1)] \u2264 Ri(t)-1 Ri(t-1)-1 \u03d5 \u2032 (R)dR. (27", "formula_coordinates": [11.0, 188.49, 169.21, 357.25, 26.29]}, {"formula_id": "formula_48", "formula_text": ")", "formula_coordinates": [11.0, 545.74, 177.6, 4.43, 9.96]}, {"formula_id": "formula_49", "formula_text": "R i (t -1) -1 \u2265 0 for all t \u2208 [1, T ].", "formula_coordinates": [11.0, 63.0, 233.83, 156.58, 17.29]}, {"formula_id": "formula_50", "formula_text": "Ri(t)-1 Ri(t-1)-1 \u03d5 \u2032 (R)dR = \u03d5 \u2032 (c 0 )[R i (t) -R i (t -1)](28)", "formula_coordinates": [11.0, 210.71, 266.96, 339.45, 26.29]}, {"formula_id": "formula_51", "formula_text": "c 0 \u2208 (R i (t -1) -1, R i (t) -1); in particular, we have c 0 < R i (t) -1. Now, from the defintion (1) observe that R i (t) -R i (t -1) = x ct i (t)r i (t) \u2264 1", "formula_coordinates": [11.0, 63.0, 304.46, 486.0, 29.24]}, {"formula_id": "formula_52", "formula_text": "x ct i (t), r i (t) \u2264 1. This implies that R i (t) -1 \u2264 R i (t -1), and hence, c 0 < R i (t -1). Finally, since \u03d5(\u2022) is concave, \u03d5 \u2032 (\u2022) is non-increasing; this implies that \u03d5 \u2032 (c 0 ) \u2265 \u03d5 \u2032 (R i (t -1)).", "formula_coordinates": [11.0, 63.0, 314.94, 486.27, 48.64]}, {"formula_id": "formula_53", "formula_text": "Regret T (\u03b2 1-\u03b1 ) \u2264 \u03b2 -\u03b1 [A -\u03b2B],(29)", "formula_coordinates": [11.0, 236.74, 441.54, 313.42, 18.44]}, {"formula_id": "formula_54", "formula_text": "A = i\u2208[N ] \u03d5 \u2032 (R i (T )) T t=1 r i (t)x ct * ,i ,(30)", "formula_coordinates": [11.0, 232.03, 484.49, 318.14, 30.94]}, {"formula_id": "formula_55", "formula_text": "B = i\u2208[N ] \u03d5 \u2032 (R i (T )) T t=1 r i (t)x ct i (t).(31)", "formula_coordinates": [11.0, 231.44, 521.8, 318.72, 30.94]}, {"formula_id": "formula_56", "formula_text": "R i (\u2022) is non-decreasing for any i \u2208 [N ]. Hence, we see that \u03d5 \u2032 (R i (t -1)) \u2265 \u03d5 \u2032 (R i (T )) for all t \u2208 [1, T ] and i \u2208 [N ]. This implies that A = i\u2208[N ] \u03d5 \u2032 (R i (T )) T t=1 r i (t)x ct * ,i \u2264 i\u2208[N ] \u03d5 \u2032 (R i (t -1)) T t=1 r i (t)x ct * ,i = A \u2032 (32) Proving B \u2032 \u2264 (1 -\u03b1) -1 (B + N ):", "formula_coordinates": [11.0, 63.0, 607.05, 487.17, 129.86]}, {"formula_id": "formula_57", "formula_text": "B \u2032 = i T t=1 \u03d5 \u2032 (R i (t -1))r i (t)x ct i (t) (a) = i T t=1 \u03d5 \u2032 (R i (t -1))[R i (t) -R i (t -1)]] (b) \u2264 i T t=1 Ri(t)-1 Ri(t-1)-1 \u03d5 \u2032 (R)dR (c) \u2264 i Ri(T ) 0 \u03d5 \u2032 (R)dR (d) = i \u03d5(R i (T )) (e) = (1 -\u03b1) -1 i \u03d5 \u2032 (R i (T ))R i (T ) (f ) = (1 -\u03b1) -1 i\u2208[N ] \u03d5 \u2032 (R i (T )) 1 + T t=1 x ct i (t)r i (t) (h) \u2264 (1 -\u03b1) -1 (B + N )(33)", "formula_coordinates": [12.0, 191.55, 105.66, 358.61, 258.14]}, {"formula_id": "formula_58", "formula_text": "B \u2032 \u2264 (1 -\u03b1) -1 (B + N ) implies that (1 -\u03b1)B \u2032 -N \u2264 B. Since \u03b2 > 0, we have \u03b2B \u2265 \u03b2(1 -\u03b1)B \u2032 -\u03b2N . Combining this with A \u2264 A \u2032 , we have that A -\u03b2B \u2264 A \u2032 -\u03b2(1 -\u03b1)B \u2032 + \u03b2N.(34)", "formula_coordinates": [12.0, 63.0, 442.79, 487.17, 61.07]}, {"formula_id": "formula_59", "formula_text": "A -\u03b2B \u2264 A \u2032 -B \u2032 + (1 -\u03b1) -1 N,(35)", "formula_coordinates": [12.0, 233.03, 547.76, 317.13, 18.44]}, {"formula_id": "formula_60", "formula_text": "Regret T (c \u03b1 ) \u2264 (1 -\u03b1) \u03b1 (A \u2032 -B \u2032 ) + c \u03b1 N = (1 -\u03b1) \u03b1 Surrogate Regret T + c \u03b1 N,(36)", "formula_coordinates": [12.0, 197.33, 610.1, 352.84, 33.39]}, {"formula_id": "formula_61", "formula_text": "Surrogate Regret T = T t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct * -x ct (t)\u27e9 = j\u2208[M ] t:ct=j \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x j * -x j (t)\u27e9 (a) \u2264 j\u2208[M ] max x j \u2022 \u2208\u2206 N t:ct=j \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x j \u2022 -x j (t)\u27e9 Regret for the j th context(37)", "formula_coordinates": [13.0, 150.01, 115.7, 400.15, 109.53]}, {"formula_id": "formula_62", "formula_text": "sizes \u03b7 t = D \u221a 2 T \u03c4 =1 \u2225g \u03c4 \u2225 2 , 1 \u2264 t \u2264 T .", "formula_coordinates": [13.0, 62.47, 373.13, 164.85, 32.46]}, {"formula_id": "formula_63", "formula_text": "Regret T \u2264 D 2 T t=1 \u2225g t \u2225 2 . (38", "formula_coordinates": [13.0, 249.62, 444.45, 296.12, 30.2]}, {"formula_id": "formula_64", "formula_text": ")", "formula_coordinates": [13.0, 545.74, 454.19, 4.43, 9.96]}, {"formula_id": "formula_65", "formula_text": "x j \u2022 \u2208\u2206 N t:ct=j \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x j \u2022 -x j (t)\u27e9 \u2264 D 2 t:ct=j \u2225\u03d5 \u2032 (R(t -1)) \u2299 r(t)\u2225 2 2 (a) \u2264 D 2 t:ct=j \u2225\u03d5 \u2032 (R(t -1))\u2225 2 2 (b) = D 2 t:ct=j i\u2208[N ] 1 R 2\u03b1 i (t -1)(39)", "formula_coordinates": [13.0, 211.0, 566.53, 339.17, 131.16]}, {"formula_id": "formula_66", "formula_text": "Surrogate Regret T \u2264 j\u2208[M ] D 2 t:ct=j i\u2208[N ] 1 R 2\u03b1 i (t -1) = M j\u2208[M ] 1 M D 2 t:ct=j i\u2208[N ] 1 R 2\u03b1 i (t -1) (a) \u2264 DM 2 M j\u2208[M ] t:ct=j i\u2208[N ] 1 R 2\u03b1 i (t -1) = D \u221a M 2 T t=1 i\u2208[N ] 1 R 2\u03b1 i (t -1) , (40", "formula_coordinates": [14.0, 175.02, 100.13, 370.72, 153.89]}, {"formula_id": "formula_67", "formula_text": ")", "formula_coordinates": [14.0, 545.74, 230.15, 4.43, 9.96]}, {"formula_id": "formula_68", "formula_text": ") implies that Surrogate Regret T \u2264 O( \u221a M N T ).(41)", "formula_coordinates": [14.0, 145.75, 276.62, 404.42, 31.2]}, {"formula_id": "formula_69", "formula_text": "T t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct (t)\u27e9 \u2265 T t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct 0 \u27e9 -Surrogate Regret T .(42)", "formula_coordinates": [14.0, 192.21, 470.86, 357.96, 65.03]}, {"formula_id": "formula_70", "formula_text": "T t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct (t)\u27e9 \u2264 i\u2208[N ] \u03d5(R i (T )). (43", "formula_coordinates": [14.0, 200.82, 579.16, 344.92, 30.94]}, {"formula_id": "formula_71", "formula_text": ")", "formula_coordinates": [14.0, 545.74, 588.9, 4.43, 9.96]}, {"formula_id": "formula_72", "formula_text": "i\u2208[N ] \u03d5(R i (T )) \u2265 T t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct 0 \u27e9 -Surrogate Regret T .(44)", "formula_coordinates": [14.0, 191.56, 647.63, 358.6, 57.77]}, {"formula_id": "formula_73", "formula_text": "T t=1 \u27e8\u03d5 \u2032 (R(t -1)) \u2299 r(t), x ct 0 \u27e9 \u2265 T t=1 \u27e8\u03d5 \u2032 (R(T )) \u2299 r(t), x ct 0 \u27e9 = i\u2208[N ] T t=1 \u03d5 \u2032 (R i (T ))r i (t) 1 N \u2265 T i\u2208[N ] \u03d5 \u2032 (R i (T )) \u03b4 N . (45", "formula_coordinates": [15.0, 183.13, 90.37, 362.61, 98.52]}, {"formula_id": "formula_74", "formula_text": ")", "formula_coordinates": [15.0, 545.74, 167.7, 4.43, 9.96]}, {"formula_id": "formula_75", "formula_text": "i\u2208[N ] \u03d5(R i (T )) \u2265 T i\u2208[N ] \u03d5 \u2032 (R i (T )) \u03b4 N -Surrogate Regret T .(46)", "formula_coordinates": [15.0, 179.05, 214.5, 371.11, 27.93]}, {"formula_id": "formula_76", "formula_text": "N T 1-\u03b1 1 -\u03b1 \u2265 T \u03d5 \u2032 (R i (T )) \u03b4 N -Surrogate Regret T ,(47)", "formula_coordinates": [15.0, 204.98, 279.73, 345.19, 31.52]}, {"formula_id": "formula_77", "formula_text": "R \u03b1 i (T ) : N (1 -\u03b1)T \u03b1 \u2265 1 R \u03b1 i (T ) \u03b4 N - Surrogate Regret T T ,(48)", "formula_coordinates": [15.0, 207.2, 310.87, 342.97, 50.64]}, {"formula_id": "formula_78", "formula_text": "1 R \u03b1 i (T ) \u2264 N \u03b4 N (1 -\u03b1)T \u03b1 + Surrogate Regret T T .(49)", "formula_coordinates": [15.0, 200.28, 378.57, 349.89, 30.61]}, {"formula_id": "formula_79", "formula_text": "1 R \u03b1 i (T ) \u2264 O N 2 \u221a M N T min(1/2,\u03b1) , \u2200i \u2208 [N ].(50)", "formula_coordinates": [15.0, 224.05, 446.83, 326.12, 33.86]}, {"formula_id": "formula_80", "formula_text": "R \u03b1 i (T ) \u2264 O( N 2 \u221a M N T \u03b1", "formula_coordinates": [15.0, 324.75, 501.27, 83.12, 24.16]}, {"formula_id": "formula_81", "formula_text": "1 R 2\u03b1 i (T ) \u2264 O( N 5 M T 2\u03b1", "formula_coordinates": [15.0, 469.31, 505.55, 72.91, 19.88]}, {"formula_id": "formula_82", "formula_text": "T \u2264 O \uf8eb \uf8ed DN 5 2 M 2 T t=2 i\u2208[N ] 1 (t -1) 2\u03b1 \uf8f6 \uf8f8 (51) If 0 \u2264 \u03b1 < 1/2, the above bound becomes Surrogate Regret T \u2264 O DN 3 M T 1 2 -\u03b1 . If \u03b1 = 1 2 , the above bound becomes Surrogate Regret T \u2264 O DN 3 M \u221a log T .", "formula_coordinates": [15.0, 63.0, 540.62, 487.17, 80.24]}, {"formula_id": "formula_83", "formula_text": "R \u03b1 i (T ) \u2264 N 2 \u221a M N T 1/2", "formula_coordinates": [15.0, 346.76, 616.3, 89.56, 32.46]}, {"formula_id": "formula_84", "formula_text": "1 R 2\u03b1 i (T ) \u2264 O N 5 M T", "formula_coordinates": [15.0, 63.0, 624.98, 486.0, 51.84]}, {"formula_id": "formula_85", "formula_text": "Surrogate Regret T \u2264 O \uf8eb \uf8ed DN 5 2 M 2 T t=2 i\u2208[N ] 1 (t -1) \uf8f6 \uf8f8 = O(DN 3 M log T ) (52)", "formula_coordinates": [15.0, 185.47, 684.76, 364.7, 52.15]}, {"formula_id": "formula_86", "formula_text": "\u00d5 \uf8eb \uf8ed E \uf8ee \uf8f0 N t:ct=j \u2225g t \u2225 2 2 + max t:ct=j \u2225g t \u2225 \u221e N t:ct=j \u2225g t \u2225 1 \uf8f9 \uf8fb \uf8f6 \uf8f8 (a) \u2264 \u00d5 \uf8eb \uf8ed E \uf8ee \uf8f0 N t:ct=j \u2225g t \u2225 2 2 + N t:ct=j \u2225g t \u2225 1 \uf8f9 \uf8fb \uf8f6 \uf8f8 (b) \u2264 \u00d5 \uf8eb \uf8ed E \uf8ee \uf8f0 N t:ct=j \u2225\u03d5 \u2032 (R(t -1))\u2225 2 2 + N t:ct=j \u2225\u03d5 \u2032 (R(t -1))\u2225 1 \uf8f9 \uf8fb \uf8f6 \uf8f8 (c) = \u00d5 \uf8eb \uf8ec \uf8edE \uf8ee \uf8ef \uf8f0 N t:ct=j i\u2208[N ] 1 R 2\u03b1 i (t -1) + N t:ct=j i\u2208[N ] 1 R \u03b1 i (t -1) \uf8f9 \uf8fa \uf8fb \uf8f6 \uf8f7 \uf8f8 (d) \u2264 \u00d5 \uf8eb \uf8ec \uf8edE \uf8ee \uf8ef \uf8f0 N t:ct=j i\u2208[N ] 1 R \u03b1 i (t -1) \uf8f9 \uf8fa \uf8fb \uf8f6 \uf8f7 \uf8f8 (e) \u2264 \u00d5 \uf8eb \uf8ec \uf8ed N t:ct=j i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f7 \uf8f8 .(61)", "formula_coordinates": [17.0, 157.76, 125.6, 392.41, 252.19]}, {"formula_id": "formula_87", "formula_text": "E \uf8ee \uf8f0 max y\u2208{e k } N k=1 t:ct=j \u27e8\u03d5 \u2032 (R(t -1) \u2299 r(t)), y -X j (t)\u27e9 \uf8f9 \uf8fb \u2264 \u00d5 \uf8eb \uf8ec \uf8ed2 N t:ct=j i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f7 \uf8f8(62)", "formula_coordinates": [17.0, 194.11, 441.87, 356.05, 79.66]}, {"formula_id": "formula_88", "formula_text": "Regret T \u2264 j\u2208[M ] \u00d5 \uf8eb \uf8ec \uf8ed N t:ct=j i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f7 \uf8f8 = M j\u2208[M ] 1 M \u00d5 \uf8eb \uf8ec \uf8ed N t:ct=j i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f7 \uf8f8 (a) \u2264 M \u00d5 \uf8eb \uf8ec \uf8ed j\u2208[M ] N M t:ct=j i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f7 \uf8f8 = \u221a M N \u00d5 \uf8eb \uf8ed T t=1 i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f8 ,(63)", "formula_coordinates": [17.0, 197.1, 564.25, 353.06, 174.7]}, {"formula_id": "formula_89", "formula_text": "j\u2208[M ] E \uf8ee \uf8f0 t:ct=j \u27e8g t , X j (t)\u27e9 \uf8f9 \uf8fb \u2265 j\u2208[M ] E \uf8ee \uf8f0 t:ct=j \u27e8g t , x j 0 \u27e9 \uf8f9 \uf8fb -Regret T .(64)", "formula_coordinates": [18.0, 226.31, 150.41, 323.86, 74.0]}, {"formula_id": "formula_90", "formula_text": "E T t=1 \u27e8g t , X ct (t)\u27e9 \u2265 E T t=1 \u27e8g t , x ct 0 \u27e9 -Regret T .(65)", "formula_coordinates": [18.0, 197.42, 258.18, 352.74, 30.2]}, {"formula_id": "formula_91", "formula_text": "E T t=1 \u27e8g t , X ct (t)\u27e9 \u2264 E \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb (66) Lower bounding \u03d5 \u2032 (R i (t -1)) by \u03d5 \u2032 (R i (T )) yields T t=1 \u27e8g t , x ct 0 \u27e9 \u2265 T t=1 \u27e8\u03d5 \u2032 (R(T )) \u2299 r(t), x ct 0 \u27e9.(67)", "formula_coordinates": [18.0, 63.0, 331.66, 487.17, 99.31]}, {"formula_id": "formula_92", "formula_text": "E T t=1 \u27e8g t , x ct 0 \u27e9 \u2265 E T t=1 \u27e8\u03d5 \u2032 (R(T )) \u2299 r(t), x ct 0 \u27e9 .(68)", "formula_coordinates": [18.0, 197.62, 462.68, 352.55, 30.2]}, {"formula_id": "formula_93", "formula_text": "[N ] E[\u03d5 \u2032 (R i (T ))] \u2022 \u03b4T N . Hence, we get E T t=1 \u27e8g t , x ct 0 \u27e9 \u2265 i\u2208[N ] E[\u03d5 \u2032 (R i (T ))] \u2022 \u03b4T N(69)", "formula_coordinates": [18.0, 136.4, 532.48, 413.77, 56.49]}, {"formula_id": "formula_94", "formula_text": "E \uf8ee \uf8f0 i\u2208[N ] \u03d5(R i (T )) \uf8f9 \uf8fb \u2265 i\u2208[N ] E[\u03d5 \u2032 (R i (T ))] \u2022 \u03b4T N -Regret T(70)", "formula_coordinates": [18.0, 185.65, 622.58, 364.52, 34.15]}, {"formula_id": "formula_95", "formula_text": "E 1 R \u03b1 i (T ) \u2264 N \u03b4 N (1 -\u03b1)T \u03b1 + Regret T T (71)", "formula_coordinates": [18.0, 214.05, 690.16, 336.12, 33.18]}, {"formula_id": "formula_96", "formula_text": "E 1 R \u03b1 i (T ) \u2264 O N 2 1 T \u03b1 + Regret T T = O N 3 \u221a M 1 T \u03b1 + log T \u221a T (a) = O N 3 \u221a M 1 T \u03b1 + T \u03f50 \u221a T = O N 3 \u221a M T min(\u03b1, 1 2 -\u03f50)(77)", "formula_coordinates": [19.0, 213.97, 389.44, 336.2, 117.2]}, {"formula_id": "formula_97", "formula_text": "Regret T \u2264 \u00d5 \uf8eb \uf8ed \u221a M N T t=1 i\u2208[N ] E 1 R \u03b1 i (t -1) \uf8f6 \uf8f8 \u2264 \u00d5 \uf8eb \uf8ed M 1 2 + 1 4 N 1 2 + 3 2 N T t=1 1 (t -1) \u03b1 \uf8f6 \uf8f8 = \u00d5(M 1 2 + 1 4 N 1 2 + 3 2 + 1 2 T 1-\u03b1 2 ) = \u00d5(M 3 4 N 5 2 T 1-\u03b1 2 ) (78)", "formula_coordinates": [19.0, 215.2, 551.37, 334.96, 112.44]}, {"formula_id": "formula_98", "formula_text": "Regret T \u2264 \u00d5 M 3 4 N 5 2 T 1-( 1 2 -\u03f5 0 ) 2 = \u00d5(M 3 4 N 5 2 T 1 4 + \u03f5 0 2 ) (79)", "formula_coordinates": [19.0, 244.23, 696.63, 305.94, 40.28]}, {"formula_id": "formula_99", "formula_text": "E \uf8ee \uf8f0 max {e k } N", "formula_coordinates": [21.0, 192.05, 170.07, 39.09, 29.91]}], "doi": "10.1145/2827872"}
