{
  "Transferable and Forecastable User Targeting Foundation Model": "Bin Dou ∗† Ant Group Hangzhou, China doubin.dou@antgroup.com Yun Zhu ∗§ Ant Group Hangzhou, China zhuyun_dcd@zju.edu.cn Xiaotong Lin Ant Group Hangzhou, China lxt203095@antgroup.com Yang Chen Ant Group Hangzhou, China cy462023@antgroup.com",
  "Baokun Wang ‡": "Ant Group Hangzhou, China yike.wbk@antgroup.com Yike Xu Ant Group Hangzhou, China xuyike.xyk@antgroup.com Yun Liu Ant Group Hangzhou, China ly319278@antgroup.com Yongchao Liu Ant Group Hangzhou, China yongchao.ly@antgroup.com Tianyi Zhang Ant Group Shanghai, China zty113091@antgroup.com Weiqiang Wang Ant Group Hangzhou, China weiqiang.wwq@antgroup.com Xiaorui Huang Ant Group Hangzhou, China huangxiaorui.hxr@antgroup.com Shaoshuai Han Ant Group Hangzhou, China hanshaoshuairs@163.com Yu Cheng Ant Group Hangzhou, China cy122623@antgroup.com Chuntao Hong Ant Group Beijing, China chuntao.hct@antgroup.com",
  "Abstract": "User targeting, the process of selecting targeted users from a pool of candidates for non-expert marketers, has garnered substantial attention with the advancements in digital marketing. However, existing user targeting methods encounter two significant challenges: (i) Poor cross-domain and cross-scenario transferability and generalization , and (ii) Insufficient forecastability in real-world applications . These limitations hinder their applicability across diverse industrial scenarios. In this work, we propose Found , an industrial-grade, transferable, and forecastable user targeting foundation model. To enhance cross-domain transferability, our framework integrates heterogeneous multi-scenario user data, aligning them with onesentence targeting demand inputs through contrastive pre-training. For improved forecastability, the text description of each user is derived based on anticipated future behaviors, while user representations are constructed from historical information. Experimental results demonstrate that our approach significantly outperforms ∗ The contributions by Bin Dou and Yun Zhu have been conducted completely during internship at Ant Group. † Also with Xi'an Jiaotong University. ‡ Corresponding author. § Also with Zhejiang University. This work is licensed under a Creative Commons Attribution 4.0 International License. WWWCompanion '25, Sydney, NSW, Australia © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1331-6/2025/04 https://doi.org/10.1145/3701716.3715266 existing baselines in cross-domain, real-world user targeting scenarios, showcasing the superior capabilities of Found. Moreover, our method has been successfully deployed on the Alipay 1 platform and is widely utilized across various scenarios.",
  "CCS Concepts": "· Information systems → Data mining ; Collaborative filtering ; Retrieval models and ranking .",
  "Keywords": "User Targeting; User Understanding; Self-supervised Pre-training; Multi-modal Pretraining",
  "ACMReference Format:": "Bin Dou, Baokun Wang, Yun Zhu, Xiaotong Lin, Yike Xu, Xiaorui Huang, Yang Chen, Yun Liu, Shaoshuai Han, Yongchao Liu, Tianyi Zhang, Yu Cheng, Weiqiang Wang, and Chuntao Hong. 2025. Transferable and Forecastable User Targeting Foundation Model. In Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/ 3701716.3715266",
  "1 Introduction": "Recently, user targeting[24, 31], the strategic practice of concentrating on specific customers or users to deliver tailored content, messaging, and product recommendations[16], has garnered significant attention with the advancement of digital marketing[5]. 1 https://global.alipay.com/platform/site/ihome WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Bin Dou et al. Precise customer targeting forms the cornerstone of any effective marketing strategy, enabling the identification of optimal target audiences for each message and fostering consistent, personalized experiences across channels for individual customers. To achieve precise user targeting, some previous studies[26] have developed domain-specific experts tailored to individual scenarios or learning common user embeddings[30] through user behavioral sequence data for e-commerce domain. However, expert methods are constrained by their exclusive focus on single scenarios, which limits their transferability and generalization. Additionally, most of them rely on labeled data to train supplementary classification heads for downstream tasks, posing challenges for application in low-resource settings, such as zero-shot learning. More recently, LLM-based methods, such as ARALLM[34], aim to enhance generalization across certain scenarios by leveraging vast repositories of commonknowledge. Nevertheless, LLMs face significant challenges in utilizing industrial multi-source data, such as payment records and user behavior patterns, due to their insufficient understanding of long-tail knowledge[1, 32]. This limitation undermines the effectiveness of LLM-based methods in complex scenarios, particularly those involving multi-modal temporal and personal data. In conclusion, current methods primarily face two challenges: (i) Poor transferability and generalization , and (ii) insufficient forecastability in complex real-world scenarios . Therefore, we propose Found, a transferable and forecastable user targeting foundation model designed to address the aforementioned challenges. To enhance transferability and generalization, we develop a user modeling framework that integrates heterogeneous multi-source data, such as billing records, super position model 2 (SPM) logs, mini-program descriptions, user information tables (containing user consumption records, financial irregularities, etc), and search texts within the Alipay application. To improve the forecastability, we collect and curate forecastable user-text pair data to align diverse modalities using contrastive learning. Considering the complexity of Alipay's data, this process is executed in two stages to ensure training stability. First, we design two novel self-supervised tasks to pretrain user models, facilitating the extraction of universal embeddings from heterogeneous, multi-scenario user data. Subsequently, we perform user-text alignment, where features from multi-modal data are fused and aligned with representations generated by a fine-tuned LLM through contrastive learning. Building on this foundation model, we propose two strategies for utilizing the pretrained models in user targeting: (1) Zero-shot Transfer, which directly inputting requirements in text format to identify targeted users based on user-text similarity. (2) Few-shot Targeting via Prompt-tuning, which Leverages a small set of seed users to enhance the text prompt's descriptive capabilities, thereby improving the performance of text-based user targeting. Extensive experiments demonstrate that Found consistently outperforms baseline methods in user targeting, particularly in real-world Alipay applications, where it showcases strong predictive capabilities in complex scenarios. Furthermore, Found achieves superior performance across diverse scenarios and domains, including security, marketing, and recommendation, highlighting its exceptional transferability and generalization. 2 Dataflow statistics generated from page burial In summary, our work makes the following contributions: · We propose Found, an industrial-grade, transferable, and forecastable user-targeting foundation model capable of understanding users through large-scale heterogeneous multisource data to accurately identify targeted users. · We design novel self-supervised tasks for pretraining user models and align different modalities through constructed user-text pair data in subsequent phases, ensuring training stability and forecastability within complex real-world scenarios while enabling the selection of targeted users based on one-sentence demands. · Extensive experiments reveal that our model surpasses baseline methods across diverse scenarios and multiple domains, demonstrating its superior transferability and forecastability in real-world Alipay applications.",
  "2 Related Work": "",
  "2.1 Multi-modal Pre-training": "Multimodal pretraining[35] has become essential for integrating diverse data types, such as text, images, audio, and video, to enhance model performance across various tasks. For instance, CLIP[29] employs contrastive learning to align image and text embeddings in a shared latent space, enabling zero-shot transfer without taskspecific fine-tuning. ALIGN[14] expands this approach by utilizing larger datasets and more complex architectures, achieving stateof-the-art results in image-text retrieval and other benchmarks. BLIP[19] advances the field by combining contrastive and generative objectives within a unified transformer framework, enriching multimodal representations and improving tasks like image captioning. Additionally, BLIP-2[18] and LLaVA[21] integrate multimodal encoders with large language models, creating robust multimodal large language models. Collectively, these methodologies facilitate the seamless integration of diverse modalities and leverage intermodal connections to boost model performance. Building upon these advancements, applications in other domains have garnered significant attention, particularly in recommendation systems, by incorporating user-item interactions with multimodal features. This synthesis enhances recommendation accuracy by capturing intricate relationships, as exemplified by models such as MGCL[22], MMSSL[36], and MMCPR[23]. In the graph domain, models such as ConGrat[4], G2P2[37], and GraphCLIP[44] extend CLIP into graph tasks, with the goal of enhancing zero-shot learning capabilities for graph-related applications. Unlike existing approaches, we integrate multi-source data of greater complexity. The distinct information across modalities complicates direct model alignment. To overcome these challenges, we propose a two-stage pretraining process: first, pretrain multimodal models using proposed novel auxiliary tasks, and second, align the pretrained models through meticulously constructed data pairs.",
  "2.2 User Modeling and Targeting": "User modeling focuses on learning transferable user patterns from user behavioral sequence data[8, 30], while user targeting involves selecting targeted users based on the trained user models. In the context of user modeling, deep representation learning frameworks[25, 39] aimed at obtaining universal user representations are still in Transferable and Forecastable User Targeting Foundation Model WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia their early stages. For instance, [25] performs multitask representation learning using an attention-based RNN architecture to capture in-depth representations of portal users. [39] propose parameterefficient transfer learning architecture named PeterRec to relieve the computational cost burden of fine-tuning. PeterRec covers five downstream tasks that include predicting user profiles like gender, age, and life status, as well as a cold-start recommendation task for browser recommender systems. [40] trains autoencoder-coupled Transformer networks that model retention, installation, and uninstallation collectively. They test the user embeddings in three downstream tasks for mobile app management scenarios. [10] proposes a behavioral consistency loss to preserve the user's longterm interest and an aggregation scheme for the benefit of model capacity. The proposed method is evaluated on user preference prediction and user profiling tasks.",
  "3 Preliminaries": "In this section, we begin by introducing the notation used in this study (Section 3.1), followed by an overview of the key downstream tasks addressed in our work (Section 3.2).",
  "3.1 Notations": "In the Alipay application, extensive non-sensitive user information and interactions are accessible. For instance, user behavioral sequence data is denoted as V = {{ 𝐵 𝑛 } 𝑁 𝑛 = 1 , { 𝑆 𝑛 } 𝑁 𝑛 = 1 , { 𝑀 𝑛 } 𝑁 𝑛 = 1 } , comprising PayBill 𝐵 , Super Position Model 𝑆 , and MiniProgram 𝑀 . Here, 𝐵 𝑛 ∈ T 𝐿 ( 𝐵 ) 𝑛 , 𝑆 𝑛 ∈ T 𝐿 ( 𝑆 ) 𝑛 , and 𝑀 𝑛 ∈ T 𝐿 ( 𝑀 ) 𝑛 represent the raw text for PayBill information, Super Position Model details, and MiniProgram descriptions, respectively. Let 𝑁 denotes the number of users and 𝑛 ∈ [ 1 , 2 , . . . , 𝑁 ] , T is the token dictionary, and 𝐿 ( 𝑀 ) 𝑛 indicates the sequence length. Tabular data is denoted as 𝑇 ∈ R 𝑁 × 𝐹 × 𝐷 , where 𝐹 represents the number of features and 𝐷 denotes the dimensionality of each feature. Search text data is represented as 𝑅 ∈ T 𝐿 ( 𝑅 ) 𝑛 . This work focuses on the user targeting task, utilizing the aforementioned data to pretrain a user model capable of selecting targeted users based on a single-sentence demand 𝑄 ∈ T 𝐿 ( 𝑄 ) .",
  "3.2 User Targeting": "To develop a user targeting foundation model, extensive user data can be utilized to pre-train a general model endowed with transferable knowledge:  where V s represents the source data of user behavioral sequences, 𝑔 𝜃 ★ denotes the pretrained user targeting foundation model, and L pretrain refers to our proposed pretraining tasks, which will be detailed in Sec. 4. With the user targeting foundation model trained, the zero-shot transfer for user targeting and few-shot user targeting via prompttuning are proposed. For zero-shot setting, the pre-trained user model can be directly deployed on target data:  where 𝑄 𝑖 denotes a single-sentence demand and 𝑈 𝑖 are the targeted users which require the demand which are sampled from user candidates U . For the few-shot setting, a limited number of training samples for each class are used for fine-tuning:  where I t | tr represents the supervised training paired data for downstream user targeting, comprising user demand and the corresponding targeted user group. ˆ 𝑈 𝑖 is the predicted user group based on demand sentence 𝑄 𝑖 . 𝑓 𝜃 ′ denotes the fine-tuned model, which will be evaluated on test samples.",
  "4 Method": "In this section, we first present the training corpus we collected and curated in Sec. 4.1. Next, we detail the proposed self-supervised tasks for user modeling in Sec. 4.2. Subsequently, we explain the user-text alignment process in Sec. 4.3. Finally, we demonstrate the application of our pre-trained model to downstream tasks in Sec. 4.4. The overall pipeline of Found is illustrated in Fig. 1.",
  "4.1 Forecastable User-Text Pair Preparation": "Based on the full amount of user bill, page clicking behavior, miniprogram browsing and ordering, homepage search and other data within the Alipay system, a natural language description of user behavior is constructed. It should be noticed that the each user's language description is built based on the future behaviors mentioned before, which will enhance the forecastability of the model pre-trained by the data. Specifically, each user-text pair can be denoted as:  where 𝑡 1 and 𝑡 2 denote the previous and future period. Additionally, different templates are designed according to different data sources, and there are five types in total, divided into purchase category, browsing category, search category, click category and payment channel category. For example, the billing information occurred by a user will be strung together into a complete description as a user behavioral alignment label based on being at a certain merchant, purchasing a certain product, belonging to a certain category, transaction amount and payment channel. The template takes the form as: Template for purchase category : The user purchased {items} amounting more than {num} dollars with {status} payment. In the above template, the placeholders are represented in blue font: 'items' denotes the purchased items, 'num' represents the amount spent by the user, and 'status' indicates whether the bill payment was successful. For instance, a user's purchase description could be expressed as: \"The user purchased gold, gold bars, etc., amounting to more than $10,000 with successful payment.\" Templates for other data sources follow similar structures. WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Bin Dou et al. (a) User Targeting Foundation Model Pre-training LoRA Future Behavioral Sequence 𝓥 𝒕𝟐 Text Template Q LLM Encoder Attention-based Feature Fusion User Encoder (II) User-Text Alignment Stage 🔥 🔥 + period 1 Previous Behavioral Sequence 𝓥 𝒕𝟏 Tabular T Searching Text R period 2 Purchase {item1},{item2}, for more than {num} dollars with {payment successful} User Encoder 🔥 Behavioral Sequence 𝓥 Tabular T Searching Text R B M S Self-Supervised Learning (I) Self-Supervised User Modeling Stage 𝒆 (𝓥) 𝒆 (𝒕𝒂𝒃) 𝒆 (𝒓) 𝒆 (𝒇) 𝒆 (𝒒) Please Select potential 3C buyers One-sentence Demands + LoRA LLM Encoder User Encoder Attention-based Feature Fusion Query Rewriting User Candidates 𝓤 (II) Few-shot User Targeting via Prompt-tuning User Encoder Attention-based Feature Fusion 𝑉 ! …… 𝑉 \" 𝑉 ! 𝑉 # …… Learnable Prompt + LoRA LLM Encoder 🔥 Forward Flow Similarity Scores Seed Users(+) Seed Users(-) (I) Zero-shot User Targeting 0.1 0.8 0.2 0.9 Gradient Flow 𝑳 𝑪𝑷 𝑳 𝑻𝒓𝒊 (𝒆 𝒇 , , 𝒆 𝒇 - , 𝒆 (𝒒) ) (b) Model Inference for User Targeting Figure 1: Pre-training and inference for user targeting foundation model. We propose an industrial framework for languagebased user targeting, i.e., selecting users from only one sentence. it' pre-trained in a two strategy, as the first stage focuses on the self-supervised user modeling while the second is employed to align user-text pairs. For user targeting, we develop two approaches: zero-shot transfer, which selects users with one-sentence input, and few-shot user targeting via prompt-tuning.",
  "4.2 Self-Supervised User Modeling": "4.2.1 User Behavioral Sequence Encoding. As user behavior contains abundant information, which greatly improves the generalization ability of user representation and empowers user modeling in many downstream tasks, behavioral sequence is selected for user modeling. At AliPay platform, PayBill 𝑃 , MiniProgram 𝑀 and SPM 𝑆 are the main data sources for user representation, thus they are utilized as the input data for user understanding. Therefore, representational embedding for user 𝑖 can formulated as:  where P denotes the average pooling function crossing all time windows along each user behavioral sequence, 𝑔 𝜃 is a composite function 𝑔 𝜃 = 𝑔 𝜇 ◦ 𝑔 𝜈 , with 𝑔 𝜇 representing a time-aware function such as GRU[7] or self-attention networks[33], and 𝑔 𝜈 serving as a text encoding function, such as ALBERT[17]. Here, 𝑧 = 𝑔 𝜈 ( 𝐵 𝑖 , 𝑀 𝑖 , 𝑆 𝑖 ) and 𝑐 = 𝑔 𝜇 ( 𝑧 ) . As the modals of user behavioral sequence are represented with text sequences, for User Behavioral Sequence Encoder 𝑔 𝜃 , we utilize ALBERT[17], a light BERT[15] for self-supervised learning of language representations, to first encode text sequences to embeddings. Then, as illustrated in Fig. 2 (a), the forecasting prediction is utilized as the main proxy self-supervised pre-training task inspired by Contrastive Predictive Coding[27], as contrastive learning is employed to train the model 3 :  where 𝑧 𝑡 represents the user behavioral embedding at timestamp 𝑡 derived from the three data modalities, 𝑐 𝑡 denotes the embedding 3 For clarity, we set batch size as 1 here. aggregated with temporal information by 𝜇 , 𝑠 is the cosine similarity function, 𝐾 is the total time length, and 𝑘 ≪ 𝐾 controls the window size for positive samples. Additionally, as the paybill and miniprogram browsing records are commonly presented cyclically, we add cyclic regularization in user behavioral sequence encoding (shown in Fig. 2 (b)), enforcing each user's representation to be close across several time windows measured by KL-divergence. Therefore, the total self-supervised pre-training loss for user behavioral sequence is:  where 𝜆 is the coefficient that controls the strength of the regularization term, KL denotes the KL divergence between the two input vectors, and 𝑇 denotes the regularization temporal cycle. Figure 2: Self-supervised pre-training tasks for user behavioral sequence. 𝑥 !\"# 𝑥 !\"$ 𝑥 !\"% 𝑥 ! 𝑔 !\"# 𝑔 !\"# 𝑔 !\"# 𝑔 !\"# 𝑧 $ 𝑔 %& 𝑔 %& 𝑔 %& 𝑔 %& 𝑐 $ 𝑔 !\"# 𝑔 !\"# 𝑔 !\"# 𝑔 !\"# Contrastive Learning Loss 𝑳 𝑪𝑳 B 𝑥 !&% 𝑥 !&$ 𝑥 !&# 𝑥 !&' 𝑥 !\"# 𝑥 !\"$ 𝑥 !\"% 𝑥 ! 𝑥 !&% 𝑥 !&$ 𝑥 !&# 𝑥 !&' 𝑔 !\"# 𝑔 !\"# 𝑔 !\"# 𝑔 !\"# 𝑧 $ 𝑔 %& 𝑔 %& 𝑔 %& 𝑔 %& 𝑐 $ Cyclic Regularization 𝑳 𝑪𝑹 𝑔 !\"# 𝑔 !\"# 𝑔 !\"# 𝑔 !\"# 𝑔 %& 𝑔 %& 𝑔 %& 𝑔 %& 𝑐 $'( M S B M S (a) Pre-training Task I: Forecasting & Contrastive Learning (b) Pre-training Task II: Cyclic Regularization Transferable and Forecastable User Targeting Foundation Model WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia 4.2.2 Tabular Encoding. In Alipay application scenarios, structural user data can be collected primarily in the form of tabular data. The key features of this tabular data pertain to account security and fund flows, facilitating transferability across multiple domains, such as risk management or marketing, for user targeting. To obtain the embedding from the user tabular, we employ transformer-based encoder (denoted as ℎ 𝜙 ). Specifically, given an input table 𝑇 = { 𝑇 𝑐𝑎𝑡 , 𝑇 𝑐𝑜𝑛𝑡 } , where 𝑇 𝑐𝑎𝑡 represents categorical tabular features and 𝑇 𝑐𝑜𝑛𝑡 represents continuous features, ℎ 𝜙 processes 𝑇 𝑐𝑎𝑡 similar to [13] (encoded with Column Embedding [13] and fed into Transformer) while handles 𝑇 𝑐𝑜𝑛𝑡 by mapping it to another feature item with MLP and also using it as the input to the transformer-based encoder. The corresponding embedding 𝑒 (tab) for user 𝑖 can be denoted as:  To pre-train the encoder, two strategies are employed: masked language modeling (MLM)[15] and replaced token detection (RTD)[20]. For MLM, we randomly mask part of the categorical features ˜ 𝑇 M ⊂ 𝑇 𝑐𝑎𝑡 . The encoder is trained by minimizing the crossentropy loss of a multi-class classifier that predicts the original values of the masked features based on the Transformer generated embeddings:  where M represents the set of masked feature IDs, CE is the cross entropy loss, ˆ ˜ 𝑇 𝑚 and ˜ 𝑇 𝑚 denote the prediction of masked feature and the ground truth respectively. In the RTD approach, the original categorical feature values are replaced with random values from the same feature set. A binary classifier is then trained to predict whether a feature has been replaced, with the loss minimized accordingly, which can be expressed as:  where R represents the replaced features, BCE is the binary classification loss, ˆ ˜ 𝑇 𝑟 is the prediction, and ˜ 𝑇 𝑟 ∈ { 0 , 1 } denotes whether the 𝑟 -th feature is replaced. Based on the aforementioned tasks and hyperparameters 𝜆 MLM and 𝜆 RTD, the tabular pre-training loss is:  4.2.3 Text Encoding. At Alipay platform, user searching text can also assist to achieve user understanding. Therefore, we utilize a light BERT model, i.e. , AlBERT[17] as the encoder 𝑓 𝜔 , to encode the user 𝑖 's searching text 𝑅 , which is pre-trained similar to [20]. The searching text encoding can be expressed as:",
  "4.3 User-Text Alignment": "4.3.1 Attention-based User Feature Fusion. For the multi-modal user representation, the embeddings from different sources are fused using cross-attention [6], which is a common feature aggregation approach for multi-modal features. The fused embedding of the user 𝑖 can be expressed as :  where CA represents the cross-attention among all data modalities. 4.3.2 Contrastive User-Text Pre-training. To achieve user targeting task with only one sentence input, we develop the user targeting foundation model inspired by CLIP[29], which aligns multi-modal features with contrastive learning. Specifically, each user's description text is developed according to Sec. 4.1, which are fed into the LLM encoder fine-tuned with LoRA[12] to generate the user text embedding:  where F represents the process of generating user description according to the template illustrated in Sec. 4.1. After obtaining the user and the corresponding encoding of behavior sequence text, we employ contrastive loss Info-NCE [27] to align the modalities, i.e. , fused user embedding 𝑒 𝑓 𝑖,𝑡 1 generated from previous multi-modal user data and text embedding 𝑒 𝑞 𝑖,𝑡 2 . The loss is applied to pulling together the representation from positive pairs while pushing apart negative pairs, takes the form as:    where 𝐵 means the batch size and 𝑠 denotes the cosine similarity function.",
  "4.4 User Targeting": "In this section, we introduce the techniques employed to adapt models for user targeting. Figure 1 (b, I) illustrates the main user targeting pipeline of our model. First, we illustrate the adaptation of our model on target data for zero-shot transfer. Then, we propose a novel prompt tuning method for few-shot user targeting. 4.4.1 Zero-shot Transfer. Once pre-trained, our model can be directly deployed on target datasets without any additional training, i.e., enabling zero-shot inference according to the one-sentence input as depicted in Figure 1 (b, I). Additionally, to enable our model accessible to language demand from non-experts, we add a module to rewrite the demand, matching it with the template designed during pre-training and enhancing its cognitive ability for user data inside Alipay application. Therefore, the module performs as the Query Rewriting . We design a simple two-stage fine-tuning scheme: the first stage is designed to refer to scaling-law to synthesize a multi-round conversation set using multi-source data from Alipay to inject domain knowledge to one LLM (QWEN2-14B[38]); then the seconda stage is to mine different people's circling intentions under <Query - Answers> for fine-tuning. 4.4.2 Few-shot User Targeting via Prompt Tuning. In low-resource scenarios, where only a few user samples (named seed users) exist for the targeting task, we also design an approach for few-shot learning via prompt-tuning, as illustrated in Figure 1 (b, II). Inspired by previous studies[41-43], the seed users can be utilized as labels to learn contexts which improves the descriptive ability of the prompt, by adding learnable tokens to the input text. In practical user targeting scenarios, some negative samples share WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Bin Dou et al. the similar behaviors with the ground-truth samples. For example, 3C digital repairers may have the similar behaviors compared to 3C digital enthusiasts. They can be defined as the hard-negative samples in user targeting task and the mistargeting of them may cause risk in real-world scenarios. To handle the hard-negative samples in our few-shot learning process, we introduce the negative samples 𝑈 -, which can obtained from business feedback, into prompt-tuning together with positive seed users 𝑈 + . Through the contrastive loss, i.e. , triplet loss[11], the learned prompt embedding.  where 𝑒 𝑓 + , 𝑒 𝑓 -and 𝑒 𝑞 denote the embeddings of positive seed users 𝑈 + , negative seed user 𝑈 -and the learnable text embedding, respectively, 𝛼 is the threshold to control embedding similarity. Once trained, the tuned prompt can be used as the input to achieve user targeting via pipeline in Figure 1 (b, I).",
  "5 Experiments": "In this section, we begins with the experimental setup, detailing the datasets, baselines, evaluation metrics, and implementation specifics (Section 5.1). Next, we present extensive experiments that demonstrate our model's effectiveness in user targeting and representation (Section 5.2). Finally, we perform an ablation study to evaluate the effectiveness of each proposed component (Section 5.3).",
  "5.1 Experimental Setups": "5.1.1 Datasets. For pre-training the industrial framework of user targeting, we employ the real-world industrial dataset, which collects heterogeneous multi-scenario user data on Alipay. Specifically, the training dataset D 𝑡𝑟𝑎𝑖𝑛 contains around 500 million pieces of user information (user-text pairs), with each piece organized according to Section 4.1. The user targeting test benchmark contains 13 scenarios, covering security risk control, recommendation and marketing domain. The dataset of each scenario contains 0.3-30 million pieces of user information, which are in the format D 𝑡𝑒𝑠𝑡 = {( 𝑢 𝑖 , 𝑙 𝑖 )} 𝑁 𝑖 = 1 , as 𝑢 𝑖 represents the same user data modal as in training while label 𝑙 𝑖 ranges from { 0 , 1 } , denoting whether the user conducts the operations in the domain, which is annotated according to realworld business scenario. For linear probe analysis, each test dataset is divided 1:4, of which the first part is used for linear layer fitting (named fitting part ) while the second (named evaluation part ) is used for evaluation. Table 1 shows the statistics of our training and test dataset. Table 1: Data statistics for user foundation model pretraining as well as the test benchmarks. 5.1.2 Baselines. The following state-of-the-art user behavioral understanding and targeting models are utilized in our comparison experiments. · ARALLM [34] Analogical Reasoning Augmented Large Language Models, which is the LLM-based method and achieves user targeting from one-sentence demand by structurally understanding the input sentence and matching the users representation with the labels. Pre-trained as well as QuestionAnswering fine-tuned models can be used for user targeting. Here we employ GPT-3.5[28] as the pre-trained model, open-source LLMs as the base model for fine-tuning, such as ChatGLM[9] and QWEN2[38]. · U-MLP One4all [30]. One4all is a general-purpose representation learning through large-scale pre-training, while U-MLP is one of its extended user targeting model which adds MLP user decoder to generate targeted users. · MSDP [8] Multi-scale Stochastic Distribution Prediction model for learning user behavioral sequence representation, which takes the prediction on user's behaviors distribution over a period of time as the self-supervision signal. In addition, TabTransformer trained with RTD & MLM [13] tasks is employed as the baseline for tabular modality (tabular encoding model), QWEN2-7B [38] is employed as the baseline for text modality (searching text encoding model) for user representation evaluation. 5.1.3 Evaluation Metrics. For User Targeting Task , as in each scenario the task can be treated as binary classification, we employ the Accuracy / Precision / Recall metrics for evaluation. For User Representation , linear probe representation analysis is conducted on all annotated datasets, which is evaluated by AUC (Area Under the ROC Curve[3]) and KS (Kolmogorov-Smirnov[2]) metrics. 5.1.4 Implementation Details. The embedding sizes of user representation and text features are set to 1024-dim. In Self-Supervised User Pre-training stage, the user behavioral sequences, i.e., paybill, SPM and miniprogram, which are in the form of text, are first encoded with AlBERT[17]. Then 6-layer Transformer is used to generate encoding of the series for further pre-training. For user tabular information, the similar configuration of TabTransformer is utilized in the tabular encoder. In the following User-Text Alignment stage, embeddings from multi-modal user features are fused using 5 cross attention layers. To generate the encoding of user text description, QWEN2-1.8B[38] is utilized as the LLM encoder, which are tuned with LoRA[12] together with user encoding model. For both of stages, the optimizer is AdamW, with cosine decay learning rate initialized at 4e-4. The model is pre-trained on 16 A100 GPUs (80G), and a single A100 is used for testing (linear probe for user embedding and zero-shot, few-shot user targeting).",
  "5.2 Comparisons": "5.2.1 Zero-shot User Targeting. We take user targeting via zeroshot transfer based on one sentence input on the benchmarks. Experimental Setup. Both ARALLM and fine-tuned ARALLM are also employed as the baseline, taking the same sentence prompt as input. Transferable and Forecastable User Targeting Foundation Model WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Table 2: Quantitative comparison on zero-shot user targeting performance on Alipay benchmarks. Accuracy / Precision / Recall are presented. Table 3: Quantitative comparison on few-shot user targeting. 10 seed users are utilized for all methods. Accuracy / Precision / Recall of user targeting performance are presented. Analysis. Quantitative results in Table 2 demonstrate the superiority of our model, as it outperforms baselines 10%/15%/10% at Accuracy/Precision/Recall metrics crossing all domains. 5.2.2 Few-shot User Targeting. We also take few-shot user targeting on the benchmarks. Experimental Setup. Both ARALLM and fine-tuned ARALLM are also employed as the baseline, together with U-MLP One4all model trained with the seed users. For all few-shot learning baselines, the number of user samples is kept at 10. To ensure the comparison fairness upon seed user number, our model is trained with 5 positive and 5 negative users (10 total). Analysis. The quantitative results in Table 3 demonstrate the superiority of our model, as our model outperforms the baselines 5%/8%/5% at Accuracy/Precision/Recall metrics crossing all domains. Zero-shot Prompt Few-shot Prompt Hard Negative Samples Untargeted User Targeted Users The visualized results are presented in Figure 3, which exhibit the performance improvement of the few-shot learned embedding intuitively. Also, the proximity to positive samples and distance from hard-negative samples proves the effectiveness of our approach, especially for handling hard-negative users. 5.2.3 User Representation Evaluation. We also evaluate the performance of the pre-trained user representation from both our model and other user modeling baselines. Experimental Setup For representation evaluation, we utilize linear probe analysis, which means employing linear network layers to the user embeddings to fit the fitting part of the dataset and evaluate on the evaluation part . Analysis As shown in Table 4, the representation from our user model outperforms other baselines trained by single-modal data (+ 4%/2% than User Behavioral Sequence, + 12%/13% than Tabular data, + 11%/10% than Searching Text). The improvement from simple feature fusion via concatenation also demonstrates the effectiveness of our user feature fusion model (attention-based fusion guided Figure 3: Few-shot User Targeting. The embeddings of both targeted, untargeted and hard-negative users are visualized with t-SNE, presented in the same coordinate system together with prompt embedding. by text description). Also, Figure 4 shows the visualization of user embeddings, as users from each consuming scenario presented as one color, demonstrating the user representation capabilities of our model.",
  "5.3 Ablation Studies": "We conduct ablation studies on user data modal, feature fusion module and the user-text alignment method. 5.3.1 Pre-training Data Modal. To verify the effect of the multisource pre-training data, we conduct the comparison between the model trained without different data modal and the full model. Comparison between Table 5 row 1-3 and row 5 shows the effectiveness of all data sources for pre-training our foundation model, while the user behavioral sequence plays the most significant role in user understanding, leading to over 6%/8%/6% Accuracy/Precison/Recall. WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Bin Dou et al. Table 4: Linear probe results. AUC/KS metrics on Alipay user targeting benchmarks are presented. Table 5: Ablation experiments on zero-shot user targeting over Alipay benchmarks. Different data sources and feature fusion module in pre-training are ablated in the experiment. Accuracy / Precision / Recall of user targeting are presented. Figure 4: t-SNE visualization of user representations. Every color represents users with different attributes (here denoting the outcomes of each user). Outcomes in 15 N <N <=100 100 <N<=1000 1000 < N <10000 days 5.3.2 Attention-based Feature Fusion. The effect of feature fusion module in our user-text alignment stage is also evaluated. among candidates. The framework is pre-trained in a two-stage strategy, where the first Self-Supervised User Modeling Stage is built to generate user representation from multi-modal user data, and the second User-Text Alignment Stage is built for fusing multimodal user features and align user embedding with corresponding language representation for user targeting. Additionally, to enhance the model predictive ability, the user-text pair for our framework pre-training is also designed, as the text is obtained via from future use behaviors while user representation is generated from pervious information. With the pre-trained framework, zero-shot transfer and few-shot user targeting via prompt-tuning are designed to achieve user targeting. On real-world benchmarks, our model outperforms baselines in cross-domain scenarios, which demonstrates the superiority of our model. Additionally, in real-world application, our model exhibits superior performance compared with other industrial frameworks. Since 2024, Found is deployed in the user selection platform of Alipay for numerous domains, obtaining an improvement for different user understanding scenarios. For example, improving 30% CTR for \"Used Car Selling Preferences\" users and 13.2% CTR for \"Video Watching in Alipay\" users. Experimental Setup. We employ the feature concatenation instead of the attention-based fusion module to fuse multi-modal user features as comparison. Analysis. As shown in 4-th row of Table 5, the absence of attention-based multi-modal user feature fusion will lead to a performance decrease of ∼ 1%/2%/1% Accuracy/Precison/Recall.",
  "6 Conclusion": "In this paper, we propose an industrial framework Found for transferable and predictive user targeting. Our model takes one-sentence form demand as input and achieve user understanding with heterogeneous multi-scenario data, enabling it to select targeted user",
  "Acknowledgments": "This work was supported by the National Key Research and Development Plan of China (2023YFB4502305), and Ant Group through Ant Research Intern Program. We acknowledge that the datasets are only used for academic research, and do not represent any real business situation. They are desensitized and encrypted, do not contain any Personal Identifiable Information and were destroyed after the experiments. Adequate data protection was carried out during the experiment to prevent the risk of data copy leakage. Transferable and Forecastable User Targeting Foundation Model WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia",
  "References": "[1] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023 (2023). [2] Vance W Berger and YanYan Zhou. 2014. Kolmogorov-smirnov test: Overview. Wiley statsref: Statistics reference online (2014). [3] Andrew P Bradley. 1997. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern recognition 30, 7 (1997), 1145-1159. [4] William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang, Brandon Roy, Jad Kabbara, and Deb Roy. 2023. Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings. arXiv preprint arXiv:2305.14321 (2023). [5] Dave Chaffey and Fiona Ellis-Chadwick. 2019. Digital marketing . Pearson uk. [6] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. 2021. Crossvit: Crossattention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision . 357-366. [7] Kyunghyun Cho. 2014. Learning phrase representations using RNN encoderdecoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014). [8] Chilin Fu, Weichang Wu, Xiaolu Zhang, Jun Hu, Jing Wang, and Jun Zhou. 2023. Robust user behavioral sequence representation via multi-scale stochastic distribution prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 4567-4573. [9] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. 2024. ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools. arXiv:2406.12793 [10] Jie Gu, Feng Wang, Qinghui Sun, Zhiquan Ye, Xiaoxiao Xu, Jingmin Chen, and Jun Zhang. 2021. Exploiting behavioral consistence for universal user representation. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 4063-4071. [11] Alexander Hermans, Lucas Beyer, and Bastian Leibe. 2017. In defense of the triplet loss for person re-identification. arXiv preprint arXiv:1703.07737 (2017). [12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [13] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. 2020. Tabtransformer: Tabular data modeling using contextual embeddings. arXiv preprint arXiv:2012.06678 (2020). [14] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and visionlanguage representation learning with noisy text supervision. In International conference on machine learning . PMLR, 4904-4916. [15] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proc. of AACL . [16] Hyeyoung Ko, Suyeon Lee, Yoonseo Park, and Anna Choi. 2022. A survey of recommendation systems: recommendation models, techniques, and application fields. Electronics 11, 1 (2022), 141. [17] Z Lan. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 (2019). [18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning . PMLR, 19730-19742. [19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning . PMLR, 12888-12900. [20] Zicheng Li, Shoushan Li, and Guodong Zhou. 2022. Pre-trained token-replaced detection model as few-shot learner. arXiv preprint arXiv:2203.03235 (2022). [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural information processing systems 36 (2024). [22] Kang Liu, Feng Xue, Dan Guo, Peijie Sun, Shengsheng Qian, and Richang Hong. 2023. Multimodal graph contrastive learning for multimedia-based recommendation. IEEE Transactions on Multimedia 25 (2023), 9343-9355. [23] Zhuang Liu, Yunpu Ma, Matthias Schubert, Yuanxin Ouyang, and Zhang Xiong. 2022. Multi-modal contrastive pre-training for recommendation. In Proceedings of the 2022 International Conference on Multimedia Retrieval . 99-108. [24] Carlo Milana and Arvind Ashta. 2021. Artificial intelligence techniques in finance and financial markets: a survey of the literature. Strategic Change 30, 3 (2021), 189-209. [25] Yabo Ni, Dan Ou, Shichen Liu, Xiang Li, Wenwu Ou, Anxiang Zeng, and Luo Si. 2018. Perceive your users in depth: Learning universal user representations from multiple e-commerce tasks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 596-605. [26] Narjes Nikzad-Khasmakhi, MA Balafar, and M Reza Feizi-Derakhshi. 2019. The state-of-the-art in expert recommendation systems. Engineering Applications of Artificial Intelligence 82 (2019), 126-147. [27] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730-27744. [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning . PMLR, 8748-8763. [30] Kyuyong Shin, Hanock Kwak, Kyung-Min Kim, Minkyu Kim, Young-Jin Park, Jisu Jeong, and Seungjae Jung. 2021. One4all user representation for recommender systems in e-commerce. arXiv preprint arXiv:2106.00573 (2021). [31] Duncan Simester, Artem Timoshenko, and Spyros I Zoumpoulis. 2020. Targeting prospective customers: Robustness of machine-learning methods to typical data challenges. Management Science 66, 6 (2020), 2495-2522. [32] Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. 2023. Headto-tail: How knowledgeable are large language models (llm)? AKA will llms replace knowledge graphs? arXiv preprint arXiv:2308.10168 (2023). [33] A Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017). [34] Junjie Wang, Dan Yang, Binbin Hu, Yue Shen, Wen Zhang, and Jinjie Gu. 2024. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 5860-5871. [35] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, and Wen Gao. 2023. Large-scale multi-modal pre-trained models: A comprehensive survey. Machine Intelligence Research 20, 4 (2023), 447-482. [36] Wei Wei, Chao Huang, Lianghao Xia, and Chuxu Zhang. 2023. Multi-modal self-supervised learning for recommendation. In Proceedings of the ACM Web Conference 2023 . 790-800. [37] Zhihao Wen and Yuan Fang. 2023. Augmenting low-resource text classification with graph-grounded pre-training and prompting. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 506-516. [38] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671 (2024). [39] Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. 2020. Parameter-efficient transfer from sequential behaviors for user modeling and recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 1469-1478. [40] Junqi Zhang, Bing Bai, Ye Lin, Jian Liang, Kun Bai, and Fei Wang. 2020. Generalpurpose user embeddings based on mobile app usage. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2831-2840. [41] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 16816-16825. [42] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learning to prompt for vision-language models. International Journal of Computer Vision 130, 9 (2022), 2337-2348. [43] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. 2023. Promptaligned gradient for prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 15659-15669. [44] Yun Zhu, Haizhou Shi, Xiaotang Wang, Yongchao Liu, Yaoke Wang, Boci Peng, Chuntao Hong, and Siliang Tang. 2024. GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs. arXiv preprint arXiv:2410.10329 (2024). WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Bin Dou et al.",
  "A Additional Model Details": "",
  "A.1 Self-Supervised User Modeling": "For User Behavioral Sequence Pre-training , the time span for each window represents one week. The behavioral sequence length is 9, divided into 5 (fed into the time-aware autoregressive model) and 4 (used as the future behaviors for contrastive learning). 𝜆 𝑐𝑦𝑐 is set at 0.1. The user behavioral sequence pre-training takes ∼ 40 hours on 8 A100 GPUs for model convergence. Figure 5: The architecture of our tabular encoder. Column Embedding Layer Normalization Categorical Features 𝑇 !\"# = (𝑇 $ , 𝑇 % , … 𝑇 & ) Continuous Features 𝑇 !'(# 𝜖ℛ !'(# Transformer ×𝑁 Multi-Head Attention Add & Norm Add & Norm Feed Forward MLP Mean Pooling 𝑓 !\"#$ = (𝑓 !% , 𝑓 !& , … 𝑓 !' ) '∗) (𝑓 !\"*+$ ) ) 𝑒 ($#-) For Tabular Pre-training , the architecture of the transformerbased encoder is shown in Fig. 5, as the categorical features 𝑇 𝑐𝑎𝑡 are encoded with Column Embedding[13] and continuous features 𝑇 𝑐𝑜𝑛𝑡 are encoded with MLP, which are both fed into the transformer blocks. Followed by mean pooling operation among all features, tabular embedding 𝑒 (tab) for each user is generated. In our model, transformer block number 𝑁 is selected as 4 and feature dimension 𝑑 is 1024. To achieve the unsupervised pre-training, MLM and RTD tasks are performed. For MLM task, given a tabular input with categorical features 𝑇 𝑐𝑎𝑡 = { 𝑇 1 , 𝑇 2 , · · · 𝑇 C } , we randomly select 𝑘 % features from index 1 to C and mask them as missing. By minimizing the objective that tries to predict the original features of the masked features, from the embedding outputted from the top-layer Transformer, MLM is conducted. Here the selection rate 𝑘 is 30. For RTD task, the original feature is replaced by a random value of that feature and the loss is designed to minimize for a binary classifier that tries to predict whether or not the feature has been replaced. It can be noticed that different binary classifiers are defined for each column rather than a shared one, as each column has its own Column Embedding. In our tabular pre-training process, 𝜆 𝑀𝐿𝑀 is set at 0.6 and 𝜆 𝑀𝐿𝑀 is 0.4. The tabular pre-training takes ∼ 50 hours on 8 A100 GPUs for model convergence.",
  "A.2 User-Text Alignment": "After pre-training the user encoder, multi-modal features are fused and further aligned with text features generated from the future user behavior to form the user targeting system. In multi-modal feature fusion, similar strategy based on crossattention in previous works [6] is performed in our model, transferring from conventional image-text data modality to our behaviortabular-text modal. The dimension for each modal and the fused feature is 1024. And for further user-text alignment, pre-trained multi-modal encoder followed with feature fusion module is the user encoder, while LLM with LoRA fine-tuning module is utilized as the text encoder. The embedding of the user information and future text are generated from the previous-seven and next-two time windows, respectively. The time span is identical to User Behavioral Sequence Pre-training in A.1, which means the user feature is generated by the multi-modal user data from the first 7 weeks and further aligned with text which is based on the template in 4.1 from the following 2 weeks using the contrastive learning. The user-text alignment takes ∼ 70 hours on 16 A100 GPUs for model convergence.",
  "B Additional Experimental Results": "We present additional zero-shot user targeting results (measured by Precision on the validation dataset) across different real-world advertising application scenarios in Alipay. Fig 6 illustrates that our foundation model outperforms the previous model significantly. Figure 6: Zero-shot user targeting results in different realworld advertising application scenarios. 0.13 0.14 0.13 0.14 0.12 0.13 0.13 0.12 0.12 0.12 0.13 0.13 0.14 0.11 0.12 0.13 0.12 0.13 0.11 0.12 0.14 0.14 0.13 0.12 0.14 0.30 0.30 0.30 0.39 0.29 0.37 0.61 0.46 0.71 0.77 0.60 0.42 0.61 0.39 0.31 0.49 0.55 0.58 0.40 0.49 0.35 0.54 0.46 0.50 0.54 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 Genshin Honkai: Star Rai Eggy Party Snowbreak: Containment Zone Leage Of Legends SPY×FAMILY Jay Chou Tomb Raiders Goda Takeshi capybara Sanxingdui Ruins Dysney Zootopia Young Leisure Travel Hotels One-Day Travel in Beijing One-Day Travel in Hangzhou One-Day Travel in Shanghai One-Day Travel in Wuhan One-Day Travel in Xi'an Stock Market Saving Money(to Yu'e Bao) Electricity bill Water, electricity and gas bills financial management Precision Zero-shot User Targeting Results ARALLM Ours",
  "C Limitations and Future Work": "Though achieving high performance in user targeting and user modeling, there are still some limitations in our model: 1. Though showing fantastic capability at Alipay platform, our model's generalizability to other platforms or industrial scenarios is not fully tested. 2. The training cost remains high though our two-stage strategy releases the requirement compared to the from-scratch strategy. Therefore, our future work will focus on the following: 1. Enhancing the model transferability across platforms. We plan to achieve this by enhancing the model's ability for processing multimodal user features (such as images, videos, knowledge graph, etc) and improving the model scalability for more data. 2. Reducing the time consumption for the foundation model pre-training.",
  "keywords_parsed": [
    "User Targeting",
    "User Understanding",
    "Self-supervised Pre-training",
    "Multi-modal Pretraining"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity"
    },
    {
      "ref_id": "b2",
      "title": "Kolmogorov-smirnov test: Overview"
    },
    {
      "ref_id": "b3",
      "title": "The use of the area under the ROC curve in the evaluation of machine learning algorithms"
    },
    {
      "ref_id": "b4",
      "title": "Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings"
    },
    {
      "ref_id": "b5",
      "title": "Digital marketing"
    },
    {
      "ref_id": "b6",
      "title": "Crossvit: Crossattention multi-scale vision transformer for image classification"
    },
    {
      "ref_id": "b7",
      "title": "Learning phrase representations using RNN encoderdecoder for statistical machine translation"
    },
    {
      "ref_id": "b8",
      "title": "Robust user behavioral sequence representation via multi-scale stochastic distribution prediction"
    },
    {
      "ref_id": "b9",
      "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools"
    },
    {
      "ref_id": "b10",
      "title": "Exploiting behavioral consistence for universal user representation"
    },
    {
      "ref_id": "b11",
      "title": "In defense of the triplet loss for person re-identification"
    },
    {
      "ref_id": "b12",
      "title": "Lora: Low-rank adaptation of large language models"
    },
    {
      "ref_id": "b13",
      "title": "Tabtransformer: Tabular data modeling using contextual embeddings"
    },
    {
      "ref_id": "b14",
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision"
    },
    {
      "ref_id": "b15",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "ref_id": "b16",
      "title": "A survey of recommendation systems: recommendation models, techniques, and application fields"
    },
    {
      "ref_id": "b17",
      "title": "Albert: A lite bert for self-supervised learning of language representations"
    },
    {
      "ref_id": "b18",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
    },
    {
      "ref_id": "b19",
      "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
    },
    {
      "ref_id": "b20",
      "title": "Pre-trained token-replaced detection model as few-shot learner"
    },
    {
      "ref_id": "b21",
      "title": "Visual instruction tuning"
    },
    {
      "ref_id": "b22",
      "title": "Multimodal graph contrastive learning for multimedia-based recommendation"
    },
    {
      "ref_id": "b23",
      "title": "Multi-modal contrastive pre-training for recommendation"
    },
    {
      "ref_id": "b24",
      "title": "Artificial intelligence techniques in finance and financial markets: a survey of the literature"
    },
    {
      "ref_id": "b25",
      "title": "Perceive your users in depth: Learning universal user representations from multiple e-commerce tasks"
    },
    {
      "ref_id": "b26",
      "title": "The state-of-the-art in expert recommendation systems"
    },
    {
      "ref_id": "b27",
      "title": "Representation learning with contrastive predictive coding"
    },
    {
      "ref_id": "b28",
      "title": "Training language models to follow instructions with human feedback"
    },
    {
      "ref_id": "b29",
      "title": "Learning transferable visual models from natural language supervision"
    },
    {
      "ref_id": "b30",
      "title": "One4all user representation for recommender systems in e-commerce"
    },
    {
      "ref_id": "b31",
      "title": "Targeting prospective customers: Robustness of machine-learning methods to typical data challenges"
    },
    {
      "ref_id": "b32",
      "title": "Headto-tail: How knowledgeable are large language models (llm)? AKA will llms replace knowledge graphs?"
    },
    {
      "ref_id": "b33",
      "title": "Attention is all you need"
    },
    {
      "ref_id": "b34",
      "title": "Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs"
    },
    {
      "ref_id": "b35",
      "title": "Large-scale multi-modal pre-trained models: A comprehensive survey"
    },
    {
      "ref_id": "b36",
      "title": "Multi-modal self-supervised learning for recommendation"
    },
    {
      "ref_id": "b37",
      "title": "Augmenting low-resource text classification with graph-grounded pre-training and prompting"
    },
    {
      "ref_id": "b38",
      "title": "Qwen2 technical report"
    },
    {
      "ref_id": "b39",
      "title": "Parameter-efficient transfer from sequential behaviors for user modeling and recommendation"
    },
    {
      "ref_id": "b40",
      "title": "Generalpurpose user embeddings based on mobile app usage"
    },
    {
      "ref_id": "b41",
      "title": "Conditional prompt learning for vision-language models"
    },
    {
      "ref_id": "b42",
      "title": "Learning to prompt for vision-language models"
    },
    {
      "ref_id": "b43",
      "title": "Promptaligned gradient for prompt tuning"
    },
    {
      "ref_id": "b44",
      "title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs"
    }
  ]
}