{"title": "'Generative CI' through Collective Response Systems", "authors": "Aviv Ovadya", "pub_date": "2023-02-01", "abstract": "This paper frames a specific kind of generative collective intelligence (CI) facilitation system: the collective response systemand the collective dialogues that it makes possible. It defines their structure, processes, key properties, and key principles with the goal of creating a useful shared language. It is intended to motivate the potential benefits of such systems and act as a concise reference text. Collective response systems enable a form of 'generative voting'where both the 'votes' and the choices of what to vote on are provided by the collective. This allows diverse populations to express their perspectives and to hear those of others-in ways that overcome the traditional limitations of polling, town halls, voting, referenda, etc. This can enable non-confrontational exploration of divisive issues, can help identify common ground, and surfaces insights from those closest to the issues; thus overcoming gridlock around conflict and governance challenges, increasing trust, and developing mandates. 1  Notable examples of existing collective response systems include Polis and Remesh. Polis has been used by Taiwan and around the world for policy-making at different levels of government [1]. The United Nations deployed Remesh, an AI-supported collective response system to understand the challenges and needs of ordinary people across a war-torn country-and with thousands of direct participants and a substantial proportion of the population engaging on social media [2][3][4]. Existing collective response systems build on technical work across recommender systems, language models, and human-computer interaction among other disciplines. 2 Continued progress toward the development and adoption of such systems could help revitalize democracies, reimagine corporate governance, transform conflict, and govern powerful AI systems-both as a complement to deeper deliberative democratic processes and as an option where deeper processes are not possible.", "sections": [{"heading": "MOTIVATION", "text": "Most current approaches for group understanding and decisionmaking don't work well at scale; particularly given the warped incentives of the modern attention economy:\n\u2022 Thoughtful deliberation doesn't scale: If a 'town hall' of a thousand people all try to talk and listen simultaneously, the result is just noise. Alternatively, if people speak one by one, that can take days. With a million people, this would take decades for a single issue, so most people impacted by a policy or conflict cannot meaningfully speak or listen.\n\u2022 People feel voiceless and disrespected: People often don't feel empowered to express their perspectives, and don't see their views and experiences reflected. This creates mistrust and causes people to tune out of governance and conflict resolution processes.\n\u2022 Valuable insights are missed: 'Knowledge keys'-information, ideas, and insights that can help overcome entrenched conflictsare often missed. These often originate in those closest to an issue, many of whom have minimal reach through standard channels.\n\u2022 Divisiveness wins over common ground: Stirring up conflict is rewarded far more than identifying and elevating common ground.\nHowever, connectivity, machine learning, and democratic practice advances 3 may allow us to overcome some of these challenges with 'simultaneous communication at a scale. ' Collective response systems are meant to enable groups of arbitrary scale to make generative decisions (e.g. decisions where the participants develop the option space). They are designed to get as close as possible to one version of the 'democratic ideal'-that in collective decisions:\n1. Everyone has the ability to respond with their perspective 2. Everyone's response is listened to and incorporated. 3. The response(s) that best represent the group is chosen. 4Collective response systems are decision and deliberation-focused, and aim to avoid the perverse incentives of entertainment or politicsfocused communication environments (e.g. social media) where attention-seeking behaviors are rewarded. They enable a form of focused simultaneous communication at scale. They also enable collectives to be treated as single agents, which can then interact with other such collective agents.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Contributions", "text": "This framing paper defines the structure, processes, key properties, and key principles of collective response systems. It is intended to provide a shared language for research and practice around collective intelligence facilitation and scaled democratic decision-making, informed by advances in human-computer interaction and artificial intelligence.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "STRUCTURE AND PROCESS", "text": "A collective response system is a collective intelligence facilitation system that satisfies the structure, processes, properties, and principles described below, in sections 2 and 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Input: A group and a prompt", "text": "There are two inputs to the system: the promptfoot_4 which might be a question, something to be completed, etc. and a group which is the set of participants.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Examples:", "text": "\u2022 For the citizens of a city: The most important challenge in education facing our city is. . .\n\u2022 For a platform like Facebook: What kinds of behavior should not be allowed on this platform?\n\u2022 For students about to graduate: Who should be our graduation speaker?\n\u2022 For an organization: The core values of our organization should be:\n\u2022 For the global population: The most important steps we should be taking to address climate change are. . .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Output:", "text": "A representative distillation of the evaluations and optionally the raw and/or derived data.\nThe core outputs of such a collective response system are an approximation or aggregation of the responses that meaningfully and representatively convey the 'best' response(s). This can most generally be referred to as a representative distillation.\nThe definition of 'best' is largely open to interpretation but is constrained by the key principles described below. For example, the responses with the most approval might be considered the best, though other approaches may be used instead (e.g. the bridgingbased ranking [5] implemented by Polis's group informed consensus [1] or by Twitter's Community Notes [7]). Outputs such as e.g. the \"best response\" may also be used in downstream systems, including as described below in future collective response processes.\nIn addition, derived data useful for reflecting back the responses and evaluations of the group may be output. For example, systems like Polis can show participants how issues and people fit on a map of perspectives. Finally, raw results such as the responses and evaluations may be output for further processing and analysis.", "publication_ref": ["b4", "b0", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Process: Responding, Evaluating, Distilling", "text": "A process facilitated by a collective response system is called a collective response process. Such processes are made up of three kinds of subprocess: 6\u2022 Responding subprocess: Everyone in the group can (optionally) respond to the prompt.\n\u2022 Evaluating subprocess: Everyone in the group evaluates some subset of the responses (potentially chosen by the system).\n\u2022 Distilling subprocess: The system approximates and/or aggregates the evaluations of each response to produce a useful output.\nPutting this all together, consider the case of five thousand people in a city participating in a collective dialogue on education policy.\nThe collective response process in this case might look like this:\n1. Responding: Participants are asked: \"What is the most important challenge in education facing our city?\" They can respond with a short answer-a response. In systems like Polis [1] and Remesh [8] these responses are usually short, e.g. one to three sentences. 2. Evaluating: Participants are assigned 'voting tasks', for example, to evaluate if they agree or disagree with a response. 3. Distilling: The system can then show the participants which responses have the most approval and also the most common ground across (e.g. political) divides. Such a process can be executed in as little as 5 minutes (e.g. synchronously with Remesh), or over days or weeks (e.g. asynchronously and iteratively with Polis [1]).", "publication_ref": ["b0", "b8", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "PROPERTIES AND PRINCIPLES", "text": "Key properties. The following properties must be satisfied for a system to be a collective response system:\n(1) Participant Agency: Participants themselves may suggest responses instead of being limited to a fixed set-this may be referred to as \"participatory perspectives\". 78 (2) Parallel Communication: Every participant doesn't need to talk to or listen to every other in order to move the process forward. This may rely on methods such as bracketing (i.e. roughly analogous to breakout groups [10]) and elicitation inference (i.e. methods for approximating how the participants would evaluate every response given evaluations of a subset) [11]. (3) Representative Distillation: There is some mechanism for approximating and/or aggregating the input of all of the participants to get evaluations representative of the whole. The resulting distilled outputs (which may include a \"map\" of output [1]), are shared back to the participants.\nKey principles. In addition, the design of a collective response system must be designed to satisfy the following key principles in order to live up to the democratic ideals described earlier:\n(1) Collective Oriented: The system design prioritizes supporting/understanding a group and its decision-making over supporting/understanding an individual and their decisionmaking. 910 (2) Prompt Oriented: The system design prioritizes the creation of outputs that best respond to the input prompt over other goals (e.g. entertainment). 11 (3) Deliberation Oriented: The system design prioritizes affordances and incentives that support participants understanding the potential choices-and each other's perspectives on them 12 -over other goals (e.g. engagement). 13 (For example, this can involve supporting actions such as identifying common ground, surfacing insight, self-reflection on group experiences, and mapping of perspectives.) (4) Fair Hearing Oriented: The system design must ensure that all participants are given a fair hearing-all responses are given the opportunity to be heard by the group such that they may end up being considered the best. 14   Properties are binary and clearcut, while the principles leave more room for interpretation; there may be additional philosophical, empirical, and normative work to be done to refine these principles further.", "publication_ref": ["b10", "b11", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Collective Dialogue", "text": "A collective dialogue system (CDS) is a collective response system with some form of iteration or feedback loop, such that participants repeatedly interact with the system to create new responses based on previous outputs (either to the same prompts or new prompts). The term collective dialogue can be used as shorthand for a collective dialogue process (the processes facilitated by a CDS).\nSuch a dialogue process enables further drilling down into details, motivations, and blockers for the most promising solutions. They can also be used for any other sense-making or iterative decisionmaking task. 9 For example, a normal recommendation system fails this test-it aims to give each user what they would want. 10 Current deliberative polls may also arguably fail this test as they focus on individual choice changes instead of collective conclusions. 11 Under this principle, arguably, connection, trust-building, practice in deliberative communication, etc. might also be prioritized in some systems, but only in the service of better prompt outputs. This is potentially worth iterating on or providing exceptions for. 12 Including potentially the relative extent of the different perspectives. 13 In other words, the purpose is to output a collective's response to a prompt in a way that supports mutual understanding, trust, and wise decision-making; or alternatively \"combines plurality and wisdom\". 14 While the ideal of the collective response system might involve fully equitable listening, depending on the system, responses with more traction may be seen more, and some responses may be shown for other reasons, e.g. to better infer what has the most common ground. (Polis does this) It may also be helpful in some contexts to have a reputation score with some influence over the process.\nConsider the example earlier asking \"What is the most important challenge in education facing our city?\". If it was part of a collective dialogue, after the first answer, a follow-up question might be asked based on the responses that are deemed most representative, in a new collective response process with the same participants. This might look like asking: \"What do you think is the best solution for addressing [that challenge]?\"", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EXISTING WORK & RELATED CONCEPTS", "text": "There are many existing partial implementations of collective response systems and a few complete ones. Some of the most notable include WikiSurvey's like Polis and All Our Ideas [1,9]. 15 Remesh and similar tools focus on approximation components which enable greater scale [4]. An alternative approach can be found in bracketing systems like PSiApp [10]. Some processes created for citizens' assemblies [12] and deliberative polls [13] may also fulfill the criteria for a collective response system. Relatedly, sortitionbased selection of participants can be used to select people within a much larger group in situations where using collective response systems for the entire group is impractical.\nCollective response systems can blur the lines between governance systems and recommender systems. For example, collective response systems may use methods such as matrix factorization to support approximation [14]. 16 More recent approaches have also involved the use of large language models [11,15].\nOther kinds of online machine learning systems are also likely to be related to collective response systems (particularly depending on where and how training data is sourced). The chaining of collective response systems together into complex decision-making networks can be somewhat analogous to both prompt chaining [16] and multi-body sortition [17]. 17 There may also be relevant similarities between prompt 'engineering' for generative AI and prompt design for generative CI.\nMore generally, collective response systems are often meant to address multi-principal-agent problems [11], so prior work related to such problems is likely to be relevant. Finally disciplines including social choice theory have much to say on potential definitions of 'best'-though the insight of other disciplines can also be incorporated. This is clearly not an exhaustive list, but simply meant to illustrate part of the breadth of related work.\nThe underlying ideas and ideals of collective response systems are not new-this paper is primarily meant to articulate them succinctly, with the goal of creating a useful shared language.\nCollective response systems support understanding of a group's values, needs, and desires by combining participant-led responses and evaluations with a reversal of attention economy incentiveswhich then helps create decisive mandates. Such systems can be used to identify and reward responses that can bridge divides-the opposite of most modern politics, media, and social media. Anecdotally, collective response systems can also be incredibly efficient at surfacing new insights from those closest to the issues, and people feel heard and respected when they see their responses incorporated into the process.", "publication_ref": ["b0", "b9", "b3", "b10", "b12", "b13", "b14", "b11", "b15", "b16", "b17", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Collective response systems are not a panacea.\nAn isolated collective response process will only elicit gut opinions, not considered judgments. This is somewhat addressed in more elaborate (and expensive) processes such as citizens assemblies and deliberative polls through access to educational materials, experts, stakeholders, compensated time, and careful neutral moderation [12,13,18]. Such resourcing and deliberative depth is crucial for decisions of significant importance. Some (but not all) of the downsides of the collective response system can be overcome with iteration, i.e. collective dialogues. Such iteration can enable deeper and more deliberative explorations. Iterated or chained collective dialogues can even be used to determine e.g. what questions are most important to ask experts, stakeholders, or the entire group. 18  However, even when using a well-designed collective dialogue system, careful process design is crucial. Particular collective response systems implementations may also have significant tradeoffs, e.g. in their capacity to be chained together into dialogues, to incorporate new information from participants mid-process, to be synchronous vs. asynchronous, to involve text vs. audio or video, etc. All of these factors must be taken into account.\nMany digitally mediated collective response systems do not, by default, enable normal human relationship formation (similar to ordinary, non-generative, voting systems). However, they do (surprisingly) appear to enable a valuable form of connection and trust despite that, likely by facilitating a form of collective introspection and helping identify areas of common ground through representative distillation.\nWhere possible, collective response processes would ideally be used as a complement to deeper deliberative processes instead of a replacement. For example, they might be used by a citizens' assembly to get open-ended public input from the public in order to support its decision-making. Due to their lower overheard, collective response processes may also be run on a regular basis, far more often than assemblies, for less important decisions or for specific components of a governance process where deep personal deliberation is less crucial. 18 They may also potentially be used to identify tradeoffs and 'cruxes' that can then be resolved.", "publication_ref": ["b12", "b13", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Opportunities", "text": "While not appropriate for all collective decisions, collective response systems may help for embed new forms of sense-making, governance, democracy, and agency at every level of society:\n\u2022 Governance & Conflict: They can be deployed by representatives, civil servants, citizen assemblies, and conflict mediators to meaningfully include the broader (busy) public.\n\u2022 'Corporate democracy': They support a new form of corporate governance, going beyond just shareholders and elite stakeholders to users, employees, and the impacted public.\n\u2022 Media & Understanding: They can support a new form of collective introspection-helping a public 'know itself', identify common ground, and thus better navigate internal and external challenges.\nThere is a significant opportunity for research to create evaluation protocols and metrics for such systems, understand the impacts of different design decisions, and thus develop increasingly better systems (and chains of systems) for different purposes and contexts.\nJust as advances in machine learning have lead to new ways of interacting with computers by enabling 'generative AI', advances in collective response systems can lead to new ways of interacting with groups by supporting 'generative collective intelligence' (CI).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Polis: Scaling deliberation by mapping high dimensional opinion spaces", "journal": "Recerca. Revista de Pensament i An\u00e0lisi", "year": "", "authors": "Christopher Small; Michael Bjorkegren; Timo Erkkil\u00e4; Lynette Shaw; Colin Megill"}, {"ref_id": "b1", "title": "Using Artificial Intelligence for Peacebuilding", "journal": "Journal of Peacebuilding & Development", "year": "2022-08", "authors": "Masood Daanish; Martin Alavi; Colin W\u00e4hlisch; Andrew Irwin;  Konya"}, {"ref_id": "b2", "title": "Making People's Voices Matter | UN Web TV", "journal": "", "year": null, "authors": ""}, {"ref_id": "b3", "title": "ASRSG Williams conducts digital dialogue with 1000 Libyans", "journal": "", "year": "2021-01", "authors": ""}, {"ref_id": "b4", "title": "Bridging-Based Ranking", "journal": "", "year": "2022-05", "authors": "Aviv Ovadya"}, {"ref_id": "b5", "title": "Language Models are Few-Shot Learners", "journal": "", "year": "", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Mateusz Litwin; Scott Gray; Benjamin Chess; Jack Clark; Christopher Berner; Sam Mccandlish; Alec Radford; Ilya Sutskever; Dario Amodei"}, {"ref_id": "b6", "title": "", "journal": "", "year": "2020", "authors": " Curran Associates;  Inc"}, {"ref_id": "b7", "title": "Birdwatch: Crowd Wisdom and Bridging Algorithms can Inform Understanding and Reduce the Spread of Misinformation", "journal": "", "year": "", "authors": "Stefan Wojcik; Sophie Hilgard; Nick Judd; Delia Mocanu; Stephen Ragain; Keith Coleman; M B Fallin; Jay Hunzaker;  Baxter"}, {"ref_id": "b8", "title": "Remesh -Qualitative Insights at Quantitative Scale", "journal": "", "year": "", "authors": ""}, {"ref_id": "b9", "title": "Wiki Surveys: Open and Quantifiable Social Data Collection", "journal": "PLOS ONE", "year": "2015-05", "authors": "Matthew J Salganik; Karen E C Levy"}, {"ref_id": "b10", "title": "PSi", "journal": "", "year": "", "authors": ""}, {"ref_id": "b11", "title": "Elicitation inference optimization for multi-principal-agent alignment", "journal": "", "year": "2022", "authors": "Andrew Konya; Lina Yeping; Michael P Qiu; Aviv Varga;  Ovadya"}, {"ref_id": "b12", "title": "Facilitating Deliberation: A Practical Guide", "journal": "", "year": "2022", "authors": "Kimbra White; Nicole Hunter; Keith Greaves"}, {"ref_id": "b13", "title": "Democracy When the People Are Thinking: Deliberation and Democratic Renewal", "journal": "Proceedings of the American Philosophical Society", "year": "2019", "authors": "James Fishkin"}, {"ref_id": "b14", "title": "Faster peace via inclusivity: An efficient paradigm to understand populations in conflict zones", "journal": "", "year": "", "authors": "Jordan Bilich; Michael Varga; Daanish Masood; Andrew Konya"}, {"ref_id": "b15", "title": "Finetuning language models to find agreement among humans with diverse preferences", "journal": "", "year": "2022-11", "authors": "A Michiel; Martin J Bakker; Hannah R Chadwick; Michael Henry Sheahan; Lucy Tessler; Jan Campbell-Gillingham; Nat Balaguer; Amelia Mcaleese; John Glaese; Matthew M Aslanides; Christopher Botvinick;  Summerfield"}, {"ref_id": "b16", "title": "PromptChainer: Chaining Large Language Model Prompts through Visual Programming", "journal": "", "year": "2022-03", "authors": "Tongshuang Wu; Ellen Jiang; Aaron Donsbach; Jeff Gray; Alejandra Molina; Michael Terry; Carrie J Cai"}, {"ref_id": "b17", "title": "Why Hybrid Bicameralism Is Not Right for Sortition", "journal": "Politics & Society", "year": "2018-09", "authors": "Terrill Bouricius"}, {"ref_id": "b18", "title": "Innovative Citizen Participation and New Democratic Institutions: Catching the Deliberative Wave", "journal": "OECD", "year": "2020-06", "authors": ""}], "figures": [], "formulas": [], "doi": ""}
