{"DSFNet: Learning Disentangled Scenario Factorization for Multi-Scenario Route Ranking": "\u2217 Jiahao Yu Yihai Duan \u2217 Longfei Xu \u2217", "ABSTRACT": "", "KEYWORDS": "Multi-scenario route ranking (MSRR) is crucial in many industrial mapping systems. However, the industrial community mainly adopts interactive interfaces to encourage users to select pre-defined scenarios, which may hinder the downstream ranking performance. In addition, in the academic community, the multi-scenario ranking works only come from other fields, and there are no works specifically focusing on route data due to lacking a publicly available MSRR dataset. Moreover, all the existing multi-scenario works still fail to address the three specific challenges of MSRR simultaneously, i.e. explosion of scenario number, high entanglement, and high-capacity demand. Different from the prior, to address MSRR, our key idea is to factorize the complicated scenario in route ranking into several disentangled factor scenario patterns. Accordingly, we propose a novel method, D isentangled S cenario F actorization Net work (DSFNet), which flexibly composes scenario-dependent parameters based on a high-capacity multi-factor-scenario-branch structure. Then, a novel regularization is proposed to induce the disentanglement of factor scenarios. Furthermore, two extra novel techniques, i.e. scenario-aware batch normalization and scenarioaware feature filtering, are developed to improve the network awareness of scenario representation. Additionally, to facilitate MSRR research in the academic community, we propose MSDR, the first large-scale publicly available annotated industrial M ultiS cenario D riving R oute dataset. Comprehensive experimental results demonstrate the superiority of our DSFNet, which has been successfully deployed in AMap to serve the major online traffic.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems ; Retrieval models and ranking . \u2217 These authors contributed equally to this research. Work done when Jiahao Yu was an intern at Alibaba Group. Longfei Xu is the corresponding author. Multi-Scenario Learning, Route Ranking, Recommender System, Route Recommendation, Scenario Disentanglement", "1 INTRODUCTION": "Route recommendation is one of the core functionalities in many online location-based applications, which aims to predict the most likely route given an origin and a destination on the road network. Many industrial mapping systems ( e.g. AMap, Apple Maps, Baidu Maps, Tencent Maps) have started to build a two-stage recall&rank framework to provide route recommendation service for users, where a lightweight efficient routing algorithm ( e.g. A* search) is applied to recall multiple feasible routes in the first stage and a series of complex ranking models are deployed to further rank the recalled routes to screen out several user-preferred ones in the second stage. This two-stage framework brings many benefits. For example, it ensures real-time recommendation services even if using deep learning techniques to model personalization. Moreover, it structurally decouples the modeling of feasibility and personalization of routes, which facilitates algorithm iteration and system troubleshooting. This paper focuses on the route ranking stage. Unfortunately, MSRR has not been well analyzed and addressed in both the academic and industrial communities. In the industrial community, mapping systems mainly define several scenarios manually before building the ranking model, and encourage users to When building the route ranking system, we found that the underlying multi-scenario characteristics of user preference are critical for ranking user-preferred routes. Specifically, one may prefer different routes in different scenarios. For example, people whocommuteshort distances in the morning rush hour prefer roads with shorter time and higher punctuality rates because they care much about time, while people who travel long distances during holidays prefer roads with more highways and fewer traffic lights because they focus much on driving comfort. This motivates us to study the problem of multi-scenario route ranking (MSRR). Jiahao et al. select the scenario they are in by clicking the scenario click box on the apps page. However, the manual division is sub-optimal and may even hinder the downstream ranking tasks. Additionally, many users usually ignore click prompts, which brings noise to the scenario label in data collection for model iteration and may even impair the recommendation quality. In the academic community, there are even no works studying the MSRR problem, since the community lacks a publicly available dataset for MSRR. Fortunately, there are still many successful multi-scenario ranking methods [20, 23, 26, 29], though they come from other fields, where ranking objects may be products, ads, etc . Nevertheless, all these works still fail to properly solve the MSRR problem. To explain the reason, we first exhaustively analyze the unique characteristics of the multi-scenario route data, which is divided into four points: 1) Scenario representation of the route is fine-grained and non-categorical ( e.g. departure time, status of road network between origin and destination). This is different from the explicitly categorical scenario in the existing multi-scenario ranking studies, which represents a pre-defined spot where products (or ads, videos, etc. ) are presented to users. 3) Since fine-grained scenario representation is rich in semantics, it leads to the phenomenon of high entanglement . Specifically, we observed that users might focus on identical factors under highly different scenarios. For example, users in night commuting and long-distance travel scenarios both focus on the quality of routes, while users in morning commuting and train catching scenarios both prefer the least time-consuming routes. Thus exploring disentangled factors of scenario that affect user preference is definitely necessary for tackling the MSRR problem. 2) Since fine-grained scenario representation values are mostly continuous in a wide range, it results in explosion of scenario number ( abbr. ESN) that combining different values of scenario representation to manually partition scenarios, which still cannot be avoided even with discretization. 4) Route features are highly abundant since a route is not only described from various aspects ( e.g. distance, traffic, toll) but also composed of a long road link sequence. It implies that the route ranking task requires a model with large model capacity, namely high-capacity demand. The above data characteristics introduce three specific challenges needing to be addressed in dealing with MSRR, i.e. ESN problem, high entanglement, and high-capacity demand. However, all the existing multi-scenario ranking studies fail to address them simultaneously. Specifically, multi-scenario ranking solutions all design a model with scenario-dependent parameters, which can be mainly divided into two categories, i.e. multi-branch method and dynamicparameter method (Fig. 1). Despite multi-branch methods having high model capacity [8], the huge memory consumption caused by the ESN problem makes them impractical for industrial systems. In contrast, dynamic-parameter methods employ a much more flexible architecture to tackle the ESN problem. However, directly learning very high-dimensional dynamic parameters from scenario conditions is sub-effective in pattern learning and inefficient in time and memory [27]. Thus, Yan et al. [27] recently utilized low-rank decomposition to improve these works. However, the low-rank Figure 1: Comparison for the schemes of multi-scenario ranking method. (a) Maintain as many network branches as scenario number and one scenario-specific branch is selected in a forward propagation. (b) Directly generate the dynamic scenario-specific parameters of the network based on the scenario feature \ud835\udc46 . (c) Our method by composing dynamic parameters in a multi-factor-scenario-branch style. Output Input \ud835\udc46 Branch selection Input \ud835\udc46 Input \ud835\udc46 G G G (a) Multi-branch (b) Dynamic-parameter (c) DSFNet Factor Scenario Learners Output Output constraint reduces the model capacity significantly, thus failing to meet the high-capacity demand. Moreover, all these works lack a module tailored to handle the highly-entangled scenario data. Moreover, since the lack of a publicly available dataset is a major obstacle to MSRR research in the academic community, we propose MSDR, a large-scale publicly available annotated industrial multiscenario driving route dataset. MSDR was collected when AMap, a top-tier location-based service provider in China, provided driving navigation services for users. In MSDR, each data sample includes route features, fine-grained scenario representation, user profiles, and a binary label representing whether the user prefers this route. Accordingly, for the first time, this paper proposes a novel methodology specifically for tackling the MSRR problem. Concretely, we first propose scenario factorization from a parametric perspective, i.e. decomposing scenario-dependent parameters into several shared parameter sets, each of which represents a factor scenario learner (FSL). The scenario representation is responsible for generating suitable gates for all FSLs. This method essentially composes dynamic parameters in a multi-factor-scenario-branch structure, thus not only solving the ESN problem but also fulfilling the high-capacity demand. Then, to resolve high entanglement, we propose a novel disentangling regularization, which consists of two terms, i.e. neuron centroid repulsion (NCR) and contrastive neuron clustering (CNC), to learn a semantically disentangled factor scenario for each FSL in a layer-wise manner. For each layer of paralleled FSLs, NCR keeps the neuron centroids as far away from each other as possible and CNC pushes directions of all neuron vectors in each FSL to cluster appropriately in parameter hyperspace, which not only induces the interpretability of factor scenarios (Sec. 5.2), but also improve the model generalization [25]. In addition, we provide two extra novel techniques, scenario-aware batch normalization (SABN) and scenario-aware feature filtering (SAFF), which significantly improve the model performance since they both enhance the network awareness of the scenario feature. Overall, the four techniques proposed essentially inject the scenario information into all the locations of an MLP with learnable parameters. Specifically, scenario factorization and disentangling regularization enhance layer-wise weights and biases, SAFF boosts the feature embedding, and SABN strengthens layer-wise batch normalization. DSFNet: Learning Disentangled Scenario Factorization for Multi-Scenario Route Ranking The major contributions can be summarized as follows. \u00b7 We are the first to specifically analyze the multi-scenario route ranking problem (MSRR) brought from the industrial background and to propose a novel method, D isentangled S cenario F actorization Net work (DSFNet), as a tailored solution to overcome the specific challenges of MSRR. \u00b7 We propose MSDR, the first large-scale publicly available annotated industrial M ultiS cenario D riving R oute dataset, to facilitate MSRR research in the academic community 1 . \u00b7 We propose four novel techniques, i.e. scenario factorization, disentangling regularization, scenario-aware batch normalization (SABN), and scenario-aware feature filtering (SAFF), to arm a vanilla MLP into our DSFNet. And we theoretically analyze the advantages of the formulation of our disentangling regularization. \u00b7 We conduct extensive experiments and both offline and online experiments demonstrate the superiority of our DSFNet. DSFNet has been successfully deployed in AMap to serve the major online traffic.", "2 RELATED WORK": "", "2.1 Multi-Scenario Ranking": "Data distributions under different scenarios are quite different, which is hard for a common model to handle. The existing multiscenario ranking methods all introduce scenario-dependent parameters to model data under different scenarios, which mainly fall into two categories, i.e. multi-branch method and dynamic-parameter method. Multi-branch method designs multiple network branches based on the multi-task learning framework [3, 12, 14, 23]. Each branch is responsible for modeling data distribution of one specific scenario, and various sharing mechanisms are designed at the branch bottom to transfer inter-scenario knowledge [19, 32, 36]. Recently, STAR [20] designed shared parameter gating to learn common knowledge. HiNet [35] introduced a hierarchical branch and utilized an attentive module to model scenario correlation. Despite the high model capacity, the number of parameters of these works grows linearly with the scenario number, which is extremely inflexible. To tackle this, the dynamic-parameter method directly generates dynamic scenario-dependent parameters of the network from scenario features [2, 10, 24, 30]. However, their parameter generation process lacks a shared component, which may cause model ignorance of the common patterns, leading to sub-effective pattern learning [27]. Recently, Yan et al. [27] exploited low-rank constraint to decompose parameters into dynamic low-rank kernels and shared weights to improve the effectiveness and efficiency of these works. Though effective, all these works are tailored for ranking objects other than routes, e.g. products, ads, videos, and do not consider the unique characteristics of the multi-scenario route data, thus failing to properly solve the MSRR problem.", "2.2 DSFNet vs. MoE & MuSeNet": "The series of classic MoE [5, 8, 18] and the recent MuSeNet [26] are both superficially similar to our DSFNet since they all have 1 Data and codes will be released after this paper is published. multi-branch structures and gating mechanisms. But they have at least three differences: 1) Different gating targets : The gating targets of MoE and MuSeNetarebothfeatures ( i.e. outputs of branches), whereas our targets are network parameters ( i.e. FSL). Actually, since parameters are fixed ( i.e. input-independent), DSFNet builds an interpretable scenario space where FSL represents coordinate basis and gating represents scenario point (Sec. 5.2.4), and that is what scenario factorization implies. 3) Different constraints on multi-branch : MoE does not set constraints. MuSeNet uses prototypical clustering [11] to induce semantic implicit scenario prototype vectors, while our DSFNet learns disentangled factor scenarios with a novel disentangling regularization on parameters. 2) Different motivations of multi-branch : MoE expects each expert branch to only handle a subset of data to reduce the learning difficulty of each branch, thus accelerating the overall learning [8]. MuSeNet aims to model specific user intention under each implicit centroid scenario. Our DSFNet decomposes parameters into several branches to learn factor scenario patterns ( i.e. coordinate bases ).", "3 METHOD": "We formulate MSRR as a click-through-rate (CTR) prediction problem. Mathematically, given a set of routes R from the previous recalling stage, for a user \ud835\udc62 \u2208 U , the goal of MSRR is to infer the route \ud835\udc5f \u2208 R most likely to be preferred by the user \ud835\udc62 considering route features \ud835\udc53 \ud835\udc5f , user profiles \ud835\udc53 \ud835\udc62 , and scenario representation \ud835\udc53 \ud835\udc60 , formally defined as solving the optimal route with the highest conditional probability:", "3.1 Scenario Factorization Network": "To model the conditional probability in Eq. (1), one common approach is to utilize a neural network f ( x , s ; \u0398 ) , where x is the concatenation of the embedding of \ud835\udc53 \ud835\udc5f and \ud835\udc53 \ud835\udc62 , s is the embedding of \ud835\udc53 \ud835\udc60 and \u0398 is the network parameter set. To better handle multi-scenario characteristics, multi-branch method redesigns the modeling as \u02dd \ud835\udc41 \ud835\udc60 \ud835\udc56 = 1 yi ( s ) f ( x , s ; \u0398i ) , where \ud835\udc41 \ud835\udc60 is scenario number and yi ( s ) is the \ud835\udc56 'th element of the one-hot scenario indicating vector y ( s ) . And dynamic-parameter method redesigns the modeling as f ( x ; \u0398 ( s )) , where \u0398 (\u00b7) is typically an MLP. Different from the prior work, we propose scenario factorization from a parametric perspective, i.e. redesigning the modeling as f ( x ; \u02dd \ud835\udc41 \ud835\udc56 = 1 \ud835\udefc \ud835\udc56 ( s ) \u02dc \u0398i ) , where \ud835\udc41 is a hyperparameter, \ud835\udefc (\u00b7) is a gating network and \u02dc \u0398i is named as factor scenario learner (FSL). As shown in Fig. 2, the dynamic parameters in the \ud835\udc59 'th layer of our network are generated via   where \ud835\udf0e (\u00b7) is the sigmoid activation and the gating network \ud835\udefc ( \ud835\udc59 ) (\u00b7) shares parameters across all layers in implementation. Jiahao et al. \u0303 \u0303 \u0303 \u0303 \u0305 \u0305 \u0305 \u0305 Figure 2: The illustration of DSFNet with four FSLs ( i.e. \ud835\udc41 = 4 ). In each layer, scenario factorization structure composes the dynamic network parameter W ( s ) via gating the four shared factor parameters { \u02dc W \ud835\udc56 } 4 \ud835\udc56 = 1 from FSLs with the gates \ud835\udefc ( s ) . The disentangling regularization constrains the four factor parameters by L \ud835\udc41\ud835\udc36\ud835\udc45 keeping the neuron centroids { \u00af w ( \ud835\udc56 ) } 4 \ud835\udc56 = 1 as far away from each other as possible and L \ud835\udc36\ud835\udc41\ud835\udc36 pushing the directions of the neurons in each \u02dc W \ud835\udc56 to cluster appropriately (Sec. 3.2). SABN and SAFF are integrated to enhance the network awareness of s (Sec. 3.3). Bias is omitted for figure simplicity. ! (\") ! (\") ! (\") ! (\") ! ! ! ! Route features \ud835\udc53 \ud835\udc5f User profiles \ud835\udc53 \" Scenario representation \ud835\udc53 \" Embedding Layer SAFF SABN \u00d7\ud835\udc41 \ud835\udc3f MLP \ud835\udc16(\ud835\udc2c) \ud835\udf0e C G \ud835\udc31 \ud835\udc2c \u2026 \u2112 \ud835\udc36\ud835\udc41\ud835\udc36 \u2112 \ud835\udc41\ud835\udc36\ud835\udc45 \u2112 \ud835\udc36\ud835\udc41\ud835\udc36 Activation function \ud835\udf0e Concatenation C G Parameter gating Output ! ! ! ! ! ! ! ! \ud835\udefc(\ud835\udc2c) ! ! (#) ! ! (#)", "3.2 Disentangled Factorization": "3.2.1 Overview. Scenario factorization without additional priors cannot spontaneously mine underlying disentangled factors from the highly entangled data. Even if it could, multiple FSLs would redundantly fit some common factor scenarios, e.g. time-concerned scenario, because neural networks could be easily biased towards the head distribution of data [31]. To address these issues, we first propose our Assumption 3.1 as follows: problem can be recast into min max \ud835\udc56 \u2260 \ud835\udc57 \u00af w ( \ud835\udc56 ) \u00b7 \u00af w ( \ud835\udc57 ) . Inspired by [25], we first define an adversarial loss L \ud835\udc40\ud835\udc40\ud835\udc34 to solve the minimax: Assumption 3.1. Keeping parameters of FSLs as angularly separate as possible can induce FSLs to mine disentangled factor scenarios. This is inspired by the observation from the studies on angular diversity [13, 25] that neural networks with angularly separated neurons in each layer are prone to capture different aspects of the data. Based on our assumption, we propose our disentangling regularization by composing two novel regularization terms, i.e. neuron centroid repulsion (NCR) and contrastive neuron clustering (CNC), to achieve disentangled factorization in a layer-wise manner. Specifically, in the \ud835\udc59 'th layer of paralleled FSLs, NCR keeps the neuron centroids, each of which represents a \u02dc W ( \ud835\udc59 ) \ud835\udc56 , as far away from each other as possible, and CNC pushes the directions of the neurons in each \u02dc W ( \ud835\udc59 ) \ud835\udc56 to cluster appropriately. The subsequent analysis is carried out for the \ud835\udc59 'th layer, thus we use W ( \ud835\udc56 ) to denote \u02dc W ( \ud835\udc59 ) \ud835\udc56 for simplicity. We define the neuron centroid as \u00af w ( \ud835\udc56 ) = \u02dd \ud835\udc40 \ud835\udc58 = 1 w ( \ud835\udc56 ) \ud835\udc58 /\u2225 \u02dd \ud835\udc40 \ud835\udc58 = 1 w ( \ud835\udc56 ) \ud835\udc58 \u2225 , where w ( \ud835\udc56 ) \ud835\udc58 \u2208 R \ud835\udc51 \u00d7 1 is the \ud835\udc58 'th neuron, i.e. the transposed row vector, of W ( \ud835\udc56 ) , \ud835\udc40 is the row number, i.e. output dimension of this layer, and \u2225 \u00b7 \u2225 denotes the Euclidean norm. 3.2.2 Neuron centroid repulsion. Keeping neuron centroids as far away from each other as possible is equivalent to maximizing the minimal pairwise distance between the centroids, which is known as the Tammes problem [22]. As the centroids are normalized, this  where \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) is the angle between \u00af w ( \ud835\udc56 ) and \u00af w ( \ud835\udc57 ) . However, we find that Eq. (4) interferes with the convergence of the main CTR prediction task after the convergence of L \ud835\udc40\ud835\udc40\ud835\udc34 , which is not expected. Thus, we analyze the gradient of L \ud835\udc40\ud835\udc40\ud835\udc34 : Although the angular-independent gradient norm is beneficial for solving gradient vanishing when \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) is close to zero at the early stage of training [25], it becomes a gradient noise after the convergence of L \ud835\udc40\ud835\udc40\ud835\udc34 that interferes with the model learning on the main CTR prediction task. To induce the gradient norm to converge to zero as neuron centroids move far away enough from each other, we propose neuron centroid repulsion (NCR) in Eq. (6).   which is derived from our Lemma 3.1, proven in Appendix A. Lemma3.1. When \ud835\udc51 \u2265 \ud835\udc41 -1 , minmax \ud835\udc56 \u2260 \ud835\udc57 \u00af w ( \ud835\udc56 ) \u00b7 \u00af w ( \ud835\udc57 ) \u21d4\u2200 \ud835\udc56 \u2260 \ud835\udc57 \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) = arccos (-1 \ud835\udc41 -1 ) \u21d4 minmax \ud835\udc56 \u2260 \ud835\udc57 \u2225 \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) -arccos (-1 \ud835\udc41 -1 )\u2225 2 . The gradient norm of NCR is further analyzed in Eq. (7). It shows that not only NCR inherits the ability from L \ud835\udc40\ud835\udc40\ud835\udc34 of solving the gradient vanishing, but also the gradient norm gradually converges DSFNet: Learning Disentangled Scenario Factorization for Multi-Scenario Route Ranking to zero as NCR converges, which no longer introduces noises to interfere with the model learning of the main CTR prediction task.  3.2.3 Contrastive neuron clustering. Pushing the directions of neurons in W ( \ud835\udc56 ) to cluster appropriately is conducive to learning disentangled factors, whereas pushing them too close to each other will lead to poor generalization due to the lack of angular diversity. To prevent the directions of neurons from being too close, we propose contrastive neuron clustering (CNC) in Eq. (8) with the idea of contrastive learning, where \ud835\udf3d \u27e8 \ud835\udc4e, \ud835\udc4f \u27e9 denotes the angle between \ud835\udc4e and \ud835\udc4f , and \ud835\udeff \ud835\udf03 is a hyperparameter. CNC first denotes the neuron w ( \ud835\udc56 ) \ud835\udc58 farthest from \u00af w ( \ud835\udc56 ) ( i.e. positive) in W ( \ud835\udc56 ) as an anchor, then constrains the angle between the anchor and the positive smaller than the angle between the anchor and the closest \u00af w ( \ud835\udc57 ) ( i.e. negative) to some extend (measured by \ud835\udeff \ud835\udf03 ).  In practice, it is more efficient to replace angle with cosine similarity since the latter can be efficiently implemented with the dot product. However, using cosine similarity in CNC leads to the vanishing of the repulsion from negative to anchor, i.e. gradient vanishing, especially in the early stage of training. Specifically, we analyze the gradient:  where \u00af \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) \ud835\udc58 denotes the angle between w ( \ud835\udc56 ) \ud835\udc58 and \u00af w ( \ud835\udc57 ) . It shows that the gradient norm is close to zero when \u00af \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) \ud835\udc58 ( i.e. the angle between negative and anchor) is small, which is prevalent in neural network[17], especially when \u00af w ( \ud835\udc57 ) is the closest negative of the anchor w ( \ud835\udc56 ) \ud835\udc58 . In contrast, CNC in angle form provides a gradient norm independent of \u00af \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) \ud835\udc58 (Eq. (10)).  3.2.4 Disentangling regularization. In practice, NCR and CNC can be efficiently implemented with matrix multiplication. Accordingly, we develop the disentangling regularization in Eq. (11) for training DSFNet, where \ud835\udc41 \ud835\udc3f denotes the layer number of our DSFNet. Thus, the total loss is shown in Eq. (12), where L \ud835\udc35\ud835\udc36\ud835\udc38 is binary cross entropy loss for CTR modeling and \ud835\udf06 is the regularization coefficient.", "3.3 Enhancing Awareness of Scenario": "Additionally, as the dimension of the route feature is much higher than that of the scenario feature, the network is prone to capture unreliable intra-correlation of the route feature, leading to weakened network awareness of the scenario feature [15]. To enhance the awareness, we further propose two practical techniques, i.e. scenario-aware batch normalization and scenario-aware feature filtering, to inject more feature interactions to DSFNet. 3.3.1 Scenario-aware batch normalization. Batch normalization applies a global normalization and learnable re-scaling across all samples [7], which may be harmful to multi-scenario data as data distributions under different scenarios are different. [20] designed partitioned normalization (PN) to normalize and re-scale samples privately within each scenario. Despite the effectiveness of addressing the differences in scenarios, it requires that samples in a mini-batch are sampled from the same scenario during training, which is inflexible. And it learns a set of parameters for each scenario, which is impractical due to the ESN problem. Therefore, we propose scenario-aware batch normalization (SABN):  where \ud835\udf07 \ud835\udc67 and \ud835\udf0e 2 \ud835\udc67 are mean and variance of \ud835\udc67 in current batch, \ud835\udf38 (\u00b7) and \ud835\udf37 (\u00b7) are two MLPs, and s \ud835\udc67 is scenario feature of sample \ud835\udc67 . Though the samples in a mini-batch are normalized by scenarioagnostic statistics, they are re-scaled by scenario-dependent parameters. Thus, it not only overcomes the limitations of PN but also induces the model to learn the feature distribution transformation from scenario-agnostic distribution to scenario-specific one. During testing, \ud835\udf07 \ud835\udc67 and \ud835\udf0e 2 \ud835\udc67 are respectively replaced with the moving average of mean and variance accumulated in the training phase. 3.3.2 Scenario-aware feature filtering. Furthermore, we seek to model the process of humans manually selecting features based on scenario information, which is common in the feature engineering phase. Inspired by the gating technique [6], we propose scenarioaware feature filtering (SAFF) via feeding s into the gating network:  where \ud835\udf0e is sigmoid activation and \u2299 denotes Hadamard product.", "4 THE PROPOSED DATASET": "Since there are no available benchmarks for evaluating MSRR models in the academic community, we propose M ultiS cenario D riving R oute dataset (MSDR), the first large-scale publicly available annotated industrial dataset, to facilitate MSRR research in the academia. Jiahao et al.", "4.1 Data Acquisition": "MSDR was collected when AMap, a top-tier location-based service provider in China, provided driving navigation services for users. Given the origin and destination, one navigation service may involve multiple route recommendation services, where the recommendation service is supported by a recall&rank framework, and in each route recommendation service it retrieves up to one hundred candidate routes in the recalling stage and screens out three routes as the system outputs from the candidates in the ranking stage. Specifically, when a user determines the destination through AMap, the navigation service enters the standby state and initiates a route recommendation, and then the three output routes are exposed to the user through the app page. When the user selects one route, the navigation service is officially started. During the navigation, once the user deviates from the selected route, a new route recommendation will be triggered at the off-route position. AMap collects the selected route set, the off-route position set, the times of deviation in a navigation(ToD), the recalled routes in the first recommendation, and the corresponding scenario information. The navigation service lasts until the user reaches the destination or ends the navigation prematurely. To obtain the user's preference labels for routes, we use the selected route set and the off-track position set to splice the groundtruth route, which is used for computing the coverage rate with all the recalled routes. Then the route with the highest coverage rate among the recalled ones is labeled as positive, while the rest are all labeled as negative. AMap further collects the route labels and the coverage rate of each route into the database. To construct a label-balanced dataset, we collect only three or four negative routes in each navigation, i.e. the nonpositive exposed routes (two or three), and one randomly sampled from the other recalled routes. Moreover, navigation data that end prematurely are discarded to avoid introducing noise.", "4.2 MSDR Dataset": "Through extracting samples from the data acquisition process, we obtained our MSDR. It contains two weeks of data, from June 25th, 2023 to July 8th, 2023, during which there are no Chinese holidays. We sampled data from eight big cities in China, i.e. Beijing, Shanghai, Guangzhou, Hangzhou, Wuhan, Zhengzhou, Chongqing, and Chengdu. Each data sample includes abundant route features, fine-grained scenario representation, user profiles, and a preference label. Specifically, Tab. 1 exhibits some route features provided in MSDR. The user profiles include gender, age, and occupation, which are all encrypted through hash and do not disclose privacy. The scenario representation includes time, date, some road network characteristics between origin and destination ( i.e. congestion situation), spherical distance between origin and destination, POI (Point-of-interest) category of destination, location type of origin and destination, and user familiarity with origin and destination. To adapt MSDR to the multi-branch methods, we select some scenario feature fields (Tab. 2) to manually divide the scenario representation into 3 \u00d7 2 \u00d7 3 \u00d7 2 \u00d7 2 = 72 scenarios. As shown in Fig. 3, we further divide the long-tail data under the 72 scenarios into four subsets, i.e. \ud835\udc46 11 1 , \ud835\udc46 36 12 , \ud835\udc46 61 37 , \ud835\udc46 72 62 , following the order from the high sample num- ber to the low in a partition ratio of 15%, 50%, and 85%, where \ud835\udc46 \ud835\udc57 \ud835\udc56 denotes samples under a scenario subset including from the \ud835\udc56 -th Table 1: Some examples of the route features provided in MSDR, along with the corresponding descriptions. Table 2: Empirical rules for manually dividing the scenarios. scenario to the \ud835\udc57 -th scenario. This division allows us to study model performances on the head and tail scenarios, respectively. MSDR contains 59,204,990 samples and 10,461,969 users, and the proportion of positives in MSDR is 23 . 33%. We divide MSDR into a train set and a test set, whose statistics are shown in Fig. 3. The 'Pos%' of the four subsets in Fig. 3 reveal that the head scenarios have higher inducing effect on users' deviation compared with the tail ones, e.g. the head ones include the long-distance travel scenario which has a large quantity of data. Finally, we compare our dataset with some representative route datasets in Tab. 3, including T-Drive [33], NYCT [4], PorT [21] and LonT [16]. Table 3: Comparison of some properties between MSDR and the other datasets. MSDR provides abundant route features (RouF), fine-grained scenario representation (FinSc), user profiles (UsPr), and ranking labels (Rank), whereas MSDR does not provide any GPS information due to privacy concerns.", "5 EXPERIMENTS": "", "5.1 Experiment Settings": "5.1.1 Datasets and metrics. Since our MSDR is the only benchmark available for evaluating MSRR models, we conduct experiments on the MSDR dataset. For metrics, we apply AUC (Area Under the ROC) DSFNet: Learning Disentangled Scenario Factorization for Multi-Scenario Route Ranking R contains 59,204,990 samples and RouF Y N N N Y FinSc Y N N N N ature fields to manually divide the \u00d7 \u00d7 \u00d7 = 2 ts, e divided the long-tail data under 2 3 72 scenarios (Tab. 2). 2 i.e. 62 12 37 1 /u1D446 /u1D446 /u1D446 /u1D446 , , , following the , 72 61 11 36 mber to the low in a partition ratio /u1D457 scenario to the denotes samples under a scenario /u1D457 del performances on the head and not provide any GPS information due to privacy concern. MSDR T-Drive [33] LonT [16] NYCT [4] PorT [21] #Users #Cities #Routes GPS 442 1 284K Y 33K 1 4.96M Y 39K 1 697.62M Y - 1 600 Y 10.46M 8 59.2M N -th scenario. This rtion of positives in MSDR is 23 . 33%. e four subsets in t and a test set, whose statistics are inducing effect on users' deviation ?? reveal that the .g. the head ones include the long- me representative route datasets in has a large quantity of data. Finally, YCT [4], PorT [21] and LonT [16]. gs ce our MSDR is the only benchmark e apply AUC (Area Under the ROC) models, we conduct experiments on Y Rank N Y N N UsPr N N N N N GLYPH<c=6,font=/AAAAAB+TimesNewRomanPSMT> GLYPH<c=7,font=/AAAAAB+TimesNewRomanPSMT> GLYPH<c=8,font=/AAAAAB+TimesNewRomanPSMT> GLYPH<c=9,font=/AAAAAB+TimesNewRomanPSMT> GLYPH<c=4,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=14,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=15,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=21,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=24,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=20,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=18,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=26,font=/AAAAAB+TimesNewRomanPSMT> GLYPH<c=7,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=18,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=10,font=/AAAAAB+TimesNewRomanPSMT> GLYPH<c=8,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=7,font=/AAAAAB+TimesNewRomanPSMT> GLYPH<c=8,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=12,font=/AAAAAB+TimesNewRomanPSMT> GLYPH<c=13,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=23,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=26,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=3,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=5,font=/AAAAAB+TimesNewRomanPSMT> Table 4: Statistics of the train and test set of MSDR. 'Pos%' indicates the proportion of the positives. ALL /u1D446 11 1 /u1D446 36 12 /u1D446 61 37 /u1D446 72 62 Train #Samples 56,234,356 36,554,052 17,720,800 1,948,207 11,297 Pos% 23.33% 23.39% 23.14% 23.78% 24.75% Test #Samples 2,970,634 1,925,341 941,531 103,164 598 Pos% 23.31% 23.40% 23.07% 24.02% 27.42% \u2022 Recall represents the exclusive routing algorithm of the ation. We further calculate subset GLYPH<c=11,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=8,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=3,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=17,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=19,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=27,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=19,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=17,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=18,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=17,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=3,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=26,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=16,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=18,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=22,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=15,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=25,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=19,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=23,font=/AAAAAB+TimesNewRomanPSMT>GLYPH<c=26,font=/AAAAAB+TimesNewRomanPSMT> recalling stage in AMap's route recommendation service. 1 7 62 /u1D456 In addition, we calculate the number compare memory complexity. enchmark our DSFNet against two ed by the multi-scenario priors: Then we compare our methods with some multi-branch methods: \u00b7 ShareBottom [3] builds scenario-specific tower networks on a shared bottom MLP, which learns common knowledge. \u00b7 MMOE [14] applies scenario-specific gates to the outputs of the shared experts to capture correlations among scenarios. Figure 3: The number of samples (#Samples) and proportion of the positives (Pos%) of the 72 scenarios, which are further divided into four subsets (by four background colors), and the statistics of the subsets are exhibited in the table. , where AUC denotes /u1D457 , and AUC 72 \u2022 Vanilla MLP is a naive multi-layer perceptron. and RelaImpr [28, 34] for evaluation. We further calculate subset AUC, i.e. AUC 11 1 , AUC 36 12 , AUC 61 37 , and AUC 72 62 , where AUC \ud835\udc57 \ud835\udc56 denotes the AUC score calculated on \ud835\udc46 \ud835\udc57 \ud835\udc56 . In addition, we calculate the number of parameters of each model to compare memory complexity. 5.1.2 Competitors. We first benchmark our DSFNet against two baselines, which are not designed by the multi-scenario priors: \u00b7 Recall represents the exclusive routing algorithm of the recalling stage in AMap's route recommendation service. Then we compare our methods with some multi-branch methods: \u00b7 Vanilla MLP is a naive multi-layer perceptron. \u00b7 ShareBottom [3] builds scenario-specific tower networks on a shared bottom MLP to learn common knowledge. \u00b7 PLE [23] builds a shared branch and several scenario-specific ones, and applies a progressive routing mechanism to further decouple common knowledge and scenario-specific one. \u00b7 MMOE [14] applies scenario-specific gates to the outputs of the shared experts to capture correlations among scenarios. \u00b7 STAR [20] applies a shared parameter gating to the parameters of scenario-specific branches to transfer common knowledge to the branches for scenario-specific domains. \u00b7 HiNet [35] recently built scenario-specific branches and a shared one with a hierarchical structure and applied an attentive module to share knowledge across the scenarios. Afterward, we compare our model with two dynamic-parameter methods: \u00b7 M2M [30] recently built a network based on meta units, which adopted an MLP architecture whereas its parameters were dynamically generated by the scenario attributes to learn explicit inter-scenario correlations. \u00b7 APG [27] recently applied the low-rank constraint to decompose dynamic parameters into a small set of dynamic scenario-dependent parameters and a set of shared parameters to improve efficiency and effectiveness. Finally, we compare our model with the recent MuSeNet [26], which is also the most related work with ours: \u00b7 MuSeNet [26] recently built a few branches to model data under the implicit scenario centroids, and exploited an alternating optimization to simultaneously perform scenario learning and ranking modeling. 5.1.3 Implementation details. We implement all methods using the Tensorflow toolbox on a Tesla V100 GPU with 16GB. Before fed into the model, x and s are normalized as \u02c6 x and \u02c6 s by moving mean and variance of batch data, respectively, except for the embedded categorical features. Besides x and s , we have a one-hot scenario indicator y ( s ) \u2208 R 72 from the scenario division. We concatenate x and s as the input for all multi-branch methods and MuSeNet, and y ( s ) is only used for branch selection in multi-branch methods, except that HiNet further exploits the embedding of y ( s ) according to [35]. In contrast, we only use x as the input for M2M, APG, and our DSFNet, while s is only used for generating dynamic parameters and as input for SABN and SAFF. We train all models using the Adam optimizer [9] for 2M iterations on MSDR train. We set the batch size to 512. The learning rate is initially set to 0 . 001 and it exponentially decays with a decay rate of 0 . 98 and a decay step of 10K. In our DSFNet, we use a five-layer MLP ( i.e. \ud835\udc41 \ud835\udc3f = 5) to model an FSL, whose hidden dimensions are 256, 128, 64, and 32, respectively. We set \ud835\udc41 = 7, \ud835\udf06 = 0 . 01, and \ud835\udeff \ud835\udf03 = 1 \ud835\udf05 arccos ( 1 1 -\ud835\udc41 ) with \ud835\udf05 = 1 . 75. In baselines, for fair comparison, we keep the depths and hidden dimensions of baselines identical to our DSFNet. Specifically, we set the number of implicit scenarios to \ud835\udc41 = 7 in MuSeNet. We thoroughly finetuned the low rank \ud835\udc3e \ud835\udc4e\ud835\udc5d\ud835\udc54 in APG and finally set \ud835\udc3e \ud835\udc4e\ud835\udc5d\ud835\udc54 = 24. We replace the multi-task layer in HiNet and M2M with an MLP with the same depth, respectively, since our problem is single-task. We keep the hidden dimension of M2M as 256 according to [30], thus resulting in the relatively high memory complexity.", "5.2 Results": "5.2.1 Performance Comparison. We report the comparison results on MSDR test in Tab. 4. As shown, our DSFNet achieves the best performance on the overall MSDR and the four subsets, respectively, and it is the only model that achieves a positive gain over the MLP in the tail \ud835\udc46 72 62 . Moreover, it achieves the best trade-off between model complexity and performance over the previous methods. Then we analyze the baselines. For the recall baseline, the gap of about 20% in AUC validates the indispensability of the ranking model, and the reason for the numerical relationship of the four subset AUCs can refer to the analysis for subset 'Pos%' in Sec. 4.2. For MLP, its performance is average but there is not much gap among the subset AUCs. For multi-branch methods, ShareBottom and MMOE both obtain improvements over the MLP, especially in the head \ud835\udc46 11 1 , \ud835\udc46 36 12 , whereas the seesaw phenomenon occurs as the performances worsen in the tail \ud835\udc46 61 37 , \ud835\udc46 72 62 . PLE and STAR alleviate the performance degradation in the tail \ud835\udc46 61 37 . HiNet further alleviates the issues and brings about AUC 36 12 reaching the highest among the subset AUCs, which is the best multi-branch method. However, some multi-branch methods ( i.e. PLE, HiNet) have nearly 20 times as many parameters as ours due to the ESN problem. For dynamicparameter methods, M2M exhibits average performance in the head \ud835\udc46 11 1 probably because its dynamic parameter generation process is merely dependent on the scenario conditions, which may cause model ignorance of the common patterns, leading to sub-effective Jiahao et al. Table 4: Comparison results of different methods on MSDR test. RelaImpr is calculated based on vanilla MLP. The best results are in boldface and the second best are underlined. Gray background color indicates the result with the highest subset AUC among the four. \u2217 indicates that the difference to the best baseline is statistically significant at 0.05 level. Table 5: Ablation analysis of our four techniques, i.e. scenario factorization (denoted as 'SF'), disentangling regularization L \ud835\udc37\ud835\udc45 , SABN, and SAFF, and comparison results of different regularizations. \u2718 of 'SABN' denotes replacing SABN with standard BN. The best results are in boldface. The number in the bracket denotes the gap with the full model, and in red are the drop gaps of at least -0.70%. Gray background color indicates the result with the highest drop gap in subset AUC among the four. pattern learning [27]. For APG, the AUC results indicate that it underfits the data in all scenarios, since low-rank decomposition makes its model capacity too low (even lower than an MLP), though it adds shared weights to the parameter generation process compared to M2M. In contrast, DSFNet not only maintains high model capacity but also integrates shared FSLs. MuSeNet is basically the second best model, whose inferiority is probably due to ignoring the specific characteristics of multi-scenario route data. 5.2.2 Ablation studies. To study the contribution of each component in DSFNet, we first conduct the ablation study of the four proposed techniques on MSDR test. As shown in Tab. 5, scenario factorization plays the most crucial role in model performance, followed by SAFF, L \ud835\udc37\ud835\udc45 , and SABN. It is visible that they four all have a more significant improvement on \ud835\udc46 36 12 , \ud835\udc46 61 37 , \ud835\udc46 72 62 , than on the head \ud835\udc46 11 1 . Specifically, the drops of AUC caused by removing SAFF or SABN validate their effectiveness, respectively. Though removing L \ud835\udc37\ud835\udc45 only brings a drop of 0.2% in the overall AUC, it results in a drop of at least 1.11% in AUC 61 37 and AUC 72 62 , which validates the effectiveness of L \ud835\udc37\ud835\udc45 in mining diverse factor scenarios. We further compare different versions of disentangling regularization in Tab. 5, where L \ud835\udc5c\ud835\udc5f\ud835\udc61\u210e = \u2225 \u00af W \ud835\udc47 \u00af W -\ud835\udc3c \u2225 2 \ud835\udc39 ( \u00af W is the concatenated matrix of { \u00af w ( \ud835\udc56 ) } \ud835\udc41 \ud835\udc56 = 1 and \u2225 \u00b7 \u2225 \ud835\udc39 is Frobenius norm) and L \ud835\udc50\ud835\udc5c\ud835\udc60 \ud835\udc36\ud835\udc41\ud835\udc36 denotes CNC in cosine form. Compared to DSFNet without regularization, L \ud835\udc5c\ud835\udc5f\ud835\udc61\u210e brings improvement to the head \ud835\udc46 36 12 , but is significantly worse than L \ud835\udc37\ud835\udc45 in the tail \ud835\udc46 61 37 , \ud835\udc46 72 62 , since L \ud835\udc5c\ud835\udc5f\ud835\udc61\u210e tends to group \u00af w ( \ud835\udc56 ) closer [25]. Against L \ud835\udc5c\ud835\udc5f\ud835\udc61\u210e , though L \ud835\udc40\ud835\udc40\ud835\udc34 brings some gain in the tail \ud835\udc46 61 37 , \ud835\udc46 72 62 , due to angular diversity, the performance drops in the head scenarios as the gradient noise brought by L \ud835\udc40\ud835\udc40\ud835\udc34 . Against L \ud835\udc40\ud835\udc40\ud835\udc34 , L \ud835\udc41\ud835\udc36\ud835\udc45 brings comprehensive improvement for addressing the gradient noise problem, but is still slightly behind L \ud835\udc37\ud835\udc45 since sufficient parameter disentangling cannot be achieved without L \ud835\udc36\ud835\udc41\ud835\udc36 . Due to gradient vanishing caused by the cosine similarity (Eq. (9)), L \ud835\udc50\ud835\udc5c\ud835\udc60 \ud835\udc36\ud835\udc41\ud835\udc36 works slightly worse in parameter disentangling, leading to the inferior performance. It is worth noting that L \ud835\udc41\ud835\udc36\ud835\udc45 and L \ud835\udc41\ud835\udc36\ud835\udc45 + L \ud835\udc50\ud835\udc5c\ud835\udc60 \ud835\udc36\ud835\udc41\ud835\udc36 both bring better performance than L \ud835\udc37\ud835\udc45 in the tail \ud835\udc46 72 62 . We argue the reason is that the two regularizations have looser constraints on neuron directions clustering than L \ud835\udc37\ud835\udc45 and it results in neurons being more angularly separated, DSFNet: Learning Disentangled Scenario Factorization for Multi-Scenario Route Ranking CNC Loss 0.0 0.5 1.0 1.5 Iteration (M) 0.0 0.5 1.0 1.5 NCR Loss NCR Loss CNC Loss 0 1 2 3 CNC Loss 1e-5 1e-4 0.0 0.5 1.0 1.5 Iteration (M) 0.0 0.5 1.0 1.5 NCR Loss NCR Loss CNC Loss 0 1 2 3 1e-5 1e-4 AUC (%) (a) Training curves of NCR & CNC Figure 4: Visualization results. a) Convergence of our disentangling regularization L \ud835\udc37\ud835\udc45 . b) Sensitivity of two key hyperparameters ( i.e. \ud835\udc41 and \ud835\udeff \ud835\udf03 ), where \ud835\udeff \ud835\udf03 = 1 \ud835\udf05 arccos (-1 6 ) and \ud835\udf05 varies linearly within each pair of adjacent graduations. 5 6 7 8 9 N 74.26 74.93 74.97 75.01 75.04 0.63M 0.73M 0.83M 0.93M 1.02M N \u03ba 1.15 1.75 3 6 10 5 6 7 8 9 N 74.26 74.93 74.97 75.01 75.04 AUC (%) 0.63M 0.73M 0.83M 0.93M 1.02M N \u03ba 1.15 1.75 3 6 10 (b) Sensitivity of \ud835\udc41 & \ud835\udf05 which brings more inducing effect on mining tail factor scenarios according to Assumption 3.1. 5.2.3 Visualization. We further conduct experiments to analyze the convergence of our L \ud835\udc37\ud835\udc45 and the sensitivity of hyperparameters. As visible in Fig. 4(a), L \ud835\udc41\ud835\udc36\ud835\udc45 and L \ud835\udc36\ud835\udc41\ud835\udc36 both gradually converge during training, especially L \ud835\udc36\ud835\udc41\ud835\udc36 converges to zero completely after oscillation, which not only validates the convergence of our L \ud835\udc37\ud835\udc45 , but also implies that the angular separation among the FSLs is indeed realized in DSFNet. We then present the impact of hyperparameters \ud835\udc41 and \ud835\udeff \ud835\udf03 on model performance in in Fig. 4(b), where we have \ud835\udeff \ud835\udf03 = 1 \ud835\udf05 arccos (-1 6 ) . For \ud835\udc41 , AUC increases with \ud835\udc41 and peaks at \ud835\udc41 = 7 and then decreases, and the reason for the drop is possibly that larger \ud835\udc41 makes data sparser for each factor scenario, thus reducing the overall performance. For \ud835\udf05 , AUC is stable when \ud835\udf05 varies between 1.5 and 3.0. But AUC decreases rapidly when \ud835\udf05 is close to 1.0 since the threshold \ud835\udeff \ud835\udf03 is close to arccos ( 1 1 -\ud835\udc41 ) where \ud835\udc41 = 7, i.e. the converged angle between neural centroids (Lemma 3.1), thus the threshold is too tight to maintain angular diversity, leading to the performance drop. Similarly, when \ud835\udf05 gets much larger, AUC gradually decreases since \ud835\udeff \ud835\udf03 in CNC becomes too loose to induce sufficient disentanglement, leading to the performance drop. 5.2.4 Interpretability. Since DSFNet actually builds an interpretable scenario space where each disentangled FSL represents an orthogonal coordinate basis, we conduct an interpretability study to unveil the disentangled factor scenario each FSL learns. Specifically, for each FSL \u02dc \u0398i , we randomly sampled 200 data samples from MSDR, each of which had the corresponding gate value \ud835\udefc \ud835\udc56 greater than 0.8. We fed these data into DSFNet and modified the gates \ud835\udefc to the one-hot vector which represents the FSL it belongs to, then obtained the logit \ud835\udc66 before sigmoid. We use | \ud835\udf15\ud835\udc66 \ud835\udf15 \u02c6 x \ud835\udc58 | to describe the sensitivity of FSL to feature \u02c6 x \ud835\udc58 , i.e. the attention to \u02c6 x \ud835\udc58 , and their values are normalized to [0,1] within each sample. Finally, we average the 200 gradient vectors to obtain the attention vector of the corresponding FSL. We select some significant route features and exhibit the corresponding attention values from all FSLs in Fig. 5, which essentially reveals the diverse user preferences under different factor scenarios. As shown, the 1st FSL learns a common factor of considering time and distance, the 2nd one gives much focus on highway information, the 3rd reflects the concern of toll, the 4th depicts a preference to user-familiar routes, the 5th may learn a wait-hating preference, the 6th may learn to care about driving \u03ba \u03ba Figure 5: Interpretability experimental results, showing the route features each FSL focuses on. The explanations of the feature names are exhibited in Tab. 1. ETASecond Distance HighwayLen NonHWLen ArrHWDis Toll FamlrRate HeadFamlr #TrafLights ArrUnkDis #Forwards LeavUnkDis LeavHWDis AlleyLen AvgKMPH 1 2 3 4 5 6 7 FSL Feature 0.2 0.4 0.6 0.8 Table 6: The relative improvements on AMap in the online A/B test, where 'Base' denotes the system with the latest previously deployed ranking model and 'Recall' denotes the systemwithout any ranking model ( i.e. only a recalling model). \u2206 is relative improvement calculated by ( \ud835\udc65 -\ud835\udc4f )/ \ud835\udc4f \u00d7 100% where \ud835\udc65 and \ud835\udc4f are metric values of DSFNet and baseline, respectively. comfort, and the last probably learns a factor of preferring routes with high traffic flow speed. In addition, the diverse attentions further validate the disentanglement among the FSLs. 5.2.5 Online A/B test. DSFNet has been deployed in AMap to provide driving navigation services for non-holidays. Before deployment, we further trained DSFNet offline on an extended version of MSDR, which included one month's data (excluding holidays) for all cities in China, then we conducted a one-week online A/B test. The results of the online test are shown in Tab. 6, which suggests the promotion of our DSFNet brought to online industrial mapping applications on all metrics. Specifically, we adopt three online metrics, i.e. deviation rate (DR), average number of deviations (AND), user coverage (UC), where DR indicates the probability of user's deviation in a route, AND indicates the expectation of the number of user's deviations in a navigation, and UC indicates the expectation of user's coverage rate of a route. Recall that AMap collects route label, selected routes set, ToD, and coverage rate of the route (Sec. 4.1), hence we can obtain DR by calculating the proportion of routes labeled as negative in the selected routes in the database, AND by averaging ToDs in the database, and UC by averaging the coverage rates of routes from the selected routes in the database.", "6 CONCLUSION": "In this paper, we specifically analyze the multi-scenario route ranking problem (MSRR) and propose DSFNet, a novel method for MSRR Jiahao et al. that composes the dynamic parameters with a multi-branches structure where each branch represents a disentangled factor scenario. Specifically, we propose four novel techniques, i.e. scenario factorization, disentangling regularization, scenario-aware batch normalization, and scenario-aware feature filtering, to arm a vanilla MLP into our DSFNet. We demonstrate the superiority of DSFNet via extensive experiments on the proposed large-scale industrial dataset MSDR and some online tests. In the future, we plan to study the applicability of DSFNet in other fields, e.g. e-commerce, to extend our method to multi-task ranking, and to explore how to automatically learn the number of factor scenarios in the training phase.", "A PROOF OF LEMMA 3.1": "Proof. First, we prove the second ' \u21d4 ' with \u2200 \ud835\udc56 \u2260 \ud835\udc57 \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) = arccos ( -1 \ud835\udc41 -1 ) \u21d4\u2200 \ud835\udc56 \u2260 \ud835\udc57 \u2225 \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) -arccos ( 1 1 -\ud835\udc41 )\u2225 2 = 0 \u21d4 max \ud835\udc56 \u2260 \ud835\udc57 \u2225 \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) -arccos ( 1 1 -\ud835\udc41 )\u2225 2 = 0 \u21d4 minmax \ud835\udc56 \u2260 \ud835\udc57 \u2225 \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) -arccos (-1 \ud835\udc41 -1 )\u2225 2 . Then, we use \u03a8 1, \u03a8 2 to denote \u03a8 1 : minmax \ud835\udc56 \u2260 \ud835\udc57 \u00af w ( \ud835\udc56 ) \u00b7 \u00af w ( \ud835\udc57 ) and \u03a8 2 : \u2200 \ud835\udc56 \u2260 \ud835\udc57 \ud835\udf3d ( \ud835\udc56,\ud835\udc57 ) = arccos (-1 \ud835\udc41 -1 ) , respectively, and we prove \u03a8 1 \u21d4 \u03a8 2 as follows. Since \u2200 \ud835\udc56 \u2225 \u00af w ( \ud835\udc56 ) \u2225 = 1, we have \ud835\udc41 ( \ud835\udc41 -1 ) max \ud835\udc56 \u2260 \ud835\udc57 \u00af w ( \ud835\udc56 ) \u00b7 \u00af w ( \ud835\udc57 ) \u2265 \u02dd \ud835\udc56 \u2260 \ud835\udc57 \u00af w ( \ud835\udc56 ) \u00b7 \u00af w ( \ud835\udc57 ) = \u2225 \u02dd \ud835\udc41 \ud835\udc56 = 1 \u00af w ( \ud835\udc56 ) \u2225 2 -\u02dd \ud835\udc41 \ud835\udc56 = 1 \u2225 \u00af w ( \ud835\udc56 ) \u2225 2 = \u2225 \u02dd \ud835\udc41 \ud835\udc56 = 1 \u00af w ( \ud835\udc56 ) \u2225 2 -\ud835\udc41 \u2265 -\ud835\udc41 , which indicates that min max \ud835\udc56 \u2260 \ud835\udc57 \u00af w ( \ud835\udc56 ) \u00b7 \u00af w ( \ud835\udc57 ) = 1 1 -\ud835\udc41 and the minimum is reached when \u2200 \ud835\udc58 \u2260 \ud835\udc59 \u00af w ( \ud835\udc58 ) \u00b7 \u00af w ( \ud835\udc59 ) = 1 1 -\ud835\udc41 . Thus, we have \u03a8 2 \u21d2\u2200 \ud835\udc58 \u2260 \ud835\udc59 \u00af w ( \ud835\udc58 ) \u00b7 \u00af w ( \ud835\udc59 ) = 1 1 -\ud835\udc41 \u21d2 \u03a8 1. But to prove \u03a8 1 \u21d2 \u03a8 2, we still need to prove that \u2200 \ud835\udc58 \u2260 \ud835\udc59 \u00af w ( \ud835\udc58 ) \u00b7 \u00af w ( \ud835\udc59 ) = 1 1 -\ud835\udc41 can be achieved in the \ud835\udc51 -dimensional space as \u00af w ( \ud835\udc56 ) \u2208 R \ud835\udc51 . It is equivalent to \u2203 \u00af W s . t . \u00af W \ud835\udc47 \u00af W = P , where \u00af W is the concatenated matrix of { \u00af w ( \ud835\udc56 ) } \ud835\udc41 \ud835\udc56 = 1 and P is an \ud835\udc41 \u00d7 \ud835\udc41 matrix where the diagonal is 1 and the remaining values are 1 1 -\ud835\udc41 . According to [1], P is a semi-positive definite matrix whose rank is \ud835\udc41 -1, thus \u00af W can be obtained through the eigendecomposition of P when \ud835\udc51 \u2265 \ud835\udc41 -1, which proves \u03a8 1 \u21d2 \u03a8 2. \u25a1", "REFERENCES": "[2] Weijie Bian, Kailun Wu, Lejian Ren, Qi Pi, Yujing Zhang, Can Xiao, Xiang-Rong Sheng, Yong-Nan Zhu, Zhangming Chan, Na Mou, Xinchen Luo, Shiming Xiang, Guorui Zhou, Xiaoqiang Zhu, and Hongbo Deng. 2022. CAN: Feature Co-Action Network for Click-Through Rate Prediction. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (Virtual Event, AZ, USA) (WSDM '22) . Association for Computing Machinery, New York, NY, USA, 57-65. https://doi.org/10.1145/3488560.3498435 [1] Sheldon Jay Axler. 1997. Linear algebra done right . Springer. [3] Rich Caruana. 1997. Multitask Learning. Machine Learning 28 (07 1997). https: //doi.org/10.1023/A:1007379606734 [5] David Eigen, Marc'Aurelio Ranzato, and Ilya Sutskever. 2013. Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314 (2013). [4] Brian Donovan and Dan Work. 2016. New York City Taxi Trip Data (2010-2013). https://doi.org/10.13012/J8PN93H8. Accessed: 2023-07-25. [6] Tongwen Huang, Qingyun She, Zhiqiang Wang, and Junlin Zhang. 2020. GateNet: Gating-Enhanced Deep Network for Click-Through Rate Prediction. CoRR abs/2007.03519 (2020). arXiv:2007.03519 https://arxiv.org/abs/2007.03519 [8] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. Adaptive Mixtures of Local Experts. Neural Computation 3, 1 (1991), 79-87. https://doi.org/10.1162/neco.1991.3.1.79 [7] Sergey Ioffe and Christian Sze. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In Proceedings of the 32nd International Conference on Machine Learning , Vol. 37. 448-456. [9] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980 [10] Feifan Li, Lun Du, Qiang Fu, Shi Han, Yushu Du, Guangming Lu, and Zi Li. 2023. DIGMN: Dynamic Intent Guided Meta Network for Differentiated User Engagement Forecasting in Online Professional Social Platforms. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining (Singapore, Singapore) (WSDM '23) . Association for Computing Machinery, New York, NY, USA, 384-392. https://doi.org/10.1145/3539597.3570420 [12] Pengcheng Li, Runze Li, Qing Da, An-Xiang Zeng, and Lijun Zhang. 2020. Improving Multi-Scenario Learning to Rank in E-Commerce by Exploiting Task Relationships in the Label Space. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management (Virtual Event, Ireland) (CIKM '20) . Association for Computing Machinery, New York, NY, USA, 2605-2612. https://doi.org/10.1145/3340531.3412713 [11] Junnan Li, Pan Zhou, Caiming Xiong, and Steven C. H. Hoi. 2021. Prototypical Contrastive Learning of Unsupervised Representations. arXiv:2005.04966 [cs.CV] [13] Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. 2018. Learning towards Minimum Hyperspherical Energy. In Advances in Neural Information Processing Systems , S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran Associates, Inc., 6222-6233. https://proceedings.neurips.cc/paper_files/paper/2018/file/ 177540c7bcb8db31697b601642eac8d4-Paper.pdf [15] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (San Francisco, California, USA) (KDD '16) . Association for Computing Machinery, New York, NY, USA, 1135-1144. https://doi.org/10.1145/2939672. 2939778 [14] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018. Modeling Task Relationships in Multi-Task Learning with Multi-Gate Mixtureof-Experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD '18) . Association for Computing Machinery, New York, NY, USA, 1930-1939. https: //doi.org/10.1145/3219819.3220007 [16] Leonardo Sol\u00e9 Rodrigues, Matteo Sammarco, Marcin Detyniecki, and Miguel Elias M. Campista. 2022. CRAWDAD ufrj/london-trajectories. https://dx.doi.org/ 10.15783/ejej-de94. Accessed: 2023-07-25. [18] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017). [17] Aruni RoyChowdhury, Prakhar Sharma, and Erik G. Learned-Miller. 2017. Reducing Duplicate Filters in Deep Neural Networks. In NIPS workshop on Deep Learning: Bridging Theory and Practice . [19] Qijie Shen, Wanjie Tao, Jing Zhang, Hong Wen, Zulong Chen, and Quan Lu. 2021. SAR-Net: A Scenario-Aware Ranking Network for Personalized Fair Recommendation in Hundreds of Travel Scenarios. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (Virtual Event, Queensland, Australia) (CIKM '21) . Association for Computing Machinery, New York, NY, USA, 4094-4103. https://doi.org/10.1145/3459637.3481948 [21] Luigi Libero Lucio Starace. 2020. Porto taxi trajectories. https://figshare.com/ articles/dataset/Porto_taxi_trajectories/12302165. Accessed: 2023-07-25. [20] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, and Xiaoqiang Zhu. 2021. One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (Virtual Event, Queensland, Australia) (CIKM '21) . Association for Computing Machinery, New York, NY, USA, 4104-4113. https://doi.org/10.1145/3459637.3481941 [22] Pieter Merkus Lambertus Tammes. 1930. Onthe origin of number and arrangement of the places of exit on the surface of pollen-grains . Ph. D. Dissertation. University of Groningen. Relation: http://www.rug.nl/ Rights: De Bussy. [24] Yichao Wang, Huifeng Guo, Bo Chen, Weiwen Liu, Zhirong Liu, Qi Zhang, Zhicheng He, Hongkun Zheng, Weiwei Yao, Muyu Zhang, Zhenhua Dong, and Ruiming Tang. 2022. CausalInt: Causal Inspired Intervention for MultiScenario Recommendation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Washington DC, USA) (KDD '22) . Association for Computing Machinery, New York, NY, USA, 4090-4099. https://doi.org/10.1145/3534678.3539221 [23] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations. In Proceedings of the 14th ACM Conference on Recommender Systems (Virtual Event, Brazil) (RecSys '20) . Association for Computing Machinery, New York, NY, USA, 269-278. https://doi.org/10.1145/3383313. 3412236 [25] Zhennan Wang, Canqun Xiang, Wenbin Zou, and Chen Xu. 2020. MMA Regularization: Decorrelating Weights of Neural Networks by Maximizing the Minimal Angles. In Advances in Neural Information Processing Systems , H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 19099-19110. https://proceedings.neurips.cc/paper_files/paper/2020/file/ dcd2f3f312b6705fb06f4f9f1b55b55c-Paper.pdf [26] Senrong Xu, Liangyue Li, Yuan Yao, Zulong Chen, Han Wu, Quan Lu, and Hanghang Tong. 2023. MUSENET: Multi-Scenario Learning for Repeat-Aware DSFNet: Learning Disentangled Scenario Factorization for Multi-Scenario Route Ranking Personalized Recommendation. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining (Singapore, Singapore) (WSDM '23) . Association for Computing Machinery, New York, NY, USA, 517-525. https: //doi.org/10.1145/3539597.3570414 [28] Ling Yan and Wu-Jun Li. 2014. Coupled Group Lasso for Web-Scale CTR Prediction in Display Advertising. In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32 (Beijing, China) (ICML'14) . JMLR.org, II-802-II-810. [27] Bencheng Yan, Pengjie Wang, Kai Zhang, Feng Li, Hongbo Deng, Jian Xu, and Bo Zheng. 2022. APG: Adaptive Parameter Generation Network for Click-Through Rate Prediction. In Advances in Neural Information Processing Systems , S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 24740-24752. https://proceedings.neurips.cc/paper_files/paper/ 2022/file/9cd0c57170f48520749d5ae62838241f-Paper-Conference.pdf [29] Xuanhua Yang, Xiaoyu Peng, Penghui Wei, Shaoguo Liu, Liang Wang, and Bo Zheng. 2022. AdaSparse: Learning Adaptively Sparse Structures for MultiDomain Click-Through Rate Prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM '22) . Association for Computing Machinery, New York, NY, USA, 4635-4639. https://doi.org/10.1145/3511808.3557541 [30] Qianqian Zhang, Xinru Liao, Quan Liu, Jian Xu, and Bo Zheng. 2022. Leaving No OneBehind: A Multi-Scenario Multi-Task Meta Learning Approach for Advertiser Modeling. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (Virtual Event, AZ, USA) (WSDM '22) . Association for Computing Machinery, New York, NY, USA, 1368-1376. https://doi.org/10.1145/ 3488560.3498479 [31] Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng. 2023. Deep Long-Tailed Learning: A Survey. arXiv:2110.04596 [cs.CV] [33] Yu Zheng. 2011. T-Drive trajectory data sample. https://www.microsoft.com/en- [32] Yuanliang Zhang, Xiaofeng Wang, Jinxin Hu, Ke Gao, Chenyi Lei, and Fei Fang. 2022. Scenario-Adaptive and Self-Supervised Model for Multi-Scenario Personalized Recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM '22) . Association for Computing Machinery, New York, NY, USA, 3674-3683. https://doi.org/10.1145/3511808.3557154 us/research/publication/t-drive-trajectory-data-sample/ T-Drive sample dataset. [34] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui [35] Jie Zhou, Xianshuai Cao, Wenhao Li, Lin Bo, Kun Zhang, Chuan Luo, and Qian Yu. 2023. Hinet: Novel multi-scenario & multi-task learning with hierarchical information extraction. In 2023 IEEE 39th International Conference on Data Engineering (ICDE) . IEEE, 2969-2975. Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for ClickThrough Rate Prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD'18) . Association for Computing Machinery, New York, NY, USA, 1059-1068. https://doi.org/10.1145/3219819.3219823 [36] Xinyu Zou, Zhi Hu, Yiming Zhao, Xuchu Ding, Zhongyi Liu, Chenliang Li, and Aixin Sun. 2022. Automatic Expert Selection for Multi-Scenario and Multi-Task Search. In SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 , Enrique Amig\u00f3, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 1535-1544. https://doi.org/10.1145/3477495.3531942"}
