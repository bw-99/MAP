{"title": "CALRec: Contrastive Alignment of Generative LLMs for Sequential Recommendation", "authors": "Yaoyiran Li; Xiang Zhai; Moustafa Alzantot; Keyi Yu; Ivan Vuli\u0107; Anna Korhonen; Mohamed Hammad", "pub_date": "2024-08-23", "abstract": "Traditional recommender systems such as matrix factorization methods have primarily focused on learning a shared dense embedding space to represent both items and user preferences. Subsequently, sequence models such as RNN, GRUs, and, recently, Transformers have emerged and excelled in the task of sequential recommendation. This task requires understanding the sequential structure present in users' historical interactions to predict the next item they may like. Building upon the success of Large Language Models (LLMs) in a variety of tasks, researchers have recently explored using LLMs that are pretrained on vast corpora of text for sequential recommendation. To use LLMs for sequential recommendation, both the history of user interactions and the model's prediction of the next item are expressed in text form. We propose CALRec, a two-stage LLM finetuning framework that finetunes a pretrained LLM in a two-tower fashion using a mixture of two contrastive losses and a language modeling loss: the LLM is first finetuned on a data mixture from multiple domains followed by another round of target domain finetuning. Our model significantly outperforms many state-of-the-art baselines (+37% in Recall@1 and +24% in NDCG@10) and our systematic ablation studies reveal that (i) both stages of finetuning are crucial, and, when combined, we achieve improved performance, and (ii) contrastive alignment is effective among the target domains explored in our experiments.", "sections": [{"heading": "Introduction and Motivation", "text": "Recommender systems aim to understand users and items and use the learned knowledge to recommend relevant items for future user interactions. Traditional collaborative filtering methods [15,25,42] map item and user IDs into a common latent embedding space learned from individual user-item interactions, but these embeddings are often static and fail to capture the dynamic nature of user interests. Recent exploration in sequential recommender employs RNN [16], CNN [54] and the transformer [22,45] to directly learn the time-evolution of user behaviors. The transformer architecture [10,49] stands out for its efficient capability of modeling sequences through the self-attention mechanism, which learns the correlation or causal relationships between different tokens and their influence on future tokens. In a large body of recommender systems (RecSys) research, each input position (token) corresponds to a single user activity, typically a user's interaction (e.g., purchase) with an item [18,22,45,57,59]. More specifically, some approaches rely on item IDs, each represented by a unique token, and model a user's interaction history as a sequence of item ID tokens [22,45]; other efforts combine more sophisticated features, such as the text embedding of item descriptions, to capture richer content information for each item [57,59].\nMotivated by the global paradigm shift towards autoregressive Language Large Models (LLMs) [1,2,46,47] with successful applications in various tasks and domains [3,11,39,55], we focus on leveraging generative LLMs for sequential recommender systems. Pretrained LLMs learn knowledge from large text corpora and are expected to perform reasonably well in general tasks, and their performance on specific tasks further improves after fine-tuning with domain-specific data. Several studies have explored utilizing LLMs in recommender systems [52], such as framing sequential recommendation as sentence-completion [27] or question-answering problems [56].\nDespite the impressive language understanding capabilities of pretrained LLMs, they require fine-tuning for highly specific tasks like sequential recommendation. An LLM should be ideally fully fine-tuned [23,51] to learn to comprehend domain-specific flattened text sequences and the underlying user interest evolution. Similar to most sequence modeling approaches like SASRec [22], BERT4REC [45], Recformer [26], and GPT4Rec [27], we adopt nextitem prediction as the sequential recommendation task.\nBased on sentence-completion [27] or question-answering [56] styled prompting, our template design is further inspired by the idea of few-shot learning [7]. Specifically, we prefix user sequence and each item in the user sequence with distinctive text indicators that 1) allow the model to better understand data pattern and text format and 2) increase the LLM response coherence of the input sentence. We further propose a Contrastive Aligned Generative LLM Recommendation (CALRec) framework to adapt an LLM for sequential recommendation, inspired by the effectiveness of contrastive learning (CL) in mapping-based RecSys [18,26] and CL research in NLP and other areas [13,28]. CALRec features the following:\n\u2022 Pure text input and text output with advanced prompt design.\n\u2022 A mixed training objective that combines customized nextitem generation objective and auxiliary contrastive objectives. \u2022 A two-stage LLM fine-tuning paradigm consisting of a multicategory joint fine-tuning stage followed by a categoryspecific fine-tuning stage. \u2022 A novel quasi-round-robin BM25 retrieval approach for item retrieval.\nSimilar to previous work [22,26,45], we leverage the Amazon Review dataset [35] and compare our results to open-source state-ofthe-art (SotA) baselines. We observed a non-negligible percentage of consecutive identical activities by the same users in the raw datasets which might trivialize the next-item prediction task. We thus train and evaluate the model on deduplicated sequences and propose to adopt 'Last Item Repeater', a training-free, text-statistic baseline for model comparison ( \u00a74 and \u00a76). We also revisit the existing evaluation metrics of the previous works, and propose to report both optimistic and pessimistic metric scores for text-based RecSys ( \u00a74). Through comprehensive experiments, we discover that our CALRec method demonstrates superior performance and outperforms existing SotA by a considerable margin in most evaluation metrics. A systematic ablation study indicates the effectiveness of the two-stage training paradigm, our contrastive alignment, and our template design among the various data categories examined in our experiments.\nFinally, we briefly summarize the contribution of our study:\n\u2022 We proposed CALRec, a novel sequential recommendation framework that features advanced prompt design, a two-stage training paradigm, a combined training objective, and a quasi-round-robin BM25 retrieval approach. \u2022 We conduct comprehensive experiments, ablation study and further analyses and show that 1) our approach outperforms existing baselines in most metrics by a considerable margin; 2) the main components of our approach are effective. We also discuss 3) the characteristics of our output and our limitations. \u2022 We revisit data preprocessing and evaluation metrics in this field.", "publication_ref": ["b14", "b24", "b42", "b15", "b55", "b21", "b45", "b9", "b49", "b17", "b21", "b45", "b58", "b60", "b21", "b45", "b58", "b60", "b0", "b1", "b46", "b47", "b2", "b10", "b39", "b56", "b53", "b26", "b57", "b22", "b52", "b21", "b45", "b25", "b26", "b26", "b57", "b6", "b17", "b25", "b12", "b27", "b21", "b25", "b45", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Traditional recommender systems like Collaborative Filtering [15] learn relevance through user-item co-occurrence. Early collaborative filtering approaches like Matrix Factorization (MF) [25] are ID-only where both user and item IDs are embedded in a shared embedding space. Later MF improvements incorporated other attributes [43] such as time and location. Nevertheless, these approaches did not capture the sequential nature of user behavior which gave rise to a class of Sequential Recommenders. The initial Sequential Recommenders, similar to MF, were ID-only models, exemplified by GRU4Rec [16], SASRec [22], and Bert4Rec [45]. Subsequent research expanded these models by integrating additional attributes beyond IDs, leading to the development of methods such as FDSA [57], S3-Rec [59], and UniSRec [18]. Due to the challenges associated with IDs in real world Recommender Systems [43], researchers explored removing them in favor of attribute-only models [18,26]. However, this shift led to quality issues, as the valuable 'memorization characteristics' provided by IDs were lost. Another thread of research explored deriving IDs from attribute embedding [17,38,44], which mitigates some of the challenges associated with random IDs, albeit at the cost of increased system complexity.\nWith the advent of pretrained LLMs, which include encoder-only models like BERT [10,26], encoder-decoder models like T5 [37], and decoder-only models like PaLM [2,9], GPT [7], and LLama [48], researchers explored leveraging these backbones in sequential recommendation tasks, harnessing the extensive 'knowledge' encoded within their parameters. Some approaches utilized zero-shot or fewshot methods [19,33,50], while others extract item text embeddings from LLMs, both with frozen LLM parameters.\nPrevious work also explored fine-tuning encoder-only, encoderdecoder or decoder-only LLMs to incorporate recommendation knowledge into the LLMs [4,20,23,26,32,37,52,56,60]. P5 [14] fine-tuned a pre-trained T5 [37] model to solve various common RecSys tasks, including sequential recommendation. In contrast to P5, which frames the sequential recommendation task as an ID generation problem, our approach focuses on generating item attribute text and features a two-tower training framework, enhancing model's understanding of items with their attributes within the context of user-item interaction sequences. GPT4Rec [27] finetunes a pre-trained GPT-2 model to generate the next-item titles. Similar to GPT4Rec, we use BM25 [40] to search for items in the item corpus. However, unlike GPT4Rec which was directly finetuned for next-item generation as a sentence completion task on each target domain respectively, we are among the first to explore using the contrastive loss as an auxiliary objective to fine-tune an autoregressive LLM on the sequential recommendation task, and our work also features a multi-stage fine-tuning framework, enabling our model to generalize to multiple target domains. We include the list of baseline models we compare against and their details in \u00a74.", "publication_ref": ["b14", "b24", "b43", "b15", "b21", "b45", "b58", "b60", "b17", "b43", "b17", "b25", "b16", "b38", "b44", "b9", "b25", "b37", "b1", "b8", "b6", "b48", "b18", "b32", "b51", "b3", "b19", "b22", "b25", "b31", "b37", "b53", "b57", "b61", "b13", "b37", "b26", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology 3.1 Preliminaries and Terminology", "text": "We assume a set of users U, where a user \ud835\udc62 \u2208 U is associated with an interaction sequence comprising \ud835\udc5b items (\ud835\udc3c 1 , \ud835\udc3c 2 , ..., \ud835\udc3c \ud835\udc5b ) arranged in chronological order (note that the value of \ud835\udc5b may vary across different users). The sequential recommendation task is then defined as follows: given the user's historical interaction sequence \ud835\udc3c 1:\ud835\udc5b-1 = (\ud835\udc3c 1 , \ud835\udc3c 2 , ..., \ud835\udc3c \ud835\udc5b-1 ), a sequential recommender aims to predict the target item \ud835\udc3c \ud835\udc5b , the last item in the user's full interaction sequence. In our study, we use pure text to represent each item and user interaction sequence ( \u00a73.2), and we adapt autoregressive LLMs pretrained on large corpora of text for sequential recommendation. Specifically, we pose the task as a sequence-to-sequence generation task where the input is the text description of \ud835\udc3c 1:\ud835\udc5b-1 and the expected output is that of \ud835\udc3c \ud835\udc5b . Our primary motivation for formulating the problem entirely within the text domain is to exploit the rich textual information inherent in real-world data and to capitalize on LLMs' strong language understanding and reasoning capabilities.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Format and Template Design", "text": "In our setup, items and user sequences are provided exclusively in text format. Instead of treating per-item features separately as in traditional recommender models [57,59]  Different from our method, many previous text-based approaches do not specify an item prefix and simply leverage a one-off user history suffix [27,56] (some approaches even do not specify any user suffix at all [26]).", "publication_ref": ["b58", "b60", "b26", "b57", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Two-Stage Fine-Tuning", "text": "Previous work [18,26] indicates that pretraining a RecSys on a large volume of data (various data categories) provides favorable inductive bias for downstream tasks (a specific and smaller-scale data category for evaluation after further fine-tuning on them). Analogously, based on an LLM already pretrained on large, diverse text corpora, we propose a two-stage fine-tuning framework that consists of Multi-Category Joint Fine-Tuning (Stage I) and Category-Specific Fine-Tuning (Stage II): the latter benefits from the former as a result of transfer learning. Our Stage I fine-tuning allows the model to adapt to the sequential recommendation problem setting and learn data patterns in a category-agnostic fashion. Inspired by Xue et al. [53], we adopt a sampling rate for each data category proportional to |U| 0.3 , where |U| is the size (number of users) of each category, for the purpose of combating the data imbalance issue, preventing a few extra-large categories from dominating the Stage I fine-tuning set. After deriving the multi-category fine-tuned model, we further adapt the model to each category respectively via Category-Specific Fine-Tuning (Stage II). In both stages, the training objective is the same, introduced in \u00a73.4.", "publication_ref": ["b17", "b25", "b54"], "figure_ref": [], "table_ref": []}, {"heading": "Training Objectives", "text": "Next-Item Generation Objective. The primary objective employed for LLM fine-tuning is the Next-Item (Text) Generation (NIG) Objective. NIG aims to generate the text description of the target item given the text description of the previous items in a user's history. Denote the text-tokenized user sequence (for a user) as t = (\ud835\udc61 1 , \ud835\udc61 2 , ..., \ud835\udc61 \ud835\udc59 ), where \ud835\udc59 is the tokenized sequence length. The first \ud835\udc5a tokens belong to all the items in the user sequence except for the last item (i.e., the target item), followed by \ud835\udc59 -\ud835\udc5a tokens for the target item. We propose this next-item generation objective to adapt an LLM for sequential recommendation:\nL \ud835\udc41 \ud835\udc3c\ud835\udc3a = E t\u223c\ud835\udc5d (t) \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \ud835\udc59 \u2211\ufe01 \ud835\udc57=\ud835\udc5a+1 log \ud835\udc43 (\ud835\udc61 \ud835\udc57 |\ud835\udc61 1:\ud835\udc57 -1 ; \ud835\udf03 ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb (1)\nwhere p(t) denotes the data distribution, and \ud835\udf03 is the whole set of trainable parameters of the chosen LLM.\nAuxiliary Contrastive Alignment. One shortcoming of the NIG is that it deals with the input and output on the token level rather than on the item/user level. Therefore, in addition to the token-level NIG objective, we also investigate auxiliary contrastive objectives that operate on the item/user level. Specifically, we adopt a twotower training framework where one tower only takes in the target item, and the mean-pooled feature in the final hidden layer is denoted as v \ud835\udc47 . The second tower takes the whole user sequence and similarly we denote the mean-pooled feature corresponding to the user history sequence as v \ud835\udc48 and the feature for the target item conditioned on the user history as v \ud835\udc47 |\ud835\udc48 (see also Fig. 1). We experiment with two contrastive losses for user-and item-level alignments following the InfoNCE loss [36], a standard and robust choice in contrastive learning research [8,13,28,31]. Below, we present the calculation of these losses on a batch of data:\nL \ud835\udc47\ud835\udc47 = - 1 \ud835\udc41 \ud835\udc4f \ud835\udc41 \ud835\udc4f \u2211\ufe01 \ud835\udc56=1 log exp(cos(v \ud835\udc47 |\ud835\udc48 \ud835\udc56 , v \ud835\udc47 \ud835\udc56 )/\ud835\udf0f \ud835\udc50 ) \ud835\udc41 \ud835\udc4f \ud835\udc57=1 exp(cos(v \ud835\udc47 |\ud835\udc48 \ud835\udc57 , v \ud835\udc47 \ud835\udc56 )/\ud835\udf0f \ud835\udc50 ) ,(2)\nL \ud835\udc48\ud835\udc47 = - 1 \ud835\udc41 \ud835\udc4f \ud835\udc41 \ud835\udc4f \u2211\ufe01 \ud835\udc56=1 log exp(cos(v \ud835\udc48 \ud835\udc56 , v \ud835\udc47 \ud835\udc56 )/\ud835\udf0f \ud835\udc50 ) \ud835\udc41 \ud835\udc4f \ud835\udc57=1 exp(cos(v \ud835\udc48 \ud835\udc57 , v \ud835\udc47 \ud835\udc56 )/\ud835\udf0f \ud835\udc50 ) ,(3)\nwhere in-batch negative samples are adopted, \ud835\udc41 \ud835\udc4f is the batch size (number of user sequences in a training batch), cos(\u2022, \u2022) calculates the cosine similarity between two input vectors, and \ud835\udf0f \ud835\udc50 is the temperature for contrastive alignments.\nOur final training objective is a mixture of L \ud835\udc41 \ud835\udc3c\ud835\udc3a and contrastive losses as shown in Fig. 1:\nL \ud835\udc36\ud835\udc34\ud835\udc3f\ud835\udc45\ud835\udc52\ud835\udc50 = (1 -\ud835\udefc -\ud835\udefd)L \ud835\udc41 \ud835\udc3c\ud835\udc3a + \ud835\udefc L \ud835\udc47\ud835\udc47 + \ud835\udefdL \ud835\udc48\ud835\udc47 .(4)", "publication_ref": ["b36", "b7", "b12", "b27", "b30"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Quasi-Round-Robin BM25 Retrieval", "text": "At inference, we prompt the model with the text description of user's history of interactions (see also \u00a73.2). To obtain more than one candidate next-item prediction, we conduct temperature sampling [12] to generate \ud835\udc41 gen pieces of text as \ud835\udc41 gen predictions for the next item. Each of the \ud835\udc41 gen predictions is associated with its sequence score \ud835\udc60 (i.e., log probability, which is a negative real number typically in the range of (-10, 0), as observed on our data). We remove duplicate generations, sort them by sequence scores in descend order, and retain at most top \ud835\udc41 preds unique predictions. Then, in order to match these generated texts with items from our corpus, we rely on BM25 retrieval [40] to calculate the matching scores between each text prediction and the whole item corpus (with size \ud835\udc41 \ud835\udc50 ): this will result in BM25 matching scores organized into a matrix of shape (\ud835\udc41 \ud835\udc50 , \ud835\udc41 preds ). In order to rank the candidate items, we devise the following heuristics which aims to select the closest matches according to both the LLM sequence scores and BM25 scores in a quasi-round-robin fashion across the different \ud835\udc41 preds text generations. First, we linearly scale each column of the matrix respectively to the range [0, 1] to make BM25 scores derived from different LLM predictions comparable. We then penalize each prediction's BM25 scores (each column) by multiplying the BM25 scores by a factor of \ud835\udc52 \ud835\udf16\ud835\udc60 , where \ud835\udc60 is the sequence score associated with the text prediction, and \ud835\udf16 is a small positive constant so that \ud835\udc52 \ud835\udf16\ud835\udc60 is smaller than but very close to 1.0. This is to further modulate the BM25 text-based retrieval scores by the LLM generative scores. After that, we max-pool along the \ud835\udc41 preds axis and finally derive modulated BM25 matching scores in a single vector of size \ud835\udc41 \ud835\udc50 , with information from all \ud835\udc41 preds recommendations fused in it (we can then derive the ranking of the ground-truth target item in the item corpus for calculating evaluation metric scores, or return top-K recommendations for users), as presented in Algorithm 1 3 . Here, we provide another view into the round-robin nature of our approach: given that the scaled BM25 score of each best-match item from the non-duplicate \ud835\udc41 preds texts generated is 1, when they are further modulated (scaled down) using \ud835\udc52 \ud835\udf16\ud835\udc60 , the final retrieval algorithm roughly follows a round-robin fashion, where the top-1 BM25 retrieval result of each of the generated text are in turn grouped as the final top \ud835\udc41 preds retrieval results, when \ud835\udf16 is close to 0.0. Since there are ties, we name our method 'quasi-' rather than 'strict-' round-robin.", "publication_ref": ["b11", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "LLM Selection. Our work leverages PaLM-2 [2,46], a state-of-theart LLM that is pretrained with a mixture of different objectives and exhibits exceptional generalization capabilities on a variety of language tasks. Among PaLM-2 model variants, we choose PaLM-2 XXS as the LLM backbone for our tasks, balancing model capability with inference latency. 4 Larger PaLM-2 S does not show a significant advantage against XXS model when both are without fine-tuning (see \u00a75). Besides, the scale of our dataset (significantly smaller than text corpora for pretraining LLMs) may lead to overfitting when fine-tuning larger models [24,34].\nDataset. Our experiments adopt the established and publicly available Amazon Review dataset 2018 [35]. Specifically, we use the 5-core subsets where each user/item is found in at least 5 interactions. We choose four item attributes: title, category, brand, and price, to construct the flattened text description of each item, and we truncate the length of their corresponding descriptions to 25, 15, 15, and 15 words, respectively. Following Li et al. [26], the same 14 product categories are involved in our research and they are all used for Stage I multi-category joint fine-tuning (3.59M users). Among them, 7 categories (3.37M users) are only for Stage I training where their entire user purchase records are usedfoot_4 ; one category 'CD and Vinyls' is used for model selection purpose (29K users) during Stage I training where the last item for each user is left for validation and only the first \ud835\udc5b -1 items are adopted for training. Finally, the remaining 6 categories are adopted for category-specific fine-tuning (Stage II) and evaluation (0.22M Users) 6 , where the first \ud835\udc5b -2 items are used for training in both Stages I and II, the penultimate item is used for validation in Stage II, and the very last item for evaluation. We also conduct data deduplication (dedup) on user sequences, which we describe later in next paragraph. For large categories including 'Office' and 'Pet', we randomly sampled 50% and 20% users respectively. The detailed statistics of our data in Stage II (after deduplication) are presented in Table 1, where 'Density' is defined as Total Purchase/(#of Users \u00d7 #of Items), and the 'Word Vocab Size' denotes the lowercase natural word types over the item corpus spanning the four attributes used.\nUser Sequence Deduplication. We have found that the Amazon Review data include a non-negligible percentage of consecutive duplicate events in user sequences, where the duplication is identified as two consecutive activities by the same user that have the same item ID (ASIN number), review text, rating, and the same review timestamp. As indicated in Table 1, around 4% to 9.4% of purchases are exact duplicates of the previous ones in their user sequences. We conduct deduplication (dedup) on all user sequences following our strict definition of duplication above, while we still keep two consecutive purchases of the same item, if they occurred at different times (i.e., they come with different timestamps) or come with different ratings/review text since they are likely the users' real actions. We note that some previous work, e.g. [26], did not conduct user sequence deduplication, which may lead a recommender system to learn a trivial strategy of always recommending the last item in the input sequence.\nImplementation Details. We train our CALRec based on XXSsized PaLM-2 on a cluster of TPUv4 chips [21]; our code is implemented with JAX [6]foot_6 and PAX 8 . We adopt a training batch size of 512 users, a learning rate of 1e-4, a maximum input length of 1, 024 tokens, and a maximum output decoding length of 80 tokens. As a data augmentation trick, during training, we randomly truncate the last \ud835\udc58 items (\ud835\udc58 \u2208 {0, 1, 2, 3, 4}) from the full user record (with \ud835\udc5b items) and always use the last item after truncation (i.e., the (\ud835\udc5b -\ud835\udc58)th item in the original user record) as the target item. We train CALRec with \ud835\udefc = 0.125, \ud835\udefd = -0.025 for up to 150, 000 steps in Stage I and adopt \ud835\udf0f \ud835\udc50 = 0.5 (see Eqs. 2 & 3) as the configuration yields best validation scores. We adopt the checkpoint with 135, 000 training steps in Stage I selected on the validation data category, and then further fine-tune the model for Stage II. There are two exceptions: Scientific 9 , and Gamesfoot_9 . All our model selection is based on the NDCG@10 metric following previous work (specifically we adopt its pessimistic estimation which will be defined later in this section). For each case, the same training objective is used in Stages I and II. At inference, the temperature for decoding is 0.5, \ud835\udc41 \ud835\udc54\ud835\udc52\ud835\udc5b = 32, \ud835\udc41 \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51 = 10 and \ud835\udf16 = 1/5000. There are two hyper-parameters inherent to BM25: we adopt the default values from the original BM25 API, \ud835\udc58 1 = 1.5 and \ud835\udc4f = 0.75, throughout all our experiments and did not further tune them since we found that the model performance is not sensitive to them.\nBaselines. In this work, we consider a comprehensive set of baseline approaches. SASRec [22] and BERT4Rec [45] are pure ID-Based approaches, Recformer [26] and the 'Last Item Repeater' (LIR) are purely text-based, and FDSA [57] and S 3 -Rec [59] leverage both ID and text. UniSRec [18] has two variants and we report its performance in both text-only mode and ID+text mode. More details about each baseline are provided in what follows. \u2022 SASRec [22] is a causal sequential recommendation approach implemented with an encoder-only unidirectional transformer. It takes in a sequence of item IDs and predicts the ID embedding of the next item in the last position. \u2022 BERT4Rec [45] adopts a bidirectional transformer architecture for sequential recommendation. Its input consists of a sequence of item IDs and it is trained with the masked language modeling objective. At inference, a 'mask' token is appended to the end of the sequence to predict the next item.\n\u2022 FDSA [57] applies two self-attentional blocks to address ID sequence and attribute feature sequence, respectively, and fuses their final representations for next-item prediction. \u2022 S 3 -Rec [59] maximizes the mutual information between ID and attribute embeddings via contrastive objectives and it is implemented with an encoder-only transformer network. It consists of a pretraining stage and a fine-tuning stage, both conducted on domain-specific data.\n\u2022 UniSRec [18] separately encodes each item's text via a pretrained and fixed BERT cascaded by a trainable adapter. Given a sequence of item embeddings, a sequence representation is then modeled with another small transformer network. We adopt the officially released UniSRec model pretrained on multiple domains and finetune it on specific Amazon categories, respectively. We report two fine-tuning setups of UniSRec: (1) text-only setup, (2) ID+text setup where an item embedding is the sum of its ID and text representations.\n\u2022 Recformer [26] relies on the encoder-only Longformer architecture [5] where token type and item position embeddings are proposed to facilitate learning item and user representations. It also features a pretraining stage on multiple categories with a combination of standard Masked Language Modeling (MLM) loss and a contrastive objective aligning user history and target item representations. We adopt Recformer's officially released pretrained checkpoint and fine-tune it on specific categories, respectively. \u2022 Last Item Repeater, or LIR hereafter, denotes a simple yet effective baseline strategy which always recommends the last item in the input sequence as our training-free, text-statistic baseline. Based on the text of the last item, we rely on BM25 to rank the items in the full item corpus.\nEvaluation Metrics. Following standard practices from prior work, we report NDCG@10, Recall@K (K\u2208 {1, 10}) and MRR@\u221e scores. Different from previous work, we report both 'optimistic' (opt) and 'pessimistic' (pes) calculations of the metrics: when there is a tie which is based on RecBole [58]: the attribute embeddings of FDSA and S 3 -Rec are text features from our strongest baseline UniSRec.\nsuch that the ground-truth target item and other candidate items have the same matching score, the ground-truth item ranks first in (opt) and ranks last in (pes) calculation, respectively. It is because 1) there are distinct items (with different item IDs) with the same text description and 2) in rare cases, we found that BM25 may yield identical scores for very similar text pieces. We still treat items in case 1) as distinct items because in real production environments vendors/content creators may intentionally or unintentionally duplicate items. Of course, this will not cause any issue for ID-based approaches due to distinct item IDs, where (opt) and (pes) scores are the same. In the official implementations of our text-based baselines, Li et al. [26] only calculates (opt) metric scores which may result in unfair comparisons with ID-based baselines. Hou et al. [18] applies \"torch.topk()\" to derive exactly top \ud835\udc3e item ids for calculating Recall/NDCG@K: if the \ud835\udc3e-th and the \ud835\udc3e + 1-th items are having the same score, only one of them will be included in the output list. So its metric score is a number between our (opt) and (pes) estimates. 12 All our evaluations are performed on usersequence-deduplicated data.", "publication_ref": ["b1", "b46", "b23", "b33", "b34", "b25", "b25", "b20", "b21", "b45", "b25", "b58", "b60", "b17", "b21", "b45", "b58", "b60", "b17", "b25", "b4", "b59", "b25", "b17"], "figure_ref": [], "table_ref": ["tab_3", "tab_3"]}, {"heading": "Experimental Results", "text": "Main Results. Table 2 demonstrates that the sequential recommendation capabilities of the proposed CALRec framework are stronger than our baselines. On the 6 Amazon Review test categories, our model outperforms all 8 baselines significantly in terms of NDCG@10, Recall@1 and MRR. In terms of Recall@10, our approach outperforms all Text-Only and ID-Only models across the board by a large margin (including the Text-Only variant of UniSRec), but slightly underperforms our strongest baseline, the ID+Text variant of UniSRec. For our method and 3 Text-Only baselines, the results in Table 2 verified that there is a gap between optimistic and pessimistic metric scores, demonstrating the necessity of reporting both. Meanwhile, it is worth noticing that even our pessimistic scores clearly surpass the optimistic scores of the 3 Text-Only baseline models on all the 6 data categories. In addition to the absolute scores, we also calculate the percentage gains comparing CALRec's optimistic scores against the baseline SotA for each data category and each evaluation metric (different method for different data category and metric). The results are presented in the last column of Table 2. We found that, CALRec is especially strong  . For text-only methods, we report both optimisitc (opt) and pessimistic (pes) as '(opt)/(pes)'. For ID-based methods, (opt) and (pes) scores are the same. Bold: the highest (opt) scores. Underline: the strongest (pes) scores. We also report the relative improvement (opt) comparing CALRec against (opt) baseline SotA marked with Blue Background per data category per metric. Evaluation metrics are computed using the Amazon Review dataset after user sequence deduplication (see also \u00a74).\nScientific Instruments Games Model Variant NDCG@10 Recall@1 Recall@10 MRR NDCG@10 Recall@1 Recall@10 MRR NDCG@10 Recall@1 Recall@ CALRec derived with both stages, it is already on par with variant A models (cf. Table 3). We also compare CALRec and pretrained 'off-the-shelf' PaLM-2 XXS in Table 3; the results show that without any fine-tuning, PaLM-2 demonstrates poor performance on our sequential recommendation benchmarks.\nContrastive Objectives: In Table 4 we calculate the average scores on all 6 categories for the setups with and without the auxiliary contrastive losses. Basically, the auxiliary contrastive objective can result in circa 0.8% -1.7% gains. We also conduct dependent paired sample t-test [41] and report the \ud835\udc5d-values in the last column of Table 4: the gains are all statistically significant at 0.05 level.\nModel Size: We are also interested in how pretrained PaLM-2 without any fine-tuning on our sequential recommendation data performs in our tasks and we present results with three PaLM-2 models of different model sizes (i.e., XXXS, XXS, and S) on 'Scientific' and 'Instruments' categories in Table 5. The input/output format is still the same: input data is from a single user. The results show that XXS and S models actually achieve similar performance and they both outperform the XXXS model.\nTemplate Design: We conducted additional experiments on prompting the raw PaLM2 XXS model for recommendation adopting another prompt from GPT4Rec [27], with results reported in Table 6. Unlike our prompt template, their template doesn't include a shared per-item prefix and user suffix but leverages a one-off user suffix. Compared with our results reported in Table 5 with the same raw PaLM-2 XXS, the new results are universally lower than ours (especially recall@1, relatively 30% \u223c 45% drop in scores), showing the effectiveness of our template design.", "publication_ref": ["b41", "b26"], "figure_ref": [], "table_ref": ["tab_5", "tab_5", "tab_5", "tab_6", "tab_6", "tab_7", "tab_7", "tab_8", "tab_8"]}, {"heading": "Further Analyses and Discussion", "text": "Further Discussion on User Sequence Dedup. Now, we further delve deeper into the issue of duplicate interactions in user sequences. Although in real world users can make repeated activities, we still posit that the high ratio of consecutive identical interactions in the raw Amazon Review data are not likely to reflect normal/real user behavior (further details in Appendix/Auxiliary Materials).\nWhile we cannot conclude that data deduplication is 'correct' or 'wrong', we reckon that the 'LIR' baseline results are meaningful and should be reported for model comparison purposes. We further run the LIR Baseline on 1) data from Li et al. [26] without dedup and compare with its results on 2) our data with dedup. Although the data statistics are slightly different, the comparisons are still meaningful, since their data sizes are comparable and they are both representative of the full distribution. Table 7 shows the optimistic metrics of LIR Baseline that always recommends last item in the input sequence on data w/o dedup (left four columns) and data w/ dedup (right four columns). All metrics suffer sharp declines due to the dedup process. Specifically, Recall@1 drops 60%-95% after removing consecutive identical events. This highlights the importance of user sequence interaction deduplication as a critical step for reliably measuring model quality.\nDoes CALRec Rely on BM25? One challenge of using generative LLMs, especially for general-purpose LLMs not tuned with taskspecific data, for recommendation is that there is no guarantee that the predicted item exists in the item corpus. Therefore we use BM25, a fuzzy lexical matching algorithm, to rank the items in the item corpus. However, the fine-tuned CALRec models can quickly master the item template format and remember items seen during training, making direct text-entity based retrieval possible without relying on fuzzy text matching methods. This motivates a simple and faster alternative to BM25. First, we found that nearly all (> 99.9%) fine-tuned model outputs adhere to the desired item text format: \"Title: **. Category: **. Brand: **. Price: **. \". We then extract the four attribute entities: title, category, brand, and price, and find that 82% -98% of model outputs correspond to an item in the item corpus that exactly matches all four attributes, and 95% -99.8% of title entities extracted from the model outputs exist in the item corpus. These statistics are summarized in Table 8. Based on these findings, we propose a heuristic hierarchical matching method. We first search exact matches in the item corpus for all four attributes extracted from model outputs; If   are also reported against BM25 retrieval.\nno match is found, we search for the exact match of the first three attributes, then the first two, and finally only the title description. Similar to our main experiments, both optimistic and pessimistic metrics are calculated, with results also presented in Table 8: we find that our hierarchical matching method has marginally worse performance compared to our BM25-based retrieval, yet it still outperforms other baselines models presented in Table 2.\nLimitations and Future Work. Due to the fact that fine-tuned CALRec usually outputs exact item descriptions seen during training, a direct limitation ensues: CALRec barely cannot address the item cold-start scenario where the ground-truth item is neither covered in the training set nor in the validation set. We leave addressing this limitation to future work. While previous approaches that also rely on matching LLM-generated item descriptions (e.g., titles) did not provide experimental results on item cold-start cases [20,27], we emphasize that our work is the first to identify this issue, which may also be present in these and similar approaches (yet to be verified). We regard this finding a minor contribution of our study. Second, although CALRec outperforms other baselines in Recall@1, NDCG@10, and MRR@\u221e by large margin, its Recall@10 underperforms one of our ID+text baselines. We attribute this to the quasi-greedy nature of temperature sampling (especially when adopting a relatively small temperature value). Unlike embeddingbased nearest neighbor search, our approach may not consistently produce the most optimal top-10 item predictions from the entire item corpus. Consequently, although CALRec excels in metrics that emphasize the relevance of the very top item prediction (like Recall@1 and MRR), there is a need for greater diversity in its top-10 item predictions.\nThird, in real-world scenarios, when the size of item corpus is extremely large (e.g., billions of items), BM25 retrieval becomes prohibitively slow and often infeasible. In addition to the hierachical entity matching method discussed earlier in this section, we may leverage CALRec as a reranker within a retrieve-and-rerank framework where any baseline sequential recommendation approach can be used to first derive, for example, top 10K recommendations focused on high recall. Then CALRec's output texts are used to rerank these items. This retrieve-and-rerank paradigm can not only leverage the strengths of CALRec but also circumvents the need for BM25 retrieval on a massive corpus.", "publication_ref": ["b25", "b19", "b26"], "figure_ref": [], "table_ref": ["tab_10", "tab_11", "tab_11", "tab_5"]}, {"heading": "Conclusion", "text": "We proposed CALRec, a contrastive-learning-assisted two-stage training framework for sequential recommendation based on LLMs, with experiments conducted using the PaLM-2 LLM as the backbone. Our approach also features careful template design inspired from few-shot learning and a novel quasi-round-robin BM25 retrieval approach. Comprehensive experiments on the Amazon Review dataset demonstrate the effectiveness of CALRec, supported by PaLM-2, where it significantly outperforms existing SotA models for sequential recommendation by a considerable margin in most evaluation metrics. We also presented a series of further analyses and revisited the issue of duplicate interactions in user sequences and existing evaluation metrics; we accordingly propose to adopt a training-free text-statistic based 'Last Item Repeater' reference baseline for model comparison purposes, and we strongly suggest reporting both optimistic and pessimistic metric scores for all textbased approaches.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Examples of Data Duplication", "text": "As discussed in \u00a74, the raw Amazon Review data contain repeated, non-distinguishable user-item interactions that share identical entity values across all attributes, including item id, user id, timestamp, item metadata, user rating, user review text, etc. An example in the 'Scientific' category is as follows:\n{ \" uid \": \" A3SFSFJZFI0OQN \", \" asin \": [\" B00002NC3K \", \" B0001MSC84 \", \" B0001MSC84 \", \" B0001MSC84 \", \" B0001MSC84 \"] , \" overall \": [3 , 3, 3, 3, 3] , \" reviewText \": [ \" Warped when exposed to the sun .\" , \" Warped when exposed to the sun .\" , \" Warped when exposed to the sun .\" , \" Warped when exposed to the sun .\" , \" Warped when exposed to the sun . In the example provided, all 5 user-item interactions, except the initial one, are identical. This behavior generally deviates from normal user activities and may potentially confuse or mislead models into recommending repetitive items. Although duplicate events make up only a small percentage of the dataset (Table 1), our analysis via the LIR baseline on raw and deduplicated data (Table 7) shows that those duplicates can significantly affect model behavior by offering an overly simplistic pattern to learn from.   \ud835\udc57 , v \ud835\udc47 \ud835\udc56 ) = ln(1.04)\ud835\udf0f \ud835\udc50 \u2248 0.02 \u226a 1, suggesting that without fine-tuning, the raw pretrained LLMs, although able to make some prediction (Table 5), does not yield embeddings for predicted items (in the user tower) that are more aligned with embeddings of the ground truth items (in the item tower) than the embeddings from random users do. After finetuning, L \ud835\udc47\ud835\udc47 \u2248 4.4 and the same ratio increases to 512\u00d7exp(-4.4) = 6.3, corresponding to cos(v", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Additional Discussion on Contrastive Alignment", "text": "92 which is close to 1, indicating that the embeddingsare aligned very well geometrically. On the contrary, without contrastive alignment, the prediction-target contrastive loss L \ud835\udc47\ud835\udc47 only drops roughly from 6.4 to 6.0 as the model learns to generate the correct next-item prediction as a pure text generation task. Fig. 2 demonstrates that the contrastive loss can efficiently align the embeddings from our two towers.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "2023", "authors": "Josh Achiam; Steven Adler; Sandhini Agarwal; Lama Ahmad; Ilge Akkaya; Florencia Leoni Aleman; Diogo Almeida; Janko Altenschmidt; Sam Altman; Shyamal Anadkat"}, {"ref_id": "b1", "title": "Palm 2 technical report", "journal": "", "year": "2023", "authors": "Rohan Anil; Andrew M Dai; Orhan Firat; Melvin Johnson; Dmitry Lepikhin; Alexandre Passos; Siamak Shakeri; Emanuel Taropa; Paige Bailey; Zhifeng Chen"}, {"ref_id": "b2", "title": "ViViT: A Video Vision Transformer", "journal": "", "year": "2021", "authors": "Anurag Arnab; Mostafa Dehghani; Georg Heigold; Chen Sun; Mario Lu\u010di\u0107; Cordelia Schmid"}, {"ref_id": "b3", "title": "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation", "journal": "Association for Computing Machinery", "year": "2023", "authors": "Keqin Bao; Jizhi Zhang; Yang Zhang; Wenjie Wang; Fuli Feng; Xiangnan He"}, {"ref_id": "b4", "title": "Longformer: The longdocument transformer", "journal": "", "year": "2020", "authors": "Iz Beltagy; Matthew E Peters; Arman Cohan"}, {"ref_id": "b5", "title": "JAX: composable transformations of Python+NumPy programs", "journal": "", "year": "2018", "authors": "James Bradbury; Roy Frostig; Peter Hawkins; Matthew James Johnson; Chris Leary; Dougal Maclaurin; George Necula; Adam Paszke; Jake Vanderplas; Skye Wanderman-Milne; Qiao Zhang"}, {"ref_id": "b6", "title": "Language models are few-shot learners", "journal": "Advances in neural information processing systems", "year": "2020", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell"}, {"ref_id": "b7", "title": "A Simple Framework for Contrastive Learning of Visual Representations", "journal": "PMLR", "year": "2020", "authors": "Ting Chen; Simon Kornblith; Mohammad Norouzi; Geoffrey Hinton"}, {"ref_id": "b8", "title": "Palm: Scaling language modeling with pathways", "journal": "Journal of Machine Learning Research", "year": "2023", "authors": "Aakanksha Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; Paul Barham; Hyung Won Chung; Charles Sutton; Sebastian Gehrmann"}, {"ref_id": "b9", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b10", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "journal": "", "year": "2021-05-03", "authors": "Alexey Dosovitskiy; Lucas Beyer; Alexander Kolesnikov; Dirk Weissenborn; Xiaohua Zhai; Thomas Unterthiner; Mostafa Dehghani; Matthias Minderer; Georg Heigold; Sylvain Gelly; Jakob Uszkoreit; Neil Houlsby"}, {"ref_id": "b11", "title": "Hierarchical Neural Story Generation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Angela Fan; Mike Lewis; Yann Dauphin"}, {"ref_id": "b12", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "journal": "", "year": "2021", "authors": "Tianyu Gao; Xingcheng Yao; Danqi Chen"}, {"ref_id": "b13", "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)", "journal": "Association for Computing Machinery", "year": "2022", "authors": "Shijie Geng; Shuchang Liu; Zuohui Fu; Yingqiang Ge; Yongfeng Zhang"}, {"ref_id": "b14", "title": "Using collaborative filtering to weave an information tapestry", "journal": "Commun. ACM", "year": "1992-12", "authors": "David Goldberg; David Nichols; Brian M Oki; Douglas Terry"}, {"ref_id": "b15", "title": "Session-based Recommendations with Recurrent Neural Networks", "journal": "", "year": "2016-05-02", "authors": "Bal\u00e1zs Hidasi; Alexandros Karatzoglou; Linas Baltrunas; Domonkos Tikk"}, {"ref_id": "b16", "title": "Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders", "journal": "Association for Computing Machinery", "year": "2023", "authors": "Yupeng Hou; Zhankui He; Julian Mcauley; Wayne Xin Zhao"}, {"ref_id": "b17", "title": "Towards Universal Sequence Representation Learning for Recommender Systems", "journal": "Association for Computing Machinery", "year": "2022", "authors": "Yupeng Hou; Shanlei Mu; Wayne Xin Zhao; Yaliang Li; Bolin Ding; Ji-Rong Wen"}, {"ref_id": "b18", "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems", "journal": "Springer-Verlag", "year": "2024-03-24", "authors": "Yupeng Hou; Junjie Zhang; Zihan Lin; Hongyu Lu; Ruobing Xie; Julian Mcauley; Wayne Xin Zhao"}, {"ref_id": "b19", "title": "GenRec: Large Language Model for Generative Recommendation", "journal": "Springer-Verlag", "year": "2024-03-24", "authors": "Jianchao Ji; Zelong Li; Shuyuan Xu; Wenyue Hua; Yingqiang Ge; Juntao Tan; Yongfeng Zhang"}, {"ref_id": "b20", "title": "Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings", "journal": "", "year": "2023", "authors": "Norm Jouppi; George Kurian; Sheng Li; Peter Ma; Rahul Nagarajan; Lifeng Nai; Nishant Patil; Suvinay Subramanian; Andy Swing; Brian Towles"}, {"ref_id": "b21", "title": "Self-Attentive Sequential Recommendation", "journal": "", "year": "2018", "authors": "Wang-Cheng Kang; Julian Mcauley"}, {"ref_id": "b22", "title": "Do llms understand user preferences? evaluating llms on user rating prediction", "journal": "", "year": "2023", "authors": "Wang-Cheng Kang; Jianmo Ni; Nikhil Mehta; Maheswaran Sathiamoorthy; Lichan Hong; Ed Chi; Derek Zhiyuan Cheng"}, {"ref_id": "b23", "title": "Overcoming catastrophic forgetting in neural networks", "journal": "Proceedings of the national academy of sciences", "year": "2017", "authors": "James Kirkpatrick; Razvan Pascanu; Neil Rabinowitz; Joel Veness; Guillaume Desjardins; Andrei A Rusu; Kieran Milan; John Quan; Tiago Ramalho; Agnieszka Grabska-Barwinska"}, {"ref_id": "b24", "title": "Matrix Factorization Techniques for Recommender Systems", "journal": "Computer", "year": "2009", "authors": "Yehuda Koren; Robert Bell; Chris Volinsky"}, {"ref_id": "b25", "title": "Text Is All You Need: Learning Language Representations for Sequential Recommendation", "journal": "Association for Computing Machinery", "year": "2023", "authors": "Jiacheng Li; Ming Wang; Jin Li; Jinmiao Fu; Xin Shen; Jingbo Shang; Julian Mcauley"}, {"ref_id": "b26", "title": "GPT4Rec: A generative framework for personalized recommendation and user interests interpretation", "journal": "", "year": "2023", "authors": "Jinming Li; Wentao Zhang; Tian Wang; Guanglei Xiong; Alan Lu; Gerard Medioni"}, {"ref_id": "b27", "title": "Translation-Enhanced Multilingual Text-to-Image Generation", "journal": "", "year": "2023", "authors": "Yaoyiran Li; Ching-Yun Chang; Stephen Rawls; Ivan Vuli\u0107; Anna Korhonen"}, {"ref_id": "b28", "title": "On Bilingual Lexicon Induction with Large Language Models", "journal": "", "year": "2023", "authors": "Yaoyiran Li; Anna Korhonen; Ivan Vuli\u0107"}, {"ref_id": "b29", "title": "Self-Augmented In-Context Learning for Unsupervised Word Translation", "journal": "", "year": "2024", "authors": "Yaoyiran Li; Anna Korhonen; Ivan Vuli\u0107"}, {"ref_id": "b30", "title": "Improving Word Translation via Two-Stage Contrastive Learning", "journal": "", "year": "2022", "authors": "Yaoyiran Li; Fangyu Liu; Nigel Collier; Anna Korhonen; Ivan Vuli\u0107"}, {"ref_id": "b31", "title": "How can recommender systems benefit from large language models: A survey", "journal": "", "year": "2023", "authors": "Jianghao Lin; Xinyi Dai; Yunjia Xi; Weiwen Liu; Bo Chen; Xiangyang Li; Chenxu Zhu; Huifeng Guo; Yong Yu; Ruiming Tang"}, {"ref_id": "b32", "title": "Is ChatGPT a Good Recommender? A Preliminary Study", "journal": "", "year": "2023", "authors": "Junling Liu; Chao Liu; Peilin Zhou; Renjie Lv; Kang Zhou; Yan Zhang"}, {"ref_id": "b33", "title": "Fast Model Editing at Scale", "journal": "", "year": "2022", "authors": "Eric Mitchell; Charles Lin; Antoine Bosselut; Chelsea Finn; Christopher D Manning"}, {"ref_id": "b34", "title": "Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects", "journal": "", "year": "2019", "authors": "Jianmo Ni; Jiacheng Li; Julian Mcauley"}, {"ref_id": "b35", "title": "Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"ref_id": "b36", "title": "Representation learning with contrastive predictive coding", "journal": "", "year": "2018", "authors": "Aaron Van Den Oord; Yazhe Li; Oriol Vinyals"}, {"ref_id": "b37", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2020-01", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b38", "title": "Recommender Systems with Generative Retrieval", "journal": "", "year": "2023", "authors": "Shashank Rajput; Nikhil Mehta; Anima Singh; Raghunandan Hulikal Keshavan; Trung Vu; Lukasz Heldt; Lichan Hong; Yi Tay; Q Vinh; Jonah Tran; Maciej Samost; Ed H Kula; Maheswaran Chi;  Sathiamoorthy"}, {"ref_id": "b39", "title": "Hierarchical text-conditional image generation with clip latents", "journal": "", "year": "2022", "authors": "Aditya Ramesh; Prafulla Dhariwal; Alex Nichol; Casey Chu; Mark Chen"}, {"ref_id": "b40", "title": "The probabilistic relevance framework: BM25 and beyond", "journal": "Foundations and Trends\u00ae in Information Retrieval", "year": "2009", "authors": "Stephen Robertson; Hugo Zaragoza"}, {"ref_id": "b41", "title": "Paired Samples T-Test", "journal": "SensePublishers", "year": "2017", "authors": "Amanda Ross; Victor L Willson"}, {"ref_id": "b42", "title": "Itembased collaborative filtering recommendation algorithms", "journal": "Association for Computing Machinery", "year": "2001", "authors": "Badrul Sarwar; George Karypis; Joseph Konstan; John Riedl"}, {"ref_id": "b43", "title": "Collaborative Filtering beyond the User-Item Matrix: A Survey of the State of the Art and Future Challenges", "journal": "ACM Comput. Surv", "year": "2014", "authors": "Yue Shi; Martha Larson; Alan Hanjalic"}, {"ref_id": "b44", "title": "Better Generalization with Semantic IDs: A case study in Ranking for Recommendations", "journal": "", "year": "2023", "authors": "Anima Singh; Trung Vu; Raghunandan Keshavan; Nikhil Mehta; Xinyang Yi; Lichan Hong; Lukasz Heldt; Li Wei; Ed Chi; Maheswaran Sathiamoorthy"}, {"ref_id": "b45", "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Fei Sun; Jun Liu; Jian Wu; Changhua Pei; Xiao Lin; Wenwu Ou; Peng Jiang"}, {"ref_id": "b46", "title": "UL2: Unifying Language Learning Paradigms", "journal": "", "year": "2023", "authors": "Yi Tay; Mostafa Dehghani; Q Vinh; Xavier Tran; Jason Garcia; Xuezhi Wei; Hyung Won Wang; Dara Chung; Tal Bahri; Steven Schuster; Denny Zheng; Neil Zhou; Donald Houlsby;  Metzler"}, {"ref_id": "b47", "title": "Gemini: a family of highly capable multimodal models", "journal": "", "year": "2023", "authors": "Gemini Team; Rohan Anil; Sebastian Borgeaud; Yonghui Wu; Jean-Baptiste Alayrac; Jiahui Yu; Radu Soricut; Johan Schalkwyk; Andrew M Dai; Anja Hauth"}, {"ref_id": "b48", "title": "LLaMA: Open and Efficient Foundation Language Models", "journal": "", "year": "2023", "authors": "Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet; Marie-Anne Lachaux; Timoth\u00e9e Lacroix; Baptiste Rozi\u00e8re; Naman Goyal; Eric Hambro; Faisal Azhar; Aurelien Rodriguez; Armand Joulin; Edouard Grave; Guillaume Lample"}, {"ref_id": "b49", "title": "Attention is All you Need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141 Ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b50", "title": "", "journal": "Curran Associates, Inc", "year": "", "authors": "U Guyon; S Von Luxburg; H Bengio; R Wallach;  Fergus"}, {"ref_id": "b51", "title": "Zero-Shot Next-Item Recommendation using Large Pretrained Language Models", "journal": "", "year": "2023", "authors": "Lei Wang; Ee-Peng Lim"}, {"ref_id": "b52", "title": "Low-Rank Adaptation for Multilingual Summarization: An Empirical Study", "journal": "", "year": "2023", "authors": "Chenxi Whitehouse; Fantine Huot; Jasmijn Bastings; Mostafa Dehghani; Chu-Cheng Lin; Mirella Lapata"}, {"ref_id": "b53", "title": "A survey on large language models for recommendation", "journal": "", "year": "2023", "authors": "Likang Wu; Zhi Zheng; Zhaopeng Qiu; Hao Wang; Hongchao Gu; Tingjia Shen; Chuan Qin; Chen Zhu; Hengshu Zhu; Qi Liu"}, {"ref_id": "b54", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer", "journal": "", "year": "2021", "authors": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant; Aditya Barua; Colin Raffel"}, {"ref_id": "b55", "title": "CosRec: 2D Convolutional Neural Networks for Sequential Recommendation", "journal": "ACM", "year": "2019", "authors": "An Yan; Shuo Cheng; Wang-Cheng Kang; Mengting Wan; Julian Mcauley"}, {"ref_id": "b56", "title": "Vector-quantized image modeling with improved vqgan", "journal": "", "year": "2021", "authors": "Jiahui Yu; Xin Li; Jing Yu Koh; Han Zhang; Ruoming Pang; James Qin; Alexander Ku; Yuanzhong Xu; Jason Baldridge; Yonghui Wu"}, {"ref_id": "b57", "title": "Recommendation as instruction following: A large language model empowered recommendation approach", "journal": "", "year": "2023", "authors": "Junjie Zhang; Ruobing Xie; Yupeng Hou; Wayne Xin Zhao; Leyu Lin; Ji-Rong Wen"}, {"ref_id": "b58", "title": "Feature-level Deeper Self-Attention Network for Sequential Recommendation", "journal": "", "year": "2019", "authors": "Tingting Zhang; Pengpeng Zhao; Yanchi Liu; Victor S Sheng; Jiajie Xu; Deqing Wang; Guanfeng Liu; Xiaofang Zhou"}, {"ref_id": "b59", "title": "Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms", "journal": "", "year": "2021", "authors": "Shanlei Wayne Xin Zhao; Yupeng Mu; Zihan Hou; Yushuo Lin; Xingyu Chen; Kaiyuan Pan; Yujie Li; Hui Lu; Changxin Wang;  Tian"}, {"ref_id": "b60", "title": "S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Kun Zhou; Hui Wang; Wayne Xin Zhao; Yutao Zhu; Sirui Wang; Fuzheng Zhang; Zhongyuan Wang; Ji-Rong Wen"}, {"ref_id": "b61", "title": "Collaborative Large Language Model for Recommender Systems", "journal": "Association for Computing Machinery", "year": "2024", "authors": "Yaochen Zhu; Liang Wu; Qi Guo; Liangjie Hong; Jundong Li"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: CALRec: Training Framework.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Algorithm 1: Quasi Round Robin BM25 Selection Input :\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc60, output texts sampled from the LLM (size = \ud835\udc41 gen ); \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc4f\ud835\udc60, LLM sequence scores of \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc60 (size = \ud835\udc41 gen ); \ud835\udc36, corpus of candidate items (size = \ud835\udc41 \ud835\udc50 ); \ud835\udc41 preds , a hyperparameter that determines the number of generated text samples used for BM25 matching; \ud835\udf16, a hyperparameter that controls the modulation of BM25 scores with LLM sequence scores. Output : The sorted list of item predictions. Keep the top \ud835\udc41 preds samples 3 top_samples \u2190 sorted_samples[: \ud835\udc41 preds ] 4 top_logprobs \u2190 sorted_logprobs[: \ud835\udc41 preds ] 5 match_scores \u2190 EmptyMatrix(\ud835\udc41 \ud835\udc50 , \ud835\udc41 preds ) 6 for \ud835\udc56 \u2190 1 to \ud835\udc41 preds do // Find BM25 scores between entire corpus and each sample", "figure_data": "// Sort \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc60 by score and remove duplicates1 sorted_samples, sorted_logprobs \u2190SortDescending((samples, logprobs), by=logprobs)2 sorted_samples, sorted_logprobs \u2190RemoveDuplicates((sorted_samples, sorted_logprobs))//"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "11  Statistics of Amazon Review dataset.", "figure_data": "Before DedupAfter DedupDataset# of Users % of duplicate Purchases # of Items Total Purchase Items/User Purchases/Item Density Words/Item Word Vocab Size Avg. Word FreqScientific94615.5%5282666447.0412.620.001322.82191786.3Instruments255774.0%105992145268.3920.240.0007918.43222568.8Arts471979.4%228284114498.7218.020.0003821.384034212.1Office44736 (50%)6.3%274823521517.8712.810.0002921.745668710.5Games509404.7%173834570608.9726.290.0005116.391708716.7Pet43135 (20%)5.9%377123806238.8210.090.0002319.874733915.8"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Evaluation results of CALRec in comparison with different ID/Text-based sequential recommendation baselines (cf. \u00a74)", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Ablation result showing the benefits of multi-stage fine-tuning and constrastive alignment to improve the performance of CALRec. Bold: the highest optimistic (opt) scores. Underline: the strongest pessimistic (pes) scores.Ablation Study. In Table3, we demonstrate the effectiveness of CALRec's key components via extensive ablation study on 3 data categories (the results on the other 3 categories show the same trends so we skip them due to space constraints).", "figure_data": "10MRR"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Average scores (opt)/(pes) on all 6 data categories: w/ contrastive alignment vs. w/o contrastive alignment.", "figure_data": "Model SizeMetricDatasetXXXSXXSS(opt)/(pes)NDCG@10 0.0299/0.0252 0.0326/0.0291 0.0321/0.0306ScientificRecall@1 Recall@100.0089/0.0034 0.0090/0.0055 0.0082/0.0058 0.0592/0.0534 0.0661/0.0623 0.0660/0.0647MRR0.0246/0.0204 0.0267/0.0234 0.0262/0.0247NDCG@10 0.0215/0.0210 0.0257/0.0254 0.0234/0.0231InstrumentsRecall@1 Recall@100.0029/0.0022 0.0041/0.0037 0.0043/0.0039 0.0480/0.0476 0.0585/0.0584 0.0512/0.0511MRR0.0162/0.0156 0.0197/0.0193 0.0188/0.0185"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Performance results of pretrained PaLM-2 models of three distinct sizes without any fine-tuning for Sequential Recommendation. Bold: the highest optimistic (opt) scores.Underline: the strongest pessimistic (pes) scores.", "figure_data": "DatasetMetric(opt)/(pes)NDCG@10 0.0265/0.0232Recall@10.0051/0.0030ScientificRecall@10 0.0582/0.0537MRR0.0217/0.0189NDCG@10 0.0240/0.0237Recall@10.0028/0.0025InstrumentsRecall@10 0.0584/ 0.0583MRR0.0179/0.0175Table 6: Performance results of GPT4Rec's template usingpretrained PaLM-2 XXS without any fine-tuning for Sequen-tial Recommendation."}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Data deduplication. Optimistic evaluations of LIR Baseline (always recommend last-item in the input sequence) on data from[26] (w/o dedup) and our data w/ dedup respectively.", "figure_data": "Output Matches an Item (%)Hierarchical Match -Metric: (opt)/(pes) & Improv. vs BM25 (%)DatasetFormat Error (%) All Four Attributes Title OnlyNDCG@10Recall@1Recall@10MRRScientific0.0%82.3%96.7%0.0771 / 0.0725 (-2.1%) 0.0525 / 0.0404 (+3.2%) 0.1054 / 0.1054 (-5.9%) 0.0693 / 0.0632 (-5.2%)Instruments0.001%97.6%99.8%0.0905 / 0.0901 (-0.4%) 0.0717 / 0.0705 (-0.1%) 0.1151 / 0.1151 (-0.6%) 0.0836 / 0.0830 (-3.2%)Arts0.01%92.5%98.9%0.0854 / 0.0844 (-1.1%) 0.0636 / 0.0611 (+0.1%) 0.1113 / 0.1113 (-2.3%) 0.0777 / 0.0764 (-4.6%)Office0.0%84.5%94.7%0.0926 / 0.0916 (-5.2%) 0.0736 / 0.0710 (-3.3%) 0.1127 / 0.1127 (-7.1%) 0.0864 / 0.0851 (-6.6%)Games0.05%98.0%99.0%0.0582 / 0.0573 (-2.1%) 0.0276 / 0.0257 (-6.8%) 0.0973 / 0.0972 (-1.3%) 0.0467 / 0.0456 (-8.4%)Pet0.03%97.3%99.0%0.0733 / 0.0690 (-0.4%) 0.0573 / 0.0468 (+0.6%) 0.0924 / 0.0922 (-1.3%) 0.0677 / 0.0620 (-2.8%)"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Hierarchical match results. The relative gains/drops in", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "L \ud835\udc41 \ud835\udc3c\ud835\udc3a = E t\u223c\ud835\udc5d (t) \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \ud835\udc59 \u2211\ufe01 \ud835\udc57=\ud835\udc5a+1 log \ud835\udc43 (\ud835\udc61 \ud835\udc57 |\ud835\udc61 1:\ud835\udc57 -1 ; \ud835\udf03 ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb (1)", "formula_coordinates": [4.0, 98.48, 224.72, 196.11, 33.45]}, {"formula_id": "formula_1", "formula_text": "L \ud835\udc47\ud835\udc47 = - 1 \ud835\udc41 \ud835\udc4f \ud835\udc41 \ud835\udc4f \u2211\ufe01 \ud835\udc56=1 log exp(cos(v \ud835\udc47 |\ud835\udc48 \ud835\udc56 , v \ud835\udc47 \ud835\udc56 )/\ud835\udf0f \ud835\udc50 ) \ud835\udc41 \ud835\udc4f \ud835\udc57=1 exp(cos(v \ud835\udc47 |\ud835\udc48 \ud835\udc57 , v \ud835\udc47 \ud835\udc56 )/\ud835\udf0f \ud835\udc50 ) ,(2)", "formula_coordinates": [4.0, 85.18, 456.34, 209.4, 29.73]}, {"formula_id": "formula_2", "formula_text": "L \ud835\udc48\ud835\udc47 = - 1 \ud835\udc41 \ud835\udc4f \ud835\udc41 \ud835\udc4f \u2211\ufe01 \ud835\udc56=1 log exp(cos(v \ud835\udc48 \ud835\udc56 , v \ud835\udc47 \ud835\udc56 )/\ud835\udf0f \ud835\udc50 ) \ud835\udc41 \ud835\udc4f \ud835\udc57=1 exp(cos(v \ud835\udc48 \ud835\udc57 , v \ud835\udc47 \ud835\udc56 )/\ud835\udf0f \ud835\udc50 ) ,(3)", "formula_coordinates": [4.0, 84.12, 493.67, 210.47, 26.57]}, {"formula_id": "formula_3", "formula_text": "L \ud835\udc36\ud835\udc34\ud835\udc3f\ud835\udc45\ud835\udc52\ud835\udc50 = (1 -\ud835\udefc -\ud835\udefd)L \ud835\udc41 \ud835\udc3c\ud835\udc3a + \ud835\udefc L \ud835\udc47\ud835\udc47 + \ud835\udefdL \ud835\udc48\ud835\udc47 .(4)", "formula_coordinates": [4.0, 87.95, 598.42, 206.64, 8.03]}], "doi": "10.1145/3640457.3688121"}
