{"Filtering Discomforting Recommendations with Large Language Models": "Jiahao Liu Fudan University Shanghai, China jiahaoliu23@m.fudan.edu.cn Yiyang Shao Fudan University Shanghai, China yyshao22@m.fudan.edu.cn Peng Zhang \u2217 Fudan University Shanghai, China zhangpeng_@fudan.edu.cn Dongsheng Li Microsoft Research Asia Shanghai, China dongsli@microsoft.com Longzhi Du Alibaba Shanghai, China du.dlz@alibaba-inc.com Hansu Gu Independent Seattle, United States hansug@acm.org Tun Lu \u2217 Fudan University Shanghai, China lutun@fudan.edu.cn", "Abstract": "Chao Chen Shanghai Jiao Tong University Shanghai, China chao.chen@sjtu.edu.cn Ning Gu Fudan University Shanghai, China ninggu@fudan.edu.cn", "Keywords": "Personalized algorithms can inadvertently expose users to discomforting recommendations, potentially triggering negative consequences. The subjectivity of discomfort and the black-box nature of these algorithms make it challenging to effectively identify and filter such content. To address this, we first conducted a formative study to understand users' practices and expectations regarding discomforting recommendation filtering. Then, we designed a Large Language Model (LLM)-based tool named DiscomfortFilter, which constructs an editable preference profile for a user and helps the user express filtering needs through conversation to mask discomforting preferences within the profile. Based on the edited profile, DiscomfortFilter facilitates the discomforting recommendations filtering in a plug-and-play manner, maintaining flexibility and transparency. The constructed preference profile improves LLM reasoning and simplifies user alignment, enabling a 3.8B opensource LLM to rival top commercial models in an offline proxy task. A one-week user study with 24 participants demonstrated the effectiveness of DiscomfortFilter, while also highlighting its potential impact on platform recommendation outcomes. We conclude by discussing the ongoing challenges, highlighting its relevance to broader research, assessing stakeholder impact, and outlining future research directions.", "CCS Concepts": "", "\u00b7 Information systems \u2192 Recommender systems .": "\u2217 Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'25, Sydney, NSW, Australia \u00a9 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1274-6/25/04 https://doi.org/10.1145/3696410.3714850 discomforting recommendation filtering, large language model", "ACMReference Format:": "Jiahao Liu, Yiyang Shao, Peng Zhang, Dongsheng Li, Hansu Gu, Chao Chen, Longzhi Du, Tun Lu, and Ning Gu. 2025. Filtering Discomforting Recommendations with Large Language Models. In Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM,NewYork,NY,USA,16pages.https://doi.org/10.1145/3696410.3714850", "1 Introduction": "Personalized algorithms, which analyze user preferences to deliver tailored content and thereby support human decision-making, are indispensable across web platforms [47-50, 52, 92]. While these algorithms are designed to enhance user experience, they can inadvertently expose users to discomforting recommendations [65]. For example, if a user searches for sensitive topics like health issues, the algorithm might suggest related health products, which could be perceived as a breach of privacy and cause unease [85]. Similarly, when a user is experiencing emotional distress, such as after a breakup, algorithms lacking contextual awareness might recommend content that evokes painful memories, potentially worsening the user's emotional state [68]. Such recommendations may not only fail to engage users but also lead to negative emotional consequences, such as anxiety, unease, or distress [65]. The perception of discomfort is highly subjective, meaning that content one user finds enjoyable may be discomforting to another [65, 67, 81, 83]. This subjectivity underscores the urgent need for a more nuanced approach to discomforting content identification that aligns more closely with individual user experiences [29, 69, 76]. In this paper, we aim to design a tool that helps users filter out discomforting recommendations. This task has two key challenges: (1) users' perceptions of discomfort are highly subjective, and (2) the algorithms recommending such content operate as black-box systems. As illustrated in Figure 2, we formalize the problem as follows: black-box personalized algorithms recommend items to a user based on the inferred preference profile (often implicit in Should it be filtered? Opaque Algorithmic Systems comprehensible and editable edited X user Algorithm-recommended items preference profile preference profile User-perceived items Identify and filter out subjectively discomforting items for the user: Opaque Algorithmic Systems Black-box personalized Incomprehensible and algorithms uneditable preference profile user Algorithm-recommended items the embeddings), and our objective is to identify and filter out subjectively discomforting items for the user. Due to the opacity of algorithmic systems, the preference profile is both incomprehensible and uneditable, making it challenging for the user to influence the algorithm's decisions. To inform our design process, we conducted a formative study to gain insights into the current landscape and user expectations (Section 3). Initially, we identified several key factors contributing to discomforting recommendations, including deviations in user behavior, biases in algorithmic modeling, and conflicting interests among stakeholders. We then examined the limitations of current feedback mechanisms, emphasizing shortcomings such as insufficient personalization, inflexibility, and a lack of transparency. Based on these findings, we established four design goals to guide the design of our tool: support conversational configuration, provide preference explanations, provide feedback channels, and operate in a plug-and-play manner. Given the natural language understanding, reasoning, and generation capabilities demonstrated by LLMs [46, 98], we propose that LLM provide a promising solution for achieving these design goals. To this end, we designed an LLM-based tool named DiscomfortFilter, specifically aimed at helping users filter out discomforting recommendations (Section 4). Figure 1 illustrates the workflow of DiscomfortFilter: (1) DiscomfortFilter identifies algorithm-recommended items based on the user's personalized perceptions, integrates the user's pairwise preferences, and ranks them to construct a comprehensible and editable preference profile tailored for the user; (2) Through a guided conversation, DiscomfortFilter assists the user in expressing personalized filtering needs, and then masks the discomforting preferences with in the profile; (3) DiscomfortFilter filters out discomforting recommendations based on the edited preference profile in a plug-and-play manner, ensuring that user-perceived items no longer includes discomforting elements. Additionally, DiscomfortFilter provides the user with access to filtering logs recorded during step (3), assisting the user in refining filtering needs in a manner similar to step (2). Overall, DiscomfortFilter empowers the user to actively influence the decisions made by personalized algorithms, enhancing control over the algorithms. We validated the efficacy of the constructed preference profile using an offline proxy task. The findings suggest that it enhances the reasoning process of LLMs, markedly decreases the challenge of aligning them with users, and allows a 3.8B open-source LLM to rival top commercial models. Additionally, we conducted a oneweek user study on Zhihu (a platform similar to Quora), China's largest Q&A community, with 24 participants (Section 5). The results demonstrate that our design goals effectively help users express their filtering needs and filter out discomforting recommendations, with DiscomfortFilter successfully achieving these goals. We also analyzed how DiscomfortFilter impacts platform recommendation outcomes by influencing the exposure of discomforting items. Finally, we conducted an in-depth discussion (Section 6), covering: challenges of filtering discomforting recommendations with LLMs, relevance to research topics in recommender systems, potential impact on platforms, and limitations and future work. The key contributions of this work are outlined below: \u00b7 We conducted a formative study with 15 participants to examine the current status and user expectations regarding the filtering of discomforting recommendations. \u00b7 We designed an LLM-based tool named DiscomfortFilter to assist users in filtering out discomforting recommendations, which is the first attempt to leverage LLMs in this important task, to the best of our knowledge. \u00b7 We evaluated DiscomfortFilter through an offline proxy experiment and a user study and the results showed that DiscomfortFilter can effectively help users express their filtering needs and filter out discomforting recommendations. \u00b7 Wediscussed the challenges and opportunities of using LLMs for filtering discomforting recommendations and shed light on its broader implications.", "2 Related Work": "Personalized algorithms primarily derive user preferences from behavioral data, leading to incomplete user modeling and discomforting recommendations [72]. In Appendix E, we provide a detailed review of studies illustrating this phenomenon across various scenarios, including privacy invasion [85], lack of contextual understanding [68], popularity bias [19], and information bubbles [73], along with their negative outcomes [83]. Although previous research has identified scenarios and causes of discomfort, few studies have focused on designing systems to identify and filter these recommendations, which our work aims to address . In Appendix E, we provide a detailed review of two potential solutions for filtering discomforting recommendations: interactive recommendation systems [22, 30, 54] and content moderation systems [13, 28, 34]. Both allow users to modify recommendations to mitigate discomfort. However, interactive systems often struggle to scale in opaque environments due to their reliance on specific algorithm designs, while moderation systems focus on objectively harmful content, which may be less effective for addressing subjective discomfort. Thus, the subjectivity of discomfort perception and the opacity of algorithms present notable challenges . In Appendix E, we present a detailed review of studies on LLMs as personal assistants [46], including commercial applications [61] and frameworks for human-centered recommender systems [78]. The impressive capabilities of LLMs, especially in handling personal data and services, underscore their potential for effectively filtering discomforting recommendations. To our knowledge, this is the first exploration of using LLMs for this specific purpose.", "3 Formative Study": "We conducted semi-structured interviews and participatory design sessions with 15 participants, each lasting approximately one hour . Additional details about the process are provided in Appendix A.", "3.1 Findings from Semi-Structured Interviews": "During the semi-structured interviews, we explored participants' experiences with discomforting recommendations and the issues they faced when using the 'Not Interested' button 1 for feedback. Our analysis identified two key findings from their responses. F1: Users may encounter discomforting recommendations for three reasons. (1) User behavior deviation. Curiosity-driven search behavior and clickbait-induced clicks may fail to reflect a user's true long-term interests, leading inaccurate user preference modeling. For example, P03 said ' Out of curiosity, I once searched for adult products, and now they keep showing up in my recommendations-so embarrassing. ' (2) Algorithmic modeling bias. Personalized algorithms cannot fully capture the nuanced interests 2 and contexts of users. For example, P06 said ' Getting horror content at night is awful, even if I watch it during the day. ' (3) Conflicting interests. For instance, platforms may promote content designed to boost user engagement, even if it may cause discomfort. Seven participants mentioned scenarios where this was the case. F2: Platforms' 'Not Interested' button faces three major limitations that reduce user engagement. (1) Lack of personalization. Thirteen participants found the options too vague, making it difficult for them to articulate their specific reasons and potentially leading to the unintended exclusion of content they might otherwise enjoy. (2) Lack of flexibility. Eleven participants expressed concern that content they temporarily wish to hide might be permanently removed from their feed. (3) Lack of transparency. Twelve participants expressed dissatisfaction with the uncertainty about whether their feedback was being processed effectively.", "3.2 Design Goals Established through Participatory Design": "In response to the issues mentioned above, during the participatory design process, we further discussed participants' specific expectations for the tool and summarized the following four design goals. G1: Support conversational configuration. Thirteen participants indicated that they prefer expressing their filtering needs 2 This stems from the fact that collaborative filtering algorithms mainly retain low- frequency information during model training [77]. using natural language because it ' isn't limited by predefined options ' (P11) and ' allows for more accurate and personalized expression ' (P04). Additionally, ten participants expressed a desire to communicate their filtering needs through conversation with the tool, as it feels ' more natural ' (P09). By supporting conversational configuration, the issue of 'lack of personalization' is addressed. G2: Provide preference explanations. As the input shifts from predefined options to open conversations, 10 participants reported difficulty in proactively articulating filtering needs. However, all participants agreed that understanding the preferences reflected in platform recommendations and their own behavior would encourage them to express these needs. For example, P05 said, ' I can review it and then provide targeted feedback on any inaccuracies. ' G3: Provide feedback channels. All participants expressed the need for the tool to exhibit transparency and be contestable. Being informed about the filtered content and the corresponding reasons can ' enhance trust in the tool ' (P13), while allowing corrections to the tool's behavior helps ' refine filtering needs ' (P12). By providing feedback channels, the issue of 'lack of transparency' is addressed. G4: Operate in a plug-and-play manner. The plug-and-play approach means that the tool operates independently of specific personalized algorithms and directly affects the outputs of these algorithms. Three key factors support this: (1) Participants recognized that their filtering needs are dynamic, as ' discomforting content varies by state ' (P06); (2) Nine participants highlighted that the tool should be user-managed, enabling it to ' work across platforms ' (P14); (3) Participants were more concerned with the discomfort caused by personalized algorithmic outputs than with understanding the algorithms themselves. By operating in a plug-and-play manner, the issue of 'lack of flexibility' is addressed.", "4 DiscomfortFilter": "Wedesigned and implemented an LLM-based tool, DiscomfortFilter, which meets the four design goals established in the formative study and aims to assist users in filtering discomforting recommendations. The workflow of DiscomfortFilter has already been illustrated in Figure 1, and this section will provide a detailed introduction.", "4.1 Overview": "Figure 3 illustrates how personalized algorithms present items to a user before the introduction of DiscomfortFilter. These algorithms analyze user behavior to recommend items, which may include both discomforting and non-discomforting items. The personalized items is then displayed on the user's device for passive consumption . The process of presenting items to the user after the introducing DiscomfortFilter. Algorithm-recommended Items Content Filter Module User-perceived Items Discomforting items caused by Filtering rules exist in the form of natural language user behavior deviation user behavior deviation <Filtering Rule 1> Need to Discomforting items caused by Discomforting items caused by isActive: True False filter? algorithmic modeling bias algorithmic modeling bias Discomforting items caused by <Filtering Rule 2> Discomtorting items caused by conflicting interests isActive: True False contlicting interests behavior data personalized algorithms 4 Save Filtering Record Non-discomforting items Non-discomforting items personalized items filtering rules list LLM 88 Cot device The process of configuring filtering rules. Configure Filtering Rules Directly User Edit & User Confirm Filtering Needs Discovery Module user behavior Add the following filtering rule: Configure Filtering Rules Clicked Items Strategy Preference Profile Explanation Through Conversation Unclicked Items Based on the content recommended by the platform and your clicking OK Cancel use behavior, believe your preference profile profile> Success It appears that the platform has modeled me incorrectly. In reality . Multi-round Conversation Candidate Rule Generation Module Preference Profile Construction Module Conversation Strategy Filtering Rule Explanation Step Analyze User's Filtering Needs Construct Profile Based on User Behavior Based on <filtering rule? have tiltered these content for you: Step Generate Rule Management Action Step2. Save Preference Profile <filtered content? because Failed: Continue Conversation LLM 88 CoT LLM + 88 CoT 8 Multi-Agent You shouldn't block <filtered content? because <reason? Multi-round Conversation LLM Filtering Record Preference  Profile Preference Profile The upper part of Figure 4 illustrates how personalized algorithms present items to the user after the introduction of DiscomfortFilter. By integrating a Content Filter Module into the original items presentation flow, DiscomfortFilter removes discomforting recommendations, ensuring that only non-discomforting items are ultimately displayed to the user. The identification of discomfort is based on user-configured filtering rules , giving the user an ability of control over the process. otherwise, they will be filtered out (G4). The identification process uses a chain-of-thought (CoT) method, with the prompt detailed in Appendix C. Filtering records will be saved and forwarded to the Filtering Needs Discovery Module. The filtering rules, existing in natural language form, are crucial for the operation of DiscomfortFilter. The lower part of Figure 4 illustrates how the user configures these rules. The user can manage them directly (green arrow) or utilize the Filtering Needs Discovery Module for conversational rule configuration (blue arrow). This conversational agent employs two strategies to help the user identify filtering needs: the first strategy relies on the preference profile constructed by the Preference Profile Construction Module , while the second strategy relies on filtering records from the Content Filter Module. The Candidate Rule Generation Module analyzes the user's filtering needs from the conversations and translates them into management actions for the filtering rules, which the user can then edit and confirm.", "4.2 Module Details": "We provide a detailed introduction to the four modules that make up the DiscomfortFilter. 4.2.1 Content Filter Module. A user can manage filtering rules directly through this module. These rules are described in natural language, specifying the discomforting recommendations the user wishes to avoid. Each filtering rule has an associated activation option. During the filtering phase, the module reviews each recommendation against all active filtering rules to determine if the content matches any discomforting criteria. Only recommendations that do not match any filtering rules will be shown to the user; 4.2.2 Preference Profile Construction Module. This module constructs a user's preference profile by analyzing the user's clicking behavior on recommendations in chronological order. This process differs from traditional personalized algorithm research in three key aspects: (1) It solely models preferences based on individual user behavior , rather than on the behavior of all users. (2) It offers more comprehensive implicit feedback by capturing both clicked items and the recommended items users choose to ignore. (3) It must be conducted in real-time , responding instantly to user clicks. The key to this process is summarizing the user's clicking behavior on recommended content into a preference profile made up of features and maintaining that profile over time. As illustrated in Figure 5, we propose an LLM-based multi-agent pipeline to complete this process. We adopt the general assumption of pairwise rank learning [70]-when two items are displayed simultaneously, the one that is clicked is more appealing to the user. For each clicked item (denoted as pos for positive), the module randomly samples an unclicked item that appeared simultaneously with pos as a negative item (denoted as neg ), and then constructs an ordered pair <pos, neg> . Note that pos and neg are unprocessed raw contents. Then, this pair is processed by the pipeline. First, the Perceive Agent identifies pos and neg from the user's perspective, analyzing why the user clicked on pos but not on neg based on the current preference profile. Different users focus on different aspects of the same item. By incorporating a user's preference profile, the Perceive Agent can accurately identify the aspects that matter most to each individual user. Second, the Summary Agent distills the reasons for selecting pos over neg into \ud835\udc5a pos feature s and \ud835\udc5b neg feature s, drawing from the analysis of the Perceive Agent. It then forms \ud835\udc5a \u00d7 \ud835\udc5b ordered pairs of Summary Agent user behavior Perceive Agent <pos? has the following features: Clicked Items negative clicked on <pos> for the following reasons: <reason> <pos feature 1> <pos teature m? Unclicked Items sampling did not click on <neg? for the following reasons: <reason> <neg> has the following features: The raw content contains various <neg feature 12 <neg feature types of side information; such as Personalized Content Perception the title; author, abstract; etc m X 1 <pos feature; neg feature? e.g Filtering Needs Discovery Module Preference Reflect Agent Profile Merge feature nodes Increase the weight for the directed edge PageRank features out of the features within the user' s Interest user's interest Comprehensive Preference Ranking Pairwise Ordered Preference Integration <pos feature, neg feature> via the Cartesian product, indicating that for each pair, the user prefers the pos feature over the corresponding neg feature . The ordered pairs that describe the partial order relationships between these features are the fundamental basis for modeling user preferences. Third, the Reflect Agent maintains a directed graph, where edges point from neg feature to pos feature , with edge weights denoting the frequency of each <pos feature, neg feature> pair. Upon receiving pairs from the Summary Agent, it first merges similar feature nodes and subsequently integrates them into the graph. During the merge process, candidate features for merging are first identified based on semantic similarity, and then the final merge result is obtained using LLM. The directed graph integrates independent ordered pairs from each user click, enabling the modeling of user preferences through comprehensive structural information. Finally, the feature nodes are ranked using the PageRank algorithm, which provides a comprehensive ranking of preferences . Features with higher rankings are generally more aligned with the user's interests, while lower-ranked features tend to be less relevant. Overall, this Module has three key characteristics: (1) Preference profile is constructed from features (rather than raw contents); (2) The features are summarized through personalized content perception ; (3) Features are globally ranked using the PageRank algorithm. The preference profile constructed with these three designs helps refine the reasoning process of LLMs and significantly reduce the difficulty of aligning LLMs with user preferences . The preference profile is stored and sent to the Filtering Needs Discovery Module to improve the personalization of the conversational agent and provide users with clear explanations. The prompt used for this multi-agent pipeline can be found in Appendix C. 4.2.3 Filtering Needs Discovery Module. This module is a conversational agent designed to help users identify potential filtering needs with two strategies (G1). The content of each conversation round will be forwarded to the Candidate Rule Generation Module for further analysis. Strategy 1: Preference Profile Explanation. This strategy begins by informing a user about the preference profile constructed by the Preference Profile Construction Module (G2). The user can then engage in multiple rounds of conversation with the conversational agent to express filtering needs, particularly where the preference profile do not match the user's expectations. Strategy 2: Filtering Record Explanation. This strategy begins by informing a user about the filtering records from the Content Filter Module, including the filtered content and the reasons for filtering (G3). The user can then engage in multiple rounds of conversation with the conversational agent to refine filtering rules, particularly where the filtering records do not match the user's expectations. It is important to emphasize that integrating the preference profile aligns the conversational agent with the user, ensuring that interactions between them are highly personalized. 4.2.4 Candidate Rule Generation Module. During the interaction between the conversational agent and a user, this module continuously analyzes the user's filtering needs from the conversation. Once these needs are successfully identified, the module evaluates their relevance to existing filtering rules and generates corresponding management actions. By generating actions based on relevance, the module helps prevent conflicts and redundancy. For example, if the new filtering needs are related to existing rules, the module will generate an update action rather than create a new rule. The user can then edit and confirm the generated management actions. If no management action is confirmed (either because no needs were identified or because the user did not confirm), the conversational agent will continue interacting with the user. Figure 10 provides a detailed illustration of the above process through a flowchart.", "4.3 Summary": "Contestability refers to the ability to challenge decisions made by algorithms, serving as a crucial safeguard against power imbalances between users and algorithms [54]. However, this concept is often overlooked [54]. Although DiscomfortFilter does not interfere with the platform's algorithm, it fundamentally aims to enhance user-perceived contestability in interactions with personalized algorithms . By constructing a comprehensible and editable", "Performance of 8 LLMs Across 5 Construction Methods": "0.350 Baseline A Baseline B 0.325 Baseline \u20ac Baseline D Our Method 0.275 0.250 0.225 Qwen-2.5 (3.1B) Qwen-2.5 (7.6B) Phi-3 (3.8B) Phi-3.5 (3.8B) Qwen-Plus Owen-Max GPT-4o-mini GPT-40 preference profile and allowing the user to mask any discomforting preferences, DiscomfortFilter narrows the gap between algorithmic recommendations and user expectations . Importantly, DiscomfortFilter inherently possesses contestabilityit does not introduce any new uncontestability while enhancing contestability in personalized algorithms .", "5 Evaluation": "Wefirst validate the Preference Profile Construction Module through offline experiments, then conduct a user study to assess whether the design goals help users filter discomforting recommendations and whether DiscomfortFilter meets those goals.", "5.1 Effectiveness of Preference Profile Construction Module": "5.1.1 Task. We validate the effectiveness of the Preference Profile Construction Module through a proxy task. Specifically, when presenting \ud835\udc3e items to a user, we instruct the LLMs to predict which item the user is most likely to click based on the preference profile, with accuracy as the evaluation metric. If the preference profile is sufficiently effective, the LLMs should accurately predict the user's behavior. For each interaction sample from a user (in chronological order), DiscomfortFilter will initially predict the user's behavior and then observe the actual click behavior to continuously update the preference profile. Notably, DiscomfortFilter only accesses the interaction records of the individual user during this process. 5.1.2 Dataset. We conducted offline experiments using the MIND dataset [90], which details negative samples recommended to users during interactions, along with side information. Given the high cost of API calls, we sampled a subset of \u223c 5,000 users for experimentation. To account for the varying interaction frequencies, we first grouped users by interaction frequency into intervals: [ 0 , 10 ) , [ 10 , 20 ) , and up to [ 100 , +\u221e) . Then, from each group, we randomly selected users, ensuring a total of at least 10,000 interactions per group. 5.1.3 Baselines. We conducted an ablation study comparing four heuristic in-context learning baselines for preference profile construction: (A) Preference profile exclude any user-specific information; (B) Raw contents are directly used as preference profile; (C) Features extracted from raw contents are used to construct preference profile; (D) Features are globally ranked using PageRank, but without adapting to personalized perception. 5.1.4 Results. We used four open-source LLMs (Qwen-2.5 (3.1B), Qwen-2.5 (7.6B), Phi-3 (3.8B), and Phi-3.5 (3.8B)) and four commercial LLMs (Qwen-Plus, Qwen-Max, GPT-4o-mini, and GPT-4o) for evaluation. Figure 6 shows their performance across various construction methods, with our method consistently achieving the best results, thus demonstrating its superior effectiveness. To understand the reasons behind the inferior performance of other baselines, we conducted an analysis as follows: A fails to incorporate user-specific data, leading to essentially random predictions; B lacks a summary of features, and the LLMs struggle to deliver precise predictions based solely on raw content; C omits a summary of unclicked features, which prevents LLMs from fully modeling user preferences; D fails to perceive content in a personalized manner, leading to an inability to capture truly important features. Our findings indicate that simply providing LLMs with extensive input is insufficient; instead, carefully curated and personalized inputs are essential for optimal performance. The profiles constructed by several ablation baselines require sufficiently powerful LLMs to achieve better performance, leading commercial models to outperform open-source LLMs on these baseline methods. However, our construction method reduces task complexity by streamlining the reasoning process through a well-constructed preference profile . This enhancement enables open-source models, e.g., Phi-3, to match or even surpass the performance of certain commercial models, e.g., GPT-4o-mini.", "5.2 User Study": "5.2.1 Implementation and Usage. We implemented DiscomfortFilter as a third-party tool on Zhihu 3 , the largest Chinese Q&A community (similar to Quora 4 ). We selected Zhihu because participants frequently mentioned it during the formative study, and many participants indicated that they regularly browse personalized content on it. Furthermore, Q&A platforms are rich in user-generated content and diverse topics, and the three types of discomforting recommendations mentioned in F1 have all been observed on Zhihu. DiscomfortFilter was implemented as a browser extension that filters discomforting recommendations by dynamically editing the DOM. We deployed Qwen2-72B-Instruct to provide LLM services for DiscomfortFilter. Aside from the LLM service, all other services run on user devices, with data stored locally. We provide three user stories and their corresponding interfaces in Appendix D to show the implementation and usage of DiscomfortFilter. 5.2.2 Process. Werecruited 24 participants to use DiscomfortFilter freely for one week . Afterward, participants completed a questionnaire and randomly selected 10 filtered and 10 unfiltered items to label whether DiscomfortFilter correctly identified them. Finally, we collected usage statistics from the participants' devices and conducted 30-minute interviews with each participant. Additional details about the process are provided in Appendix B. 5.2.3 Preliminary Statistical Results. On average, DiscomfortFilter processed 1,093 items per participant, filtering out 124 of them. Table 1 presents the interaction data between participants and DiscomfortFilter. The overall acceptance rate for the management actions on candidate filtering rules generated by DiscomfortFilter (via strategy 1 and strategy 2) was 93.8%. 5.2.4 Results of the questionnaire. Guided by the Technology Acceptance Model (TAM) [14, 57], we developed three evaluation questions for each design goal and the overall tool, focusing on perceived usefulness (PU), perceived ease of use (PEOU), and behavioral intention (BI). Figure 9 shows the specific questions and the corresponding score distributions. For each question, at least three-quarters of participants rated it 4 or 5, indicating overall satisfaction with the role of DiscomfortFilter in assisting users to filter out discomforting recommendations. With a Cronbach's \ud835\udefc of 0.80, we believe the results are reliable for subsequent analysis. 5.2.5 Results of the interview. We compared the platform's 'Not Interested' button with DiscomfortFilter and asked participants for their opinions. All participants unanimously preferred DiscomfortFilter for filtering discomforting recommendations, which can be summarized into the following five key results. R1. Natural language filtering rules helped participants in personalizing their filtering needs (G1). Some participants also found emotional value in being able to ' complain to the tool about the platform's recommendations ' (P31) and ' set rules regarding mood ' (P38). However, false associations in LLMs sometimes hindered accurate identification of discomforting recommendations. Seven participants noted that DiscomfortFilter occasionally overextended the rules, leading to unintended content filtering. This reduced precision - as shown in Table 2, where 24 participants annotated 480 recommended contents, resulting in a precision rate of 173/240=72.1% and a recall rate of 173/(173+32)=84.4%. R2. Providing preference explanations helps participants identify and articulate their filtering needs (G2). Participants were sometimes unclear about their filtering needs, and the preference explanations enabled them to ' discuss with the tool how to establish rules ' (P31). However, two participants felt that the preference profile lacked sufficient detail , offering only a ' broad overview of the recommended content ' (P17). R3. Feedback channels help participants refine their filtering needs and build trust in DiscomfortFilter (G3). Participants noted that some filtering needs were ' hard to express precisely in one attempt ' (P18). When DiscomfortFilter behaved unexpectedly, feedback channels convinced participants that ' the tool had the ability to evolve ' (P31), and encouraged them to ' refine the rules instead of abandoning the tool ' (P28). R4. Participants exhibited varying usage habits for the two strategies in the conversational agent (G1, G2, G3). Participants indicated that configuring filtering rules was the most challenging aspect, and the conversational agent helped simplify this process. Most participants reported a preference for using strategy 1 to create new filtering rules because ' the gaps highlighted new filtering needs ' (P31), while strategy 2 was typically used to update existing rules due to ' errors prompting corrections to the filtering rules ' (P23). This observation is further supported by Table 1. R5. The plug-and-play approach facilitates flexible configuration of dynamic filtering needs (G4) , adapting to contextual changes and short-term interests. However, three participants expressed dissatisfaction with the absence of an efficient method for managing numerous filtering rules , noting that ' reviewing all filtering rules to decide which to activate is cumbersome ' (P33). 5.2.6 Case Study. To demonstrate how participants use DiscomfortFilter, we provide an example. After searching related topics, P35 found through strategy 1 that the platform mistakenly assumed she was interested in mother-in-law and daughter-in-law relationships, which also raised privacy concerns. She then used strategy 1 to set a filtering rule: ' I do not want to see content related to mother-in-law and daughter-in-law relationships. ' Later, through strategy 2, P35 realized that DiscomfortFilter had also unintentionally filtered out content from a novel she liked (caused by false association in LLMs). She then adjusted the rule using strategy 2: ' I do not want to see content related to mother-in-law and daughter-in-law relationships, except for the fictional content in novels. ' 5.2.7 Impact on platform recommendation outcomes. Assuming DiscomfortFilter processes \ud835\udc41 items using a filtering rule, with \ud835\udc5b items identified as discomforting, we define \ud835\udc5b / \ud835\udc41 as the filtering burden of this rule. We calculated the daily average filtering burden for all filtering rules that remained active for more than five days during the seven-day user study. The trend of daily average filtering burden from the day they were configured is shown in Figure 7. Over time, the filtering burden steadily declined, indicating that the platform was recommending progressively fewer discomforting items. This decline can be attributed to the introduction of DiscomfortFilter, which reduces users' exposure to discomforting recommendations, thereby resulting in fewer interactions with such items over time. As a result, the platform's recommender systems can dynamically adjust to users' evolving preferences, further reducing the likelihood of discomforting items being recommended. It is important to emphasize that, from the users' perspective, discomforting recommendations vanish immediately, as they were filtered out entirely.", "6 Discussion": "", "6.1 Challenges of Filtering Discomforting Recommendations with LLMs": "Our evaluation has identified two main challenges in filtering discomforting recommendations using LLMs. (1) False association in LLMs. Despite careful design, LLMs sometimes misinterpret non-discomforting recommendations as containing discomforting elements, leading to unintended exclusions (R1). While LLMs' associative abilities enhance creativity, they require careful control when making decisions for users. (2) Insufficient perceptual alignment. Despite our meticulous design of the Preference Profile Construction Module, LLMs struggle to fully grasp users' subjective experiences, undermining the effectiveness of profile explanation (R2). A significant gap remains in helping LLMs transition from 'seeing what users see' to 'perceiving what users perceive'.", "6.2 Relevance to Research Topics in Recommender Systems": "Recent recommender system studies increasingly emphasize user experience. Recommendation unlearning [45, 96], a process that allows models to forget specific user interests, enhances transparency [95] and controllability [87]. Context-aware recommendations [2] improve user satisfaction by accurately modeling contextual factors. Bias-mitigating recommendations [12] address information cocoons by prioritizing diversity [44] and fairness [88]. Our study emphasizes human-centered aspects of recommender system design, enhancing previous research primarily focused on algorithmic design. Most existing recommendation unlearning techniques [45, 96] achieve only approximate forgetting; however, integrating them with DiscomfortFilter enables exact interest forgetting . Traditional context-aware recommendations [2] rely solely on passively collected data, such as spatio-temporal information, while combining with DiscomfortFilter can better meet the personalized needs of users . Recent studies indicate that the perceived diversity among users cannot be achieved merely by providing diverse recommendations but instead relies on their active exploration [97]. While the impact of our study on information cocoons remains uncertain, we believe that, with appropriate guidance, DiscomfortFilter can enhance users' understanding of recommended content and help them escape these cocoons actively .", "6.3 Potential Impact on Platforms": "While DiscomfortFilter does not directly modify the platform's algorithms, it influences the items that users encounter. This influence can alter user behavior and, in turn, affect the platform's data collection and user modeling indirectly. We believe that this influence is beneficial for two primary reasons. First , studies indicate that even minimal control over recommendations significantly enhances users' willingness to engage [16]. DiscomfortFilter empowers users with greater control over the recommendation process, fostering trust and increasing their willingness to use the platform. Second , preventing data collection from users' interactions with discomforting recommendations enables the platform to model users more accurately.", "6.4 Limitations and Future Work": "We present the limitations and potential improvements from four aspects: (1) Performance. The two challenges outlined in Section 6.1 primarily arise from LLMs not being aligned with users. One potential solution is to introduce an interactive verification process that aligns LLMs based on user feedback. (2) Function. Our study currently focuses exclusively on user click behavior and text content. Future developments could incorporate other behaviors and multimodal content. Additionally, an efficient rule management solution is necessary. (3) Evaluation. Most participants recruited for this study hold bachelor's degrees and were assessed on a single platform over a period of one week. A large-scale deployment is needed for a long-term evaluation that encompasses a broader demographic and multiple platforms. (4) Application. DiscomfortFilter could be extended to other scenarios, such as parental monitoring and controlling the online content accessible to children. This requires targeted design and careful consideration of legal and ethical issues.", "7 Conclusion": "Building on insights from a formative study, we developed an LLMbased tool named DiscomfortFilter to assist users in identifying and filtering discomforting recommendations from recommender systems. Results from an offline experiment and a user study demonstrated DiscomfortFilter's effectiveness. In-depth discussions illuminated future directions and the potential broad impact of our work. We believe our study can strengthen the recommender systems community's focus on human-centered design.", "Acknowledgments": "This work is supported by National Natural Science Foundation of China (NSFC) under the Grant No. 61932007, 62372113, and 62172106. Tun Lu is also a faculty of Shanghai Key Laboratory of Data Science, Fudan Institute on Aging, MOE Laboratory for National Development and Intelligent Governance, and Shanghai Institute of Intelligent Electronics & Systems, Fudan University.", "References": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Gediminas Adomavicius and Alexander Tuzhilin. 2010. Context-aware recommender systems. In Recommender systems handbook . Springer, 217-253. [3] Imtiaz Ahmad, Rosta Farzan, Apu Kapadia, and Adam J Lee. 2020. Tangible privacy: Towards user-centric sensor designs for bystander privacy. Proceedings of the ACM on Human-Computer Interaction 4, CSCW2 (2020), 1-28. [4] Sharifa Alghowinem. 2019. A safer youtube kids: An extra layer of content filtering using automated multimodal analysis. In Intelligent Systems and Applications: Proceedings of the 2018 Intelligent Systems Conference (IntelliSys) Volume 1 . Springer, 294-308. [5] Fedor Bakalov, Marie-Jean Meurs, Birgitta K\u00f6nig-Ries, Bahar Sateli, Ren\u00e9 Witte, Greg Butler, and Adrian Tsang. 2013. An approach to controlling user models and personalization effects in recommender systems. In Proceedings of the 2013 international conference on Intelligent user interfaces . 49-56. [6] Jack Bandy and Nicholas Diakopoulos. 2021. Curating quality? How Twitter's timeline algorithm treats different types of news. Social Media+ Society 7, 3 (2021), 20563051211041648. [7] Garfield Benjamin. 2022. # FuckTheAlgorithm: algorithmic imaginaries and political resistance. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency . 46-57. [8] Svetlin Bostandjiev, John O'Donovan, and Tobias H\u00f6llerer. 2012. TasteWeights: a visual interactive hybrid recommender system. In Proceedings of the sixth ACM conference on Recommender systems . 35-42. [9] Simon Bruns, Andr\u00e9 Calero Valdez, Christoph Greven, Martina Ziefle, and Ulrik Schroeder. 2015. What should i read next? a personalized visual publication recommender system. In International Conference on Human Interface and the Management of Information . Springer, 89-100. [10] Taina Bucher. 2019. The algorithmic imaginary: Exploring the ordinary affects of Facebook algorithms. In The social power of algorithms . Routledge, 30-44. [11] Eshwar Chandrasekharan, Chaitrali Gandhi, Matthew Wortley Mustelier, and Eric Gilbert. 2019. Crossmod: A cross-community learning-based system to assist reddit moderators. Proceedings of the ACM on human-computer interaction 3, CSCW (2019), 1-30. [12] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023. Bias and debias in recommender system: A survey and future directions. ACM Transactions on Information Systems 41, 3 (2023), 1-39. [13] Stefano Cresci, Amaury Trujillo, and Tiziano Fagni. 2022. Personalized interventions for online moderation. In Proceedings of the 33rd ACM Conference on Hypertext and Social Media . 248-251. [14] Fred D Davis et al. 1989. Technology acceptance model: TAM. Al-Suqri, MN, Al-Aufi, AS: Information Seeking Behavior and Technology Adoption 205 (1989), 219. [15] Michael Ann DeVito, Ashley Marie Walker, and Julia R Fernandez. 2021. Values (mis) alignment: Exploring tensions between platform and LGBTQ+ community design values. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1-27. [16] Berkeley J Dietvorst, Joseph P Simmons, and Cade Massey. 2018. Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them. Management science 64, 3 (2018), 1155-1170. [17] Anthony Elliott. 2024. Algorithms of Anxiety: Fear in the Digital Age . John Wiley & Sons. [18] Motahhare Eslami, Karrie Karahalios, Christian Sandvig, Kristen Vaccaro, Aimee Rickman, Kevin Hamilton, and Alex Kirlik. 2016. First I\" like\" it, then I hide it: Folk Theories of Social Feeds. In Proceedings of the 2016 cHI conference on human factors in computing systems . 2371-2382. [19] Andres Ferraro. 2019. Music cold-start and long-tail recommendation: bias in deep representations. In Proceedings of the 13th ACM Conference on Recommender Systems . 586-590. [20] Mirko Franco, Ombretta Gaggi, and Claudio E Palazzi. 2023. Analyzing the use of large language models for content moderation with chatgpt examples. In Proceedings of the 3rd International Workshop on Open Challenges in Online Social Networks . 1-8. [21] Liza Gak, Seyi Olojo, and Niloufar Salehi. 2022. The distressing ads that persist: Uncovering the harms of targeted weight-loss ads among users with histories of disordered eating. Proceedings of the ACM on Human-Computer Interaction 6, CSCW2 (2022), 1-23. [22] Yingqiang Ge, Shuchang Liu, Zuohui Fu, Juntao Tan, Zelong Li, Shuyuan Xu, Yunqi Li, Yikun Xian, and Yongfeng Zhang. 2022. A survey on trustworthy recommender systems. ACM Transactions on Recommender Systems (2022). [23] Mohammed Muheeb Ghori, Arman Dehpanah, Jonathan Gemmell, Hamed QahriSaremi, and Bamshad Mobasher. 2022. Does the user have a theory of the recommender? a grounded theory study. In Adjunct Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization . 167-174. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia [24] Tarleton Gillespie. 2018. Custodians of the Internet: Platforms, content moderation, and the hidden decisions that shape social media . Yale University Press. [25] Mark Graham and William H Dutton. 2019. Society and the internet: How networks of information and communication are changing our lives . Oxford University Press. [26] Jonathan Gruber, Eszter Hargittai, G\u00f6k\u00e7e Karaoglu, and Lisa Brombach. 2021. Algorithm awareness as an important internet skill: the case of voice assistants. International Journal of Communication 15 (2021), 1770-1788. [27] Ziwei Gu, Jing Nathan Yan, and Jeffrey M Rzeszotarski. 2021. Understanding user sensemaking in machine learning fairness assessment systems. In Proceedings of the Web Conference 2021 . 658-668. [28] Necdet Gurkan, Mohammed Almarzouq, and Pon Rahul Murugaraj. 2024. Personalized Content Moderation and Emergent Outcomes. arXiv preprint arXiv:2405.09640 (2024). [29] Richard Jackson Harris and Lindsay Cook. 2011. How content and co-viewers elicit emotional discomfort in moviegoing experiences: Where does the discomfort come from and how is it handled? Applied Cognitive Psychology 25, 6 (2011), 850-861. [30] Chen He, Denis Parra, and Katrien Verbert. 2016. Interactive recommender systems: A survey of the state of the art and future research challenges and opportunities. Expert Systems with Applications 56 (2016), 9-27. [31] Manoel Horta Ribeiro, Justin Cheng, and Robert West. 2023. Automated content moderation increases adherence to community guidelines. In Proceedings of the ACM web conference 2023 . 2666-2676. [32] Shagun Jhaver, Iris Birman, Eric Gilbert, and Amy Bruckman. 2019. Humanmachine collaboration for content regulation: The case of reddit automoderator. ACM Transactions on Computer-Human Interaction (TOCHI) 26, 5 (2019), 1-35. [33] Shagun Jhaver, Quan Ze Chen, Detlef Knauss, and Amy X Zhang. 2022. Designing word filter tools for creator-led comment moderation. In Proceedings of the 2022 CHI conference on human factors in computing systems . 1-21. [34] Shagun Jhaver, Alice Qian Zhang, Quan Ze Chen, Nikhila Natarajan, Ruotong Wang, and Amy X Zhang. 2023. Personalizing content moderation on social media: User perspectives on moderation choices, interface design, and labor. Proceedings of the ACM on Human-Computer Interaction 7, CSCW2 (2023), 1-33. [35] Jialun Aaron Jiang, Peipei Nie, Jed R Brubaker, and Casey Fiesler. 2023. A tradeoff-centered framework of content moderation. ACM Transactions on ComputerHuman Interaction 30, 1 (2023), 1-34. [36] Ray Jiang, Silvia Chiappa, Tor Lattimore, Andr\u00e1s Gy\u00f6rgy, and Pushmeet Kohli. 2019. Degenerate feedback loops in recommender systems. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society . 383-390. [37] Yucheng Jin, Karsten Seipp, Erik Duval, and Katrien Verbert. 2016. Go with the flow: effects of transparency and user control on targeted advertising using flow charts. In Proceedings of the international working conference on advanced visual interfaces . 68-75. [38] Antti Kangasr\u00e4\u00e4si\u00f6, Dorota Glowacka, and Samuel Kaski. 2015. Improving controllability and predictability of interactive recommendation interfaces for exploratory search. In Proceedings of the 20th international conference on intelligent user interfaces . 247-251. [39] Nadia Karizat, Dan Delmonaco, Motahhare Eslami, and Nazanin Andalibi. 2021. Algorithmic folk theories and identity: How TikTok users co-produce Knowledge of identity and engage in algorithmic resistance. Proceedings of the ACM on human-computer interaction 5, CSCW2 (2021), 1-44. [40] Erin Klawitter and Eszter Hargittai. 2018. 'It's like learning a whole other language': The role of algorithmic skills in the curation of creative goods. International Journal of Communication 12 (2018), 3490-3510. [41] Mahi Kolla, Siddharth Salunkhe, Eshwar Chandrasekharan, and Koustuv Saha. 2024. LLM-Mod: Can Large Language Models Assist Content Moderation?. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems . 1-8. [42] Sherrie YX Komiak and Izak Benbasat. 2006. The effects of personalization and familiarity on trust and adoption of recommendation agents. MIS quarterly (2006), 941-960. [43] Deepak Kumar, Yousef AbuHashem, and Zakir Durumeric. 2023. Watch your language: large language models and content moderation. arXiv preprint arXiv:2309.14517 (2023). [44] Matev\u017e Kunaver and Toma\u017e Po\u017erl. 2017. Diversity in recommender systems-A survey. Knowledge-based systems 123 (2017), 154-162. [45] Yuyuan Li, Chaochao Chen, Xiaolin Zheng, Yizhao Zhang, Biao Gong, Jun Wang, and Linxun Chen. 2023. Selective and collaborative influence function for efficient recommendation unlearning. Expert Systems with Applications 234 (2023), 121025. [46] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. 2024. Personal llm agents: Insights and survey about the capability, efficiency and security. arXiv preprint arXiv:2401.05459 (2024). [47] Jiahao Liu, Dongsheng Li, Hansu Gu, Tun Lu, Jiongran Wu, Peng Zhang, Li Shang, and Ning Gu. 2023. Recommendation unlearning via matrix correction. arXiv preprint arXiv:2307.15960 (2023). [48] Jiahao Liu, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, and Ning Gu. 2022. Parameter-free dynamic graph embedding for link prediction. Advances in Neural Information Processing Systems 35 (2022), 27623-27635.", "A The Detailed Process of the Formative Study": "We recruited 15 participants (8 male, 7 female), most of whom were between the ages of 18 and 35 (13/15), representing the primary users of web platforms. They mostly spend 2-3 hours daily on various web platforms. Table 3 details the demographics of participants in the formative study. First, we conducted semi-structured interviews to systematically understand the specific scenarios in which participants encountered discomforting recommendations on web platforms, how they handled it, and the challenges they faced. Next, we designed a draft framework and used a participatory design approach to explore their expectations for the tool. Based on their feedback, we refined our design. Each participant spent an average of about one hour on this process, and appropriate compensation was provided. All interviews were recorded with participants' consent and transcribed using automated tools as well as by the first author. To ensure privacy and security, we will not share or disclose any data. After completing the semi-structured interviews and participatory design process, we used thematic analysis methods [58] to analyze participants' feedback and interview logs related to standard procedures [3, 60]. Initially, three authors independently conducted open coding on all feedback and collaborated thereafter to establish a set of axial codes. Subsequently, the authors reviewed interview logs, iteratively refining the coding scheme over three rounds to address any identified deficiencies. The final stage involved focused coding, aimed at synthesizing evolving conceptual categories into comprehensive topics. Throughout this process, regular communication is maintained among all authors to ensure conceptual coherence and reliability. The coding process concluded once consensus was reached among the authors regarding the conclusions.", "B The Detailed Process of the User Study": "The detailed user study process is as follows. Step1. We recruited 24 participants (12 male, 12 female), all aged between 18 and 35 years. The majority of them (22/24) browse personalized content on Zhihu for over 30 minutes daily. Table 4 details the demographics of participants in the user study. Step2. We first introduced the participants to the features and usage of DiscomfortFilter, and then invited them to use DiscomfortFilter at their discretion to filter out discomforting recommendations over the course of a week, without imposing any restrictions. We only intervened if participants encountered problems. Step3. After using DiscomfortFilter for one week, we invited participants to complete a questionnaire to gather their feedback. The questionnaire employed a 5-point Likert scale. To assess the performance of DiscomfortFilter and its alignment with the design goals, we incorporated three dimensions into the evaluation, grounded in the Technology Acceptance Model (TAM): (1) Perceived Usefulness (PU): Assesses the effectiveness of DiscomfortFilter in helping users filter out discomforting recommendations. (2) Perceived Ease of Use (PEOU): Evaluates how easy it is for users to understand and use DiscomfortFilter. (3) Behavioral Intention (BI): Gauges users' willingness to continue using DiscomfortFilter in the future. Step4. We asked participants to select 10 filtered and 10 unfiltered pieces of content and indicate whether DiscomfortFilter correctly identified each one. feedback block this author horrifyinglBloody vulgarllow-class don't this content don't like this author this content is outdated disturbing cover image misleading titlelclickbait don't want to watch this content is unfriendly block this category advertisement too similar items many this author immersive mode block this content; keyword or author The quality of this content is poor vulgarllow-class inappropriate content this category too much of this type of content #JR report JRiF report already recommended not interested exaggeratedlbloated like Zhihu Weibo Bilibili Xiaohongshu PU: Conversation helps me express my filtering needs more accurately PU: The \"profile explanation helps me express my filtering needs PU: The feedback mechanism helps me refine my filtering needs. PEOU: Configuring through conversation is easy for me to use, PEOU: The \"profile explanation\" is easy for me to understand. PEOU: The feedback mechanism is easy for me to understand. am willing to continue using conversation for configuration. am willing to continue using the \"profile explanation\" am willing to continue using the feedback mechanism. G1. Support Conversational Configuration G2. Provide Preference Explanations G3. Provide Feedback Channels PU: The plug-and-play manner helps me configure my filtering needs flexibly. PU: The tool helps me filter out discomforting content. color score PEOU: The plug-and-play manner is easy for me to understand. PEOU: The tool is easy for me to understand and use. average score am to continue using the plug-and-play manner. am willing to continue using the tool G4. Operate in a Plug-and-play Manner Overall willing Start Analyze Filtering Needs No Success? Yes Analyze the Correlation Type Unrelated Overlapping Consistent Create a New Rule Update a Existing Rule User Edit & Confirm Continue Conversation End", "C Prompt": "The original prompt is in Chinese, but we have translated it into English for presentation purposes. All prompts include a system instruction stating, 'You are a helpful assistant', which we will exclude here. In practice, we ask the LLM to provide responses in JSON format and have implemented several carefully designed constraints. However, for the sake of simplicity in presentation, we will focus only on the main logic and exclude the details.", "C.1 Content Filter Module": "user: There is a question on the Zhihu platform titled <title> , with the summary <summary> . Please analyze the topics that this question may relate to, ensuring not to overextend the discussion.", "assistant: [Response]": "user: Based solely on our conversation and without any additional elaboration, do you believe this user should filter out this question? assistant: [Response] Step 5. With participants' consent, we collected log data from their use of DiscomfortFilter and conducted interviews with each participant, averaging about half an hour in length. The interview revolved around the questionnaire, discussing DiscomfortFilter's strengths and weaknesses based on their usage experience. We then provided compensation to the participants. Filtering Discomforting Recommendations with Large Language Models Content Filter Module Hi, UserB! Let's Go~ Strategy 1. Preference Profile Explanation Strategy 2. Filtering Rule Explanation", "C.2 Preference Profile Construction Module": "C.2.1 Perceive Agent. Please act as a Zhihu user with the following preference in- formation: Very liked: <feature list> Fairly liked: <feature list> Neutral: <feature list> Fairly disliked: <feature list> Very disliked: <feature list> Now, for a question titled <title> , assuming you have (or have not) interacted with it, please explain your reasons. A Zhihu user has (or has not) interacted with the question titled <title> , and here are the reasons she/he provided: <reasons> . Based on the reasons, please summarize the key features of the question.", "C.2.3 Reflect Agent.": "Can <feature> be merged with the features listed below? If so, please provide the details of the merged feature. <feature list>", "D User Stories": "Figure 11 shows the main page of DiscomfortFilter, featuring three entry points: the Content Filter Module and two conversational agent strategies. Figure 12 illustrates the workflow and corresponding interfaces for three user stories. Story (a). A patient with a mental health condition searched for related content but felt that repeated health product recommendations invaded the privacy. To address this, the user can add a rule to the Content Filter Module: ' I do not want to see content related to mental health. ' Story (b). A user noticed that the platform's recommendations and her behavior showed an interest in horror content through interactions with conversational agent strategy 1. However, she doesn't want such content recommended late at night. She can express dissatisfaction and set a rule: ' I do not want to see content containing horror elements. ' Story (c). Through interaction with conversational agent strategy 2, the user in story (b) noticed that some primarily comedic content with minor horror elements was mistakenly filtered. She reports this to DiscomfortFilter, which will analyze the issue and generate refined filtering rules for her to edit and confirm.", "E Detailed Review of Related Work": "", "E.1 Discomforting Recommendations": "Personalized algorithms infer user preferences primarily from behavioral data, often leading to an incomplete understanding of the user and resulting in discomforting recommendations [72]. Ur et al. [85] highlight that these algorithms can lead users to perceive privacy violations, particularly among individuals with psychological disorders, causing significant anxiety [21]. Moreover, these algorithms frequently fail to consider recent personal experiences, leading to inappropriate content that may trigger emotional distress [68]. The issue is compounded by the inherent popularity bias in personalized algorithms [19], which can generate discomforting recommendations for users outside the algorithm's representative groups [15, 74, 80]. This bias also increases exposure to trending misinformation [6]. Even when recommendations are accurate, users may develop negative perceptions upon realizing they are confined to a 'filter bubble' [36, 71, 73]. Research on short video recommendation has identified various types of discomforting elements, including political issues, dance challenges, eating shows, and specific visual or auditory triggers [65]. Given the subjective, dynamic, and ambiguous nature of discomfort perception [67, 81], identifying such content should be guided by individual user experiences [29, 76] rather than relying solely on broad predefined categories [65]. When users are recommended discomforting content and feel misunderstood, they often attempt to influence the algorithm through their behavior [83]. However, due to their limited understanding and control over algorithmic systems [25, 26, 40, 79], users develop 'folk theories'-assumptions about how the system operates [10, 18]. Based on these theories, they deliberately engage in specific behaviors to elicit desired outcomes from personalized systems. Users must be cautious in their content selection to prevent the platform from distorting their digital identity on social media [79]. When these unconventional actions fail to influence recommendations, it can lead to algorithmic irritation [23, 94], potentially triggering radical collective protests [7, 39, 56]. This frustration may further erode trust in personalized algorithms [42] and foster the feeling of being manipulated [17].", "E.2 Potential Solutions for Filtering Discomforting Recommendations": "We present two categories of studies that appear relevant for identifying and filtering discomforting recommendations-one focusing on interactive recommendation systems [22, 30, 54] and the other on content moderation systems [13, 28, 34]. E.2.1 Interactive Recommendation Systems. An interactive recommender system utilizes data visualization techniques to create a Hi, UserA! Let'$ Go~ Click This All Rules Save Edit Here se0 conlent relaled mental healih Content Filter Module Save Delete Platform Filter Items Accordihg Rules Add Activate Hi, UserB! Let's Go~ Click This Edit guess your pretoronce horror content night Add Rule: Horror movies watchthe content containing honor elements Strategy 1. Preference Profile Explanation Food tromn all over the world Complete Rule Configuration Edit & Confirm Chat with Agent (b) Hi, UserB! Let's Go ~ User Edlt & Conflrm Fillered based on dont watch content containing Ghostbusters( 1984) ~Edit Click This hlter Ghostbusters(1984) Although there ar0 ghost Strategy 2. Filtering Rule Explanation elements In Ghostbuslers So thals Yes why you may no Send Chat with Agent Complete Rule Configuration User Wont (c) transparent and controllable recommendation process, enabling users to actively participate in customizing personalized content. We highlighted some typical works in this field. TasteWeights [8] visualizes recommendations in three layers and allows fine-tuning of each node. Graph embedding [86] uses node-link diagrams to cluster similar movies, with filters for customization. TIGRS [9] visualizes recommendations with keyword filtering for refinement. SetFusion [66] links recommendations to techniques using a Venn diagram with color cues. PARIS [37] displays user profiles and recommendation steps, supporting user control. Bakalov et al. [5] visualize and adjust user interests using circular zones. Intent Radar [38] represents interest by keyword distance, allowing position adjustments. E.2.2 Content Moderation Systems. Content moderation is used to regulate harmful content, such as hate speech and abuse [4, 20, 32, 51, 64, 93]. The process has evolved from manual review [24, 75] to automated methods using NLP technologies [11, 31, 53], which has led to reduced transparency in moderation and less feedback for users [32, 89]. To address these challenges, RECAST [89] provides mechanisms for explanation, revision, and user feedback in the moderation process. Recent research on using LLMs for content moderation [55, 59, 63, 93] highlights their potential in tasks like toxicity detection [43] and rule violation identification [41]. At the same time, there is increasing support for decentralizing content moderation, shifting control from centralized platforms to individual users, thus better enabling them to manage content they wish to avoid on social media [33]. This shift recognizes that a one-size-fits-all approach to content regulation fails to meet the diverse needs of the user base [34, 35].", "E.3 LLMs as Personal Assistants": "LLMs possess semantic understanding and reasoning capabilities, enabling them to engage in human-like conversations and act as decision-making proxies [98]. They not only help users efficiently obtain information and complete tasks but also provide more intelligent, convenient, and enriched interactions [46]. Some of them are closely integrated with personal data and devices, functioning as personal assistants [46]. Several commercial products, such as Microsoft's Copilot, New Bing, Google's Bard, and Gemini, as well as offerings from smartphone manufacturers, have integrated LLMs to boost productivity and enhance web and on-device experiences [1, 61, 62]. In the area of human-centered recommender systems, the RAH framework [78], an LLM-based multi-agent system, analyzes user preferences by observing ratings and reviews and incorporates a reflection mechanism to improve the accuracy.", "F More Discussion": "Here are some discussions that help readers gain a deeper understanding of our work. F.0.1 Generalizability to non-text-based items. Expanding to nontext scenarios is a natural progression, requiring the use of multimodal models to understand such content. However, the efficiency of processing, especially for videos, remains a concern. Preprocessing non-text content offline may offer a viable solution. F.0.2 Differences between discomforting and disliked items. We believe this primarily manifests in the impact perceived by users, with 'discomforting' items having a greater impact. For uninteresting or disliked items, users may simply choose to ignore them. However, as we mentioned in the introduction, discomforting recommendations may not only fail to engage users but also lead to negative emotional consequences, such as anxiety, unease, or distress. This is a highly subjective matter and has a dynamic nature. For example, as illustrated in our formative study (Section 3.1, F1), one participant mentioned that he might search for horror content to watch during the day, but encountering the same horror content at night would make him feel very uncomfortable, even to the extent of affecting his sleep. In our formative study, we observed that most users do not express strong hostility toward uninteresting (disliked) content but are more averse to 'discomforting' content. Thus, from the users' perspective, filtering discomforting content is a more pressing issue than merely addressing disinterest, and this study focuses on addressing the former. F.0.3 Discussion of critiquing-based recommender systems. Critiquingbased systems [84, 91] adjust recommendations end-to-end based on feedback, whereas our work adopts a plug-and-play filtering approach, enabling immediate real-world applicability. As noted in Section 6.2, our work complements algorithmic designs like critiquing systems. F.0.4 Computational overhead and scalability. There are three areas where LLMs are needed: (1) to determine whether an item should be filtered, (2) for conversational agents, and (3) to build preference profiles. Among these, (1) and (3) have higher real-time requirements. For (1), assume a user has \ud835\udc5a filtering rules. Processing each item requires 1 + \ud835\udc5a LLM calls, including one call for analyzing the item and \ud835\udc5a calls to check individually whether the item should be filtered according to each filtering rule. In the user study, we recruited 24 participants, and they did not notice any significant delay with the LLM service during their use. For (3), suppose the algorithm recommends \ud835\udc41 items, and a user clicks on \ud835\udc5b of them ( \ud835\udc41 \u226b \ud835\udc5b ). One direct way to construct a user's preference profile is to analyze all the items recommended by the algorithm, treating the clicked items as positive samples and the unclicked ones as negative samples. This process is similar to pointwise matrix factorization in collaborative filtering, with a complexity of \ud835\udc42 ( \ud835\udc41 ) . However, we adopted the pairwise ranking approach, where for each positive sample clicked by the user, a negative sample is sampled, reducing the complexity to \ud835\udc42 ( 2 \ud835\udc5b ) (with \ud835\udc5b positive samples and corresponding \ud835\udc5b negative samples). This process is similar to the matrix factorization of BPR Loss in collaborative filtering with a negative sampling ratio equal to 1. Additionally, in the user study, we found that users could smoothly use the tool without experiencing delays, indicating good real-time performance. We are working hard to publish our work to the extension store of the Chrome browser, where users can access commercial LLM services through a private API Key. Meanwhile, we are exploring collaborations with commercial partners, and when our work is applied to real-world applications, certain engineering designs will be considered, such as processing items offline and using different sizes of LLMs for different tasks. In the future, we believe that all our services, including LLM services, have the potential to run entirely on mobile devices. This is also why we chose to conduct offline experiments with the Phi series LLM, which has 3.8 billion parameters. From its inception, the Phi series model was designed with running on mobile devices in mind, and we believe that LLMs on user devices will become a reality in the near future. F.0.5 Discussion on the echo chamber problem. Our work provides users with understandable and editable preference profiles, giving them greater agency over the recommended content. In this context, if systems similar to the critiquing-based recommender systems can effectively capture users' preferences in real time and then deliver diverse recommendations, it becomes possible to prevent the formation of information silos. This discussion further underscores why we believe our human-centered approach complements algorithmcentered work in the research and development of recommender systems. F.0.6 Comparison with recommendation unlearning. Recommendation unlearning emphasizes the model's capability, with a primary focus on the model's completeness, efficiency, and performance after forgetting. These methods typically predefine which interactions should be forgotten, assess the completeness of forgetting through Membership Inference Attacks (MIA), measure the efficiency of forgetting through runtime analysis, and evaluate post-forgetting performance through recommendation accuracy. In contrast, DiscomfortFilter filters interactions from the perspective of user needs-due to its subjectivity, it is difficult to clearly define which interactions should be filtered in advance. Therefore, it is hard to make a quantitative comparison with recommendation unlearning, as current unlearning methods cannot handle subjective user inputs. We believe that these two types of work focus on model design and human-centered design, respectively, and are complementary to each other. F.0.7 The effect of ephemeral filtering criteria. We first selected filtering rules with more than 600 items processed. For each group of 30 items, we calculated the filtering burden of the rules. The results showed that the average filtering burden of the rules decreased in oscillations, and the corresponding fitted curve yielded conclusions similar to those from the analysis at a daily granularity. F.0.8 Only the MIND dataset was used in the offline experiment. DiscomfortFilter constructs positive and negative sample pairs by observing the content recommended by the algorithm and the user's click behavior on that content. Specifically, our negative sampling is not performed randomly from the entire item set; instead, it is selected from items that appeared alongside the clicked content but were not clicked. This approach places high demands on the dataset and is often difficult to satisfy. Among commonly used datasets, we found that only the MIND dataset provides both unclicked items that appeared alongside the clicked ones and side information for those items. Additionally, some less common datasets, such as the ZhihuRec-Dataset, provide unclicked items but lack detailed side information, offering only token vectors. Furthermore, while classic CTR datasets include labels indicating whether a user clicked on an item, they are not suitable for our purpose, as their features are anonymous and do not contain textual information. F.0.9 Insufficient diversity of platform and user in user study. During the user study, we ceased recruiting participants when we observed that additional participants were no longer providing new insights. Due to the substantial time investment required for user research (including interviews, transcription, coding, and analysis), it became challenging to expand the study further. For instance, two related studies recruited 15 and 12 participants, respectively [27, 82]. We are currently trying to collaborate with industry partners to implement the DiscomfortFilter concept in real-world recommender systems, which will allow for broader research in this area. F.0.10 Can DiscomfortFilter be used to capture interest? We believe the process can be understood as re-ranking algorithmically recommended content on the user side. We agree that the DiscomfortFilter could indeed be applied for such purposes and believe this might be a very promising direction for the future. F.0.11 Is it possible to use the 'Not Interested' button to enhance the feedback? While we agree that the 'Not Interested' button can enhance feedback, we find it inappropriate for our goals. In the formative study, we identified drawbacks of the 'Not Interested' button, such as its lack of personalization, which prevents users from providing nuanced feedback, and its lack of flexibility, which can result in content disappearing permanently even if the disinterest is temporary. Including such a button could introduce uncontrollability and potentially harm users. F.0.12 DiscomfortFilter or DisinterestFilter? In our formative study, we observed that most users do not have strong hostility toward uninteresting content but are more averse to 'discomforting' content. Thus, from users' perspective, filtering discomforting content is a more pressing issue than merely addressing disinterest, and the study focuses on the former. That said, DiscomfortFilter could also be used to filter uninteresting content-the tool's functionality depends on the user. F.0.13 The A/B test for the control group. Our findings revealed that DiscomfortFilter's design inherently accounts for contestability, ensuring transparency and user control over its decisions. For instance, users can review filtering log and understand the rationale behind each filtering record, with the ability to make adjustments if the filtering records do not meet their expectations. Consequently, when we presented users with a mock filtering interface-where no actual or only random filtering occurred-they quickly discerned they were part of the control group, leading to immediate expressions of dissatisfaction. Moreover, during the user study in current version, participants expressed satisfaction with the performance of the functionalities, not just their design. In summary, we posit that the mere presence of a filtering interface is unlikely to enhance user satisfaction. Instead, satisfaction levels are significantly influenced by the efficacy of the actual filtering performance. Regarding other design elements, such as explanations of preference profiles and filtering rules, these strategies emerged from our participatory design process as necessities identified by participants. Furthermore, during the evaluation phase, these features garnered high satisfaction in both questionnaires and interviews with regards to their effects and design. Therefore, we believe that our current conclusions sufficiently demonstrate the effectiveness of these designs without necessitating a control group experiment. In conclusion, the results of the A/B test do not alter our original conclusions."}
