{"title": "CAPRI-FAIR: Integration of Multi-sided Fairness in Contextual POI Recommendation Framework", "authors": "Jeffrey Chan; Francis Zac; Dela Cruz; Flora D Salim; Yonchanok Khaokaew", "pub_date": "2024-08-14", "abstract": "Point-of-interest (POI) recommendation considers spatio-temporal factors like distance, peak hours, and user check-ins. Given their influence on both consumer experience and POI business, it's crucial to consider fairness from multiple perspectives. Unfortunately, these systems often provide less accurate recommendations to inactive users and less exposure to unpopular POIs. This paper develops a post-filter method that includes provider and consumer fairness in existing models, aiming to balance fairness metrics like item exposure with performance metrics such as precision and distance. Experiments show that a linear scoring model for provider fairness in re-scoring items offers the best balance between performance and long-tail exposure, sometimes without much precision loss. Addressing consumer fairness by recommending more popular POIs to inactive users increased precision in some models and datasets. However, combinations that reached the Pareto front of consumer and provider fairness resulted in the lowest precision values, highlighting that tradeoffs depend greatly on the model and dataset.", "sections": [{"heading": "Introduction", "text": "Recommender systems are pivotal in digital environments, tailoring content across platforms like Spotify, YouTube, TikTok, and Facebook. Specifically, point-of-interest (POI) recommender systems suggest locations such as restaurants and stores, emphasizing geographical proximity as a crucial factor [19]. While these systems significantly enhance user experience, the critical issue of fairness within these algorithms has gained substantial attention. Research in machine learning fairness has evolved, addressing various definitions and the multifaceted nature of biases in recommender systems [6,9]. Existing work [1,2] has begun to address multi-stakeholder recommendation frameworks that consider the needs of both consumers and providers. Consumers benefit most from recommendations that accurately reflect their interests and intentions, while providers gain from equitable exposure of their items. Despite these advancements, current models often struggle to balance these interests effectively without sacrificing the quality of recommendations or exacerbating existing biases.\nOur research targets this gap by integrating a 'fairness factor' into POI recommender systems through post-processing techniques adaptable to various underlying models. This approach aims to refine the balance between provider and consumer interests, often overlooked in conventional systems. Additionally, we explore multiobjective fairness in POI recommender systems. This area has seen limited investigation, particularly in how different fairness objectives can be simultaneously optimized without detriment to the overall system performance [11,12].\nThis paper contributes to the field by proposing a methodology that reconciles the often conflicting objectives of different stakeholders within the POI recommendation context. By doing so, we aim to push the boundaries of what is currently possible in fairnessoriented recommender systems, providing a template for future research to build upon [10,13]. We pose the following research questions to guide our investigation:\nRQ1: How does the scoring model for the provider fairness factor affect the recommender system's performance? RQ2: What are the impacts of the provider fairness factor on performance, fairness, and distance metrics compared to the consumer fairness factor? RQ3: How does the recommender system perform when integrating provider and consumer fairness factors? Aligned with these questions, our investigation will test several hypotheses: H1: The exposure model has an impact on the system's performance. H2: The provider fairness factor improves long-tail item exposure and the consumer fairness factor enhances precision for inactive users, both without compromising accuracy. H3: Integrating both provider and consumer fairness factors potentially improves GCE fairness metrics. To address these questions, we explore the complexities of designing fairness-oriented systems. The literature supports various fairness definitions, from Aristotle's principle of treating equals equally, to more nuanced considerations of group and individual fairness in machine learning contexts [5,6]. The spatial dimensions of POI recommendations, which consider the proximity of recommended sites to the user, add another layer of complexity, especially given the often sparse data on user interactions with potential POIs [7,19]. This study does not only address theoretical aspects of fairness and performance but also practical implementations, examining existing methodologies and proposing new ways to balance consumer and provider interests within the framework of POI recommender systems. By integrating considerations of geographical closeness, social connections, and categorical preferences, we aim to formulate a comprehensive system that aligns with contemporary needs for fairness and utility in digital recommendations.", "publication_ref": ["b18", "b5", "b8", "b0", "b1", "b10", "b11", "b9", "b12", "b4", "b5", "b6", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Modelling the Task and the Fairness Factors", "text": "Problem Statement This project aims to develop a recommender system model that incorporates a user \ud835\udc62 \u2208 \ud835\udc48 , a point-of-interest (POI) \ud835\udc5d \u2208 \ud835\udc43, and contextual factors \ud835\udc36 (\ud835\udc62, \ud835\udc5d) (e.g., distance, time, social connections), all of which are normalized to ensure values range between 0 and 1. Additionally, it integrates consumer and provider fairness through abstract functions \ud835\udc39 \ud835\udc50 (\ud835\udc62) and \ud835\udc39 \ud835\udc5d (\ud835\udc62), which also range from 0 to 1 for consumer and provider fairness, respectively. The model, denoted by M, combines these contexts to produce personalized POI recommendations. Fairness is incorporated into this framework by adjusting the ranking score as follows:\nM(\ud835\udc62, \ud835\udc5d) = M (\ud835\udc62, \ud835\udc5d, \ud835\udc36 (\ud835\udc62, \ud835\udc5d)) + \ud835\udefc \u2022 \ud835\udc39 \ud835\udc5d (\ud835\udc62) + \ud835\udefd \u2022 \ud835\udc39 \ud835\udc50 (\ud835\udc62)(1)\nwhere the tunable parameters \ud835\udefc and \ud835\udefd modulate the impact of fairness considerations and ensure the score M(\ud835\udc62, \ud835\udc5d) remains within the range of 0 to 1. Different configurations will be evaluated to illustrate their effects on system performance and fairness, aiming to identify optimal balances in a Pareto front.\nIn this front, we propose a framework CAPRI-FAIR 1 , where we extend a pre-existing framework for context-aware POI recommendation and incorporate fairness factors. We also add evaluations that consider user and item fairness.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Consumer Fairness", "text": "Consumer fairness addresses the disparity between active and inactive users on the platform, distinguished by their frequency of check-ins. Active users benefit from more personalized recommendations due to their extensive interaction data, enhancing profile accuracy. Conversely, for inactive users, who may need a basic understanding of popular options to start exploring, a strategy 1 https://github.com/cruiseresearchgroup/CAPRI-FAIR of recommending widely popular POIs proves beneficial. This approach aligns with findings from Rahmani [15], indicating that less active users frequently attend popular POIs.\nWe propose recommending nearby, popular POIs to inactive users, defining \"nearby\" as POIs within the closest 20% of distances to any previously visited POI [18]. This method focuses on enhancing recommendation precision for inactive users without altering the accuracy of active ones.", "publication_ref": ["b14", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Provider Fairness", "text": "Each POI has an associated popularity, indicated by user check-ins, which follows a power law distribution. Most check-ins are concentrated among a few POIs, while a large majority fall into the 'long tail' with minimal exposure. This distribution characteristic underscores the challenge of achieving fairness among providers, as most POIs receive scant attention. To address this, we model popularity with a power law, where the score of a POI is inversely proportional to its check-in count, helping to elevate less popular POIs. For a given number of \ud835\udc65 check-ins, the estimated number of POIs \ud835\udc66 with that popularity is expressed inline as \ud835\udc66 = \ud835\udc64 0 \u2022 \ud835\udc65 \ud835\udc64 1 . This relationship undergoes a log-log transformation to fit a linear regression, where parameters are determined using L2 regularization to manage data skew, with a regularization factor of 10.0 ensuring stable predictions. In addition to the power law model, we explore linear and logistic regression models for comparison. The linear model does not scale down exposure as sharply with increasing popularity, offering a more gradual adjustment for moderately popular POIs. In contrast, logistic regression treats slight increases in popularity more harshly, impacting even moderately popular POIs significantly. To assess provider fairness, we measure each item's exposure across recommendations, where each POI is assumed to represent a unique provider. Exposure (\ud835\udc38) is quantified using an attention function \ud835\udc4e(\ud835\udc5d, \ud835\udc43 \ud835\udc45 ), which, for simplicity, is binary in our experiments, indicating whether an item \ud835\udc5d is present or absent in the recommendation list. The exposure of an item \ud835\udc5d is defined as (\ud835\udc38 \ud835\udc5d = \ud835\udc43 \ud835\udc45 \u2282\ud835\udc43 \ud835\udc4e(\ud835\udc5d, \ud835\udc43 \ud835\udc45 )), aligning with methods in related work that scale exposure based on item visibility or rank within the list, facilitating comparison of different ranking algorithms' impact on provider fairness.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment Setup Datasets", "text": "We employ the Gowalla [3] and Yelp [8] datasets, recognized for their comprehensive feature sets which facilitate model comparisons [14,18]. Following standard preprocessing, we remove POIs visited by fewer than 10 users and users who have visited fewer than 10 POIs. After this filtering process, the dataset includes 7135 users for Yelp and 5628 users for Gowall. We divide each user's sequence of POI visits into 70% training, 20% validation, and 10% testing, maintaining the chronological order to simulate real-world scenarios and ensure robust evaluations of our recommendation system. Users are categorized as 'active' or \"inactive' based on their activity, with the top 20% being active. This is based on the defaults given by the original CAPRI framework [17] and the Pareto principle (i.e., where 20% of users account for 80% of check-ins). Items are similarly divided into 'short-head' and 'long-tail' groups based on the top 20% of popularity metrics. ", "publication_ref": ["b2", "b7", "b13", "b17", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Metrics", "text": "For evaluation, we primarily use Precision@k as our accuracy metric, which assesses if the target POI is among the top \ud835\udc58 entries in the recommendation list-suitable for scenarios with sparse hits. We measure both Precision and Exposure across different groups to evaluate user and item fairness. For fairness assessment, we employ the Generalized Cross-Entropy (GCE) with Pearson's \ud835\udf12 2 measure, noted for its robustness to outliers [4,15]. This measure quantifies item exposure disparities, focusing on long-tail exposure improvements. For consumer fairness, recommendation gain for each user is defined based on the presence of recommended POIs in their previous visits. Similarly, provider fairness is evaluated by whether POIs appear in recommendation lists. Each POI's latitude and longitude are used to compute distances, and the user's position is averaged from the coordinates of their previously visited POIs. Distances are then calculated using the standard geographical distance formula, ensuring accurate measurement of proximity.", "publication_ref": ["b3", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Benchmarking and Experimentation", "text": "Our model is benchmarked against three widely used POI recommenders [16][17][18]20]: USG [19], GeoSoCa [21], and LORE [22]. USG integrates user preferences, social influence, and geographic data. GeoSoCa leverages geographical, social, and categorical correlations, while LORE combines geographic, temporal, and social contexts. These methods are chosen for their robust integration of diverse contextual factors crucial for effective POI recommendations. Future evaluations may extend to include graph-based, deep learning-based, and generative AI-based methods for a more comprehensive comparison. Consistent datasets are used as per [17] to maintain comparability.\nOur experiments systematically evaluate the effects of fairness factors on model performance and fairness metrics. We assess three exposure models-power law, linear, and logistic-across \ud835\udefc values from 0 to 1, analyzing their impact on precision and long-tail exposure at \ud835\udc58 = 10. Both provider and consumer fairness factors are explored for their influence on item exposure and precision for inactive users. We employ the Kruskal-Wallis test to assess differences among the three exposure models starting at \ud835\udefc = 0.25. Pairwise Mann-Whitney U tests further analyze specific model comparisons, enhancing our evaluation of each model's impact.", "publication_ref": ["b15", "b16", "b17", "b19", "b18", "b20", "b21", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Results and Discussion", "text": "The benchmark models were initially run without modifications, then progressively with increased provider fairness factor \ud835\udefc coefficients to assess the impact delineated in H1. The first assessment focused on the effect of the provider fairness model on precision and long-tail exposure metrics, which it aims to improve. Results presented in Table 1 highlight the outcomes of these evaluations.\nFor precision, the key performance metric, the linear exposure model consistently outperforms others, as indicated in bold in the results. This model's relative leniency on moderately popular POIs is illustrated by the curves in Figure 5 in the appendix. Conversely, the logistic model boosts exposure for the least popular POIs. The power law model, however, underperforms for the GeoSoCa and LORE models, particularly at higher \ud835\udefc weights. Differences between the exposure models validate H1 that the choice of exposure model is crucial. Specifically, the precision of the linear model is statistically significantly greater than that of the power law model. Pairwise comparisons between the linear and logistic models and logistic and power law models show no significant differences. Therefore, the linear model is chosen for the provider fairness factor due to its superior performance. We acknowledge this choice's limitations and will include a comprehensive table of all pairwise results across models in Appendix A.3 for detailed comparison.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": ["tab_0"]}, {"heading": "Impact of the provider fairness factor", "text": "After adopting the linear model as the provider fairness factor, we observed its effects on performance metrics related to H2. As shown in Table 1, increasing \ud835\udefc reduces precision but significantly enhances long-tail exposure. For instance, in the USG model, where baseline exposure is nearly zero, the increase in \ud835\udefc substantially boosts exposure, as detailed in Figure 1. This supports H2, demonstrating that provider fairness can enhance visibility for underrepresented POIs without overly compromising precision. As depicted in Figure 2, adjusting \ud835\udefd in the Yelp dataset initially improves precision for the GeoSoCa and LORE models from 2.3% to 2.6%. However, further increases cause precision to decline, notably in the USG model, where it drops from 3.0% to 2.8%. A Wilcoxon signed rank test confirms significant changes in precision across @5 @10 @20 @5 @10 @20 @5 @10 @20 @5 @10 @20 GeoSoCa (0.0, 0. models with minor adjustments in \ud835\udefd, underscoring the impact of fairness modifications on recommender systems. ", "publication_ref": [], "figure_ref": ["fig_0", "fig_1"], "table_ref": ["tab_0"]}, {"heading": "Tradeoff between provider and consumer fairness", "text": "Exploring the impact of combining provider and consumer fairness factors, as outlined in H3, can be seen through GCE metrics for different tradeoffs, detailed in Figure 3 and Table 2. This figure plots the tradeoff between user precision GCE and item exposure GCE metrics, using color to denote precision levels and shapes to differentiate between models and \ud835\udc58 values. Across the Yelp and Gowalla datasets, the Pareto front typically occupies the upper right region of each plot, highlighting a significant tradeoff where high user GCE often comes at the expense of item GCE, especially evident in the USG model which shows optimal precision. Table 2 further reveals that while GeoSoCa and LORE achieve the best precision with full weight on consumer fairness, this approach diminishes provider fairness. Conversely, the USG model demonstrates better performance with balanced fairness weights. Such adjustments to fairness weights reveal nuanced impacts on mean median distances, generally enhancing provider fairness across all models. This analysis underscores how specific properties of each model and dataset characteristics influence the effectiveness of combined fairness strategies, thus affecting overall outcomes in POI recommendation systems. For more details, see Appendix A.4.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Conclusion", "text": "To integrate consumer and provider fairness into pre-existing POI recommendation models, a post-filter approach was employed using 2 fairness factors added into the pre-filter score with varying weights. The provider factor gives higher scores for less popular POIs, using a linear model fit to the popularity distribution, so moderately popular recommendations are not severely penalized. The consumer factor attempted to increase recommendation quality for inactive users by recommending more popular POIs near previously visited ones. The results show that increasing the provider factor's weight resulted in improved exposure to the long-tail, up to 3-fold in the Yelp dataset, especially for USG where exposure was almost none. This came at the cost of lower precision, varying between models. In some cases, the consumer factor improved precision but failed to enhance performance for inactive users. Nevertheless, combining both factors can yield reasonable results. Overall, there is a tradeoff between these factors, with the provider fairness factor achieving better long-tail exposure with minimal precision decreases for models like USG. Unfortunately, when comparing GCE metrics across models, a distinct correlation exists between low item exposure GCE values and higher precision, although this depends on the dataset, as the LORE model reached a middle ground between the metrics in the Gowalla dataset.\nIn the future, we plan to expand our research to include various models and strategies for consumer fairness and other objectives. We will explore different splitting methods and refine our accuracy assessments, aiming to comprehensively address complex fairness issues and enhance POI recommender systems' effectiveness.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Auxiliary A.1 Evaluation Metrics", "text": "For evaluation, we will primarily use Precision@k as our accuracy metric, as this is the most commonly used one in the field. Precision@k simply checks if the target POI to be predicted lies in the first \ud835\udc58 entries in the recommendation list, which would suffice for a use case where hits (i.e. visits to a POI) are sparse.\nWe can measure precision and exposure across the user and item groups to evaluate user and item fairness. For a single measurable metric of fairness, the Generalized Cross-Entropy (GCE) of the metric distribution can be used for both cases [15] [4]. For categorical attributes \ud835\udc4e \u2208 \ud835\udc34 with respect to which fairness will be computed, the GCE for a given model \ud835\udc5a is defined as such:\n\ud835\udc3a\ud835\udc36\ud835\udc38 (\ud835\udc5a, \ud835\udc4e) = 1 \ud835\udefd \u2022 (1 -\ud835\udefd) \u2211\ufe01 \ud835\udc4e \ud835\udc57 \ud835\udc5d \ud835\udefd \ud835\udc53 (\ud835\udc4e \ud835\udc57 )\ud835\udc5d (1-\ud835\udefd ) \ud835\udc5a (\ud835\udc4e \ud835\udc57 ) -1 (2)\nHere, \ud835\udc5d \ud835\udc5a and \ud835\udc5d \ud835\udc53 represent the metric distributions of the model outputs and a theoretical fair model, respectively. The index \ud835\udefd represents the index of the unfairness measure being used, which includes the Hellinger distance (\ud835\udefd = 1\n2 ), Pearson's \ud835\udf12 2 (\ud835\udefd = 2), Kullback-Leibler divergence (lim \ud835\udefd\u21921 ), and a few others. As mentioned in [4], the Pearson's \ud835\udf12 2 measure is more robust to outliers, so a value of \ud835\udefd = 2 will be used.\nTo adapt this GCE formulation to consumer fairness, we define a recommendation gain for each user based on the lists recommended to the user. Each of the POIs has an associated gain to them. In the case of recommendation hits or relevance, we can add 1 for each POI in each recommendation result that corresponds to one visited by the user. We can define this similarly to the attention function used in calculating item exposure \ud835\udc4e(\ud835\udc5d, \ud835\udc43 \ud835\udc45 ). Suppose we define \ud835\udf19 (\ud835\udc62, \ud835\udc5d) as an indicator function that equals either 0 or 1 depending on whether or not the user has visited the POI. In that case, we can define the recommendation gain, as well as the overall metric distribution as follows:\n\ud835\udc5d \ud835\udc5a (\ud835\udc4e \ud835\udc56 ) = 1 \ud835\udc4d \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 \ud835\udc4e \ud835\udc56 \ud835\udc5f\ud835\udc54 \ud835\udc62 = 1 \ud835\udc4d \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 \ud835\udc4e \ud835\udc56 \u2211\ufe01 \ud835\udc43 \ud835\udc45\ud835\udc62 \u2282\ud835\udc43 \u2211\ufe01 \ud835\udc5d \u2208\ud835\udc43 \ud835\udc45\ud835\udc62 \ud835\udf19 (\ud835\udc62, \ud835\udc5d) \u2022 \ud835\udc4e(\ud835\udc5d, \ud835\udc43 \ud835\udc45\ud835\udc62 ) (3)\nNote that \ud835\udc4d is a simple normalization factor which ensures that the sum of the metric distribution values equals 1. We can define a similar metric for provider fairness. We tweak the indicator function to indicate if a POI is in a recommendation list, and restate it as \ud835\udeff (\ud835\udc5d, \ud835\udc43 \ud835\udc45\ud835\udc62 ). The resulting recommendation gain for POIs is as follows:\n\ud835\udc5d \ud835\udc5a (\ud835\udc4e \ud835\udc56 ) = 1 \ud835\udc4d \u2211\ufe01 \ud835\udc56 \u2208\ud835\udc3c \ud835\udc4e \ud835\udc56 \ud835\udc5f\ud835\udc54 \ud835\udc56 = 1 \ud835\udc4d \u2211\ufe01 \ud835\udc56 \u2208\ud835\udc3c \ud835\udc4e \ud835\udc56 \u2211\ufe01 \ud835\udc43 \ud835\udc45\ud835\udc62 \u2282\ud835\udc43 \ud835\udeff (\ud835\udc5d, \ud835\udc43 \ud835\udc45\ud835\udc62 )(4)\nBecause of the constraints of the POI recommendation task, metrics with respect to these constraints can also be defined. For geographical distance, a simple average distance of recommended POIs to users may be employed.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Modelling the provider fairness factor using various functions", "text": "As mentioned above, the popularity of POIs follows a power law distribution, which can be modelled with a power law, where the score of a POI is inversely proportional to its check-in count, helping to elevate less popular POIs. The histogram of popularity values are shown in Figure 4, which includes the power law model determined via a linear regression on the log-log transformed data. In addition to the power law, we also use linear and logistic models that represent different methods of scaling the fairness factor to popularity. These are also plotted in Figure 5.   6, referencing the values in Table 1. Note that the differences in long-term exposure are immediately significant even for low factor weights. The significance in differences in precision come a little bit later with higher factor weights, specifically in the Yelp dataset.", "publication_ref": [], "figure_ref": ["fig_3", "fig_4"], "table_ref": ["tab_0"]}, {"heading": "A.4 More results on the tradeoff between provider and consumer fairness", "text": "Now that we have the 2 fairness factors to consider, the next question is to see how a combination of the 2 will affect the same metrics.\nFor the graphs, a weight of 1.0 is shared between the consumer and provider factors. Although we see that even a high consumer fairness factor will decrease accuracy metrics for the USG model, it still affects it less than a high provider fairness factor. For GeoSoCa and LORE in particular, the performance increases as more weight is given to consumer fairness, and away from producer fairness. USG shows a peak midway, at a weight distribution of \ud835\udefc = 0.3, \ud835\udefd = 0.7 for the Yelp dataset, and \ud835\udefc = 0.2, \ud835\udefd = 0.8 for the Gowalla dataset. When looking at the fairness metrics for both long-tail items and inactive users, we see the expected changes, where an increase in the consumer fairness factor's weight corresponds to a decrease in long-tail exposure, and an increase in precision for inactive users. In particular, even a small addition of consumer fairness weight can cause a large decrease in exposure, especially for USG, where a weight of \ud835\udefd = 0.2 can cause a 3-fold decrease in long-tail exposure.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5 Dataset statistics", "text": "The table below displays the details about the 2 datasets used in the study, Yelp and Gowalla. Note that the sparsity is below even 1%, highlighting the sparsity of this kind of recommendation task. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Multistakeholder recommendation: Survey and research directions", "journal": "User Modeling and User-Adapted Interaction", "year": "2020", "authors": "Himan Abdollahpouri; Gediminas Adomavicius; Robin Burke; Ido Guy; Dietmar Jannach; Toshihiro Kamishima; Jan Krasnodebski; Luiz Pizzato"}, {"ref_id": "b1", "title": "Balanced neighborhoods for multi-sided fairness in recommendation", "journal": "PMLR", "year": "2018", "authors": "Robin Burke; Nasim Sonboli; Aldo Ordonez-Gauger"}, {"ref_id": "b2", "title": "Friendship and mobility: user movement in location-based social networks", "journal": "", "year": "2011", "authors": "Eunjoon Cho; Seth A Myers; Jure Leskovec"}, {"ref_id": "b3", "title": "A flexible framework for evaluating user and item fairness in recommender systems", "journal": "", "year": "2021", "authors": "Yashar Deldjoo; Vito Walter Anelli; Hamed Zamani; Alejandro Bellogin; Tommaso Di; Noia "}, {"ref_id": "b4", "title": "Fairness through Awareness (ITCS '12)", "journal": "Association for Computing Machinery", "year": "2012", "authors": "Cynthia Dwork; Moritz Hardt; Toniann Pitassi; Omer Reingold; Richard Zemel"}, {"ref_id": "b5", "title": "Fairness in information access systems", "journal": "Foundations and Trends\u00ae in Information Retrieval", "year": "2022", "authors": "Anubrata Michael D Ekstrand; Robin Das; Fernando Burke;  Diaz"}, {"ref_id": "b6", "title": "Lars: A location-aware recommender system", "journal": "IEEE", "year": "2012", "authors": "Justin J Levandoski; Mohamed Sarwat; Ahmed Eldawy; Mohamed F Mokbel"}, {"ref_id": "b7", "title": "An experimental evaluation of point-of-interest recommendation in location-based social networks", "journal": "", "year": "2017", "authors": "Yiding Liu; Tuan-Anh Nguyen Pham; Gao Cong; Quan Yuan"}, {"ref_id": "b8", "title": "A survey on bias and fairness in machine learning", "journal": "ACM Computing Surveys (CSUR)", "year": "2021", "authors": "Ninareh Mehrabi; Fred Morstatter; Nripsuta Saxena; Kristina Lerman; Galstyan "}, {"ref_id": "b9", "title": "Sustainability-Oriented Recommender Systems", "journal": "Association for Computing Machinery", "year": "2023", "authors": "Pavel Merinov"}, {"ref_id": "b10", "title": "Cpfair: Personalized consumer and producer fairness re-ranking for recommender systems", "journal": "", "year": "2022", "authors": "Mohammadmehdi Naghiaei; Hossein A Rahmani; Yashar Deldjoo"}, {"ref_id": "b11", "title": "Fairrec: Two-sided fairness for personalized recommendations in two-sided platforms", "journal": "", "year": "2020", "authors": "Arpita Gourab K Patro; Niloy Biswas; Krishna P Ganguly; Abhijnan Gummadi;  Chakraborty"}, {"ref_id": "b12", "title": "Towards safety and sustainability: Designing local recommendations for post-pandemic world", "journal": "", "year": "2020", "authors": "Abhijnan Gourab K Patro; Ashmi Chakraborty; Niloy Banerjee;  Ganguly"}, {"ref_id": "b13", "title": "The role of context fusion on accuracy, beyond-accuracy, and fairness of point-of-interest recommendation systems", "journal": "Expert Systems with Applications", "year": "2022", "authors": "A Hossein; Yashar Rahmani; Tommaso Deldjoo; Noia Di"}, {"ref_id": "b14", "title": "The unfairness of active users and popularity bias in point-of-interest recommendation", "journal": "Springer", "year": "2022-04-10", "authors": "A Hossein; Yashar Rahmani; Ali Deldjoo; Mohammadmehdi Tourani;  Naghiaei"}, {"ref_id": "b15", "title": "Exploring the Impact of Temporal Bias in Point-of-Interest Recommendation", "journal": "", "year": "2022", "authors": "A Hossein; Mohammadmehdi Rahmani; Ali Naghiaei; Yashar Tourani;  Deldjoo"}, {"ref_id": "b16", "title": "CAPRI: Context-Aware Interpretable Point-of-Interest Recommendation Framework", "journal": "", "year": "2023", "authors": "Ali Tourani; A Hossein; Mohammadmehdi Rahmani; Yashar Naghiaei;  Deldjoo"}, {"ref_id": "b17", "title": "Points of interest recommendations: methods, evaluation, and future directions", "journal": "Information Systems", "year": "2021", "authors": "Heitor Werneck; Nicollas Silva; Matheus Viana; C M Adriano; Fernando Pereira; Leonardo Mourao;  Rocha"}, {"ref_id": "b18", "title": "Exploiting geographical influence for collaborative point-of-interest recommendation", "journal": "", "year": "2011", "authors": "Mao Ye; Peifeng Yin; Wang-Chien Lee; Dik-Lun Lee"}, {"ref_id": "b19", "title": "A category-aware deep model for successive POI recommendation on sparse check-in data", "journal": "", "year": "2020", "authors": "Fuqiang Yu; Lizhen Cui; Wei Guo; Xudong Lu; Qingzhong Li; Hua Lu"}, {"ref_id": "b20", "title": "Geosoca: Exploiting geographical, social and categorical correlations for point-of-interest recommendations", "journal": "", "year": "2015", "authors": "Jia-Dong Zhang; Chi-Yin Chow"}, {"ref_id": "b21", "title": "Lore: Exploiting sequential influence for location recommendations", "journal": "", "year": "2014", "authors": "Jia-Dong Zhang; Chi-Yin Chow; Yanhua Li"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Long-tail Exposure v.s. Provider fairness factor \ud835\udefc Impact of the consumer fairness factor Increasing the weight of the consumer fairness factor, denoted as \ud835\udefd, aims to enhance precision for inactive users, aligning with H3.As depicted in Figure2, adjusting \ud835\udefd in the Yelp dataset initially improves precision for the GeoSoCa and LORE models from 2.3% to 2.6%. However, further increases cause precision to decline, notably in the USG model, where it drops from 3.0% to 2.8%. A Wilcoxon signed rank test confirms significant changes in precision across", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Precision@10 v.s. Consumer fairness factor \ud835\udefd", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Scatterplot showing the tradeoff between GCE for users, GCE for items, and precision", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Histogram of popularity /check-in counts in the Yelp training dataset, with ridge regression linear model, \ud835\udefc = 10.0.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Different provider fairness scoring models.", "figure_data": ""}, {"figure_label": "896", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 :Figure 9 :Figure 6 :896Figure 8: Exposure for Long tail items (bottom 80%)", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Precision@10 v.s. Tradeoff between consumer and provider fairness factors", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Precision and Long-tail Exposure by Provider Fairness Weights and Exposure Models in CAPRI-FAIR framework.", "figure_data": "DatasetYelpGowallaModelExposure ModelPrecisionLong-tail ExposurePrecisionLong-tail Exposure0.250.500.7510.250.500.7510.250.500.7510.250.500.751GeoSoCaLinear0.0134 0.0117 0.0111 0.0107 3.4467 4.15254.50264.68880.0300 0.0287 0.0274 0.0266 0.97961.07341.16261.2465Logistic0.0173 0.00890.00730.00733.0696 4.5693 4.6969 4.6970 0.0316 0.01820.01400.01321.2142 1.9263 2.0336 2.0459PowerLaw0.01370.01090.00960.00813.16363.52782.50251.84290.02770.02420.02220.0203 1.3231 1.80732.0207 2.0636LORELinear0.0179 0.0147 0.0132 0.0124 2.79753.61414.13614.4426 0.0427 0.0405 0.0389 0.0378 0.93561.07571.19581.2961Logistic0.01830.00960.00740.0072 3.4217 4.8180 4.9503 4.9565 0.02830.01490.01270.0125 1.6326 2.0425 2.0815 2.0842PowerLaw0.0196 0.01360.01060.00822.22842.16471.88871.57680.03920.02600.01910.01561.29831.65271.77241.7771USGLinear0.0314 0.0278 0.0228 0.0186 0.03410.54241.82143.07650.0504 0.0477 0.0446 0.0424 0.10560.15670.25230.3689Logistic0.02960.02570.01230.0042 0.0461 1.1532 4.5227 5.6810 0.05000.03340.01950.0103 0.2319 1.4315 2.0190 2.2654PowerLaw0.0300 0.0291 0.01520.00360.02090.27912.94183.9288 0.0525 0.04680.02930.01580.15130.78761.65312.0789"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Precision, GCE, and Mean Median Distance Metrics for Different Weights of Providers and Consumers, Yelp Dataset.", "figure_data": "ModelFairness WeightsPrecisionItem fairness GCEUser fairness GCEMean median distance(\ud835\udefc, \ud835\udefd)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Figure 10: Details about the datasets used.", "figure_data": "YelpGowallaUsers7,1355,628POIs16,62131,803Checkins774,320483,846Sparsity0.65%0.27%Active/Inactive Users1,427 / 5,7081,125 / 4,503Long-tail/Short-head12,413 / 3,16224,700 / 6,243POIsLong-tail/Short-head280,255 / 494,065 164,459 / 319387Check-ins"}], "formulas": [{"formula_id": "formula_0", "formula_text": "M(\ud835\udc62, \ud835\udc5d) = M (\ud835\udc62, \ud835\udc5d, \ud835\udc36 (\ud835\udc62, \ud835\udc5d)) + \ud835\udefc \u2022 \ud835\udc39 \ud835\udc5d (\ud835\udc62) + \ud835\udefd \u2022 \ud835\udc39 \ud835\udc50 (\ud835\udc62)(1)", "formula_coordinates": [2.0, 85.4, 490.72, 209.18, 9.98]}, {"formula_id": "formula_1", "formula_text": "\ud835\udc3a\ud835\udc36\ud835\udc38 (\ud835\udc5a, \ud835\udc4e) = 1 \ud835\udefd \u2022 (1 -\ud835\udefd) \u2211\ufe01 \ud835\udc4e \ud835\udc57 \ud835\udc5d \ud835\udefd \ud835\udc53 (\ud835\udc4e \ud835\udc57 )\ud835\udc5d (1-\ud835\udefd ) \ud835\udc5a (\ud835\udc4e \ud835\udc57 ) -1 (2)", "formula_coordinates": [6.0, 77.6, 252.28, 216.99, 23.02]}, {"formula_id": "formula_2", "formula_text": "\ud835\udc5d \ud835\udc5a (\ud835\udc4e \ud835\udc56 ) = 1 \ud835\udc4d \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 \ud835\udc4e \ud835\udc56 \ud835\udc5f\ud835\udc54 \ud835\udc62 = 1 \ud835\udc4d \u2211\ufe01 \ud835\udc62 \u2208\ud835\udc48 \ud835\udc4e \ud835\udc56 \u2211\ufe01 \ud835\udc43 \ud835\udc45\ud835\udc62 \u2282\ud835\udc43 \u2211\ufe01 \ud835\udc5d \u2208\ud835\udc43 \ud835\udc45\ud835\udc62 \ud835\udf19 (\ud835\udc62, \ud835\udc5d) \u2022 \ud835\udc4e(\ud835\udc5d, \ud835\udc43 \ud835\udc45\ud835\udc62 ) (3)", "formula_coordinates": [6.0, 57.04, 491.58, 237.54, 35.63]}, {"formula_id": "formula_3", "formula_text": "\ud835\udc5d \ud835\udc5a (\ud835\udc4e \ud835\udc56 ) = 1 \ud835\udc4d \u2211\ufe01 \ud835\udc56 \u2208\ud835\udc3c \ud835\udc4e \ud835\udc56 \ud835\udc5f\ud835\udc54 \ud835\udc56 = 1 \ud835\udc4d \u2211\ufe01 \ud835\udc56 \u2208\ud835\udc3c \ud835\udc4e \ud835\udc56 \u2211\ufe01 \ud835\udc43 \ud835\udc45\ud835\udc62 \u2282\ud835\udc43 \ud835\udeff (\ud835\udc5d, \ud835\udc43 \ud835\udc45\ud835\udc62 )(4)", "formula_coordinates": [6.0, 89.11, 591.53, 205.47, 25.35]}], "doi": "10.1145/3640457.3688170"}
