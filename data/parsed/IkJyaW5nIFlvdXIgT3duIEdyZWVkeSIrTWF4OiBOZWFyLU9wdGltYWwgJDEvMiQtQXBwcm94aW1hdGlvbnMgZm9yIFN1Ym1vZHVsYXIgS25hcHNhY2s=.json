{"title": "\"Bring Your Own Greedy\"+Max: Near-Optimal 1 /2-Approximations for Submodular Knapsack", "authors": "Dmitrii Avdiukhin; Grigory Yaroslavtsev; Samson Zhou", "pub_date": "2019-10-15", "abstract": "The problem of selecting a small-size representative summary of a large dataset is a cornerstone of machine learning, optimization and data science. Motivated by applications to recommendation systems and other scenarios with query-limited access to vast amounts of data, we propose a new rigorous algorithmic framework for a standard formulation of this problem as a submodular maximization subject to a linear (knapsack) constraint. Our framework is based on augmenting all partial Greedy solutions with the best additional item. It can be instantiated with negligible overhead in any model of computation, which allows the classic Greedy algorithm and its variants to be implemented. We give such instantiations in the offline (Greedy+Max), multi-pass streaming (Sieve+Max) and distributed (Distributed Sieve+Max) settings. Our algorithms give ( 1 /2 -)-approximation with most other key parameters of interest being near-optimal. Our analysis is based on a new set of first-order linear differential inequalities and their robust approximate versions. Experiments on typical datasets (movie recommendations, influence maximization) confirm scalability and high quality of solutions obtained via our framework. Instance-specific approximations are typically in the 0.6-0.7 range and frequently beat even the (1 -1/e) \u2248 0.63 worst-case barrier for polynomial-time algorithms.", "sections": [{"heading": "Introduction", "text": "A fundamental problem in many large-scale machine learning, data science and optimization tasks is finding a small representative subset of a big dataset. This problem arises from applications in recommendation systems [LKG + 07, EAG11, BMSC17, MBN + 17, YXC18, AMYZ19], exemplar-based clustering [GK10], facility location [LWD16], image processing [IB19], viral marketing [HMS08], principal component analysis [KGPK15], and document summarization [LB11, WLKB13,SSSJ12] and can often be formulated as constrained monotone submodular optimization under various constraints such as cardinality [BMKK14, BEM18, KMZ + 19], knapsack [HKY17], matchings [CK14], and matroids [CCPV11, AHN + 19] due to restrictions demanded by space, budget, diversity, fairness or privacy. As a result, constrained submodular optimization has been recently and extensively studied in various computational models, including centralized [NWF78], distributed [MKSK13, KMVV15, dPBENW15, MZ15, MZK16, dPBENW16, LV19], streaming [BMKK14, BFS15, NTM + 18, ASS19, KMZ + 19], and adaptive [GK11, BS18, BRS19, FMZ19, EN19b, CQ19] among others.\nIn this paper we focus on monotone submodular maximization under a knapsack constraint, which captures the scenario when the representative subset should have a small cost or size. While a number of algorithmic techniques exist for this problem, there are few that robustly scale to large data and can be easily implemented in various computing frameworks. This is in contrast with a simpler cardinality-constrained version in which only the number of elements is restricted. In this setting the celebrated Greedy algorithm of [NWF78] enjoys both an optimal approximation ratio and a simplicity that allows easy adaptation in various environments. For knapsack constraints, such a simple and universal algorithm is unlikely. In particular, Greedy does not give any approximation guarantee.\nWe develop a framework that augments solutions constructed by Greedy and its variations and gives almostfoot_0 /2-approximations 1 in various computational models. For example, in the multipass streaming setting we achieve optimal space and almost optimal number of queries and running time. We believe that our framework is robust to the choice of the computational model as it can be implemented with essentially the same complexity as that of running Greedy and its variants.", "publication_ref": ["b26", "b44", "b32", "b31", "b33", "b56", "b54", "b30", "b14", "b53", "b53"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries and our contributions", "text": "A set function f : 2 U \u2192 R is submodular if for every S \u2286 T \u2286 U and e \u2208 U it holds that f (e \u222a T ) -f (T ) \u2264 f (e \u222a S) -f (S). Moreover, f is monotone if for every S \u2286 T \u2286 U it holds that f (T ) \u2265 f (S). Intuitively, elements in the universe contribute non-negative utility, but their resulting gain is diminishing as the size of the set increases. In the monotone submodular maximization problem subject to a knapsack constraint, each item e has cost c(e). Given a parameter K > 0, the task is to maximize a non-negative monotone submodular function f (S) under the constraint c(S) := e\u2208S c(e) \u2264 K. Without loss of generality, we assume that min e\u2208S c(e) \u2265 1, which can be achieved by rescaling the costs and taking all items with cost 0. Then K = min(n, K) is an upper bound on the number of elements in any feasible solution.\nAny algorithm for submodular maximization requires query access to f . As query access can be expensive, the number of queries is typically considered one of the performance metrics. Furthermore, in some critical applications of submodular optimization such as recommendation systems, another constraint often arises from the fact that only queries to feasible sets are allowed (e.g. when click-through rates can only be collected for sets of ads which can be displayed to the users). Practical algorithms for submodular optimization hence typically only make such queries, an assumption commonly used in the literature (see e.g. [NTM + 18]). For any algorithm that only makes queries on feasible sets, it is easy to show that \u2126(nfoot_1 ) queries are required to go beyond 1 /2-approximation under various assumptions on f (Theorem 2.14). Hence it is natural to ask whether we can get a 1 /2-approximation, while keeping other performance metrics of interest nearly optimal and hence not compromising on practicality. We answer this question positively.\nWe first state the following simplified result in the most basic offline model (i.e. when an algorithm can access any element at any time) to illustrate the main ideas and then improve parameters in our other results. In this model, we are given an integer knapsack capacity K \u2208 Z + and a set E of elements e 1 , . . . , e n from a finite universe U . 2 Theorem 1.1 (Offline Greedy+Max) Let K = min(n, K). There exists an offline algorithm Greedy+Max (Algorithm 1) that gives a 1 /2-approximation for the submodular maximization problem under a knapsack constraint with query complexity and running time O Kn (Theorem 2.6).\nIn the single-pass streaming model, the algorithm is given K and a stream E consisting of elements e 1 , . . . , e n \u2208 U , which arrive sequentially. The objective is to minimize the auxiliary space used by algorithm throughout the execution. In the multi-pass streaming model, the algorithm is further allowed to make multiple passes over E. This model is typically used for modeling storage devices with sequential access (e.g. hard drives) while using a small amount of RAM. In this setting minimizing the number of passes becomes another key priority. Note that since \u2126( K) is a trivial lower bound on space and \u2126(n) is a trivial lower bound on time and query complexity of any approximation algorithm that queries feasible sets, our next result is almost optimal in most parameters of interest.\nTheorem 1.2 (Multi-pass streaming algorithm Sieve+Max) Let K = min(n, K). There exists a multi-pass streaming algorithm Sieve+Max (Algorithm 2) that uses O K space and O (1/ ) passes over the stream and outputs a ( 1 /2-)-approximation to the submodular maximization problem under a knapsack constraint, with query complexity and running time 3 O n(1/ + log K) (see Theorem 2.10).\nWe also give an algorithm in the massively-parallel computation (MPC) model [KSV10] used to model MapReduce/Spark-like systems. We use the most restrictive version, which only allows linear total memory, running time and communication per round [ANOY14]. In this model, the input set E of size n is arbitrarily distributed across m machines, each with s = O(n/m) memory so that the overall memory is O (n). A standard setting of parameters for submodular optimization is m = n/ K and s = O( n K) (see e.g. [LV19,AMYZ19]). One of the machines is designated as the central machine and outputs the solution in the end. The machines communicate to each other in a number of synchronous rounds. In each round, each machine receives an input of size O( n K), performs a local linear-time computation, and sends an output of size O( n K) to other machines before the next round begins. The primary objective in this model is minimizing the number of rounds. Our main result in this model is given below.\nTheorem 1.3 (MPC algorithm Distributed Sieve+Max) Let K = min(n, K). There exists an MPC algorithm Distributed Sieve+Max (Algorithm 3)that runs in O (1/ ) rounds on n/ K machines, each with O( n K) memory. Each machine uses query complexity and runtime O( n K) per round. The algorithm outputs a ( 1 /2 -)-approximation to the submodular maximization problem under a knapsack constraint(see Theorem 2.13).\nIn particular, our algorithm uses execution time O( n K/ ) and total communication, CPU time and number of queries O (n/ ).", "publication_ref": ["b38", "b43", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Relationship to previous work", "text": "The classic version of the problem considered in this work sets c(e) = 1 for all e \u2208 U and is known as monotone submodular maximization under a cardinality constraint and has been extensively studied. The celebrated result of [NWF78] gives a 1 -1 /e \u2248 0.63-approximation using Greedy, which is optimal unless P = N P , [Fei98]. The problem of maximizing a monotone submodular function under a knapsack constraint was introduced by [Wol82], who gave an algorithm with \u2248 0.35-approximation. [KMN99] gave a simple GreedyOrMax algorithm with 1 -1 / \u221a e \u2248 0.39approximation as well as a more complicated algorithm PartialEnum+Greedy which requires a partial enumeration over an initial seed of three items and hence runs in O Kn 4 time. Par-tialEnum+Greedy was later analyzed by [Svi04] who showed a (1 -1 /e) \u2248 0.63-approximation, matching the hardness of [Fei98]. The subsequent search for more efficient algorithms has motivated a number of further studies. [BV14] and [EN19a] give algorithms with approximation 1 -1 /e -. However while these algorithms are theoretically interesting, they are self-admittedly impractical due to their exponential dependence on large polynomials in 1/ .\nCompared to the well-studied cardinality-constrained case, streaming literature on monotone submodular optimization under a knapsack constraint is relatively sparse. A summary of results in the streaming setting is given in Figure 1. Prior to our work, the best results in streaming are by [HKY17,HK19]. While the most recent work of [HK19] achieves the ( 1 /2 -)-approximation, its space, runtime and query complexities are far from optimal and depend on large polynomials of 1/ , making it impractical for large data. Compared to this result, our Theorem 1.2 gives an improvement on all main parameters of interest, leading to near-optimal results. On the other hand, for the cardinality-constrained case, an optimal single-pass ( 1 /2 -)-approximation has very recently been achieved by [KMZ + 19]. While using different ideas, our multi-pass streaming result matches theirs in terms of approximation, space and improves slightly on the number of queries and runtime (from O n log K/ to O n( 1 / + log K) ) only at the cost of using a constant number of passes for constant .", "publication_ref": ["b53", "b23", "b57", "b34", "b55", "b23", "b9", "b21", "b30", "b29", "b29"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Reference", "text": "Approx. Passes Space Runtime and Queries [HKY17] 1 /3 - In the distributed setting, [MKSK13] give an elegant two round protocol for monotone submodular maximization subject to a knapsack constraint that achieves a subconstant guarantee.\n1 O 1 K log K O 1 n log K [HKY17] 4 /11 - 1 O 1 K log K O 1 n log K [HKY17] 2 /5 - 3 O 1 2 K log 2 K O 1 n log K [HK19] 1 /2 - O ( 1 / ) O 1 7 K log 2 K O 1 8 n log 2 K Sieve+Max (Alg. 2) 1 /2 - O ( 1 / ) O (K) O n 1 + log K\n[KMVV15] later give algorithms for both cardinality and matroid constraints that achieve a constant factor approximation, but the number of rounds is \u0398(log \u2206), where \u2206 is the maximum increase in the objective due to a single element, which is infeasible for large datasets since \u2206 even be significantly larger than the size of the entire dataset. [dPBENW15,dPBENW16] subsequently give a framework for both monotone and non-monotone submodular functions under cardinality, matroid, and p-system constraints. Specifically, the results of [dPBENW16] achieves almost 1 /2-approximation using two rounds, a result subsequently matched by Liu and Vondr\u00e1k without requiring the duplication of items, as well as a (1 -1 /e -) approximation using O ( 1 / ) rounds. [dPBENW15] also gives a two-round algorithm for a knapsack constraint that achieves roughly 0.17-approximation in expectation.\nFor extensions to other constraints, non-monotone objectives and other generalizations see e.g. [CK14, CGQ15, CHJ + 17, EDFK17, ELVZ17, MJK18, FKK18, CQ19].", "publication_ref": ["b30", "b48", "b16", "b17", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Our techniques", "text": "Let f (e | S) = f (e \u222a S) -f (S) be the marginal gain and \u03c1 (e | S) = f (e | S) /c(e) be the marginal density of e with respect to S. Greedy starts with an empty set G and repeatedly adds an item that maximizes \u03c1 (e | G) among the remaining items that fit. While by itself this does not guarantee any approximation, the classic result of [KMN99] shows that GreedyOrMax algorithm, which takes the best of the greedy solution and the single item with maximum value, gives a 0.39-approximation but cannot go beyond 0.44-approximation. Our algorithm Greedy+Max (Algorithm 1) instead attempts to augment every partial greedy solution with the item giving the largest marginal gain. For each i, let G i be the set of the first i items taken by greedy. We augment this solution with the item s i which maximizes f (s i | G i ) among the remaining items that fit. Greedy+Max then outputs the best solution among such augmentations.\nOur main technical contribution lies in the analysis of this algorithm and its variants, which shows a 1 /2-approximation (this analysis is tight, see Example 2.1 ). Let o 1 be the item from OPT with the largest cost. The main idea is to consider the last partial greedy solution such that o 1 still fits. Since o 1 has the largest cost in OPT, we can augment the partial solution with any element from OPT, and all of them have a non-greater marginal density than the next selected item. While Greedy+Max augments partial solutions with the best item, for the sake of analysis it suffices to consider only augmentations with o 1 (note that the item itself is unknown to the algorithm).\nTo simplify the presentation, in the analysis we rescale f and the costs so that f (OPT) = 1 and K = 1. Suppose that at some point, the partial greedy solution has collected elements with total cost x \u2208 [0, 1]. We use a continuous function g(x) to track the performance of Greedy. We also introduce a function g 1 (x) to track the performance of augmentation with o 1 and then show that g and g 1 satisfy a differential inequality g 1 (x) + (1 -c (o 1 ))g (x) \u2265 1 (Lemma 2.5), where g denotes the right derivative. To give some intuition about the proof, consider the case when there exists a partial greedy solution of cost exactly 1 -c (o 1 ). If g 1 (1 -c (o 1 )) \u2265 1 /2, then the augmenation with o 1 gives a 1 /2-approximation. Otherwise, by the differential inequality,\ng (1 -c (o 1 )) \u2265 1 /2(1-c(o 1 )). Since g(0) = 0 and g is non-increasing, g(1 -c (o 1 )) \u2265 (1 -c (o 1 ))g (1 -c (o 1 )) \u2265 1 /2.\nSee full analysis for how to handle the cases when there is no partial solution of cost exactly 1 -c (o 1 ).\nOur streaming algorithm Sieve+Max and distributed algorithm Distributed Sieve+Max approximately implement Greedy+Max in their respective settings. Sieve+Max makes O ( 1 / ) passes over the data, and for each pass it selects items with marginal density at least a threshold cf (OPT) K(1+ ) i in the i-th pass for some constant c > 0. This requires having a constant-factor approximation of f (OPT) which can be computed using a single pass. Distributed Sieve+Max combines the thresholding approach with the sampling technique developed by [LV19] for the cardinality constraint. The differential inequality which we develop for Greedy+Max turns out to be robust to various sources of error introduced through thresholding and sampling. As we show, it continues to hold with functions and derivatives replaced with their (1 + )-approximations, which results in ( 1 /2 -)-approximation guarantees for both algorithms.", "publication_ref": ["b34", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithms and analysis", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Offline algorithm Greedy+Max", "text": "We introduce the main ideas by first describing our offline algorithm Greedy+Max which is then adapted to the streaming and distributed settings. As this algorithm is a modification of the standard Greedy algorithm we describe Greedy first. Greedy starts with an empty set G and in each iteration selects an item e with the highest marginal density \u03c1 (e | G) that still fits into the knapsack. We refer to the resulting solution as the greedy solution and denote it as G. Greedy+Max is based on augmenting each partial solution constructed by Greedy with the item of the largest marginal value (as opposed to density) and taking the best among such augmentations. Recall that G i is the set of the first i items in the greedy solution. Greedy+Max finds for each i an augmenting item s i which maximizes f (s i \u222a G i ) among all items that still fit, i.e. c(\ns i \u222a G i ) \u2264 K.\nThe final output is the best among all such augmented solutions. Implementation is given as Algorithm 1. In the rest of this section we show that Greedy+Max gives 1 /2-approximation. This Algorithm 1: Offline algorithm Greedy+Max Input: Set of elements E = e 1 , . . . , e n , knapsack capacity K, cost function c(\u2022), non-negative monotone submodular function f ; Output: 1 2 -approximation for submodular maximization under knapsack constraint;\nG \u2190 \u2205, S \u2190 \u2205; while E = \u2205 do s \u2190 argmax e\u2208E f (e | G); if f (S) < f (G \u222a s) then S \u2190 G \u222a s; a \u2190 argmax e\u2208E \u03c1 (e | G); G \u2190 G \u222a a; K \u2190 K -c(a);\nRemove all elements e \u2208 E with c(e) > K; return S analysis is tight as illustrated by the following example:\nExample 2.1 Let e 1 , e 2 , e 3 be three items such that f (e 1 ) = f (e 2 ) = 1 2 and f (e 3 ) = 1 2 + for any > 0. Let c(e 1 ) = c(e 2 ) = 1 2 and c(e 3 ) = 1+ 2 . Let f be a linear function, i.e. f (S) = e\u2208S f (e). Then OPT = {e 1 , e 2 } has value 1 while Greedy+Max outputs {e 3 } of value 1 2 + .\nAs discussed in Section 1.3, our analysis is based on a number of differential inequalities for functions tracking the performance of our algorithm. We assume that these functions are continuous and piecewise smooth, and by \u03be (x) we denote the right-hand derivative of \u03be at point x. All these inequalities are of the form \u03be(x) + \u03b1\u03be (x) \u2265 \u03b2 for some function \u03be, applied in a certain range [u, v] and have some initial condition \u03be(u). We frequently need to integrate these inequalities to get a lower bound on \u03be(v) which can be done as follows:\nOur proof proceeds by case analysis on whether o 1 , the item of the largest cost in OPT, is included in the greedy solution G or not. We first show that if o 1 \u2208 G, then f (G) is at least a 1 /2-approximation.\nLet OPT be the optimal solution, i.e. the maximizer of f (OPT) under c(OPT) \u2264 K. Let o 1 be the element of the largest cost in OPT. W.l.o.g. and only for the sake of analysis of approximation we rescale the function values and costs so that f (OPT) = 1 and c(OPT) = K = 1foot_2 . We first define a greedy performance function g(x) which allows us to track the performance of the greedy solution in a continuous fashion. Let G be the greedy solution computed by Algorithm 1 and let g 1 , g 2 , . . . , g m be the elements in G in the order they were added and recall that G i = {g 1 , . . . , g i }.\nFor a fixed x, let its greedy index i be the smallest index such that c(G i ) > x.\nDefinition 2.2 (Greedy performance function) For x \u2208 [0, 1] we define g(x) as:\ng(x) = f (G i-1 ) + (x -c(G i-i ))\u03c1 (g i | G i-1 ) .\nNote that g is a continuous and monotone piecewise-linear function such that g(0) = 0. Since an important role in the analysis is played by the derivative of this function we further define g to be the right derivative for g so that g is defined everywhere on the interval [0, c(G)) and is always non-negative.\nWe now define a function g + (x) which tracks the performance of Greedy+Max when the greedy solution collects a set of cost x. Note that the cost of the last item which Greedy+Max uses to augment the solution does not count in the argument of this function.\nDefinition 2.3 (Greedy+Max performance function) For any fixed x, let i be the smallest index such that c(G i ) > x. We define g\n+ (x) = g(x) + f (v | G i-1 ), where v = argmax e\u2208E\\G i-1 :c(e\u222aG i-1 )\u2264K f (e | G i-1 )\nis the element with the largest marginal gain with respect to the current partial greedy solution G i-1 .\nFor technical reasons which we describe below instead of working directly with g + it is easier to work with a lower bound on it g 1 which has some nicer properties. For g 1 we only consider adding o 1 , the largest item from OPT, to the current partial greedy solution. Note that hence g 1 is only defined while this item still fits. Consider the last item added by the greedy solution before the cost of this solution exceeds 1 -c(o 1 ). We define c * so that 1 -c(o 1 ) -c * is the cost of the greedy solution before this item is taken.\nDefinition 2.4 (Greedy+Max performance lower bound) For x \u2208 [0, 1 -c (o 1 ) -c * ] we define g 1 (x) = g(x) + f (o 1 | G i-1 ) so that g 1 (x) \u2264 g + (x).\nLemma 2.5 (Greedy+Max inequality) Let g denote the right derivative of g. Then for all x \u2208 [0, 1 -c (o 1 ) -c * ], the following differential inequality holds:\ng 1 (x) + (1 -c (o 1 ))g (x) \u2265 1\nProof : Similarly to the proof of the standard greedy inequality it suffices to show the statement only for points where x = c(G i-1 ) for some i \u2265 1. Hence, we have g\n1 (x) = g(c(G i-1 ))+f (o 1 | G i-1 ) = f (G i-1 \u222ao 1 ). Since we normalized f (OPT) = 1, then by monotonicity, 1 = f (OPT) \u2264 f (G i-1 \u222aOPT).\nHence:\n1 \u2264 f (G i-1 \u222a OPT) = f (G i-1 \u222a o 1 ) + f (OPT \\ (o 1 \u222a G i-1 ) | G i-1 \u222a o 1 ) \u2264 g 1 (x) + e\u2208OPT\\(o 1 \u222aG i-1 ) f (e | G i-1 \u222a o 1 ) = g 1 (x) + e\u2208OPT\\(o 1 \u222aG i-1 ) c(e)\u03c1 (e | G i-1 \u222a o 1 ) ,\nwhere the second inequality is by submodularity and the definition of g 1 and the last equality is by the definition of marginal density. Since x \u2264 1 -c (o 1 ) -c * , then all items in OPT \\ (o 1 \u222a G i-1 ) still fit, as o 1 is the largest item in OPT. Since the greedy algorithm always selects the item with the largest marginal density, then max e\u2208OPT\\(o\n1 \u222aG i-1 ) \u03c1 (e | G i-1 \u222a o 1 ) \u2264 g (x)\n. Hence:\n1 \u2264 g 1 (x) + e\u2208OPT\\(o 1 \u222aG i-1 ) c(e)\u03c1 (e | G i-1 \u222a o 1 ) \u2264 g 1 (x) + e\u2208OPT\\(o 1 \u222aG i-1 ) c(e)\u03c1 (e | G i-1 ) \u2264 g 1 (x) + e\u2208OPT\\(o 1 \u222aG i-1 ) c(e)g (x) = g 1 (x) + g (x) e\u2208OPT\\(o 1 \u222aG i-1 ) c(e) = g 1 (x) + g (x)c(OPT \\ (o 1 \u222a G i-1 )) \u2264 g 1 (x) + g (x)(1 -c (o 1 )),\nwhere the last inequality follows from the normalization of c(OPT) \u2264 1 and the fact that o 1 \u2208 OPT. 2 Theorem 2.6 Recall that K = min(n, K) is an upper bound on the number of elements in feasible solutions. Then Greedy+Max gives a 1 /2-approximation to the submodular maximization problem under a knapsack constraint and runs in O Kn time.\nProof : By applying Lemma 2.5 at the point x = 1 -c (o 1 ) -c * , we have:\ng 1 (1 -c (o 1 ) -c * ) + (1 -c (o 1 ))g (1 -c (o 1 ) -c * ) \u2265 1 If g 1 (1 -c(o 1 ) -c * ) \u2265 1 2 , then we have 1 2 -approximation, because g 1 (1 -c(o 1 ) -c *\n) is a lower bound on the value of the augmented solution when the cost of the greedy part is 1 -c(o 1 ) -c * . Otherwise:\ng (1 -c (o 1 ) -c * ) \u2265 1 -g 1 (1 -c (o 1 ) -c * ) 1 -c (o 1 ) > 1 2(1 -c (o 1 ))\n.\nNote that since g(0) = 0 and g is non-increasing by the definition of Greedy, for any x \u2208 [0, 1] we have g(x) \u2265 g (x) \u2022 x:\ng(x) \u2265 x \u03c7=0 g (\u03c7)d\u03c7 \u2265 x \u03c7=0 g (x)d\u03c7 = g (x) \u2022 x, Therefore, applying this inequality at x = 1 -c (o 1 ) -c * : g(1 -c (o 1 ) -c * ) \u2265 (1 -c (o 1 ) -c * )g (1 -c (o 1 ) -c * ) \u2265 1 -c (o 1 ) -c * 2(1 -c (o 1 )) .\nRecall that 1 -c (o 1 ) -c * was the last cost of the greedy solution when we could still augment it with o 1 ; therefore, the next element e that the greedy solution selects has the cost at least\n(1 -c (o 1 )) -(1 -c (o 1 ) -c * ) = c * .\nThus, the function value after taking e is at least\ng(1 -c (o 1 ) -c * ) + c * g (1 -c (o 1 ) -c * ) \u2265 1 -c (o 1 ) -c * 2(1 -c (o 1 )) + c * 2(1 -c (o 1 )) = 1 2\nHence, Algorithm 1 gives a 1 2 -approximation to the submodular maximization problem under a knapsack constraint. It remains to analyze the running time and query complexity of Algorithm 1. Since K is the maximum size of a feasible set, Algorithm 1 makes at most K iterations. In each iteration, it makes O (n) oracle queries, so the total number of queries and runtime is O Kn . 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Streaming algorithm Sieve+Max", "text": "Our multi-pass streaming algorithm is given as Algorithm 2. To simplify the presentation, we first give the algorithm under the assumption that it is given a parameter \u03bb, which is a constant-factor approximation of f (OPT). We then show how to remove this assumption using standard techniques in Theorem B.3. As discussed in the description of our techniques Sieve+Max uses O (1/ ) passes over the data to simulate the execution of Greedy+Max approximately.\nIn the analysis, which gives the proof of Theorem 1.2, we define functions t, t + , and t 1 analogous to g, g + , and g 1 respectively, based on T i , the first i items collected by the thresholding algorithm. We show that t and t 1 satisfy the same differential inequalities as g and g 1 respectively, up to (1 + ) factors, and similar to before, our analysis then proceeds by casework on whether o 1 , the largest item in OPT, is included in the thresholding solution T or not.\nWe first show that if o 1 \u2208 T , then f (T ) is at least a 1 2 --approximation. Let T be the set of items constructed Sieve+Max (as in Algorithm 2) and let t 1 , t 2 , . . . be the order that they are collected. We refer to the part of the algorithm which constructs T as \"thresholding\" and the rest as \"augmentation\" below. We use T i to denote the set containing the i items {t 1 , t 2 , . . . , t i }. We again use o 1 to denote the item with highest cost in OPT. Similar to the above, we define two functions representing the values of our thresholding algorithm, and augmented solutions given the utilized proportion of the knapsack.\nDefinition 2.7 (Thresholding performance function) For any x \u2208 [0, 1], let i be the smallest index such that c(T i ) > x. We define t(x) = f (T i-1 ) + (x -c(T i-i ))\u03c1 (t i | T i-1 ) and t (x) to be the right derivative of t. For each i, let G i be the first i selected in the construction of T above and let s i = \u2205; Take a pass over the stream; for each read item e do // Augmentation stage if e / \u2208 T then\nj = max{i|c(G i ) + c(e) \u2264 K}; if f (G j \u222a s j ) < f (G j \u222a e) then s j \u2190 {e}; return argmax f (G i \u222a s i )\nWe define a function t 1 (x) that lower bounds the performance of Sieve+Max when the thresholding solution collects a set of cost x: Definition 2.8 (Sieve+Max performance function and lower bound) For any fixed x, let i be the smallest index such that c(T i ) > x. Then we define t 1 (x) = t(x) + f (o 1 | T i-1 ), where o 1 = argmax e\u2208OPT c(e).\nIn order to analyze the output of the algorithm, we prove a differential inequality for t 1 rather than t + . If c(T ) \u2265 1 -c(o 1 ) then let c * \u2265 0 be defined so that 1 -c(o 1 ) -c * is the cost of the thresholding solution before the algorithm takes the item which makes the cost exceed 1 -c(o 1 ). Lemma 2.9 (Sieve+Max Inequality) If c(T ) \u2265 1 -c(o 1 ) then for all x \u2208 [0, 1 -c (o 1 ) -c * ], then t and t 1 satisfy the following differential inequality:\nt 1 (x) + (1 + )(1 -c (o 1 ))t (x) \u2265 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof :", "text": "First, note that for x \u2208 [0, p] where p is the total cost of items taken in the first pass the inequality holds trivially since t (x) \u2265 1 (as in the proof of the standard thresholding inequality). Hence assume that x \u2208 [p, 1 -c(o 1 ) -c * ] is fixed and consider any pass after the first one. Similarly to other proofs it suffices to only consider left endpoints of the intervals of the form [c(T i-1 ), c(T i )) so let x = c(T i-1 ). Since we normalized f (OPT) = 1, then by monotonicity,\n1 = f (OPT) \u2264 f (T i-1 \u222a OPT). Hence: 1 \u2264 f (T i-1 \u222a OPT) = f ((T i-1 \u222a o 1 ) \u222a (OPT \\ o 1 )) = f (T i-1 \u222a o 1 ) + f (OPT \\ (o 1 \u222a T i-1 ) | T i-1 \u222a o 1 ) \u2264 t 1 (x) + e\u2208OPT\\(o 1 \u222aT i-1 ) f (e | T i-1 \u222a o 1 ) = t 1 (x) + e\u2208OPT\\(o 1 \u222aT i-1 ) c(e)\u03c1 (e | T i-1 \u222a o 1 ) ,\nwhere the second inequality is by submodularity and the last line is by the definition of marginal density. Since o 1 has the maximum cost in OPT. x \u2264 1 -c (o 1 ), all items in OPT \\ (o 1 \u222a T i-1 ) still fit into the remaining knapsack capacity. In all passes after the first one, the thresholding algorithm always selects an element which gives 1 1+ -approximation of the highest possible marginal density:\n(1 + )t(x) \u2265 max e\u2208OPT\\(o 1 \u222aT i-1 ) \u03c1 (e | o 1 \u222a T i-1 ) .\nCombining with the inequality above:\n1 \u2264 t 1 (x) + e\u2208OPT\\(o 1 \u222aT i-1 ) c(e)\u03c1 (e | T i-1 \u222a o 1 ) \u2264 t 1 (x) + (1 + )t (x) e\u2208OPT\\(o 1 \u222aT i-1 ) c(e) = t 1 (x) + (1 + )t (x)c(OPT \\ (o 1 \u222a T i-1 )) \u2264 t 1 (x) + (1 + )t (x)(1 -c (o 1 )),\nwhere the last equality is by the normalization of c(OPT) = 1 and the fact that o 1 \u2208 OPT. 2\nTheorem 2.10 There exists an algorithm that uses O K space and O ( 1 / ) passes over the stream, makes O n / + n log K queries, and outputs a ( 1 /2 -)-approximation to the submodular maximization problem under a knapsack constraint.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof :", "text": "We can use existing algorithm from Theorem B.3 to obtain a constant factor approximation \u03bb to f (OPT). We thus analyze the correctness of Algorithm 2 given an input \u03bb that is a constant factor approximation to f (OPT). The proof is similar to proof of Theorem 2.6.\nBy applying Lemma 2.9 at the point x = 1 -c (o 1 ) -c * , we have:\nt 1 (1 -c (o 1 ) -c * ) + (1 + )(1 -c (o 1 ))t (1 -c (o 1 ) -c * ) \u2265 1 If t 1 (1 -c (o 1 ) -c * ) \u2265 1 2 , then we have 1 2 -approximation, because t 1 (1 -c(o 1 ) -c *\n) is a lower bound on the value of the augmented solution when the cost of the thresholding solution is 1 -c(o 1 ) -c * . Otherwise:\nt (1 -c (o 1 ) -c * ) \u2265 1 -g 1 (1 -c (o 1 ) -c * ) (1 -c (o 1 ))(1 + ) > 1 2(1 -c (o 1 ))(1 + )\nNote that since t(0) = 0, for any x \u2208 [0, 1] we have t(x) \u2265 t (x)\u2022x 1+ :\nt(x) \u2265 x \u03c7=0 t (\u03c7)d\u03c7 \u2265 x \u03c7=0 t (x) 1 + d\u03c7 = t (x) \u2022 x 1 + ,\nwhere we used the fact that t is a 1 1+ approximation of the maximum marginal density, which does not increase. Therefore, applying this at x = 1 -c (o 1 ) -c * :\nt(1 -c (o 1 ) -c * ) \u2265 (1 -c (o 1 ) -c * )t (1 -c (o 1 ) -c * ) \u2265 1 -c (o 1 ) -c * 2(1 -c (o 1 ))(1 + ) .\nRecall that 1 -c (o 1 ) -c * was the last cost of the thresholding solution when we could still augment it with o 1 ; therefore, the next element e that the thresholding solution selects has the cost at least (1 -c (o 1 )) -(1 -c (o 1 ) -c * ) = c * . Thus, the function value after taking e is at least\ng(1 -c (o 1 ) -c * ) + c * g (1 -c (o 1 ) -c * ) \u2265 1 -c (o 1 ) -c * 2(1 -c (o 1 ))(1 + ) + c * 2(1 -c (o 1 ))(1 + ) = 1 2(1 + ) = 1 2 - 2(1 + ) \u2265 1 2 -.\nHence, Algorithm 2 gives a 1 2 --approximation to the submodular maximization problem under knapsack constraints, given a constant factor approximation to f (OPT). Note that it suffices to consider only thresholds up to \u03c4 2K since t (x) < 1 2 implies that t(x) > 1 2 by Lemma A.3. Using existing algorithms to obtain a constant factor approximation \u03bb (e.g., by setting = 1 6 in Theorem B.3) that use additional O n log K queries, then correctness of Algorithm 2 follows. It remains to analyze the space and query complexity of Algorithm 2. Since each item has cost at least 1, at most K items are stored by the thresholding algorithm, and at most K items are stored by the augmented solution S. Hence, the space complexity of Algorithm 2 is O K . If \u03c4 is an \u03b1-approximation to f (OPT) for some constant \u03b1, then the algorithm makes log 1+ 1 2\u03b1 = O 1 passes over the input stream. Each pass makes at most n queries, so the number of queries is at most O n . 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Distributed algorithm Distributed Sieve+Max", "text": "In this section, we assume that there are m = n / K machines M 1 , . . . , M m , each with O n m = O( n K) amount of local memory. Our distributed algorithm (Algorithm 3) follows a similar thresholding approach as our streaming algorithm: at each round, machines collect items whose marginal densities exceed the threshold corresponding to the round.\nWe require the following form of Azuma's inequality for submartingales.\nTheorem 2.11 (Azuma's Inequality) Suppose X 0 , X 1 , . . . , X n is a submartingale and |X i -\nX i+1 | \u2264 c i . Then Pr [X n -X 0 \u2264 -t] \u2264 exp -t 2 2 i c 2 i .\nWe first bound the total number of elements sent to the central machine.\nLemma 2.12 In Algorithm 3, with probability 1 -e -\u2126(K) , the total number of elements sent to the central machine is n K.\nProof : Since each element is sampled with probability K n , the expected number of elements in \u0393 i is 4 n K for any round i. Hence |\u0393 i | \u2265 3 n K with probability at least 1 -e -\u2126( K) by standard Chernoff bounds. Let N i denote the total number of elements with marginal density at least f (OPT) (1+ ) i K with respect to T i-1 , so that the number of elements sent to the central unit in round i is exactly\nN i + |\u0393 i |.\nSuppose \u0393 i is partitioned into at least 3 K chunks of size n K elements. If there are less than n K remaining elements before each chunk whose marginal density with respect to T i-1 exceeds f (OPT) (1+ ) i K , then certainly at most n K elements are sent to the central machine.\nOn the other hand, if there are at least n K remaining elements before each chunk whose marginal density with respect to T i-1 exceeds f (OPT) (1+ ) i K . Then an additional element is added to N i with probability at least 1 -1 -\nK n \u221a n K > 1/2.\nTo use a martingale argument to bound the number of elements selected in \u0393 i , we let X i be the indicator random variable for the event that at least one element is selected from the i th block so that we have E K) , in which case no elements are sent to the central machine.\n[X i | X 1 , . . . , X i-1 ] \u2265 1 2 . Let Y i = i j=1 (X i -1/2) so that the sequence Y 1 , Y 2 , . . . is a submartingale, i.e., E[Y i | Y 1 , . . . , Y i-1 ] \u2265 Y i-1 and |Y i -Y i-1 | \u2264 1. By Azuma's inequality (Theorem 2.11), Pr[Y 3 K < -1 2 K] < e -\u2126( K) , so that 3 K j=1 X j = Y K + 3 2 K \u2265 K with probability at least 1 -e -\u2126(\n2\nWe now analyze the approximation guarantee and performance of Algorithm 3.\nTheorem 2.13 There exists an algorithm Distributed Sieve+Max which uses O ( 1 / ) rounds of communication between n / K machines, each with O n K memory. With high probability, the total number of elements sent to the central machine is n K and the algorithm outputs a ( 1 /2 -)-approximation to the submodular maximization problem with a knapsack constraint.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof :", "text": "Correctness follows from the observation that the algorithm performs thresholding in the same manner as Algorithm 2. The space bounds follow from Lemma 2.12. 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Query lower bound", "text": "We show a simple query lower bound under the standard assumption [NTM + 18, KMZ + 19] that the algorithm only queries f on feasible sets.\nTheorem 2.14 For \u03b1 > 1 /2, any \u03b1-approximation algorithm for maximizing a function f under a knapsack constraint that succeeds with constant probability and only queries values of the function f on feasible sets (i.e. sets of cost at most K) must make at least \u2126(n 2 ) queries if f is either: 1) nonmonotone submodular, 2) monotone and submodular on the feasible sets, 3) monotone subadditive.\nProof : Let e 1 , . . . , e n be the set of elements and set c(e i ) = K/2 for all i. By Yao's principle it suffices to consider two hard distributions D 1/2 and D 1 such that the optimum for every instance in the support of these distributions is 1/2 and 1 respectively and then show that no algorithm making o(n 2 ) deterministic queries can distinguish the two distributions with constant probability. The distributions D 1/2 and D 1 are as follows:\n\u2022 D 1/2 has f (S) = 1/2 for all S = \u2205.\n\u2022 D 1 is constructed by picking two items e i = e j uniformly at random and assigning f (S) = 1 for S = {e i , e j }. Otherwise, set f (S) = 1/2 for all S = \u2205 and S = {e i , e j }.\nFix the set of deterministic queries Q that the algorithm makes. Since the algorithm is only allowed to make queries to sets of cost at most K, all sets in Q have size at most two. Furthermore, note that f (e i ) = 1/2 for all i under both D 1/2 and D 1 . Thus, only queries to sets of size exactly two can help the algorithm distinguish the two distributions. All such queries give value 1/2 under both distributions except for a single query (i, j) under D 1 which gives value 1. Since (i, j) is chosen uniformly at random under D 1 the probability that a fixed set Q contains it is given as |Q|/ n 2 . Hence if the algorithm succeeds with a constant probability then it must be the case that |Q| = \u2126(n 2 ).\nNote that the construction of f results in a non-monotone submodular function but f is monotone when restricted to feasible sets of size at most two items. By changing D 1 so that the functions in this distribution take value 1 on all sets of size more than 2 one can ensure monotonicity of f . However, f is still submodular on the feasible sets and subadditive everywhere (recall that a subadditive function satisfies f (S) + f (T ) \u2265 f (S \u222a T ) for all S, T \u2286 U ). 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental results", "text": "We compare our offline algorithm Greedy+Max and our streaming algorithm Sieve+Max with baselines, answering the following questions: (1) What are the approximation factors we are getting on real data? (2) How do the objective values compare? (3) How do the runtimes compare? (4) How do the numbers of queries compare? We compare Greedy+Max to the following baselines:\n(1) PartialEnum+Greedy [Svi04]. Given an input parameter d, this algorithm creates a separate knapsack for each combination of d items, and then runs the Greedy algorithm on each of the knapsacks. At the end, the algorithm outputs the best solution among all knapsacks, so that the total runtime is \u2126(Kn d+1 ). In fact, PartialEnum+Greedy is only feasible for d = 1 and our smallest dataset. (2) Greedy. This algorithm starts with an empty knapsack and repeatedly adds the item with the highest marginal density with respect to the collected items in the knapsack, until no more item can be added to the knapsack. (3) GreedyOrMax [KMN99]. This algorithm compares the value of the best item with the value of the output of the Greedy algorithm and outputs the better of the two. In streaming we compare Sieve+Max to Sieve [BMKK14] and SieveOrMax [HKY17], which are similar thresholding-based algorithms. Sieve starts with an empty knapsack and collects all items whose marginal density with respect to the items in the knapsack exceed a given threshold (which is initially equal to 1 2 ), while SieveOrMax uses a similar approach, but compares the items collected by the thresholding algorithm to the best single item, and outputs the better of the two solutions. We also implemented a single-pass BranchingMRT by [HKY17] that uses thresholding along with multiple branches and gives a 4 /11 \u2248 0.36-approximation. We did not implement [HK19] as their algorithms are orders of magnitude slower than BranchingMRT which is already several orders of magnitude slower than other algorithms.\nOur code is available at https://github.com/aistats20submodular/aistats20submodular.", "publication_ref": ["b55", "b34", "b5", "b30", "b30", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Objectives and Datasets", "text": "Graph coverage. For a graph G(V, E) and Z \u2282 V , the objective is to maximize the neighborhood vertex coverage function f (Z) := |Z \u222a N (Z)|/|V |, where N (Z) is the set of neighbors of Z. The cost of each node is roughly proportional to the value of the node. Specifically, the cost of each node\nv \u2208 V is c(v) = \u03b2 |V | (|N (v)| -\u03b1)\n, where \u03b1 = 1 20 and \u03b2 is a normalizing factor so that c(v) \u2265 1, so that the cost of each node is roughly proportional to the value of the node. We ran experiments on two graphs from SNAP [LK14]: 1) ego-Facebook(4K vertices, 81K edges), 2) com-DBLP (317K vertices, 1M edges).\nMovie ratings. We also analyze a dataset of movies to model the scenario of movie recommendation. The objective function, defined as in [AMYZ19], is maximized for a set of movies that is similar to a user's interests and the cost of a movie is set to be roughly proportional to its value. Each movie is assigned a rating in the range [1, 5] by users. Let r x,u be the rating assigned by user u to movie x and r avg be the average rating across all movies. For each movie x, we normalize the ratings to produce a vector v x by setting v x,u = 0 if user u did not rate movie x and v x,u = r x,u -r avg otherwise. We then define the similarity between two movies x 1 and x 2 as the dot product v x 1 , v x 2 of their vectors. Given a set X of movies, to quantify how representative a subset of movies Z is, we consider a parameterized objective function f X (Z) = x\u2208X max z\u2208Z v z , v x . Hence, the maximizer of f X (Z) corresponds to a set of movies that is similar to the user's interests. We analyze the ml-20 MovieLens dataset [Gro15], which contains approximately 27K movies and 20M ratings.", "publication_ref": ["b40", "b2", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We first give instance-specific approximation factors for different values of K for offline (Fig. 2) and streaming (Fig. 3) algorithms. These approximations are computed using upper bounds on f (OPT) which can be obtained using the analysis of Greedy. Greedy+Max and Sieve+Max typically perform at least 20% better than their 1 /2 worst-case guarantees. In fact, our results show that the output value can be improved by up to 50%, both by Greedy+Max upon Greedy (Figure 4) and by Sieve+Max upon Sieve (Figure 5).\nRunning time. We point out that the runtimes of Greedy+Max and GreedyOrMax algorithms are similar, being at most 20% greater than the runtime of Greedy, as shown in Figure 6. On the other hand, even though PartialEnum+Greedy does not outperform Greedy+Max, it is only feasible for d = 1 and the ego-Facebook dataset and uses on average almost 500 times as much runtime for K = 10 across ten iterations of each algorithm, as shown in Figure 6. The runtimes of Sieve+Max, SieveOrMax, and Sieve are generally similar; however in the case of the com-dbpl dataset, the runtime of Sieve+Max grows with K. This can be explained by the fact that oracle calls on larger sets typically require more time, and augmented sets typically contain more elements than sets encountered during execution of Sieve. On the other hand, the runtime of BranchingMRT was substantially slower, and we did not include its runtime for scaling purposes, ", "publication_ref": [], "figure_ref": ["fig_1", "fig_14"], "table_ref": []}, {"heading": "OPT approximation", "text": "(c) ml-20", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Greedy Greedy+Max GreedyOrMax PartialEnum+Greedy", "text": "Fig. 2: Instance-specific approximations for different K. Greedy+Max performs substantially better than its worst-case 1 /2-approximation guarantee and typically beats even the (1 -1 /e) \u2248 0.63 bound. Despite much higher runtime, PartialEnum+Greedy does not beat Greedy+Max even on the only dataset where its runtime is feasible (ego-Facebook). ", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "OPT approximation", "text": "(c) ml-20", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sieve Sieve+Max SieveOrMax BranchingMRT", "text": "Fig. 3: Instance-specific approximations for different K. Sieve+Max performs substantially better than its worst-case ( 1 /2 -)-approximation guarantee and robustly dominates all other approaches. It can improve by up to 40% upon Sieve. Despite much higher runtime, BranchingMRT does not beat Sieve+Max (some data points not shown for BranchingMRT as it did not terminate under a 200-second time limit).\nas for K = 5, the runtime of BranchingMRT was already a factor 80K more than Sieve. Error bars for the standard deviations of the runtimes of the streaming algorithms are given in Figure 8.\nOracle calls. We also compare the number of oracle calls performed by the algorithms. Greedy+Max, GreedyOrMax and Greedy require the same amount of oracle calls, since computing marginal gains and finding the best element for augmentation compute the objective on the same set. On the other hand, PartialEnum+Greedy requires 544x more calls than Greedy for K = 8. For the streaming algorithms, the number of oracle calls made by Sieve, Sieve+Max, and Sieve, never differed by more than a factor of two, while BranchingMRT requires a factor 125K more oracle calls than Sieve for K = 8. We illustrate the number of oracle calls made by these algorithms in Figure 9. ", "publication_ref": [], "figure_ref": ["fig_14"], "table_ref": []}, {"heading": "Ratio to Greedy time", "text": "(c) ml-20", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Greedy+Max GreedyOrMax", "text": "Fig. 6: Ratio of runtime of offline algorithms to the runtime of Greedy, for different values of K.\nObserve that Greedy+Max and GreedyOrMax show similar running time, which is at most 20% greater than Greedy running time. The ratio of PartialEnum+Greedy runtime is not displayed, due to it being several orders of magnitude larger, e.g., 1000 times larger for K = 15. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sieve+Max SieveOrMax", "text": "Fig. 7: Ratio of average runtime of streaming algorithms to the average runtime of Sieve for different values of K, across ten iterations. The larger ratios can be explained from the oracle calls made on larger sets by Sieve+Max being more expensive than the average oracle call made by Sieve. The ratio of BranchingMRT runtime is not displayed, due to being several orders of magnitude larger, e.g., 80K times larger for K = 5.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Standard greedy and thresholding inequalities", "text": "In this section we prove the standard greedy inequality g(x) \u2265 1 -e -x , where x is the cost of a partial greedy solution. To prove it, we first show that a differential inequality g(x)+g (x) \u2265 1 holds, and then integrate it using Proposition A.1. For the thresholding algorithm a similar approximate inequality holds. Proof : First, consider the case when \u03be is smooth. \u03be(x) + \u03b1\u03be (x) \u2265 \u03b2 implies that \u03be(x)e  \n\u03b1 ) dx \u2265 \u03b2e x \u03b1 implies v u d(\u03be(x)\u03b1e x \u03b1 ) \u2265 v u \u03b2e x \u03b1 dx (\u03be(x)\u03b1e x \u03b1 ) v u \u2265 \u03b1\u03b2e x \u03b1 v u \u03be(v)\u03b1e v \u03b1 -\u03be(u)\u03b1e u \u03b1 \u2265 \u03b1\u03b2e v \u03b1 -\u03b1\u03b2e u \u03b1 .\nDividing both sides by \u03b1,\n\u03be(v)e v \u03b1 -\u03be(u)e u \u03b1 \u2265 \u03b2e v \u03b1 -\u03b2e u \u03b1 \u03be(v) \u2265 \u03b2 + (\u03be(u) -\u03b2)e u-v \u03b1 .\nFor a piecewise smooth \u03be, let u = x 0 < x 1 < \u2022 \u2022 \u2022 < x t = v, such that \u03be is smooth on a segment (x i , x i+1 ) for any i. By induction, we prove that the inequality holds for x 0 , x i for any i:\n\u03be(x i ) \u2265 \u03b2 + (\u03be(x 0 ) -\u03b2)e x i -x 0 \u03b1 .\nThe statement is true for i = 0. Induction step:\n\u03be(x i+1 ) \u2265 \u03b2 + (\u03be(x i ) -\u03b2)e x i -x i+1 \u03b1 \u2265 \u03b2 + (\u03be(x 0 ) -\u03b2)e x 0 -x i \u03b1 e x i -x i+1 \u03b1 \u2265 \u03b2 + (\u03be(x 0 ) -\u03b2)e x 0 -x i+1 \u03b12\nTheorem A.2 (Standard greedy inequality) For all x \u2208 [0, 1 -c (o 1 )], the greedy performance function g satisfies the following differential inequality:\ng(x) + g (x) \u2265 1,\nand hence also its integral version: g(x) \u2265 1 -e -x .\nProof :\nLet x \u2208 [0, 1 -c (o 1 )\n] and recall that by definition G i-1 is the largest set of elements selected by the greedy solution without exceeding total cost of x. Note that it suffices to show the inequality only for the left endpoints of the piecewise linear intervals of the form [c(G i-1 ), c(G i )) as inside these intervals g stays constant while g can only increase and hence the inequality holds. Hence we can assume that x = c(G i-1 ) in the proof below which implies that g(x) = f (G i-1 ).\nSince we normalized f (OPT) = 1, by monotonicity:\n1 = f (OPT) \u2264 f (OPT \u222a G i-1 ) = f (G i-1 ) + f (OPT \\ G i-1 | G i-1 ) .\nThen by submodularity and using the fact that by definition f (G i-1 ) = g(x):\n1 \u2264 f (G i-1 ) + f (OPT \\ G i-1 | G i-1 )\n\u2264 g(x) + The desired differential inequality follows from the observation that c(OPT \\ G i-1 ) \u2264 c(OPT) \u2264 1. Finally, by integrating from 0 to x using the initial condition g(0) = 0, it follows that g(x) \u2265 1 -e -x (by Proposition A.1). 2\nTheorem A.3 (Standard thresholding inequality) For all x \u2208 [0, 1 -c(o 1 )], the thresholding performance function t satisfies the following differential inequality: t(x) + (1 + )t (x) \u2265 1.\nAnd hence also its integral version: t(x) \u2265 1 -e -x 1+ .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof :", "text": "Let p \u2208 [0, 1] be the total cost of the elements collected by the thresholding algorithm in the first pass. First, note that for the first pass when x \u2208 [0, p] the differential inequality follows trivially as t (x) \u2265 \u03bb \u03b1K \u2265 1 since \u03bb \u2265 \u03b1f (OPT) and by our normalization f (OPT) = K = 1. Fix x \u2208 [p, 1 -c (o 1 )] and recall that by definition T i-1 is the largest set of elements selected by the thresholding algorithm without exceeding total cost of x. Similarly to the previous proofs it suffices to consider only the left endpoints of the intervals of the form [c(T i-1 , T i )) so we assume x = c(T i-1 ). Since we normalized f (OPT) = 1, then by monotonicity:\n1 = f (OPT) \u2264 f (OPT \u222a T i-1 ) = f (T i-1 ) + f (OPT \\ T i-1 | T i-1 ) .\nThen by submodularity and using the fact that by definition t(x) = f (T i-1 ): for all e \u2208 OPT \\ T i-1 . Indeed, note that in all passes except the first one the thresholding algorithm always selects an item whose marginal density is at least (1 + ) -1 times the best marginal density available. Since t (x) is the density of this item and all items in OPT \\ T i-1 still fit (as x \u2264 1 -c(o 1 )) we have (1 + )t (x) \u2265 max e\u2208OPT\\T i-1 \u03c1 (e | T i-1 ) as desired. Hence:\n1 \u2264 f (T i-1 ) + f (OPT \\ T i-1 | T i-1 ) \u2264 t(x) +\n1 \u2264 t(x) + e\u2208OPT\\T i-1\nc(e)t (x)(1 + ) = t(x) + (1 + )t (x)c(OPT \\ T i-1 ).\nThe desired differential inequality follows from the observation that c(OPT \\ T i-1 ) \u2264 c(OPT) = 1.\nFor the integral version we integrate the differential inequality between 0 and x with the initial condition t(0) = 0 (formally, apply Proposition A.1 with \u03b1 = 1 + , \u03b2 = 1, u = 0, v = x) and get t(x) \u2265 1 -e -x 1+ , as desired. 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Omitted proofs", "text": "Fact B.1 For all 0 \u2264 x \u2264 1,\n(1 -x)e 2x-1 \u2264 1 2 .\nProof : Let r(x) = (1 -x)e 2x-1 and note that r (x) = (1 -2x)e 2x-1 so that r (x) > 0 for x \u2208 0, 1 2 and r (x) \u2264 0 for x \u2208 1 2 , 1 . Hence, it follows that r 1 2 = 1 2 is a local maximum and so (1 -x)e 2x-1 \u2264 1 2 for all 0 \u2264 x \u2264 1. 2 Hence it suffices to show that e 1+ \u2264 1 + 2 , which follows from the fact that d dx e\nx 1+x \u2264 2 for 0 \u2264 x \u2264 1.\n2\nWe now describe a generalization to a knapsack constraint of the algorithm of [KMZ + 19] that computes a constant factor approximation to maximum submodular maximization under a cardinality constraint, using small space and a small number of queries.\nTheorem B.3 There exists a one-pass streaming algorithm that outputs a 1 3 --approximation to the submodular maximization under knapsack constraint that uses O K space and O n log K total queries.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof :", "text": "Since Algorithm 4 uses the same threshold as Algorithm 2 in [HKY17], it outputs a Hence, by setting = 1 6 , we obtain the following:\nCorollary B.4 There exists a one-pass streaming algorithm that outputs a 1 6 -approximation to the submodular maximization under knapsack constraint that uses O(K) space and O(n log K) total queries.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Algorithm 3: Distributed Sieve+Max: A O 1 -round MapReduce algorithm for submodular maximization under knapsack constraints.\nInput: Set of elements E = e 1 , . . . , e n , knapsack capacity K, cost function c(\u2022), non-negative monotone submodular function f , \u03c4 that is \u03b1-approximation of f (OPT) for some constant \u03b1 > 0;\nOutput: A set S that is a ( 1 2 -)-approximation for submodular maximization with a knapsack constraint;\nSend \u0393 and T to all machines including a central machine C; for each machine M i (in parallel) do\nSend T to all machines; for each machine M i (in parallel) do\nFor each i, let G i denote the first i items that a greedy algorithm would select from T and initialize ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "", "authors": " Ahn"}, {"ref_id": "b1", "title": "Structured robust submodular maximization: Offline and online algorithms", "journal": "PMLR", "year": "2019-04", "authors": "Nima Anari; Nika Haghtalab; Seffi Naor; Sebastian Pokutta; Mohit Singh; Alfredo Torrico"}, {"ref_id": "b2", "title": "Adversarially robust submodular maximization under knapsack constraints", "journal": "", "year": "", "authors": "Slobodan Dmitrii Avdiukhin; Grigory Mitrovic; Samson Yaroslavtsev;  Zhou"}, {"ref_id": "b3", "title": "", "journal": "BFS", "year": "", "authors": ""}, {"ref_id": "b4", "title": "Online submodular maximization with preemption", "journal": "", "year": "2015", "authors": "Niv Buchbinder; Moran Feldman; Roy Schwartz"}, {"ref_id": "b5", "title": "Streaming submodular maximization: Massive data summarization on the fly", "journal": "ACM", "year": "2014", "authors": "Ashwinkumar Badanidiyuru; Baharan Mirzasoleiman; Amin Karbasi; Andreas Krause"}, {"ref_id": "b6", "title": "Robust submodular maximization: A non-uniform partitioning approach", "journal": "", "year": "2017", "authors": "Ilija Bogunovic; Slobodan Mitrovi\u0107; Jonathan Scarlett; Volkan Cevher"}, {"ref_id": "b7", "title": "An exponential speedup in parallel running time for submodular maximization without loss in approximation", "journal": "", "year": "2019", "authors": "Eric Balkanski; Aviad Rubinstein; Yaron Singer"}, {"ref_id": "b8", "title": "The adaptive complexity of maximizing a submodular function", "journal": "STOC", "year": "2018", "authors": "Eric Balkanski; Yaron Singer"}, {"ref_id": "b9", "title": "Fast algorithms for maximizing submodular functions", "journal": "", "year": "2014", "authors": "Ashwinkumar Badanidiyuru; Jan Vondr\u00e1k"}, {"ref_id": "b10", "title": "Maximizing a monotone submodular function subject to a matroid constraint", "journal": "SIAM J. Comput", "year": "2011", "authors": "Gruia C\u0103linescu; Chandra Chekuri; Martin P\u00e1l; Jan Vondr\u00e1k"}, {"ref_id": "b11", "title": "Streaming algorithms for submodular function maximization", "journal": "", "year": "2015", "authors": "Chandra Chekuri; Shalmoli Gupta; Kent Quanrud"}, {"ref_id": "b12", "title": "", "journal": "", "year": "", "authors": " Chj +"}, {"ref_id": "b13", "title": "Online submodular maximization with free disposal: Randomization beats 0.25 for partition matroids", "journal": "", "year": "2017", "authors": "T.-H Hubert Chan; Zhiyi Huang; H.-C Shaofeng; Ning Jiang; Zhihao Gavin Kang;  Tang"}, {"ref_id": "b14", "title": "Submodular maximization meets streaming: Matchings, matroids, and more", "journal": "", "year": "2014", "authors": "Amit Chakrabarti; Sagar Kale"}, {"ref_id": "b15", "title": "Submodular function maximization in parallel via the multilinear relaxation", "journal": "", "year": "2019", "authors": "Chandra Chekuri; Kent Quanrud"}, {"ref_id": "b16", "title": "power of randomization: Distributed submodular maximization on massive datasets", "journal": "", "year": "2015", "authors": "Rafael Da; Ponte Barbosa; Alina Ene; L Huy; Justin Nguyen;  Ward"}, {"ref_id": "b17", "title": "A new framework for distributed submodular maximization", "journal": "", "year": "2016", "authors": "Rafael Da; Ponte Barbosa; Alina Ene; L Huy; Justin Nguyen;  Ward"}, {"ref_id": "b18", "title": "Beyond keyword search: discovering relevant scientific literature", "journal": "ACM", "year": "2011", "authors": "Khalid El-Arini; Carlos Guestrin"}, {"ref_id": "b19", "title": "Streaming weak submodularity: Interpreting neural networks on the fly", "journal": "", "year": "2017", "authors": "Ethan R Elenberg; Alexandros G Dimakis; Moran Feldman; Amin Karbasi"}, {"ref_id": "b20", "title": "Submodular optimization over sliding windows", "journal": "", "year": "2017", "authors": "Alessandro Epasto; Silvio Lattanzi; Sergei Vassilvitskii; Morteza Zadimoghaddam"}, {"ref_id": "b21", "title": "A nearly-linear time algorithm for submodular maximization with a knapsack constraint", "journal": "", "year": "2019", "authors": "Alina Ene; L Huy;  Nguyen"}, {"ref_id": "b22", "title": "Submodular maximization with nearly-optimal approximation and adaptivity in nearly-linear time", "journal": "", "year": "2019", "authors": "Alina Ene; L Huy;  Nguyen"}, {"ref_id": "b23", "title": "A threshold of ln n for approximating set cover", "journal": "J. ACM", "year": "1998", "authors": "Uriel Feige"}, {"ref_id": "b24", "title": "Do less, get more: Streaming submodular maximization with subsampling", "journal": "NeurIPS", "year": "2018", "authors": "Moran Feldman; Amin Karbasi; Ehsan Kazemi"}, {"ref_id": "b25", "title": "Submodular maximization with nearly optimal approximation, adaptivity and query complexity", "journal": "", "year": "2019", "authors": "Matthew Fahrbach; S Vahab; Morteza Mirrokni;  Zadimoghaddam"}, {"ref_id": "b26", "title": "Budgeted nonparametric learning from data streams", "journal": "", "year": "2010", "authors": "Ryan Gomes; Andreas Krause"}, {"ref_id": "b27", "title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "journal": "Journal of Artificial Intelligence Research", "year": "2011", "authors": "Daniel Golovin; Andreas Krause"}, {"ref_id": "b28", "title": "MovieLens Datasets", "journal": "", "year": "2015", "authors": " Grouplens"}, {"ref_id": "b29", "title": "Multi-pass streaming algorithms for monotone submodular function maximization", "journal": "WADS", "year": "2019", "authors": "Chien-Chung Huang; Naonori Kakimura"}, {"ref_id": "b30", "title": "Streaming algorithms for maximizing monotone submodular functions under a knapsack constraint", "journal": "", "year": "2017", "authors": "Chien-Chung Huang; Naonori Kakimura; Yuichi Yoshida"}, {"ref_id": "b31", "title": "Optimal marketing strategies over social networks", "journal": "ACM", "year": "2008", "authors": "Jason Hartline; Vahab Mirrokni; Mukund Sundararajan"}, {"ref_id": "b32", "title": "Near optimal algorithms for hard submodular programs with discounted cooperative costs", "journal": "PMLR", "year": "2019-04", "authors": "Rishabh Iyer; Jeffrey Bilmes"}, {"ref_id": "b33", "title": "Sparse Submodular Probabilistic PCA", "journal": "PMLR", "year": "2015-05", "authors": "Rajiv Khanna; Joydeep Ghosh; Russell Poldrack; Oluwasanmi Koyejo"}, {"ref_id": "b34", "title": "The budgeted maximum coverage problem", "journal": "Inf. Process. Lett", "year": "1999", "authors": "Samir Khuller; Anna Moss; Joseph Naor"}, {"ref_id": "b35", "title": "Fast greedy algorithms in mapreduce and streaming", "journal": "ACM Transactions on Parallel Computing (TOPC)", "year": "2015", "authors": "Ravi Kumar; Benjamin Moseley; Sergei Vassilvitskii; Andrea Vattani"}, {"ref_id": "b36", "title": "", "journal": "", "year": "", "authors": " Kmz +"}, {"ref_id": "b37", "title": "Submodular streaming in all its glory: Tight approximation, minimum memory and low adaptive complexity", "journal": "", "year": "2019", "authors": "Ehsan Kazemi; Marko Mitrovic; Morteza Zadimoghaddam; Silvio Lattanzi; Amin Karbasi"}, {"ref_id": "b38", "title": "A model of computation for mapreduce", "journal": "", "year": "2010", "authors": "Howard J Karloff; Siddharth Suri; Sergei Vassilvitskii"}, {"ref_id": "b39", "title": "A class of submodular functions for document summarization", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Hui Lin; Jeff Bilmes"}, {"ref_id": "b40", "title": "SNAP Datasets: Stanford large network dataset collection", "journal": "", "year": "2014-06", "authors": "Jure Leskovec; Andrej Krevl"}, {"ref_id": "b41", "title": "", "journal": "", "year": "", "authors": " Lkg +"}, {"ref_id": "b42", "title": "Cost-effective outbreak detection in networks", "journal": "ACM", "year": "2007", "authors": "Jure Leskovec; Andreas Krause; Carlos Guestrin; Christos Faloutsos; Jeanne Van-Briesen; Natalie Glance"}, {"ref_id": "b43", "title": "Submodular optimization in the mapreduce model", "journal": "", "year": "2019", "authors": "Paul Liu; Jan Vondr\u00e1k"}, {"ref_id": "b44", "title": "Leveraging sparsity for efficient submodular data summarization", "journal": "", "year": "2016", "authors": "Erik M Lindgren; Shanshan Wu; Alexandros G Dimakis"}, {"ref_id": "b45", "title": "", "journal": "", "year": "", "authors": " "}, {"ref_id": "b46", "title": "Streaming robust submodular maximization: A partitioned thresholding approach", "journal": "", "year": "2017", "authors": "Slobodan Mitrovi\u0107; Ilija Bogunovic; Ashkan Norouzi-Fard; Jakub Tarnawski; Volkan Cevher"}, {"ref_id": "b47", "title": "Streaming nonmonotone submodular maximization: Personalized video summarization on the fly", "journal": "", "year": "2018", "authors": "Baharan Mirzasoleiman; Stefanie Jegelka; Andreas Krause"}, {"ref_id": "b48", "title": "Distributed submodular maximization: Identifying representative elements in massive data", "journal": "", "year": "2013", "authors": "Baharan Mirzasoleiman; Amin Karbasi; Rik Sarkar; Andreas Krause"}, {"ref_id": "b49", "title": "Randomized composable coresets for distributed submodular maximization", "journal": "STOC", "year": "2015", "authors": "S Vahab; Morteza Mirrokni;  Zadimoghaddam"}, {"ref_id": "b50", "title": "Fast distributed submodular cover: Public-private data summarization", "journal": "", "year": "2016", "authors": "Baharan Mirzasoleiman; Morteza Zadimoghaddam; Amin Karbasi"}, {"ref_id": "b51", "title": "NTM +", "journal": "", "year": "", "authors": ""}, {"ref_id": "b52", "title": "Beyond 1/2-approximation for submodular maximization on massive data streams", "journal": "", "year": "2018", "authors": "Ashkan Norouzi-Fard; Jakub Tarnawski; Slobodan Mitrovic; Amir Zandieh; Aidasadat Mousavifar; Ola Svensson"}, {"ref_id": "b53", "title": "An analysis of approximations for maximizing submodular set functions -I", "journal": "Math. Program", "year": "1978", "authors": "George L Nemhauser; Laurence A Wolsey; Marshall L Fisher"}, {"ref_id": "b54", "title": "Temporal corpus summarization using submodular word coverage", "journal": "ACM", "year": "2012", "authors": "Ruben Sipos; Adith Swaminathan; Pannaga Shivaswamy; Thorsten Joachims"}, {"ref_id": "b55", "title": "A note on maximizing a submodular set function subject to a knapsack constraint", "journal": "Oper. Res. Lett", "year": "2004", "authors": "Maxim Sviridenko"}, {"ref_id": "b56", "title": "Using document summarization techniques for speech data subset selection", "journal": "", "year": "2013", "authors": "Kai Wei; Yuzong Liu; Katrin Kirchhoff; Jeff Bilmes"}, {"ref_id": "b57", "title": "Maximising real-valued submodular functions: Primal and dual heuristics for location problems", "journal": "Math. Oper. Res", "year": "1982", "authors": "Laurence A Wolsey"}, {"ref_id": "b58", "title": "Streaming algorithms for news and scientific literature recommendation: Monotone submodular maximization with a $d$ -knapsack constraint", "journal": "IEEE Access", "year": "2018", "authors": "Qilian Yu; Easton Li Xu; Shuguang Cui"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 :1Fig. 1: Monotone submodular maximization under a knapsack constraint in the streaming model.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Algorithm 2 :2Multi-pass streaming algorithm Sieve+Max Input: Stream e 1 , . . . , e n , knapsack capacity K, cost function c(\u2022), non-negative monotone submodular function f , \u03bb which is an \u03b1-approximation of f (OPT) for some fixed constant \u03b1>0, > 0; Output: ( 1 /2 -)-approx. for submodular maximization under a knapsack constraint; T \u2190 \u2205, \u03c4 \u2190 \u03bb \u03b1K ; while \u03c4 > \u03bb 2K do // Thresholding stage Take a new pass over the stream; for each read item e do if \u03c1 (e | T ) \u2265 \u03c4 and c(e \u222a T ) \u2264 K then T \u2190 T \u222a {e}; \u03c4 \u2190 \u03c4 /(1 + );", "figure_data": ""}, {"figure_label": "45", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 4 :Fig. 5 :45Fig. 4: Ratio of the objective of offline algorithms to the objective of Greedy for different values of K. Greedy+Max can improve by almost 50% upon Greedy, but by definition, Greedy+Max and GreedyOrMax cannot perform worse than Greedy. Despite its runtime, PartialEnum+Greedy does not outperform Greedy+Max on the ego-Facebook dataset.", "figure_data": ""}, {"figure_label": "89", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Fig. 8 :Fig. 9 :89Fig.8: Ratio of average runtime of streaming algorithms compared to the average runtime of Sieve, with error bars representing one standard deviation for each algorithm on the corresponding knapsack constraint across ten iterations.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Proposition A. 11Let \u03be be a continuous and piecewise smooth function [u, v] \u2192 R + . If for some \u03b1, \u03b2 > 0 we have \u03be(x) + \u03b1\u03be (x) \u2265 \u03b2 for u \u2264 x \u2264 v, then \u03be(v) \u2265 \u03b2 + (\u03be(u) -\u03b2)e u-v \u03b1 .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "by e x \u03b1 . Observe that \u03be(x)e x \u03b1 + \u03b1\u03be (x)e x \u03b1 is the derivative of \u03be(x)\u03b1e x \u03b1 . Hence, d(\u03be(x)\u03b1e", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "x", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "e\u2208OPT\\G i- 1 f1(e | G i-1 ) . Since f (e | G i-1 ) = c(e)\u03c1 (e | G i-1 ): 1 \u2264 g(x) + e\u2208OPT\\G i-1 f (e | G i-1 ) = g(x) + e\u2208OPT\\G i-1 c(e)\u03c1 (e | G i-1 ) \u2264 g(x) + e\u2208OPT\\G i-1 c(e)g (x),where the last inequality follows because greedy always picks the item with the largest marginal density and since x \u2264 1 -c(o 1 ) every item in OP T \\ G i-1 can still fit into the knapsack. Hence,1 \u2264 g(x) + g (x) e\u2208OPT\\G i-1 c(e)= g(x) + g (x)c(OPT \\ G i-1 ).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "e\u2208OPT\\T i- 1 f1(e | T i-1 ) .Since f (e | T i-1 ) = c(e)\u03c1 (e | T i-1 ):1 \u2264 t(x) + e\u2208OPT\\T i-1 f (e | T i-1 ) = t(x) + e\u2208OPT\\T i-1 c(e)\u03c1 (e | T i-1 ) \u2264 t(x) + e\u2208OPT\\T i-1 c(e)t (x)(1 + ),where the last inequality follows because after the first pass t (x) \u2265 \u03c1(e | T i-1 ) 1+", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Fact", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "1 3 -3-approximation. On the other hand, by Theorem 1 in [KMZ + 19], Algorithm 4 uses space O K and query complexity O n log K . 2", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "1 O 1 K log K O 1 n log K [HKY17] 4 /11 - 1 O 1 K log K O 1 n log K [HKY17] 2 /5 - 3 O 1 2 K log 2 K O 1 n log K [HK19] 1 /2 - O ( 1 / ) O 1 7 K log 2 K O 1 8 n log 2 K Sieve+Max (Alg. 2) 1 /2 - O ( 1 / ) O (K) O n 1 + log K", "formula_coordinates": [4.0, 108.17, 466.17, 376.85, 86.53]}, {"formula_id": "formula_1", "formula_text": "g (1 -c (o 1 )) \u2265 1 /2(1-c(o 1 )). Since g(0) = 0 and g is non-increasing, g(1 -c (o 1 )) \u2265 (1 -c (o 1 ))g (1 -c (o 1 )) \u2265 1 /2.", "formula_coordinates": [5.0, 72.0, 531.78, 468.0, 24.18]}, {"formula_id": "formula_2", "formula_text": "s i \u222a G i ) \u2264 K.", "formula_coordinates": [6.0, 477.56, 263.61, 62.44, 10.63]}, {"formula_id": "formula_3", "formula_text": "G \u2190 \u2205, S \u2190 \u2205; while E = \u2205 do s \u2190 argmax e\u2208E f (e | G); if f (S) < f (G \u222a s) then S \u2190 G \u222a s; a \u2190 argmax e\u2208E \u03c1 (e | G); G \u2190 G \u222a a; K \u2190 K -c(a);", "formula_coordinates": [6.0, 88.94, 374.43, 134.39, 102.38]}, {"formula_id": "formula_4", "formula_text": "g(x) = f (G i-1 ) + (x -c(G i-i ))\u03c1 (g i | G i-1 ) .", "formula_coordinates": [7.0, 205.39, 247.46, 201.22, 10.63]}, {"formula_id": "formula_5", "formula_text": "+ (x) = g(x) + f (v | G i-1 ), where v = argmax e\u2208E\\G i-1 :c(e\u222aG i-1 )\u2264K f (e | G i-1 )", "formula_coordinates": [7.0, 224.29, 390.26, 184.69, 44.32]}, {"formula_id": "formula_6", "formula_text": "Definition 2.4 (Greedy+Max performance lower bound) For x \u2208 [0, 1 -c (o 1 ) -c * ] we define g 1 (x) = g(x) + f (o 1 | G i-1 ) so that g 1 (x) \u2264 g + (x).", "formula_coordinates": [7.0, 72.0, 561.17, 468.0, 26.13]}, {"formula_id": "formula_7", "formula_text": "g 1 (x) + (1 -c (o 1 ))g (x) \u2265 1", "formula_coordinates": [7.0, 239.29, 640.23, 133.41, 10.63]}, {"formula_id": "formula_8", "formula_text": "1 (x) = g(c(G i-1 ))+f (o 1 | G i-1 ) = f (G i-1 \u222ao 1 ). Since we normalized f (OPT) = 1, then by monotonicity, 1 = f (OPT) \u2264 f (G i-1 \u222aOPT).", "formula_coordinates": [8.0, 72.0, 89.06, 468.0, 24.18]}, {"formula_id": "formula_9", "formula_text": "1 \u2264 f (G i-1 \u222a OPT) = f (G i-1 \u222a o 1 ) + f (OPT \\ (o 1 \u222a G i-1 ) | G i-1 \u222a o 1 ) \u2264 g 1 (x) + e\u2208OPT\\(o 1 \u222aG i-1 ) f (e | G i-1 \u222a o 1 ) = g 1 (x) + e\u2208OPT\\(o 1 \u222aG i-1 ) c(e)\u03c1 (e | G i-1 \u222a o 1 ) ,", "formula_coordinates": [8.0, 185.24, 139.92, 241.53, 92.16]}, {"formula_id": "formula_10", "formula_text": "1 \u222aG i-1 ) \u03c1 (e | G i-1 \u222a o 1 ) \u2264 g (x)", "formula_coordinates": [8.0, 275.41, 284.88, 139.76, 12.05]}, {"formula_id": "formula_11", "formula_text": "1 \u2264 g 1 (x) + e\u2208OPT\\(o 1 \u222aG i-1 ) c(e)\u03c1 (e | G i-1 \u222a o 1 ) \u2264 g 1 (x) + e\u2208OPT\\(o 1 \u222aG i-1 ) c(e)\u03c1 (e | G i-1 ) \u2264 g 1 (x) + e\u2208OPT\\(o 1 \u222aG i-1 ) c(e)g (x) = g 1 (x) + g (x) e\u2208OPT\\(o 1 \u222aG i-1 ) c(e) = g 1 (x) + g (x)c(OPT \\ (o 1 \u222a G i-1 )) \u2264 g 1 (x) + g (x)(1 -c (o 1 )),", "formula_coordinates": [8.0, 197.03, 311.94, 217.95, 154.69]}, {"formula_id": "formula_12", "formula_text": "g 1 (1 -c (o 1 ) -c * ) + (1 -c (o 1 ))g (1 -c (o 1 ) -c * ) \u2265 1 If g 1 (1 -c(o 1 ) -c * ) \u2265 1 2 , then we have 1 2 -approximation, because g 1 (1 -c(o 1 ) -c *", "formula_coordinates": [8.0, 72.0, 602.89, 383.07, 39.14]}, {"formula_id": "formula_13", "formula_text": "g (1 -c (o 1 ) -c * ) \u2265 1 -g 1 (1 -c (o 1 ) -c * ) 1 -c (o 1 ) > 1 2(1 -c (o 1 ))", "formula_coordinates": [8.0, 165.0, 663.12, 277.77, 27.45]}, {"formula_id": "formula_14", "formula_text": "g(x) \u2265 x \u03c7=0 g (\u03c7)d\u03c7 \u2265 x \u03c7=0 g (x)d\u03c7 = g (x) \u2022 x, Therefore, applying this inequality at x = 1 -c (o 1 ) -c * : g(1 -c (o 1 ) -c * ) \u2265 (1 -c (o 1 ) -c * )g (1 -c (o 1 ) -c * ) \u2265 1 -c (o 1 ) -c * 2(1 -c (o 1 )) .", "formula_coordinates": [9.0, 72.0, 108.44, 360.92, 106.03]}, {"formula_id": "formula_15", "formula_text": "(1 -c (o 1 )) -(1 -c (o 1 ) -c * ) = c * .", "formula_coordinates": [9.0, 72.0, 251.12, 167.77, 12.58]}, {"formula_id": "formula_16", "formula_text": "g(1 -c (o 1 ) -c * ) + c * g (1 -c (o 1 ) -c * ) \u2265 1 -c (o 1 ) -c * 2(1 -c (o 1 )) + c * 2(1 -c (o 1 )) = 1 2", "formula_coordinates": [9.0, 122.79, 273.24, 365.22, 27.45]}, {"formula_id": "formula_17", "formula_text": "j = max{i|c(G i ) + c(e) \u2264 K}; if f (G j \u222a s j ) < f (G j \u222a e) then s j \u2190 {e}; return argmax f (G i \u222a s i )", "formula_coordinates": [10.0, 88.94, 300.86, 182.67, 50.97]}, {"formula_id": "formula_18", "formula_text": "t 1 (x) + (1 + )(1 -c (o 1 ))t (x) \u2265 1.", "formula_coordinates": [10.0, 223.39, 562.9, 165.23, 10.63]}, {"formula_id": "formula_19", "formula_text": "1 = f (OPT) \u2264 f (T i-1 \u222a OPT). Hence: 1 \u2264 f (T i-1 \u222a OPT) = f ((T i-1 \u222a o 1 ) \u222a (OPT \\ o 1 )) = f (T i-1 \u222a o 1 ) + f (OPT \\ (o 1 \u222a T i-1 ) | T i-1 \u222a o 1 ) \u2264 t 1 (x) + e\u2208OPT\\(o 1 \u222aT i-1 ) f (e | T i-1 \u222a o 1 ) = t 1 (x) + e\u2208OPT\\(o 1 \u222aT i-1 ) c(e)\u03c1 (e | T i-1 \u222a o 1 ) ,", "formula_coordinates": [11.0, 72.0, 75.39, 353.94, 129.44]}, {"formula_id": "formula_20", "formula_text": "(1 + )t(x) \u2265 max e\u2208OPT\\(o 1 \u222aT i-1 ) \u03c1 (e | o 1 \u222a T i-1 ) .", "formula_coordinates": [11.0, 200.74, 275.24, 210.52, 17.62]}, {"formula_id": "formula_21", "formula_text": "1 \u2264 t 1 (x) + e\u2208OPT\\(o 1 \u222aT i-1 ) c(e)\u03c1 (e | T i-1 \u222a o 1 ) \u2264 t 1 (x) + (1 + )t (x) e\u2208OPT\\(o 1 \u222aT i-1 ) c(e) = t 1 (x) + (1 + )t (x)c(OPT \\ (o 1 \u222a T i-1 )) \u2264 t 1 (x) + (1 + )t (x)(1 -c (o 1 )),", "formula_coordinates": [11.0, 198.11, 323.64, 215.78, 89.94]}, {"formula_id": "formula_22", "formula_text": "t 1 (1 -c (o 1 ) -c * ) + (1 + )(1 -c (o 1 ))t (1 -c (o 1 ) -c * ) \u2265 1 If t 1 (1 -c (o 1 ) -c * ) \u2265 1 2 , then we have 1 2 -approximation, because t 1 (1 -c(o 1 ) -c *", "formula_coordinates": [11.0, 72.0, 569.75, 382.96, 36.0]}, {"formula_id": "formula_23", "formula_text": "t (1 -c (o 1 ) -c * ) \u2265 1 -g 1 (1 -c (o 1 ) -c * ) (1 -c (o 1 ))(1 + ) > 1 2(1 -c (o 1 ))(1 + )", "formula_coordinates": [11.0, 204.7, 634.51, 201.41, 56.05]}, {"formula_id": "formula_24", "formula_text": "t(x) \u2265 x \u03c7=0 t (\u03c7)d\u03c7 \u2265 x \u03c7=0 t (x) 1 + d\u03c7 = t (x) \u2022 x 1 + ,", "formula_coordinates": [12.0, 193.57, 98.68, 224.86, 28.58]}, {"formula_id": "formula_25", "formula_text": "t(1 -c (o 1 ) -c * ) \u2265 (1 -c (o 1 ) -c * )t (1 -c (o 1 ) -c * ) \u2265 1 -c (o 1 ) -c * 2(1 -c (o 1 ))(1 + ) .", "formula_coordinates": [12.0, 180.74, 176.76, 250.53, 44.22]}, {"formula_id": "formula_26", "formula_text": "g(1 -c (o 1 ) -c * ) + c * g (1 -c (o 1 ) -c * ) \u2265 1 -c (o 1 ) -c * 2(1 -c (o 1 ))(1 + ) + c * 2(1 -c (o 1 ))(1 + ) = 1 2(1 + ) = 1 2 - 2(1 + ) \u2265 1 2 -.", "formula_coordinates": [12.0, 106.71, 281.23, 397.39, 54.99]}, {"formula_id": "formula_27", "formula_text": "X i+1 | \u2264 c i . Then Pr [X n -X 0 \u2264 -t] \u2264 exp -t 2 2 i c 2 i .", "formula_coordinates": [12.0, 72.0, 647.88, 323.05, 40.21]}, {"formula_id": "formula_28", "formula_text": "N i + |\u0393 i |.", "formula_coordinates": [14.0, 72.0, 197.82, 44.76, 10.63]}, {"formula_id": "formula_29", "formula_text": "K n \u221a n K > 1/2.", "formula_coordinates": [14.0, 252.36, 294.53, 69.72, 31.75]}, {"formula_id": "formula_30", "formula_text": "[X i | X 1 , . . . , X i-1 ] \u2265 1 2 . Let Y i = i j=1 (X i -1/2) so that the sequence Y 1 , Y 2 , . . . is a submartingale, i.e., E[Y i | Y 1 , . . . , Y i-1 ] \u2265 Y i-1 and |Y i -Y i-1 | \u2264 1. By Azuma's inequality (Theorem 2.11), Pr[Y 3 K < -1 2 K] < e -\u2126( K) , so that 3 K j=1 X j = Y K + 3 2 K \u2265 K with probability at least 1 -e -\u2126(", "formula_coordinates": [14.0, 72.0, 343.66, 468.0, 64.34]}, {"formula_id": "formula_31", "formula_text": "v \u2208 V is c(v) = \u03b2 |V | (|N (v)| -\u03b1)", "formula_coordinates": [16.0, 98.15, 217.23, 146.55, 15.83]}, {"formula_id": "formula_32", "formula_text": "\u03b1 ) dx \u2265 \u03b2e x \u03b1 implies v u d(\u03be(x)\u03b1e x \u03b1 ) \u2265 v u \u03b2e x \u03b1 dx (\u03be(x)\u03b1e x \u03b1 ) v u \u2265 \u03b1\u03b2e x \u03b1 v u \u03be(v)\u03b1e v \u03b1 -\u03be(u)\u03b1e u \u03b1 \u2265 \u03b1\u03b2e v \u03b1 -\u03b1\u03b2e u \u03b1 .", "formula_coordinates": [25.0, 173.21, 481.66, 220.97, 89.83]}, {"formula_id": "formula_33", "formula_text": "\u03be(v)e v \u03b1 -\u03be(u)e u \u03b1 \u2265 \u03b2e v \u03b1 -\u03b2e u \u03b1 \u03be(v) \u2265 \u03b2 + (\u03be(u) -\u03b2)e u-v \u03b1 .", "formula_coordinates": [25.0, 212.68, 599.79, 186.64, 33.7]}, {"formula_id": "formula_34", "formula_text": "\u03be(x i ) \u2265 \u03b2 + (\u03be(x 0 ) -\u03b2)e x i -x 0 \u03b1 .", "formula_coordinates": [25.0, 234.15, 677.09, 143.7, 16.2]}, {"formula_id": "formula_35", "formula_text": "\u03be(x i+1 ) \u2265 \u03b2 + (\u03be(x i ) -\u03b2)e x i -x i+1 \u03b1 \u2265 \u03b2 + (\u03be(x 0 ) -\u03b2)e x 0 -x i \u03b1 e x i -x i+1 \u03b1 \u2265 \u03b2 + (\u03be(x 0 ) -\u03b2)e x 0 -x i+1 \u03b12", "formula_coordinates": [26.0, 211.44, 95.0, 328.56, 82.58]}, {"formula_id": "formula_36", "formula_text": "g(x) + g (x) \u2265 1,", "formula_coordinates": [26.0, 266.11, 232.8, 79.79, 9.57]}, {"formula_id": "formula_37", "formula_text": "Let x \u2208 [0, 1 -c (o 1 )", "formula_coordinates": [26.0, 129.0, 279.2, 99.56, 10.7]}, {"formula_id": "formula_38", "formula_text": "1 = f (OPT) \u2264 f (OPT \u222a G i-1 ) = f (G i-1 ) + f (OPT \\ G i-1 | G i-1 ) .", "formula_coordinates": [26.0, 220.27, 369.74, 171.47, 27.17]}, {"formula_id": "formula_39", "formula_text": "1 \u2264 f (G i-1 ) + f (OPT \\ G i-1 | G i-1 )", "formula_coordinates": [26.0, 222.69, 431.75, 166.62, 10.63]}, {"formula_id": "formula_40", "formula_text": "1 = f (OPT) \u2264 f (OPT \u222a T i-1 ) = f (T i-1 ) + f (OPT \\ T i-1 | T i-1 ) .", "formula_coordinates": [27.0, 221.09, 321.53, 169.82, 27.17]}, {"formula_id": "formula_41", "formula_text": "1 \u2264 f (T i-1 ) + f (OPT \\ T i-1 | T i-1 ) \u2264 t(x) +", "formula_coordinates": [27.0, 223.51, 384.25, 164.98, 29.07]}, {"formula_id": "formula_42", "formula_text": "1 \u2264 t(x) + e\u2208OPT\\T i-1", "formula_coordinates": [27.0, 218.87, 648.2, 99.99, 23.62]}], "doi": ""}
