{"Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention": "", "Ziru Liu": "City University of Hong Kong Hong Kong, China ziruliu2-c@my.cityu.edu.hk Shuchang Liu Kuaishou Technology Beijing, China liushuchang@kuaishou.com Zijian Zhang City University of Hong Kong Hong Kong, China zhangzj2114@mails.jlu.edu.cn Qingpeng Cai* Kuaishou Technology Beijing, China cqpcurry@gmail.com", "Lantao Hu": "Kuaishou Technology Beijing, China hulantao@kuaishou.com", "Xiangyu Zhao*": "City University of Hong Kong Hong Kong, China xianzhao@cityu.edu.hk", "Kesen Zhao": "City University of Hong Kong Hong Kong, China kesenzhao2-c@my.cityu.edu.hk Peng Jiang* Kuaishou Technology Beijing, China jp2006@139.com", "ABSTRACT": "In Recommender System (RS) applications, reinforcement learning (RL) has recently emerged as a powerful tool, primarily due to its proficiency in optimizing long-term rewards. Nevertheless, it suffers from instability in the learning process, stemming from the intricate interactions among bootstrapping, off-policy training, and function approximation. Moreover, in multi-reward recommendation scenarios, designing a proper reward setting that reconciles the inner dynamics of various tasks is quite intricate. To this end, we propose a novel decision transformer-based recommendation model, DT4IER, to not only elevate the effectiveness of recommendations but also to achieve a harmonious balance between immediate user engagement and long-term retention. The DT4IER applies an innovative multi-reward design that adeptly balances short and long-term rewards with user-specific attributes, which serve to enhance the contextual richness of the reward sequence, ensuring a more informed and personalized recommendation process. To enhance its predictive capabilities, DT4IER incorporates a high-dimensional encoder to identify and leverage the intricate interrelations across diverse tasks. Furthermore, we integrate a contrastive learning approach within the action embedding predictions, significantly boosting the model's overall performance. Experiments on three real-world datasets demonstrate the effectiveness of DT4IER against state-of-the-art baselines in terms of both Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '24, July 14-18, 2024, Washington, DC, USA https://doi.org/10.1145/3626772.3657829 Kun Gai Unaffiliated Beijing, China gai.kun@qq.com immediate user engagement and long-term retention. The source code is accessible online to facilitate replication 1 .", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "Recommender Systems; Decision Transformer; Multi-task Learning", "ACMReference Format:": "Ziru Liu, Shuchang Liu, Zijian Zhang, Qingpeng Cai*, Xiangyu Zhao*, Kesen Zhao, Lantao Hu, Peng Jiang*, and Kun Gai. 2024. Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention. In Proceedings of Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24). ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3626772.3657829", "1 INTRODUCTION": "In today's digital age, platforms spanning from social media to e-commerce have led to an explosion in the amount of information available online, underscoring the importance of efficient navigation tools [1]. Recommender Systems (RS) have emerged as a crucial technology in this realm, adeptly improving content suggestions and optimizing recommendations according to user interests inferred from their historical engagement. In recent years, researchers have proposed various methods for Recommender Systems, encompassing collaborative filtering [42], matrix factorization-based approaches [27], and those powered by deep learning [7, 14, 69]. Among them, transformer-based models, notably BERT4Rec [50] and SASRec [26] have risen to prominence, redefining the landscape of recommendation systems. It is believed that the strength of transformers lies in the attention mechanisms [57], which can dynamically capture the inherent dependencies in data, offering an accurate understanding of user patterns. This adaptability and precision make transformers well-suited for navigating the realm of user preferences inference and immediate feedback prediction. In practice, a range of metrics such as clicks, likes, and ratings are widely used as indicators of user preferences. Though effective, these immediate metrics offer limited insights into the user's lasting impression in the long term. In some cases, they can be misleading indicators of content quality [62, 66]. For instance, content with an eye-catching title but poor content quality may initially draw attention and obtain positive feedback, but only to later abuse the users' trust and undermine their loyalty. This highlights a pressing need to balance the RS's focus between short-term user engagement and long-term user satisfaction [58] . To this end, methods based on Reinforcement Learning (RL) have emerged [11]. By modeling sequential user behaviors using a Markov Decision Process (MDP), RL-based systems can dynamically adapt recommendations at every user interaction juncture [39, 79]. The strength of RL lies in its capability to optimize cumulative rewards over extended sequences in the future, allowing it to capture long-term objectives and trends, and ensure sustained user engagement. A recent finding [4] shows that RL-based RS can even directly optimize user retention, a crucial metric often ignored but paramount in real-world business contexts [13, 81] that is closely related to daily active users (DAU). Despite their promises, RL-based systems have some inherent challenges. Firstly, the long-term credit assignment through bootstrapping leads to the Deadly Triad issue which emerges from the complex interplay between bootstrapping, off-policy training, and function approximation, often making the learning process unstable [10]. Secondly, the standard practice of discounting future rewards in Temporal Difference (TD) learning can inadvertently drive the system to be overly focused on immediate gains at the expense of long-term objectives [64]. To circumvent these problems and unlock the full potential of RL-based recommendation systems, the innovative Decision Transformer (DT) [8] has been introduced and then applied in RS. During inference, the recommendation policy is conditioned on the rewards, i.e. returns-to-go (RTG). During training, it reformulates the reinforcement learning paradigms into sequence modeling tasks, and the transformer model is used to capture not only the interactions but also the user rewards. With this transformation, DT converts the intricate landscape of RL into a tractable one close to supervised learning. Additionally, DT has also been proven to be efficient in boosting recommendation performance with respect to user retention [72]. Weposit that focusing solely on immediate user feedback or longterm retention is insufficient. A more holistic approach requires optimizing both metrics simultaneously, offering a comprehensive perspective on user behavior. This approach frames the problem as a long short-term multi-task learning challenge. Adapting Decision Transformers (DT) to multi-reward contexts, however, presents significant challenges. Firstly, the intricate dynamics among various user responses in historical data underscore the complexity involved in designing corresponding rewards. This is crucial as it directly impacts the quality of the recommendations [47]. Secondly, the presence of multiple objectives introduces additional complexity to the training process. This arises from the sophisticated and often unpredictable interdependencies among different tasks, potentially impairing the model's learning efficiency. To address these challenges, we introduce DT4IER , a novel framework based on the Decision Transformer, crafted for long short-term multi-task recommendation scenarios. Our approach deploys an innovative multi-reward configuration, reinforced by a high-dimensional encoder designed to capture the intricate relationships among different tasks effectively. Furthermore, we apply an innovative approach to reward structuring, skillfully balancing short-term and long-term rewards. It does so by incorporating userspecific attributes, which serve to enhance the contextual richness of the reward sequence ensuring a more informed and personalized recommendation process. We also introduce a contrastive learning objective to ensure that the predicted action embeddings for distinct rewards do not converge too closely. Our key contributions in this paper can be summarized as follows: \u00b7 We emphasize the importance of long short-term multi-task sequential recommendations and introduce DT4IER, a novel Decision Transformer-based model engineered for integrated user engagement and retention. \u00b7 Our innovative framework applies a novel multi-reward setting that balances immediate feedback with long-term retention by user-specific features, and then complements by a corresponding high-dimensional embedding module and a contrastive loss term. \u00b7 We validate the performance of DT4IER through extensive experimentation compared with state-of-the-art Sequential Recommender Systems (SRSs) and Multi-Task Learning (MTL) models on three real-world datasets.", "2 PRELIMINARIES": "In this section, we provide an overview of the foundational concepts and primary notations used throughout this paper.", "2.1 Offline Reinforcement Learning": "Given a Markov decision process (MDP) formulated as (S , A , \ud835\udc43, R , \ud835\udefe ) where S is the set of state \ud835\udc60 \u2208 R \ud835\udc51 , A is the set of action \ud835\udc4e , \ud835\udc43 ( \ud835\udc60 \u2032 | \ud835\udc60, \ud835\udc4e ) is the transition probabilities from state \ud835\udc60 to new state \ud835\udc60 \u2032 given action \ud835\udc4e , R is the reward function for specific state-action pairs and \ud835\udefe is the discount rate. The trajectory from timestamp 0 to \ud835\udc47 can be written as ( \ud835\udc60 0 , \ud835\udc4e 0 , \ud835\udc5f 0 , \u00b7 \u00b7 \u00b7 , \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 , \ud835\udc5f \ud835\udc61 , \u00b7 \u00b7 \u00b7 , \ud835\udc60 \ud835\udc47 , \ud835\udc4e \ud835\udc47 , \ud835\udc5f \ud835\udc47 ) , where ( \ud835\udc60 \ud835\udc61 , \ud835\udc4e \ud835\udc61 , \ud835\udc5f \ud835\udc61 ) are state, action and reward pairs at timestamp \ud835\udc61 . The learning objective of RL is to determine an optimal policy that maximizes the expected cumulative return E GLYPH<2>\u02dd \ud835\udc47 \ud835\udc61 = 1 \ud835\udefe \ud835\udc61 \ud835\udc5f \ud835\udc61 GLYPH<3> given a specific reward function and discount rate.", "2.2 Decision Transformer": "Rather than employing traditional RL algorithms such as training an agent policy or approximating value functions [23], the decision transformer follows a different routine that recasts RL into a sequence modeling problem with supervised learning objectives. To equip the transformer with the capability to discern significant patterns, the corresponding trajectory representation with \ud835\udc47 timestamps is designed as: In this representation, b \ud835\udc45 \ud835\udc61 = \u02dd \ud835\udc47 \ud835\udc58 = \ud835\udc61 \ud835\udc5f \ud835\udc58 defines the returns-to-go (RTG) which represents the cumulative reward from time \ud835\udc61 through to time \ud835\udc47 , without applying any discount. Given the RTG and state information, the DT is capable of predicting the next action \ud835\udc4e \ud835\udc47 by a causally masked transformer architecture with layered self-attention and residual connections [8]. In each layer, \ud835\udc5a input embeddings, denoted as { \ud835\udc65 \ud835\udc56 } \ud835\udc5a \ud835\udc56 = 1 , are processed to yield corresponding output embeddings { \ud835\udc67 \ud835\udc56 } \ud835\udc5a \ud835\udc56 = 1 . Each token's position, \ud835\udc56 , dictates its transformation into a key \ud835\udc58 \ud835\udc56 , a query \ud835\udc5e \ud835\udc56 , and a value \ud835\udc63 \ud835\udc56 [57]. The resultant output for the same position is derived by adjusting values, \ud835\udc63 \ud835\udc57 , using the normalized dot product between the query \ud835\udc5e \ud835\udc56 and its associated keys \ud835\udc58 \ud835\udc57 which is further processed by a softmax activation function \ud835\udf0e \ud835\udc60 : In the realm of recommendation systems, the inference process utilizing the DT can be concisely described as follows: Initially, the model receives a combination of state and action inputs. These inputs are processed through the Decision Transformer, resulting in an output that consists of an action embedding specifically prompted by a designed reward which is integral to guiding the model's decision-making process. Subsequently, this action embedding undergoes a decoding process, which ultimately yields a sequence of recommended items.", "3 THE PROPOSED FRAMEWORK": "In the following section, we provide a comprehensive overview of our proposed method, DT4IER. We begin with the problem formulation, setting the stage for a deeper understanding, and then detailing the intricate design and settings of the specific modules.", "3.1 Problem Defination": "In the conventional sequential recommendation scenario, the objective is to recommend items based on a user's historical sequence optimized for specific indicators. However, from a business perspective, focusing on a single metric can be limiting since user behavior can be influenced by various factors and can exhibit different patterns over time. Therefore, we prioritize the optimization of two key performance indicators: click-through rate (CTR) and return frequency. The former serves as a widely recognized business metric in numerous real-world applications, offering immediate insight into user engagement. The latter is intrinsically tied to vital operational metrics, including daily active users (DAU), which are essential for sustained platform growth and user retention. This optimization strategy is applicable to a variety of digital platforms, encompassing streaming services, e-commerce websites, and social media networks, where both immediate user engagement and longterm user retention are critical for success. To realize this, we adapt the DT framework to a multi-reward setting, whose architecture can be naturally extended to handle multiple reward signals. Given the input trajectory \ud835\udf0f = GLYPH<16> b R 1 , s 1 , a 1 , . . . , b R \ud835\udc61 , s \ud835\udc61 , a \ud835\udc61 , . . . , b R \ud835\udc47 , s \ud835\udc47 GLYPH<17> , the state-action and RTG is defined as: \u00b7 Session \ud835\udc61 represents an individual timestamp within a trajectory of length \ud835\udc47 . It also signifies a specific day for a particular user. \u00b7 State s \ud835\udc61 \u2208 R \ud835\udc3b represents the historical interaction information for a user before session \ud835\udc61 , which comprises user-clicked item IDs, zero-padded to length \ud835\udc3b . It will be updated based on clicked items in the current action. \u00b7 Action a \ud835\udc61 \u2208 R \ud835\udc41 is the recommendation list containing item IDs of length \ud835\udc41 , denoting the action based on state \ud835\udc60 \ud835\udc61 . \u00b7 Reward r \ud835\udc61 = ( \ud835\udc5f \ud835\udc60,\ud835\udc61 , \ud835\udc5f \ud835\udc59,\ud835\udc61 ) \u2208 R 2 represents the feedback corresponding to the actions \ud835\udc4e \ud835\udc61 executed at session \ud835\udc61 . In this paper, our focus is primarily on two pivotal metrics in real-world recommender systems. The short-term indicator \ud835\udc5f \ud835\udc60,\ud835\udc61 is quantified as the click-through rate for recommended action \ud835\udc4e \ud835\udc61 at session \ud835\udc61 . And long-term indicator \ud835\udc5f \ud835\udc59,\ud835\udc61 is defined as the return frequency for a given session \ud835\udc61 to measure user retention. \u00b7 Return-to-go (RTG) b R \ud835\udc61 \u2208 R 2 denote the accumulative reward accumulated from session \ud835\udc61 through to \ud835\udc47 , without the introduction of any discount factor, which can be expressed as: Our method aims to optimize a spectrum of objectives that encompass both short-term and long-term rewards. This approach can be regarded as both an evolution and a specialization within the MTL framework, one that is finely tuned to balance and achieve both immediate and enduring user engagement metrics.", "3.2 Framework Overview": "The overall structure of DT4IER is illustrated in Figure 1. While it shares foundational elements with the conventional DT [8], our design incorporates modifications designed to accommodate recommendation tasks with a multi-task setting. Here's a step-by-step overview of DT4IER's forward process: \u00b7 Adaptive RTG Balancing Block : To effectively capture the intricate dynamics of short-term and long-term user behaviors, we propose to apply user feature-based reweighting to the RTG sequence. Further insights can be found in Section 3.3. \u00b7 Embedding Module : The modified trajectory data then undergoes an embedding transformation to be represented as dense vectors. To be specific, the state and action sequences are processed using a Gated Recurrent Unit (GRU) [15], detailed further in Appendix B.1. In contrast, the reward sequence employs a high-dimensional encoder which is further explained in Section 3.4. Additionally, a positional session encoding is integrated. \u00b7 Transformer Decision Block : Upon processing, the dense trajectory vectors serve as context, guiding the generation of action embeddings for the subsequent timestamp. This decision-making module is underpinned by the transformer model, detailed further in the Appendix B.2. \u00b7 Action Decoding Block : Using the predicted action embeddings, our decoding unit strives to construct an action sequence that aligns closely with the reference of ground truth actions, detailed further in Appendix B.3. \u00b7 Training Objective : We employ an objective function designed to minimize the differences between the predicted and ground truth actions. Besides, a supplementary contrastive learning objective is introduced, serving as a catalyst to amplify the model's robustness. More details are given in Section 3.5. ... ... State Embedding Action Embedding MLP GRU GRU Decision Transformer ... + Pos Encoding + Pos Encoding + Pos Encoding Action Decoder Embedding MLP MLP Balancing Embedding Module Categorical Feature Numerical Feature Balanced Reward Loss", "3.3 Adaptive RTG Balancing": "In settings that involve long short-term reward optimization, striking a balance between short-term and long-term performance metrics is notably challenging. This complexity arises from inherent conflicts between immediate and deferred objectives, as well as from the convoluted interdependencies among tasks [47]. To adeptly balance this equilibrium, we propose a novel strategy: the balancing of the RTG sequence, adaptively controlled by distinct user features. Since these user features hold a wealth of information about user preferences, they serve as a reliable guide for understanding user behaviors, bridging the gap between their immediate feedback and long-term retention. Considering the diverse nature of user features, it is crucial to treat numerical and categorical data distinctly. We process categorical features through an embedding layer to capture their unique attributes effectively. Meanwhile, numerical features are refined using a Multilayer Perceptron (MLP). The outputs of both processes are then combined and fed into another MLP which is designed to refine the data further and produce tailored weights that strike a balance between immediate responses and long-term engagement in user behavior. Specifically, given a set of user features U = ( u n , u c ) , where u n are numerical features and u c are categorical features. The whole process can be summarized as: \u00b7 Feature Extraction: The transformation of categorical features into an embedded space is given by: where \ud835\udc38 represents the embedding function, and e \ud835\udc56 is the embedding for the \ud835\udc56 -th categorical feature. The embedded vectors are then concatenated to form u e = [ e 1 , e 2 , . . . , e \ud835\udc5b ] . The numerical features are transformed by an MLP with the sigmoid activation function to obtain: To derive the long short-term balancing weights, the combined feature vector u com = [ u e , u M ] is processed by another MLP with softmax activation function to yield a set of weights B = ( \ud835\udc4f \ud835\udc60 , \ud835\udc4f \ud835\udc59 ) which indicates the importance of each reward indicator: where \ud835\udc67 \ud835\udc56 represents the \ud835\udc56 -th element of the output vector z , and \ud835\udc3e is the total number of elements in z , representing weights for balancing immediate response and long-term retention. \u00b7 Balanced Reward Loss: To guide the user-specific weight optimization process, we hope the learned balancing weights can maximize the overall weighted sum of immediate response reward \ud835\udc5f \ud835\udc60,\ud835\udc56 and long-term retention reward \ud835\udc5f \ud835\udc59,\ud835\udc56 and also balance them to reach a better balance. This objective can be written as: where \ud835\udc3e is the number of users, \ud835\udc3c \ud835\udc58 is the number of interacted items for user \ud835\udc58 . \ud835\udc4f \ud835\udc60 and \ud835\udc4f \ud835\udc59 represent the weights for immediate feedback and long-term retention with condition \ud835\udc4f \ud835\udc60 + \ud835\udc4f \ud835\udc59 = 1. To achieve a balanced consideration of rewards for immediate feedback and long-term retention, we implement an \ud835\udc3f 2 regularization approach which imposes a penalty on substantial deviations between the weighted values of short-term and long-term rewards. Such a strategy is designed to prevent the system from disproportionately favoring one type of reward over the other, thus maintaining an equitable focus on both immediate and sustained user engagement. Then the second inequality in Equation (8) is derived from the fact that RTG is the sum of rewards. To maximize the objective O , we design the corresponding BalancedRewardLoss function as follows: where \ud835\udefe is a hyperparameter to control the balance term. The overall design of the BalancedRewardLoss function aligns with the objective of achieving a sustainable and effective recommendation strategy, addressing both immediate user responses and long-term user retention. The existing RTG sequence is further rebalanced by the weights \ud835\udc35 and we also use the same notation for the RTG sequence b R \ud835\udc61 .", "3.4 Multi-reward Embedding": "In the architecture of DT, the reward mechanism is crucial as it drives the process of predicting actions. The reward signals to the model what outcomes to strive for, influencing its predictive decisions. However, the use of a simplistic embedding strategy for representing these rewards falls short, as it fails to preserve the essential partial order relationships among them. This limitation is particularly acute in scenarios involving multiple rewards, where the model must understand and respect the hierarchy of rewards to make accurate and contextually relevant embeddings. In order to effectively solve this problem, we introduce an innovative multi-reward embedding module specifically tailored for our RTG setting. This method employs learnable weights derived from an MLP, anchoring the weights directly to the reward values. Such an approach not only allows for a more nuanced representation of rewards but also ensures that the model remains adaptive to shifts in user behavior patterns. This process can be delineated as follows: \u00b7 Discretization of Rewards With the given numerical reward value b R \ud835\udc61 = [ b \ud835\udc45 \ud835\udc60,\ud835\udc61 , b \ud835\udc45 \ud835\udc59,\ud835\udc61 ] as a starting point, we apply a discretization technique enabling us to extract distinct meta-embeddings E \ud835\udc45 = [ \ud835\udc38 \ud835\udc45 \ud835\udc60,\ud835\udc61 , \ud835\udc38 \ud835\udc45 \ud835\udc59,\ud835\udc61 ] tailored to each task's reward. The principle behind this is to transform continuous reward values into categorical bins, each associated with a specific embedding, allowing for more subtle representations. \u00b7 Weighted Score Generation The reward values, once processed, are channeled into an MLP with a specific architecture, which translates the reward value into a multi-weighted score. To be specific, the MLP layer can be formulated as follows: where h \ud835\udc5b represent the \ud835\udc5b -th hidden layer, characterized by its weight W \ud835\udc5b and bias b \ud835\udc5b . For this layer, the activation function used is Leaky-ReLU, denoted as \ud835\udf0e . Meanwhile, the output layer, symbolized by h \ud835\udc41 , employs the softmax function \ud835\udf0e \u2217 . Here the output of MLP is a 2-D vector with w = h \ud835\udc41 = [ \ud835\udc64 1 , \ud835\udc64 2 ] . The core purpose of this step is to harness the potential of deep learning to derive a relational significance score for each task, highlighting the underlying relationship between rewards. \u00b7 Embedding Concatenation Upon obtaining the weighted metaembeddings specific to each task, we proceed to concatenate them to ensure a unified, comprehensive reward representation, providing a holistic view of the reward dynamics across tasks: where \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61\ud835\udc52 () represents the concatenation operation. This embedding module corresponds to the shared representation in MTL, drawing from both types of rewards to offer a comprehensive understanding of the tasks. Thus, while not being classic MTL, our approach borrows the foundational principle of jointly optimizing for multiple objectives to enhance overall performance.", "3.5 Objective Function with Contrastive Learning Term": "While DT typically predicts actions based on the prospect of obtaining the maximum reward, this may inadvertently sideline data samples associated with lower rewards [72]. Furthermore, for optimal performance, it's essential that actions with different reward values are distinctly separable in the embedding space. To achieve this, we introduce a contrastive learning-based loss term, ensuring the model effectively differentiates between actions corresponding to varied rewards. It can be written as: where b E \ud835\udc34 is the predicted action embedding, \u03a9 -is the set for negative samples b E \ud835\udc34 -, and function \ud835\udc37 ( \ud835\udc65,\ud835\udc66 ) calculates the similarity of two sequences. In our context, negative samples are identified as data points where the rewards \ud835\udc5f 1 ,\ud835\udc61 and \ud835\udc5f 2 ,\ud835\udc61 are consistently below 0.6. This threshold signifies that these samples underperform, falling below average in both click rate and retention metrics. Besides, in the original Decision Transformer model, the \ud835\udc3f 2 loss is employed for scenarios involving a continuous action space. However, in our specific setting, the action comprises video IDs with a length of 30, representing a discrete space. To adapt to this context, each element within the action space is converted into a one-hot encoded label. Consequently, we utilize the cross-entropy loss function, which effectively measures the divergence between the predicted action distribution and the ground truth action: where \ud835\udc4d is the length of the action sequence, \ud835\udc66 \ud835\udc5c,\ud835\udc67 is the binary indicator equal to 1 if the current predicted action \ud835\udc5c is inside the action table with label \ud835\udc67 , \ud835\udc5d \ud835\udc5c,\ud835\udc67 is predicted probability for current predicted action \ud835\udc5c is of class \ud835\udc67 . Ultimately, the overall objective function can be expressed as: where \ud835\udefc is the contrastive learning loss weight.", "4 EXPERIMENT": "In this section, we assess the performance of the DT4IER framework using experiments conducted on two real-world datasets.", "4.1 Dataset": "We carried out our experiment using three datasets. \u00b7 Kuairand-Pure 2 is an unbiased sequential recommendation dataset featuring random video exposures. \u00b7 MovieLens-25M 3 , a widely-used benchmark for SRSs, boasts a more extensive scale but with a sparser distribution. \u00b7 RetailRocket 4 dataset is collected from a real-world e-commerce website. To optimize the transformer's memory requirements, item IDs have been reindexed. Underline: the best baseline model. Bold : the best performance among all models.", "4.2 Evaluation Metrics": "We evaluate DT4IER's effectiveness using various metrics, focusing on both short-term recommendation accuracy and long-term user retention. Detailed metrics are provided below: \u00b7 MMoE [37]: Renowned as a robust multi-task learning model, MMoE employs gating mechanisms to manage the interplay between the shared foundation and task-specific layers. \u00b7 BLEU [44] assesses the precision of the predicted recommendation list which is a common metric in SRSs. \u00b7 ROUGE [29] calculates the recall rate of the predicted recommendation list. \u00b7 HR@K quantifies the likelihood of ground-truth items ranking within the top-K recommendations. \u00b7 NDCG@K [61] calculates the normalized cumulative gain within the top-K recommendations, factoring in positional relevance. \u00b7 Similarity-Based User Return Score (SB-URS) [72] is an established metric for assessing the retention impact of recommended lists, determined by the weighted sum of the actual user retention score. In our approach, we categorize samples into eight distinct classes based on their reward values, uniformly ranging from 0 to 1. The similarity, represented by the BLEU score, is then computed by comparing the predicted recommendations with the ground truth for each class: Here, \ud835\udc60\ud835\udc56\ud835\udc5a \ud835\udc50 represents the similarity for class \ud835\udc50 , \ud835\udc5f \ud835\udc50 denotes the corresponding ground truth retention reward, and \ud835\udc41 \ud835\udc50 signifies the count of samples with a reward classification of \ud835\udc50 .", "4.3 Baselines": "In our analysis, we contrast the performance of our approach with state-of-the-art (SOTA) models spanning both multi-task learning and decision transformer-based sequential recommendations: \u00b7 PLE [56]: Standing out as a leading multi-task learning solution, PLE handles complex task interrelations by employing a blend of shared experts and task-specific expert layers. \u00b7 RMTL [35]: This is a reinforcement learning-based multi-task recommendation model with adaptive loss weights. \u00b7 BERT4Rec [50]: It employs a bidirectional Transformer architecture to effectively capture sequential patterns in user behavior with a masked language model. \u00b7 SASRec [26]: This model applies a left-to-right unidirectional Transformer to capture user preference. \u00b7 DT4Rec [72]: Operating on the decision transformer paradigm, this Sequential Recommendation System (SRS) model is meticulously tailored for optimizing user retention. Furthermore, we've adapted the model architectures of MMoE and PLE to facilitate sequential recommendations, utilizing a weighted score derived from multiple tasks.", "4.4 Implementation Details": "For the Decision Transformer model, we use a trajectory length \ud835\udc47 of 20 for both datasets, and we set the maximum sequence length the same for state and action \ud835\udc3b = \ud835\udc41 = 30. The model configuration includes 2 Transformer layers, 8 heads, and an embedding size of 128. We employ the Adam optimizer, with a batch size set to 128. The learning rates are 0.005 for Kuairand-Pure and 0.02 for ML-25M and RetailRocket. The action decoder is capped at a maximum sequence length of 30. The balanced reward loss utilizes a balance term \ud835\udefe of 0.5, and a contrastive loss parameter \ud835\udefc of 0.1. For other baseline models, we either adopt the optimal hyper-parameters suggested by their original authors or search within the same ranges as our model. All results are showcased using the optimal configurations for each model, as detailed in Appendix A.", "4.5 Overall Performance and Comparison": "We assessed the efficacy of our proposed DT4IER model against four baselines across two datasets. A comprehensive performance summary is shown in Table 1, yielding the following insights: \u00b7 Limitations of MMoE : Among all models, MMoE demonstrates the least satisfactory performance concerning both recommendation accuracy and long-term retention across the datasets. While MMoE's design proficiently handles multiple tasks by balancing parameter interactions between shared components and taskspecific towers, its architecture, optimized for parallel task processing, struggles to adapt to the dynamic and evolving nature of sequential data. Similarly, PLE faces the same challenge, potentially resulting in diminished outcomes in sequential recommendation contexts. \u00b7 Strengths of DT4Rec : DT4Rec achieves the best performance in both recommendation accuracy and retention among all baseline models. Its unique auto-discretized reward prompt design guides the model training towards boosting long-term user engagement, thus enhancing user retention appreciably. \u00b7 Superiority of DT4IER : The DT4IER model consistently outperforms the four baselines in both the realms of recommendation accuracy and user retention score across datasets. Particularly on the Kuairand-Pure dataset, DT4IER exhibits 0.07-0.09 improvement in recommendation accuracy compared to the topperforming baseline. By interpreting RL as an autoregressive setting and integrating a structure designed for multiple rewards, our model strikes a balance between immediate feedback and long-term retention, resulting in significant enhancements in recommendation performance while retaining user engagement. To conclude, the DT4IER model represents a significant advancement over existing state-of-the-art multi-task learning (MTL) frameworks and transformer-based sequential recommendation systems. It excels in providing superior recommendation accuracy and sustaining long-term user retention across diverse real-world datasets. This is largely due to its sophisticated reward structure, which has been meticulously designed to harmonize the delivery of immediate feedback and long-term retention.", "4.6 Ablation Study": "In this subsection, we delve into an ablation study to underscore the significance of the distinct modules integrated within our proposed model. By contrasting variants of the primary model with specific modules omitted, we aim to measure the impact of each component. The variant models are delineated as follows: \u00b7 NAW represents the model variant devoid of the adaptive RTG weighting module, with all other components kept constant. \u00b7 NRE In this configuration, a standard reward embedding is employed without considering intricate task relations. \u00b7 NCL This variant exclusively leverages the cross-entropy loss in its objective functions without the contrastive loss component. The outcomes of our ablation study, conducted on the DT4IER model utilizing the Kuairand-Pure dataset, are illustrated in Figure 2. From the results, we have several key insights: \u00b7 Importance of RTG Balancing: Our DT4IER model consistently outperforms the NAW variant across diverse metrics, spanning recommendation accuracy to long-term retention metrics. Specifically, it achieves improvements of 1.50% in BLEU, 1.90% in NDCG, and 0.08% in SB-URS. This can be largely attributed to the RTG balancing module, which adaptively weights the immediate reward and long-term retention by distinct user features. The result effectively underscores the contribution of this module. \u00b7 Limitations of NRE : The NRE configuration achieves the most modest performance across both immediate feedback and longterm retention metrics. Compared with the NRE variant, the DT4IER model achieves improvements of 1.80% in BLEU, 2.30% in NDCG, and 0.19% in SB-URS. Its primary shortcoming arises from its encoder module, which struggles to efficiently map 2D rewards or discern the intrinsic connections between tasks. This underscores the efficiency of our proposed multi-reward embedding module in driving better performance. \u00b7 Impact of Contrastive Loss : The absence of contrastive loss in the NCL variant notably diminishes its performance. This is mainly because, without this component, the action embeddings for distinct rewards aren't adequately separated. The DT4IER model achieves improvements of 1.02% in BLEU, 1.60% in NDCG, and 0.03% in SB-URS against NCL. Notably, while the SB-URS metrics between DT4IER and the NCL variant are comparable, our model manages to boost recommendation accuracy without compromising on long-term retention capabilities. NAW NRE NCL DT4IER 0.860 0.865 0.870 0.875 0.880 0.885 0.890 0.895 (a) BLEU Metric NAW NRE NCL DT4IER 0.812 0.817 0.822 0.827 0.832 0.837 0.842 0.847 0.852 (b) NDCG Metric NAW NRE NCL DT4IER 527500 527800 528100 528400 528700 529000 529300 529600 (c) SB-URS", "4.7 RTG Prompting Analysis": "The inference mechanism of the Decision Transformer (DT) operates on the principle of supervised action prediction, conditioned on the highest possible Return-to-Go (RTG) values. In our specific context, this RTG value is represented as [1,1], implying an anticipated 100% click rate along with the expectation that the user will return in the subsequent session. However, in practical applications, this ideal scenario isn't always achieved which underscores the potential benefit of utilizing RTG prompting with a reduced proportion to potentially boost model performance. Drawing from this observation, we evaluate the model's performance over RTG values ranging from 0.4 to the upper limit of 1.0, with 1.0 representing the utilization of the maximum RTG. The results are presented in Figure 3, offering insights into the interplay between RTG proportions and recommendation efficacy. From the figure presented, a distinct pattern emerges in the performance metrics. Notably, both the BLEU and NDCG scores exhibit a consistent ascent with increasing RTG proportions, predominantly in the 0.4 to 0.8 range. The best performance for these metrics is achieved at an RTG proportion of 0.8, beyond which no incremental benefit is observed. This pattern suggests that while higher RTG promptings enhance recommendation accuracy, there exists a saturation point beyond which further increments do not translate to performance gains. The result exactly validates our hypothesis regarding RTG prompting. 0.30.40.50.60.70.80.91.0 RTG Propotion 0.79 0.82 0.85 0.88 0.91 BLEU (a) BLEU 0.30.40.50.60.70.80.91.0 RTG Propotion 0.68 0.72 0.76 0.80 0.84 0.88 NDCG (b) NDCG", "4.8 Case Study": "In this subsection, we highlight DT4IER's effectiveness in improving recommendation performance through a case study on the Kuairand-Pure dataset. The selected user for this study has a history of four interactions, with the ground truth action sequence comprising details of five different videos. Figure 2 shows that when provided with the user's state and the goal of maximizing retention, DT4Rec recommends four videos. This recommendation achieves a BLEU score of 0.625 and an NDCG of 0.710. In contrast, DT4IER outperforms by recommending five actions, all of which align with the ground truth action sequence. This leads to a higher prediction accuracy, with a BLEU score of 0.887 and an NDCG of 0.947. These results underscore our model's enhanced efficiency, attributed to its consideration of short-term clicks and long-term retention. User State Information [3310, 3706, 1018, 0, 0, \u2026 , 0, 0] DT4Rec DT4IER [5246, 6767, 7438, 0, 0, \u2026 , 0, 0] [5246, 6767, 7438, 399 , 0, 0, \u2026 , 0, 0] BLUE 0.625   NDCG 0.710 BLUE 0.887   NDCG 0.947 Ground Truth Action [5246, 6767, 399, 7298, 7438, 0, 0, \u2026 , 0, 0]", "5 RELATED WORK": "In this section, we briefly discuss existing research related to sequential recommender systems, RL-based recommender systems, and MTL-based recommender systems.", "5.1 Sequential Recommender Systems": "Sequential Recommendation refers to a recommendation system paradigm that models patterns of user behavior and items over time to suggest relevant products or content [20, 28, 32, 33, 67]. Among the myriad of approaches available, Markov Chains (MCs) [48] and Recurrent Neural Networks (RNNs) stand out for their prowess in sequence modeling [17]. The adaptability and capability of RNNs in integrating diverse information make them especially effective for crafting sequential recommendations. RU4Rec [55] was a pioneer in leveraging RNNs, tapping into their potential to process sequential data. Subsequently, GRU4Rec [22] unveiled a parallel RNN structure to process item features, thus boosting recommendation quality. Further, the generalization power of the transformer architecture has given rise to its prominence in Sequential Recommendation Systems, birthing models such as BERT4Rec [50] and SASRec [26]. Building upon these foundations, the decision transformer was conceptualized to tackle user retention challenges, aiming to directly predict actions using a reward-driven autoregressive framework [72]. However, a gap remains in current research concerning the optimization of multi-reward settings.", "5.2 Reinforcement Learning Based Recommender Systems": "The validity of applying RL-based solutions [2, 52, 60, 68, 73, 75] for sequential recommendation comes from the assumption of Markov Decision Process [49]. And the key advantage of RL solutions is the ability to improve the expected cumulative reward of future interactions with users, rather than optimizing the one-step recommendation. Specifically, for scenarios with small recommendation spaces, one can use tabular-based [39, 41] or value-based methods [24, 54, 77, 80] to directly evaluate the long-term value of the recommendation; For scenarios where action spaces are large, policy gradient methods [6, 9, 51, 78] and actor-critic methods [3, 5, 16, 18, 30, 31, 34, 46, 53, 63, 76] are adopted to guide the policy towards better recommendation quality. Knowing that the web service may want to optimize multiple metrics, several works have discussed the challenge of multi-objective optimization [5, 12] where the user behaviors might have different distributional patterns. Among all user feedback signals, user retention has been considered one of the most challenging to optimize, while recent work has shown a possible RL-based approach [4] for this uphill struggle. This work aims to simultaneously optimize immediate feedback and user retention. Similar to our work, to overcome the gap between experiments on real user environments and offline evaluations, user simulators are widely used to bypass this paradox [25, 71, 74]. Our approach can be thought of as a likelihood-based method while employing a sequence modeling objective rather than relying on variational techniques.", "5.3 Multi-task Learning in Recommender Systems": "Multi-task learning (MTL) is a machine learning technique that addresses multiple tasks simultaneously [59, 70]. It captures a shared representation of the input through a shared bottom layer and then processes each task using distinct networks with task-specific Predicted Action weights. The overall performance is further enhanced by the knowledge transfer between tasks. This approach has become particularly popular in recommender systems, thanks to its prowess in effectively sharing data across tasks and in recognizing a range of user behaviors [21, 36, 38, 43, 45]. A significant portion of recent research targets the enhancement of these architectures to promote more effective knowledge sharing. Notably, some innovations focus on introducing constraints to task-specific parameters [19, 40, 65], while others aim to clearly demarcate shared from task-specific parameters [37, 56]. The principal goal of these strategies is to amplify knowledge transfer through improved feature representation. Additionally, some researchers are exploring the potential of reinforcement learning to enhance the MTL model by adjusting loss function weights [35]. While many approaches prioritize item-wise modeling, our research delves into the challenges of balancing both short-term and long-term rewards in sequential recommendation settings. Furthermore, we have refined the methodologies of both the PLE and MMoE models for sequential recommendations.", "6 CONCLUSION": "In this work, we introduced DT4IER, an advanced decision transformer tailored for sequential recommendations within a multi-task framework. Our primary objective was twofold: augmenting recommendation accuracy and ensuring a harmonious balance between immediate user responses and long-term retention. To achieve this, DT4IER leverages an innovative multi-reward mechanism that adeptly integrates immediate user responses with long-term retention signals, tailored by user-specific attributes. Furthermore, the reward embedding module is enriched by a high-dimensional encoder that deftly navigates the intricate relationships between different tasks. Empirical results on three business datasets consistently positioned DT4IER ahead of prevalent baselines, highlighting its transformative potential in recommender systems.", "ACKNOWLEDGEMENTS": "This research was partially supported by Kuaishou, Research Impact Fund (No.R1015-23), APRC - CityU New Research Initiatives (No.9610565, Start-up Grant for New Faculty of CityU), CityU HKIDS Early Career Research Grant (No.9360163), Hong Kong ITC Innovation and Technology Fund Midstream Research Programme for Universities Project (No.ITS/034/22MS), Hong Kong Environmental and Conservation Fund (No. 88/2022), and SIRG - CityU Strategic Interdisciplinary Research Grant (No.7020046, No.7020074).", "A HYPER-PARAMETER SELECTION": "In our study, we undertook a hyperparameter tuning process for the DT4IER model to ensure the robustness and credibility of our experimental results. These results represent the aggregated performance across two datasets. A comprehensive breakdown of the hyperparameter choices is provided in Table 2 for further reference.", "B OTHER MODEL DETAILS": "", "B.1 State-action Encoder": "Given the state and action sequence of length \ud835\udc3b = \ud835\udc41 , we apply GRU to process the input sequence and specify by state s \ud835\udc61 : where \ud835\udc67 \ud835\udc5b , \ud835\udc5c \ud835\udc5b are update gate vector and reset gate vector for \ud835\udc5b -th item, \u210e \ud835\udc5b is the output vector, \ud835\udf0e \u2032 is the logistic function, \ud835\udc4a,\ud835\udc48,\ud835\udc4f are parameter matrices and vector, b E \ud835\udc60 \ud835\udc61 is the state embedding at \ud835\udc61 timestamp which is also the last output vector for \ud835\udc41 .", "B.2 Transformer Block": "We employ a unidirectional transformer layer equipped with a multi-head self-attention mechanism as our primary model architecture. To combat overfitting, skip-connections are integrated, and feed-forward neural layers are utilized for feature transformation: where b E \ud835\udc34 is the predicted action embedding, \ud835\udf49 \u2032 is the trajectory information containing state-action and RTG embedding. MHA is a multi-head self-attentive layer and FNN is the feed-forward neural with Gaussian Error Linear Units (GELU) activation function.", "B.3 Action Decoder": "Given predicted action embedding b E \ud835\udc34 with \ud835\udc61 -th rows as \ud835\udc34 \ud835\udc61 and user interaction histories \ud835\udc56 \ud835\udc5b , the action decoder aims to decode sequences of items of interest to users with the GRU module: where b \u210e \ud835\udc5b is the \ud835\udc5b -th output vector. To forecast the first item without any prior information, we employ 'start' as a checkpoint and initialize b \ud835\udc56 0 arbitrarily. The decoding process can be expressed as: where b a \ud835\udc61 represents the predicted action for \ud835\udc61 -th instance.", "REFERENCES": "[1] Giuseppe Aceto, Valerio Persico, and Antonio Pescap\u00e9. 2020. Industry 4.0 and health: Internet of things, big data, and cloud computing for healthcare 4.0. Journal of Industrial Information Integration 18 (2020), 100129. [2] MMehdi Afsar, Trafford Crump, and Behrouz Far. 2021. Reinforcement learning based recommender systems: A survey. ACM Computing Surveys (CSUR) (2021). [3] Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton. 2007. Incremental natural actor-critic algorithms. Advances in neural information processing systems 20 (2007). [4] Qingpeng Cai, Shuchang Liu, Xueliang Wang, Tianyou Zuo, Wentao Xie, Bin Yang, Dong Zheng, Peng Jiang, and Kun Gai. 2023. Reinforcing User Retention in a Billion Scale Short Video Recommender System. In Companion Proceedings of the ACM Web Conference 2023 . 421-426. [5] Qingpeng Cai, Zhenghai Xue, Chi Zhang, Wanqi Xue, Shuchang Liu, Ruohan Zhan, Xueliang Wang, Tianyou Zuo, Wentao Xie, Dong Zheng, et al. 2023. TwoStage Constrained Actor-Critic for Short Video Recommendation. In Proceedings of the ACM Web Conference 2023 . 865-875. [6] Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang, Yuzhou Zhang, and Yong Yu. 2019. Large-scale interactive recommendation with tree-structured policy gradient. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 3312-3320. [7] Jingfan Chen, Wenqi Fan, Guanghui Zhu, Xiangyu Zhao, Chunfeng Yuan, Qing Li, and Yihua Huang. 2022. Knowledge-enhanced black-box attacks for recommendations. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 108-117. [8] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems 34 (2021), 15084-15097. [9] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H Chi. 2019. Top-k off-policy correction for a REINFORCE recommender system. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining . 456-464. [10] Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. 2019. Generative adversarial user model for reinforcement learning based recommendation system. In International Conference on Machine Learning . PMLR, 1052-1061. [11] Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, and Xianzhi Wang. 2021. A survey of deep reinforcement learning in recommender systems: A systematic review and future directions. arXiv preprint arXiv:2109.03540 (2021). [12] Xiaocong Chen, Lina Yao, Aixin Sun, Xianzhi Wang, Xiwei Xu, and Liming Zhu. 2021. Generative inverse deep reinforcement learning for online recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 201-210. [13] Xiong-Hui Chen, Bowei He, Yang Yu, Qingyang Li, Zhiwei Qin, Wenjie Shang, Jieping Ye, and Chen Ma. 2023. Sim2rec: A simulator-based decision-making approach to optimize real-world long-term user engagement in sequential recommender systems. In 2023 IEEE 39th International Conference on Data Engineering (ICDE) . IEEE, 3389-3402. [14] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [15] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014). [16] Thomas Degris, Patrick M Pilarski, and Richard S Sutton. 2012. Model-free reinforcement learning with continuous action in practice. In 2012 American Control Conference (ACC) . IEEE, 2177-2182. [17] Tim Donkers, Benedikt Loepp, and J\u00fcrgen Ziegler. 2017. Sequential user-based recurrent neural network recommendations. In Proceedings of the eleventh ACM conference on recommender systems . 152-160. [18] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. 2015. Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679 (2015). [19] Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser. In Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing (volume 2: short papers) . 845-850. [20] Jingtong Gao, Xiangyu Zhao, Muyang Li, Minghao Zhao, Runze Wu, Ruocheng Guo, Yiding Liu, and Dawei Yin. 2024. SMLP4Rec: An Efficient all-MLP Architecture for Sequential Recommendations. ACM Transactions on Information Systems 42, 3 (2024), 1-23. [21] Guy Hadash, Oren Sar Shalom, and Rita Osadchy. 2018. Rank and rate: multi-task learning for recommender systems. In Proceedings of the 12th ACM Conference on Recommender Systems . 451-454. [22] Bal\u00e1zs Hidasi, Massimo Quadrana, Alexandros Karatzoglou, and Domonkos Tikk. 2016. Parallel recurrent neural network architectures for feature-rich session-based recommendations. In Proceedings of the 10th ACM conference on recommender systems . 241-248. [23] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019. SlateQ: A Tractable Decomposition for Reinforcement Learning with Recommendation Sets. In Proceedings of the Twenty-eighth International Joint Conference on Artificial Intelligence (IJCAI-19) . Macau, China, 2592-2599. See arXiv:1905.12767 for a related and expanded paper (with additional material and authors).. [24] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019. SlateQ: A tractable decomposition for reinforcement learning with recommendation sets. (2019). [25] Eugene Ie, Chih wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. 2019. RecSim: A Configurable Simulation Platform for Recommender Systems. (2019). arXiv:1909.04847 [cs.LG] [26] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE, 197-206. [27] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30-37. [28] Muyang Li, Xiangyu Zhao, Chuan Lyu, Minghao Zhao, Runze Wu, and Ruocheng Guo. 2022. MLP4Rec: A pure MLP architecture for sequential recommendations. arXiv preprint arXiv:2204.11510 (2022). [29] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out . Association for Computational Linguistics, Barcelona, Spain, 74-81. https://aclanthology.org/W04-1013 [30] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen, Huifeng Guo, and Yuzhou Zhang. 2018. Deep reinforcement learning based recommendation with explicit user-item interactions modeling. arXiv preprint arXiv:1810.12027 (2018). [31] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen, Huifeng Guo, Yuzhou Zhang, and Xiuqiang He. 2020. State representation modeling for deep reinforcement learning based recommendation. Knowledge-Based Systems 205 (2020), 106170. [32] Langming Liu, Liu Cai, Chi Zhang, Xiangyu Zhao, Jingtong Gao, Wanyu Wang, Yifu Lv, Wenqi Fan, Yiqi Wang, Ming He, et al. 2023. Linrec: Linear attention mechanism for long-term sequential recommender systems. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 289-299. [33] Qidong Liu, Fan Yan, Xiangyu Zhao, Zhaocheng Du, Huifeng Guo, Ruiming Tang, and Feng Tian. 2023. Diffusion augmentation for sequential recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 1576-1586. [34] Ziru Liu, Kecheng Chen, Fengyi Song, Bo Chen, Xiangyu Zhao, Huifeng Guo, and Ruiming Tang. 2024. AutoAssign+: Automatic Shared Embedding Assignment in streaming recommendation. Knowledge and Information Systems 66, 1 (2024), 89-113. [35] Ziru Liu, Jiejie Tian, Qingpeng Cai, Xiangyu Zhao, Jingtong Gao, Shuchang Liu, Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, et al. 2023. Multi-Task Recommendations with Reinforcement Learning. In Proceedings of the ACM Web Conference 2023 . 1273-1282. [36] Yichao Lu, Ruihai Dong, and Barry Smyth. 2018. Coevolutionary recommendation model: Mutual learning between ratings and reviews. In Proceedings of the 2018 World Wide Web Conference . 773-782. [37] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [38] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 1137-1140. [39] Tariq Mahmood and Francesco Ricci. 2007. Learning and adaptivity in interactive recommender systems. In Proceedings of the ninth international conference on Electronic commerce . 75-84. [40] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. 2016. Cross-stitch networks for multi-task learning. In Proceedings of the IEEE conference on computer vision and pattern recognition . 3994-4003. [41] Omar Moling, Linas Baltrunas, and Francesco Ricci. 2012. Optimal radio channel recommendations with explicit and implicit feedback. In Proceedings of the sixth ACM conference on Recommender systems . 75-82. [42] Raymond J Mooney and Loriene Roy. 2000. Content-based book recommending using learning for text categorization. In Proceedings of the fifth ACM conference on Digital libraries . 195-204. [43] Huashan Pan, Xiulin Li, and Zhiqiang Huang. 2019. A Mandarin Prosodic Boundary Prediction Model Based on Multi-Task Learning.. In Interspeech . 4485-4488."}
