{"Knowledge Enhanced Multi-intent Transformer Network for Recommendation": "Ding Zou \u2217 Wei Wei \u2020", "Feida Zhu": "CCIIP Lab School of Computer Science and Technology Huazhong University of Science and Technology Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL) Wuhan, China Taotian Group Hangzhou, China m202173662@hust.edu.cn Chuanyu Xu Taotian Group Hangzhou, China tracy.xcy@taobao.com CCIIP Lab School of Computer Science and Technology Huazhong University of Science and Technology Property & Casualty Research (HPL) Wuhan, China Joint Laboratory of HUST and Pingan weiw@hust.edu.cn", "Tao Zhang": "Taotian Group Hangzhou, China guyan.zt@taobao.com", "ABSTRACT": "Incorporating Knowledge Graphs (KGs) into Recommendation has attracted growing attention in industry, due to the great potential of KG in providing abundant supplementary information and interpretability for the underlying models. However, simply integrating KG into recommendation usually brings in negative feedback in industry, mainly due to the ignorance of the following two factors: i) users' multiple intents, which involve diverse nodes in KG. For example, in e-commerce scenarios, users may exhibit preferences for specific styles, brands, or colors. ii) knowledge noise, which is a prevalent issue in Knowledge Enhanced Recommendation (KGR) and even more severe in industry scenarios. The irrelevant knowledge properties of items may result in inferior model performance compared to approaches that do not incorporate knowledge. To tackle these challenges, we propose a novel approach named Knowledge Enhanced Multi-intent Transformer Network for Recommendation (KGTN), which comprises two primary modules: Global Intents Modeling with Graph Transformer, and Knowledge Contrastive Denoising under Intents. Specifically, Global Intents with Graph Transformer focuses on capturing learnable user intents, by incorporating global signals from user-itemrelation-entity interactions with a well-designed graph transformer, \u2217 Work done during internship at Taotian Group. \u2020 Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'24 Companion, May 13-17, 2024, Singapore, Singapore. \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0172-6/24/05...$15.00 https://doi.org/10.1145/3589335.3648296 Chengfu Huo Taotian Group Hangzhou, China chengfu.huocf@taobao.com and meanwhile learning intent-aware user/item representations. On the other hand, Knowledge Contrastive Denoising under Intents is dedicated to learning precise and robust representations. It leverages the intent-aware user/item representations to sample relevant knowledge, and subsequently proposes a local-global contrastive mechanism to enhance noise-irrelevant representation learning. Extensive experiments conducted on three benchmark datasets show the superior performance of our proposed method over the state-of-the-arts. And online A/B testing results on Alibaba large-scale industrial recommendation platform also indicate the real-scenario effectiveness of KGTN. The implementations are available at: https://github.com/CCIIPLab/KGTN.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "Knowledge Enhanced Recommendation, Graph Transformer, Graph Neural Networks", "ACMReference Format:": "Ding Zou, Wei Wei, Feida Zhu, Chuanyu Xu, Tao Zhang, and Chengfu Huo. 2024. Knowledge Enhanced Multi-intent Transformer Network for Recommendation. In Companion Proceedings of the ACM Web Conference 2024 (WWW '24 Companion), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3589335.3648296", "1 INTRODUCTION": "Knowledge graphs (KGs) have emerged as a promising approach to enhance the accuracy and interpretability of recommender systems in both academic and industry scenarios. By incorporating entities and relations, KGs provide a rich source of information for user/item representation learning, which not only captures the diverse relationships among items (such as the same item brand), Singapore Management University Singapore, Singapore fdzhu@smu.edu.sg WWW'24 Companion, May 13-17, 2024, Singapore, Singapore. Ding Zou et al. \ud835\udc621 \ud835\udc56 1 \ud835\udc56 2 \ud835\udc521 \ud835\udc522 \ud835\udc501 ... \ud835\udc502 \ud835\udc50\ud835\udc58 \ud835\udc501 : Passing time : Long-term interest : Social reason \ud835\udc502 \ud835\udc50\ud835\udc58 ... \ud835\udc523 \ud835\udc524 \ud835\udc56 \ud835\udc5b \ud835\udc622 : Style is : Type of : Color is (a) Intent Case 0 0.05 0.1 0.15 0.2 Music Book Movie Recall@20 Datasets Models CF-based KG-based (b) Comparison Music Book Movie CF-based 0.174 0.0525 0.1512 KG-based 0.1532 0.048 0.1364 Figure 1: (a) A simple case for illustrating multiple user intents with global information; (b)Performance comparison. but also allows for the interpretation of user preferences (such as attributing a user's selection of a clothing to its fashionable style). In an effort to effectively integrate the item-side KG information into recommendation, considerable research efforts have been devoted to Knowledge Enhanced Recommendation ( aka. KGR). Early studies [7, 19, 33] directly integrate knowledge graph embeddings with items to enhance their representations. Some subsequent studies [6, 16, 25] enrich the interactions via meta-paths that capture relevant connectivities between users and items with KG. They either select prominent paths over KG [17], or represent the interactions with multi-hop paths from users to items [6, 25]. Nevertheless, most of them heavily rely on manually designed meta-paths, which makes it hard to optimize in reality. As a result, later methods have embraced Graph Neural Networks (GNNs) [22, 23] to automatically aggregate high-order information over KG, which iteratively integrate multi-hop neighbors into representations and have demonstrated promising performance for recommendation. Most recently, there have been efforts to incorporate Contrastive Learning (CL) into KGR for addressing noisy knowledge and long-tail problems [27, 29, 37] via contrasting the user-item (collaborative part) and item-entity (knowledge part) graphs. However, current KGR methods usually bring poor performance in large-scale industry scenarios, due to their commonly overlooking two crucial factors: 1) Users' multiple intents underlying interaction behavior. For instance, as depicted in Figure 1(a), users may have diverse intentions when shopping in Alibaba E-commerce platform, such as long-term interest, passing time, or social reason, etc . 2) Redundant Knowledge information. In the context of user intents, some knowledge facts in the KG may be irrelevant noise [3], which can potentially disrupt the learning process of user/item representations. As shown in Figure 1(b), incorporating KGs may result in a worse model performance than the models without KG utilization (the details of comparison could refer to Section 4.2 ). But still, it's not trivial to model user intents in KGR, since user intents may be composed of multiple heterogeneous information, including items, relations, and entities. Previous multi-intent modeling methods usually define the intents as a linear combination of either interacted items [24] or entire relation sets [23], then update the intent representations through local aggregation in the userintent-item heterogeneous graph. Nevertheless, such a multi-intent learning paradigm may not fully meet the requirements for KGR, as it neglects the global information in intent defining and learning. To illustrate this, we present an example in Figure 1(a). In this example, user \ud835\udc62 1 may purchase the item \ud835\udc56 1 for the intent \ud835\udc50 1 of long-term interest, resulting in a focus on clothing style ( e.g., whether it is fashionable), which means intent \ud835\udc50 1 is associated with KG relation \ud835\udc5f 1 and entity \ud835\udc52 1; while \ud835\udc62 1 may buy the item \ud835\udc56 \ud835\udc5b for the intent \ud835\udc50 \ud835\udc58 of social reason (such as friend \ud835\udc62 2 recommend), which means intent \ud835\udc50 \ud835\udc58 is associated with user \ud835\udc62 2 and item \ud835\udc56 \ud835\udc58 . In this paper, we focus on modeling user intents behind interaction behaviors with global collaborative (user-item) and knowledge (item-relation-entity) information, and exploiting these modeled intents to guide knowledge sampling, facilitating fine-grained and accurate user/item representation learning. We propose a novel model, KGTN, which comprises two essential components for solving the foregoing limitations: i) Global Intents Modeling with Graph Transformer. We predefine \ud835\udc3e intent representations for user/item, then learn these intents with global information from collaborative and knowledge graphs. Specifically, it first merges knowledge information into items, then propose a novel graph transformer in the user-item graph to learn global intents and generate intent-aware user/item representations. ii) Knowledge Contrastive Denoising under Intents. KGTN first exploits the intent-aware user/item representations to guide the knowledge sampling, effectively pruning the irrelevant knowledge. Then a novel local-global contrastive mechanism is proposed here to denoise the user/item representations. Empirically, KGTN outperforms the state-of-the-art models on three benchmark datasets in offline testing, and achieves significant improvements in online A/B testing. Our contributions of this work can be summarized as follows: \u00b7 General Aspects: We emphasize the importance of intent modeling with global information, which plays a crucial role in finegrained representation learning and knowledge denoising. \u00b7 Novel Methodologies: Wepropose a novel model KGTN, which models user intents from global signals with a novel graph transformer; and denoises item representations with i) knowledge denoising under intents, and ii) local-global graph contrastive learning. \u00b7 Multifaceted Experiments: We conduct extensive offline experiments on three benchmark datasets and online A/B testing on Alibaba recommendation platform. The results demonstrate the advantages of our KGTN in better representation learning.", "2 PROBLEM FORMULATION": "In this section, we begin by formulating the structural data of CF (user-item interactions) and KG (item-relation-entity knowledge) in KGR, then present the problem statement. Interaction Data . In a typical recommendation scenario, let U = { \ud835\udc62 1 , \ud835\udc62 2 , . . . , \ud835\udc62 \ud835\udc40 } be a set of \ud835\udc40 users and V = { \ud835\udc63 1 , \ud835\udc63 2 , . . . , \ud835\udc63 \ud835\udc41 } a set of \ud835\udc41 items. Let Y \u2208 R \ud835\udc40 \u00d7 \ud835\udc41 be the user-item interaction matrix, where \ud835\udc66 \ud835\udc62\ud835\udc63 = 1 indicates that user \ud835\udc62 engaged with item \ud835\udc63 , such as behaviors like clicking or purchasing; otherwise \ud835\udc66 \ud835\udc62\ud835\udc63 = 0. Knowledge Graph . A KG stores luxuriant real-world facts associated with items, encompassing item attributes or external commonsense knowledge, in the form of a heterogeneous graph [16]. Let G = {( \u210e, \ud835\udc5f, \ud835\udc61 ) | \u210e, \ud835\udc61 \u2208 E , \ud835\udc5f \u2208 R} be the KG, where \u210e , \ud835\udc5f , \ud835\udc61 represent the head, relation, tail of a knowledge triple, respectively; E and R denote the sets of entities and relations in G . In many recommendation scenarios, an item \ud835\udc63 \u2208 V corresponds to one entity \ud835\udc52 \u2208 E . We hence establish a set of item-entity alignments Knowledge Enhanced Multi-intent Transformer Network for Recommendation WWW'24 Companion, May 13-17, 2024, Singapore, Singapore. A = {( \ud835\udc63, \ud835\udc52 )| \ud835\udc63 \u2208 V , \ud835\udc52 \u2208 E} , where ( \ud835\udc63, \ud835\udc52 ) indicates that item \ud835\udc63 can be aligned with an entity \ud835\udc52 in KG. With the alignments between items and KG entities, KG is able to profile items and offer complementary information to the interaction data. Problem Statement . Given the user-item interaction matrix Y and the KG G , KGR aims to learn a function that can predict how likely a user would adopt an item.", "3 METHODOLOGY": "We now present the proposed Knowledge Enhanced Multi-intent Transformer Network for Recommendation (KGTN). KGTN aims at modeling user intents with global information and exploiting user intents to denoise KG for accurate and robust user/item representation learning. Figure 2 displays the framework of KGTN, which mainly consists of two key components: 1) Global Intent Modeling with graph transformer. Initially, KGTN defines a set of \ud835\udc3e learnable global intents for users and items. It then models these intents and learns intent-aware user/item representations, via integrating global signals with a graph transformer in the user-item graph, where knowledge information has been encoded into items. 2) Knowledge Contrastive Denoising under intents. It first exploits the learned intent-aware user/item representations to sample intentrelevant knowledge, then designs a contrastive self-supervised task between the local aggregation and global aggregation features within the sampled graph to facilitate robust representation learning.", "3.1 Global Intents Modeling with Graph Transformer": "3.1.1 Intent Initialization with Global signals. When interacting with items, users often have diverse intents, such as preferences for specific clothing brands and styles, friends recommending, or passing time with randomly clicking [14, 23]. To capture these diverse intents, we assume \ud835\udc3e different intents \ud835\udc50 \ud835\udc62 and \ud835\udc50 \ud835\udc63 from the user and item sides, respectively, where the intents on the item side can also be understood as the theme or context of the item, for example, a user who intends to purchase a fashionable dress may like clothes of 'young' topic. Our predictive objective of user-item preference can be presented as follows:  Specifically, we define \ud835\udc3e global intent prototypes { c \ud835\udc58 \ud835\udc62 \u2208 R \ud835\udc51 } \ud835\udc3e \ud835\udc58 = 1 and { c \ud835\udc58 \ud835\udc63 \u2208 R \ud835\udc51 } \ud835\udc3e \ud835\udc58 = 1 for user and item, respectively. With these predefined intent prototypes, we then are supposed to integrate them into user/item representations, and update them with related global signals. 3.1.2 Intent Modeling with graph transformer. Towards accurately modeling user intents with global information and learning intentaware user/item representations, we perform an intent-aware information propagation with these learnable intents. Specifically, intentaware user/item embeddings are acquired by an attentive sum of the intent prototypes, and user/item embeddings of each layer are updated by aggregating the global user/item/relation/entity signals. Formally, we could get intent-aware user/item representations at the \ud835\udc59 -th user/item embedding layer, by aggregating information across different \ud835\udc3e learnable intent prototypes (including c \ud835\udc62 and c \ud835\udc63 ), using the following design:   where the \ud835\udc43 ( c \ud835\udc58 \ud835\udc62 | e \ud835\udc59 \ud835\udc62 ) and \ud835\udc43 ( c \ud835\udc58 \ud835\udc63 | e \ud835\udc59 \ud835\udc63 ) denotes the importance score of c \ud835\udc58 \ud835\udc62 for \ud835\udc59 -th user embeddings that has encodes the global signals. Similarly, the \ud835\udc43 ( c \ud835\udc58 \ud835\udc63 | e \ud835\udc59 \ud835\udc63 ) denotes the importance score of c \ud835\udc58 \ud835\udc62 for \ud835\udc59 -th item embeddings. As for the way of calculating the \ud835\udc59 -th user/item embeddings, we propose to adopt a two-step process to encode the global user/item/ relation/entity information in the whole heterogeneous graph. The first step is to merge the knowledge information (including both relation and entity) into item embeddings with a proposed relationaware graph aggregation, making the item representation more comprehensive and informative. It injects the relational context into the embeddings of the neighboring entities, and weighting them with the knowledge rationale scores (It's worth noting that items are a subset of knowledge entities), as follows:  where | | denotes concat operation, \ud835\udc41 \ud835\udc56 denotes the set of neighboring entities. Then the second step is to apply a novel graph transformer among user-item graph, which encodes global user/item/entity information into user/item representations. By doing so, the user/item representations of each layer are integrated with global signals, which would be exploited into intent modeling and representation updating, as follows:  where \ud835\udc3b denotes the number of attention heads (indexed by \u210e ). \ud835\udc5a \ud835\udc63,\ud835\udc63 \u2032 is the binary indicator to decide whether to calculate the attentive relations between user \ud835\udc62 and item \ud835\udc56 . \ud835\udefd \u210e \ud835\udc62,\ud835\udc56 denotes the attention weight for user-item interaction pair ( \ud835\udc62, \ud835\udc56 ) w.r.t. the \u210e -th head representation space. W \u210e Q , W \u210e K , W \u210e V \u2208 R \ud835\udc51 / \ud835\udc3b \u00d7 \ud835\udc51 denotes the query, key, the value embedding projection for the \u210e -th head, respectively. WWW'24 Companion, May 13-17, 2024, Singapore, Singapore. Ding Zou et al. Figure 2: Overall framework illustration of the proposed KGTN model. Best viewed in color. Knowledge Injection \ud835\udc621 \ud835\udc56 1 \ud835\udc621 \ud835\udc56 2 \ud835\udc622 \ud835\udc56 2 ... \ud835\udc62\ud835\udc40 \ud835\udc56\ud835\udc41 Global Intent Modeling with Graph Transformer Sampled graph \ud835\udc621 \ud835\udc622 \ud835\udc56 1 \ud835\udc56 2 \ud835\udc56 3 \ud835\udc56 1 \ud835\udc56 2 \ud835\udc56 3 Local-Global Contrastive Learning Light-GCN BPR Recommendation CL InfoNCE \ud835\udc621 \ud835\udc622 \ud835\udc56 1 \ud835\udc56 2 \ud835\udc56 3 \ud835\udc521 \ud835\udc522 \ud835\udc523 \ud835\udc56 1 \ud835\udc56 2 \ud835\udc56 3 \ud835\udc501 \ud835\udc502 ... \ud835\udc524 \ud835\udc50\ud835\udc3e Local Global \ud835\udc7b \u00b7         / d/H H \ud835\udefd\ud835\udc62,\ud835\udc56 \u210e = exp\ud835\udefd \ud835\udc62,\ud835\udc56 \u210e exp\u2061 \ud835\udefd \ud835\udc62,\ud835\udc56 \u210e \ud835\udc56 \ud835\udefd\ud835\udc62,\ud835\udc56 \u210e \ud835\udc56 \u00b7 + \ud835\udc4a\ud835\udc44 \u210e \ud835\udc4a\ud835\udc3e \u210e \ud835\udc4a\ud835\udc49 \u210e \ud835\udc50\ud835\udc62 \ud835\udc50\ud835\udc56 Intent-aware Embeddings e \ud835\udc62 \ud835\udc59 = c \ud835\udc62 \ud835\udc58 \ud835\udc3e \ud835\udc58 \ud835\udc43( c \ud835\udc62 \ud835\udc58 | e \ud835\udc62 \ud835\udc59 ) By integrating global information into users/items, we could learn intent-aware user/item representations and update the learnable intents according to Equation 2. from KG and CF to denoise, we further propose a local-global contrastive mechanism to improve the robustness of representation learning.", "3.2 Knowledge Contrastive Denoising under Intents": "It is intuitive that noisy or irrelevant connections between entities in knowledge graphs can lead to suboptimal representation learning, which is opposite to original purpose of introducing the KG. To eliminate the noise effect in the KG and distill informative signals that benefit the recommendation task, we propose to highlight important connections consistent to user intents, while removing the irrelevant ones. 3.2.1 Knowledge Sampling under intents. With the intent-aware user/item representations, we then try to denoise the item-entity graph by removing the irrelevant edges and nodes and sampling the important ones. We first exploit the intent-aware representations to calculate the importance score of knowledge triplets ( i.e., the item-relation-entity pairs) same as Equation 4, then add the Gumbel noise [8] to the learned importance scores to improve the sampling robustness, as follows:  where \ud835\udf16 is a random variable sampled from a uniform distribution. Then it follows a top-k sampling strategy for generating the new item-entity graph that removes the irrelevant edges and nodes:  where b \ud835\udefd ( \ud835\udc56, \ud835\udc5f, \ud835\udc63 ) is the sampled triples in item-entity graph, which would be used to replace the original graph structure in the following user/item representation learning. 3.2.2 Local-Global Knowledge Contrastive Learning. With the sampled item-entity graph, we then propose to iteratively update the intent-aware representations in it. And inspired by previous contrastive learning based methods that align the item representations Specifically, we exploit the user-item graph and sampled itementity graph to perform light information aggregation with intentaware user/item representations e \ud835\udc62 , e \ud835\udc56 as input z ( 0 ) \ud835\udc62 , z ( 0 ) \ud835\udc56 , for acquiring a robust and effective intent-aware user/item representations, as follows:  where z ( 0 ) \ud835\udc62 , z ( 0 ) \ud835\udc56 memorize the global signals, and we hence get final representations of user/item z ( \ud835\udc59 ) \ud835\udc62 , z ( \ud835\udc59 ) \ud835\udc56 ( \ud835\udc59 \u2208 \ud835\udc3f ) . Besides the supervised user/item representation learning, we propose to perform a contrastive learning between the nodes embeddings that encode global signals and local signals, which is different from traditional cl-based methods that contrast the CF and KG parts. We perform information aggregation in the sampled graph with the initial user/item representations e ( 0 ) \ud835\udc62 , e ( 0 ) \ud835\udc56 to acquire the local results z ( \ud835\udc59 ) \ud835\udc62,\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59 , z ( \ud835\udc59 ) \ud835\udc56,\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59 ( \ud835\udc59 \u2208 \ud835\udc3f ) , while utilizing the intent-aware user/item representations e \ud835\udc62 , e \ud835\udc56 that contains global signals to acquire the global results z ( \ud835\udc59 ) \ud835\udc62 , z ( \ud835\udc59 ) \ud835\udc56 ( \ud835\udc59 \u2208 \ud835\udc3f ) . Then perform layer-wise contrastive learning between local and global results. Thelocal aggregation layer embeddings z ( \ud835\udc59 ) \ud835\udc62,\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59 , z ( \ud835\udc59 ) \ud835\udc56,\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59 and global aggregation layer embeddings z ( \ud835\udc59 ) \ud835\udc62 , z ( \ud835\udc59 ) \ud835\udc56 are made to be contrasted in a layer-wise way. We generate each positive pair using the embeddings of the same user (item) from the local view and each of the global view, and other nodes form the negative pairs. We could get the contrastive loss of users as follows:  where s (\u00b7) denotes the cosine similarity calculating, and \ud835\udf0f denotes a temperature parameter. And similarly we could get the contrastive loss of item L \ud835\udc56 \ud835\udc50 . By summing the two contrastive losses we hence have the total local-global contrastive loss L \ud835\udc50 . Knowledge Enhanced Multi-intent Transformer Network for Recommendation WWW'24 Companion, May 13-17, 2024, Singapore, Singapore. Table 1: Statistics for the three datasets.", "3.3 Model Prediction": "After learning intent-aware user/item representations with global signals and performing contrastive learning between local and global information, we have multi-layer intent-aware representations for user/item. By summing all the layers' representations, we have the final user/item representations and predict their matching score through inner product, as follows:  By adopting a BPR loss [15] to reconstruct the historical data, which encourages the prediction scores of a user's historical items to be higher than the unobserved items, we acquire the supervised loss:  where \ud835\udc76 = GLYPH<8> ( \ud835\udc62, \ud835\udc56, \ud835\udc57 ) | ( \ud835\udc62, \ud835\udc56 ) \u2208 \ud835\udc76 + , ( \ud835\udc62, \ud835\udc57 ) \u2208 \ud835\udc76 -GLYPH<9> is the training dataset consisting of the observed interactions \ud835\udc76 + and unobserved counterparts \ud835\udc76 -; \ud835\udf0e is the sigmoid function.", "3.4 Multi-task Training": "To combine the recommendation task with the self-supervised task, we optimize the whole model with a multi-task training strategy. We combine the local-global contrastive loss with BPR loss, and learn the model parameter via minimizing the following objective function:  where \u0398 is the model parameter set, \ud835\udefc is a hyperparameter to determine the local-global contrastive loss ratio, \ud835\udefd and \ud835\udf06 are two hyperparameters to control the contrastive loss and \ud835\udc3f 2 regularization term, respectively.", "4 EXPERIMENT": "Aiming to answer the following research questions, we conduct both offline experiments and online A/B tests on three public datasets and Alibaba online platform: \u00b7 RQ1: How does KGTN perform, compared to present models? \u00b7 RQ2: How do the main components in KGTN affect its effectiveness? \u00b7 RQ3: Howdodifferent hyper-parameter settings affect KGTN? \u00b7 RQ4: How does KGTN perform with noisy injection? \u00b7 RQ5: How does KGTN perform in a live system serving billions of users?", "4.1 Experiment Settings": "4.1.1 Dataset and Metrics. Three benchmark datasets are utilized to evaluate the effectiveness of KGTN: Last.FM 1 , Book-Crossing 2 , and MovieLens-1M 3 . The detailed statistics of them are summarized in Table 1, which vary in size and sparsity and make our experiments more convincing. As for the data pre-process, we first follow RippleNet [18] to transform their explicit feedback into implicit one, and randomly sample negative samples from his unwatched items with the size equal to his positive ones to construct the negative parts. As for the sub-KG construction, we follow RippleNet [18] and use Microsoft Satori 4 to construct it for MovieLens-1M, Book-Crossing, and Last.FM datasets. Each sub knowledge graph that follows the triple format is a subset of the whole KG with a confidence level greater than 0.9. We evaluate our method in two experimental scenarios: (1) In click-through rate (CTR) prediction, we apply the trained model to predict each interaction in the test set. We adopt two widely used metrics [18, 21] \ud835\udc34\ud835\udc48\ud835\udc36 and \ud835\udc39 1 to evaluate CTR prediction. (2) In top\ud835\udc3e recommendation, we use the trained model to select \ud835\udc3e items with the highest predicted click probability for each user in the test set, and we choose Recall@ \ud835\udc3e to evaluate the recommended sets. 4.1.2 Baselines. To demonstrate the effectiveness of our proposed KGTN, we compare it with four types of KGR methods: CF-based methods (BPRMF [15]), embedding-based method (CKE [33], RippleNet [18]), path-based method (PER [32]), GNN-based methods(KGCN [21], KGNN-LS [20], KGAT [22], CKAN [26], KGIN [23], CG-KGR [3]), CL-based methods (KGCL[29], MCCLK [37]). 4.1.3 Parameter Settings. We implement our KGTN and all baselines in Pytorch and carefully tune the key parameters. For a fair comparison, we fix the embedding size to 64 for all models, and the embedding parameters are initialized with the Xavier method [4]. We optimize our method with Adam [9] and set the batch size to 2048. A grid search is conducted to confirm the optimal settings, we tune the learning rate \ud835\udf02 among { 0 . 0001 , 0 . 0003 , 0 . 001 , 0 . 003 } and \ud835\udf06 of \ud835\udc3f 2 regularization term among { 10 -7 , 10 -6 , 10 -5 , 10 -4 , 10 -3 } . Other hyper-parameter settings are provided in Table 1. The best settings for hyper-parameters in all comparison methods are researched by either empirical study or following the original papers.", "4.2 Performance Comparison (RQ1)": "We report the empirical results of all methods in Table 2 and Table 3. The improvements and statistical significance test are performed between KGTN and the strongest baselines (highlighted with underline). Analyzing such performance comparison, we have the following observations: \u00b7 Our proposed KGTN achieves the best results. KGTN consistently outperforms all baselines across three datasets in terms of all measures, which achieves significant improvements over the 1 https://grouplens.org/datasets/hetrec-2011/ 2 http://www2.informatik.uni-freiburg.de/~cziegler/BX/ 3 https://grouplens.org/datasets/movielens/1m/ 4 https://searchengineland.com/library/bing/bing-satori WWW'24 Companion, May 13-17, 2024, Singapore, Singapore. Ding Zou et al. Table 2: The result of \ud835\udc34\ud835\udc48\ud835\udc36 and \ud835\udc39 1 in CTR prediction. The best results are in boldface and the second best results are underlined. * denotes statistically significant improvement by unpaired two-sample \ud835\udc61 -test with \ud835\udc5d < 0 . 001 . Table 3: The result of \ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 @10 and \ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 @20 in top\ud835\udc3e recommendation. strongest baselines w.r.t. AUC by 2.76%, 1.20%, and 2.41% in Book, Movie, and Music respectively, and demonstrates its effectiveness. We attribute such improvements to the following aspects: (1) By modeling user intents with global signals, KGTN is able to learn user/item representations in a more fine-grained and comprehensive manner; (2) The knowledge sampling strategy under intents could remove less relevant knowledge information for a robust representation learning; (3) The local-global contrastive learning improves the representation learning in a self-supervised manner, via contrasting the local and global information. \u00b7 Incorporating KG not always benefits recommender system. Comparing CKE with BPRMF, leaving KG untapped limits the performance of BPRMF, which shows the effectiveness of KG information. While PER gets a worse performance than BPRMF, which means that only incorporating suitable knowledge could benefit the model. This fact stresses the importance of knowledge sampling and knowledge denoising. \u00b7 GNN has a strong power of graph learning. Most of the GNN-based methods perform better, suggesting the importance of modeling long-range connectivity for graph representation learning. This fact inspires us to go beyond the local aggregation paradigm, and to consider the global signals. \u00b7 Contrastive Learning is effective. The most recently proposed CL-based methods have the best performance, which shows the effectiveness of incorporating a self-supervised task for improving representation learning. It inspires us to design proper contrastive mechanisms to denoise the knowledge and improve the model performance.", "4.3 Ablation Studies (RQ2)": "As shown in Figure 3, here we examine the contributions of main components in our model to the final performance by comparing KGTN with the following three variants: 1) KGTN \ud835\udc64 / \ud835\udc5c \ud835\udc46 : In this variant, the knowledge sampling under intents module is removed. 2) KGTN \ud835\udc64 / \ud835\udc5c \ud835\udc36 : This variant removes local-global contrastive mechanism. 3) KGTN \ud835\udc64 / \ud835\udc5c \ud835\udc3c : This variant removes the multi-intent modeling, which means both global intent modeling and knowledge contrastive denoising do not exist in this variant. The results of two variants and KGTN are reported in Figure 3, from which we have the following observations: \u00b7 Removing both knowledge sampling and local-global contrasting would degrade model performance, which shows their effectiveness in representation learning. \u00b7 Ablating the multi-intent modeling brings the worst performance, which shows the importance of incorporating global signals and considering multiple intents.", "4.4 Sensitivity Analysis (RQ3)": "4.4.1 Impact of graph transformer depth. To study the influence of graph transformer depth, we vary \ud835\udc3f in range of {1, 2, 3} on book, movie, and music datasets. As shown in Table 4, KGTN performs best when \ud835\udc3f = 1. It convinces that one iteration is enough for Knowledge Enhanced Multi-intent Transformer Network for Recommendation WWW'24 Companion, May 13-17, 2024, Singapore, Singapore. Figure 3: Effect of ablation study. AUC F1 0.762 0.768 0.774 0.78 0.786 AUC Book-Crossing KGTN KGTN w / o S KGTN w / o C KGTN w / o I 0.67 0.674 0.678 0.682 0.686 F1 AUC F1 0.932 0.933 0.934 0.935 0.936 0.937 AUC MovieLens-1M 0.857 0.858 0.859 0.86 0.861 0.862 0.863 0.864 F1 Table 4: Impact of graph transformer depth. F1 Figure 4: Impact of intent number \ud835\udc3e . 1 16 32 64 128 256 0.580 0.624 0.668 0.712 0.756 0.800 AUC AUC F1 0.664 0.669 0.674 0.679 0.684 0.689 (a) Book 1 16 32 64 128 256 0.870 0.876 0.882 0.888 0.894 0.900 AUC AUC F1 0.760 0.768 0.776 0.784 0.792 0.800 F1 (b) Music Figure 5: Impact of contrastive loss ratio \ud835\udefc . 1 0.1 0.01 0.0010.0001 0.580 0.624 0.668 0.712 0.756 0.800 AUC AUC F1 0.570 0.594 0.618 0.642 0.666 0.690 F1 (a) Book 1 0.1 0.01 0.0010.0001 0.600 0.660 0.720 0.780 0.840 0.900 AUC AUC F1 0.580 0.624 0.668 0.712 0.756 0.800 F1 (b) Music integrating the global signals into user/item representations, which shows its low reliance on model depth. 4.4.2 Impact of intent number \ud835\udc3e . To investigate the impact of the intent number, we vary it from the range {16, 32, 64, 128, 256} and the model performance is shown in Figure 4, from which we could draw the following conclusions: i) Ignoring the multiple intents ( \ud835\udc3e = 1) results in the worst performance, which convinces the effectiveness of incorporating multi-intent modeling. ii) The model performance first arises then drops with the intent increasing. A suitable intents number boosts the model performance with fine-grained preference learning, while too many intents mean too fine-grained modeling and inversely introduce noise into representation learning. 4.4.3 Impact of contrastive loss ratio \ud835\udefc . The parameter \ud835\udefc determines the importance of the contrastive loss during the multi-task Figure 6: Noise Analysis in Music and Book datasets. 0 0.05 0.1 0.15 0.2 Noisy Ratio 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 %Drop Percent KGTN MCCLK (a) Book 0 0.05 0.1 0.15 0.2 Noisy Ratio 0.0 0.2 0.4 0.6 0.8 1.0 %Drop Percent KGTN MCCLK (b) Music training. Hence we vary it in range {1, 0.1, 0.01, 0.001} to study its influence. As shown in Figure 5, we observe that: \ud835\udefc = 0 . 1 brings the best model performance, the main reason is that changing the contrastive loss to a fairly equal level to recommendation task loss could boast the model performance.", "4.5 Denoising Analysis (RQ4)": "We additionally conduct a denoising experiment here, for checking the model robustness under noisy interactions. Specifically, we contaminate the training set by adding a certain proportion of adversarial examples ( i.e., 5%, 10%, 15%, 20% negative user-item interactions), meanwhile keeping the validation and testing sets unchanged, following SGL [28]. This experiment could show the model ability of noise-irrelevant representation learning, from an overall perspective. The experimental results on Baby are shown in Figure 6, where the Noisy Ratio means the percentage of noisy interactions added for the model, and the %Drop Percentage represents the percentage of performance degradation. From the experimental results, we could clearly find that: Although adding noise degrades the model performance of both KGTN and MCCLK, the proposed KGTN is clearly less influenced than the GNN-based and CL-based MCCLK. It is more apparent with a bigger noise ratio, since the performance dropping gap between KGTN and MCCLK becomes larger and larger as the noise ratio increases, which suggests that KGTN is more robust to noisy perturbation. AUC F1 0.876 0.879 0.882 0.885 0.888 AUC Last.FM 0.784 0.787 0.79 0.793 0.796 0.799 F1 WWW'24 Companion, May 13-17, 2024, Singapore, Singapore. Ding Zou et al. Table 5: Results of online A/B testing.", "4.6 Online A/B Testing (RQ5)": "We conduct online A/B testing in our recommender system in Alibaba, to validate the benefits of KGTN in the real business scenario. In our online A/B testing, users are randomly divided into A group and B group. When group A browses the website, the system recommends results provided by the previous model, while the group B is recommended with results provided by our KGTN model. Experiments are run for three weeks, during which both the control and experiment models are trained continuously with new interactions and feedback being used as training data. As shown in Table 5, KGTN improves item page view (ipv) per user by 2.3%, unique visitor list to order (uv l2o) by 2.22%, and unique visitor click through rate (uv ctr) by 2.3% relatively compared with DMR [12], which is the last version of CTR model in our system. This reveals the practical application value of KGTN.", "5 RELATED WORK": "", "5.1 Knowledge Enhanced Recommendation": "Existing Knowledge Enhanced recommendation methods can be roughly categorized into three lines: embedding-based, path-based, and GNN-based methods. Embedding-based methods [7, 19, 34] pre-train the KG entity embeddings with knowledge graph embeddings methods (KGE) [1, 10], for enriching item representations, such as CKE [33] and KTUP [2], and RippleNet [18]. Pathbased methods [16, 31, 35] explore various patterns of connections among items in KG to provide additional guidance for the recommendation, such as PER [32] and MCRec[6]. KPRN [25] further automatically extracts paths between users and items, modeling these paths with RNNs. GNN-based methods are founded on the information aggregation mechanism of graph neural networks (GNNs) [5, 30], which integrates multi-hop neighbors into node representations, modeling long-range connectivity. KGCN [21] and KGNN-LS [20] firstly utilize GNN on KG side, then KGAT[22] propose to utilizes GAT on the unified user-item-entity heterogeneous graph. Then CKAN [26] separately models collaborative signals and knowledge signals, and CG-KGR [3] exploits the collaborative signals to guide the aggregation on KG. KGIN [23] models user-item interactions at an intent level, which reveals user intents behind the KG interactions and performs GNN on the user-intent-item-entity graph. More recently, CL-based Methods such as MCCLK [37], KGIC [38], and KGCL [29] combine a contrastive learning paradigm wtih GNN-based methods, and build cross-view contrastive frameworks as additional self-discrimination supervision signals to enhance robustness.", "5.2 Multi-intent Modeling": "Current multi-intent modeling usually adopts a disentangled representation learning paradigm, which splits the user embedding into multiple chunks and tries to learn each chunked emebedding respectively for representing each user intent. In graph representation learning area, DisenGCN [13], IPGDN [11] and ADGCN [36] adopt such a paradigm and utilize the Hilbert-Schmidt Independence Criterion (HSIC) and adversarial learning for ensuring the effectiveness of intent modeling. As for recommendation scenarios, DGCF [24] proposes to disentagnle the user representations and adopts a mutual information restraint for the independence of all intents. And KGIN [23] considers each intent as an attentive combination of KG relations, then use a local aggregation manner for the intent modeling. MIDGN [39] performs fine-grained intent disentanglement from the hierarchical structure in bundle recommendation, together with an intent contrastive mechanism. Despite the success of disentangled learning attempts in previous methods, they commonly learns the multi-intent representations with local information, while ignore the importance of global signals. Hence, our work focuses on learning intent-aware representations with global information, and exploits the intent features to denoise the knowledge information.", "6 CONCLUSION": "In this paper, we focus on modeling multiple intents with global information, and leveraging intents to denoise the knowledge information. We propose a novel framework, KGTN, which achieves fine-grained and robust user/item representation learning from two dimensions: 1) KGTN models global intents and learns intent-aware user/item representations with a proposed graph transformer, by integrating global signals into learnable intents. 2) KGTN exploits the user intents to sample the relevant knowledge, and designs a localglobal contrastive mechanism within the sampled graph, for robust representation learning. Extensive experiments on three public datasets demonstrate that KGTN significantly improves the recommendation performance over baselines on both Click-Through rate prediction and Top-K recommendation tasks. Furthermore, the online A/B testing on recommender system of Alibaba demonstrates the practical application value of KGTN.", "ACKNOWLEDGMENTS": "This work was supported in part by the National Natural Science Foundation of China under Grant No. 62276110, No. 62172039 and in part by the fund of Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL). The authors would also like to thank the anonymous reviewers for their comments on improving the quality of this paper.", "REFERENCES": "[1] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Neural Information Processing Systems (NIPS) . 1-9. [2] Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. 2019. Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences. In WWW . 151-161. [3] Yankai Chen, Yaming Yang, Yujing Wang, Jing Bai, Xiangchen Song, and Irwin King. 2022. Attentive Knowledge-aware Graph Convolutional Networks with Collaborative Guidance for Personalized Recommendation. In ICDE . 299-311. [4] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In AISTATS . 249-256. [5] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In NeurIPS . 1025-1035. Knowledge Enhanced Multi-intent Transformer Network for Recommendation WWW'24 Companion, May 13-17, 2024, Singapore, Singapore. [6] Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S Yu. 2018. Leveraging meta-path based context for top-n recommendation with a neural co-attention model. In SIGKDD . 1531-1540. [7] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y Chang. 2018. Improving sequential recommendation with knowledge-enhanced memory networks. In SIGIR . 505-514. [8] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparametrization with gumble-softmax. In International Conference on Learning Representations (ICLR 2017) . OpenReview. net. [9] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [10] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In AAAI . [11] Yanbei Liu, Xiao Wang, Shu Wu, and Zhitao Xiao. 2020. Independence promoted graph disentangled networks. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 4916-4923. [12] Ze Lyu, Yu Dong, Chengfu Huo, and Weijun Ren. 2020. Deep match to rank model for personalized click-through rate prediction. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 156-163. [13] Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. 2019. Disentangled graph convolutional networks. In International conference on machine learning . PMLR, 4212-4221. [14] Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, and Chao Huang. 2023. Disentangled Contrastive Collaborative Filtering. arXiv preprint arXiv:2305.02759 (2023). [15] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv (2012). [16] Chuan Shi, Binbin Hu, Wayne Xin Zhao, and S Yu Philip. 2018. Heterogeneous information network embedding for recommendation. IEEE Transactions on Knowledge and Data Engineering (2018), 357-370. [17] Zhu Sun, Jie Yang, Jie Zhang, Alessandro Bozzon, Long-Kai Huang, and Chi Xu. 2018. Recurrent knowledge graph embedding for effective recommendation. In RecSys . 297-305. [18] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2018. Ripplenet: Propagating user preferences on the knowledge graph for recommender systems. In CIKM . 417-426. [19] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep knowledge-aware network for news recommendation. In WWW . 1835-1844. [20] Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, Jure Leskovec, Miao Zhao, Wenjie Li, and Zhongyuan Wang. 2019. Knowledge-aware graph neural networks with label smoothness regularization for recommender systems. In SIGKDD . 968977. [21] Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. 2019. Knowledge graph convolutional networks for recommender systems. In WWW . 3307-3313. [22] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat: Knowledge graph attention network for recommendation. In SIGKDD . 950-958. [23] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, and Tat-Seng Chua. 2021. Learning Intents behind Interactions with Knowledge Graph for Recommendation. In WWW . 878-887. [24] Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng Chua. 2020. Disentangled graph collaborative filtering. In Proceedings of the 43rd"}
