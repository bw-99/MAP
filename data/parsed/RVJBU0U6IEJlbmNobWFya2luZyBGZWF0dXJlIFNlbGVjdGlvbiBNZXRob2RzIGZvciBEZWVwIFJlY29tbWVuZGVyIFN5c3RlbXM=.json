{
  "ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems": "",
  "Pengyue Jia ‚àó": "",
  "Yejing Wang ‚àó": "",
  "Zhaocheng Du ‚àó": "jia.pengyue@my.cityu.edu.hk City University of Hong Kong",
  "Xiangyu Zhao ‚Ä†": "xianzhao@cityu.edu.hk City University of Hong Kong Wanyu Wang wanyuwang4-c@my.cityu.edu.hk City University of Hong Kong yejing.wang@my.cityu.edu.hk City University of Hong Kong Yichao Wang wangyichao5@huawei.com Huawei Noah's Ark Lab Huifeng Guo ‚Ä† huifeng.guo@huawei.com Huawei Noah's Ark Lab",
  "ABSTRACT": "zhaochengdu@huawei.com Huawei Noah's Ark Lab Bo Chen chenbo116@huawei.com Huawei Noah's Ark Lab Ruiming Tang tangruiming@huawei.com Huawei Noah's Ark Lab",
  "KEYWORDS": "Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods. This approach is typically computationally infeasible for identifying the optimal hyperparameters and overlooks evaluating the robustness and stability of these methods. To bridge these gaps, this paper presents ERASE, a comprehensive b E nchma R k for fe A ture SE lection for DRS. ERASE comprises a thorough evaluation of eleven feature selection methods, covering both traditional and deep learning approaches, across four public datasets, private industrial datasets, and a real-world commercial platform, achieving significant enhancement. Our code is available online 1 for ease of reproduction.",
  "CCS CONCEPTS": "",
  "¬∑ Information systems ‚Üí Recommender systems .": "‚àó Authors contributed equally to this research. ‚Ä† Corresponding author. 1 https://github.com/Applied-Machine-Learning-Lab/ERASE Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '24, August 25-29, 2024, Barcelona, Spain ¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0490-1/24/08. https://doi.org/10.1145/3637528.3671571 Benchmark, Feature Selection, Deep Recommender System",
  "ACMReference Format:": "Pengyue Jia, Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Yichao Wang, Bo Chen, Wanyu Wang, Huifeng Guo, and Ruiming Tang. 2024. ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 25-29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671571",
  "1 INTRODUCTION": "Recommender systems have become indispensable in many sectors ranging from e-commence to content streaming in the realm of information explosion [37, 38]. With the application of deep learning techniques, Deep Recommender System (DRS) exhibits amplified prediction ability of user preference, providing personalized experience and dominating the deployment landscape [7, 16, 43]. To improve the accuracy of recommendations, DRS is progressively integrating an expanding array of feature fields into their predictive models, which can number in the hundreds or even thousands [19]. The significance of each feature varies, resulting in the accumulation of superfluous or extraneous features. Consequently, feature selection, which concentrates on pinpointing and leveraging the most critical features, is becoming increasingly crucial in modern DRS [5, 52]. One immediate benefit of feature selection is the enhancement of prediction performance, achieved by eliminating non-contributory features that could otherwise adversely affect predictions. From an industrial viewpoint, selecting predictive features is also essential for meeting deployment criteria regarding memory usage since unnecessary storage demands can often be inflated by the presence of redundant features. Hand-crafted feature selection usually requires lots of expert knowledge and labor efforts, which is usually infeasible to achieve optimal results when the candidate set contains thousands of features. Researchers have designed various methods to automatically select predictive features, including statistical methods [31, 41, 42], learning methods [4, 6, 13], and agent-based methods [10, 34, 50]. Despite the satisfactory results these methods achieved, they also reveal the following issues hindering the development of this field: KDD '24, August 25-29, 2024, Barcelona, Spain Pengyue Jia, et al. ¬∑ Experimental Differences. Recent years have witnessed a variety of methods designed for DRS [17, 26, 29, 35, 44-46]. However, these works conduct experiments with varied settings. For example, MultiSFS [44] is designed to select features for multitask DRS. SHARK [46] suggests the feature selection method F-permutation and a quantization method. Optfs [35] selects the feature from the value-level while others mainly select from the field-level. These experimental differences usually lead to unfair or unavailable comparisons, making the subsequent researchers struggle to generate practical insights. ¬∑ Insufficiency. Existing feature selection benchmarks are predominantly tailored for conventional downstream tasks, such as classification [2, 3]. They are primarily built upon synthetic or domain-specific datasets [1, 9, 12], which diverges significantly from the complex, large-scale datasets encountered in DRS. This divergence results in a notable deficiency in guiding feature selection specifically for DRS, due to the benchmarks' disparate scope and focus. DeepLasso [8], while being among the benchmarks most closely related to DRS, primarily addresses tabular learning-a context that, despite similarities, does not fully align with the intricacies of recommendation tasks. Furthermore, such benchmarks often limit their exploration to traditional selection methods and rely on datasets of a much smaller scale, thereby omitting crucial, in-depth analysis from an industrial perspective for DRS. Moreover, while related literature surveys [5, 52] provide comprehensive reviews of the field, they fall short in offering empirical evidence or experimental results that could motivate practical deployments in DRS, lacking the necessary data-driven support to inform and inspire future research directions. ¬∑ Assessment Deficit. A pivotal hyperparameter for feature selection methods is the number of features to be selected, denoted as ùëò in this study. Feature selection methods often exhibit significant variability in performance across different values of ùëò , and the optimal ùëò is not the same for all methods [29, 35, 45]. This variability on ùëò introduces two significant challenges in evaluating feature selection methods. First, the direct comparison of methods at their respective optimal ùëò may not constitute a fair assessment. Such comparisons fail to account for the different memory requirements associated with varying optimal ùëò values for different selection methods. Furthermore, this approach neglects the performance variability under sub-optimal hyperparameter settings, thereby obscuring insights into the methods' robustness and stability across a range of ùëò values. Second, the exhaustive search for the optimal ùëò across the entire spectrum of possible values is time-consuming and computationally intensive. This necessitates an evaluation methodology capable of effectively assessing a method's performance based on partial ùëò values, offering a more efficient means to gauge feature selection effectiveness without finishing complete iterations. To address these issues, we propose ERASE, a comprehensive b E nchma R k for fe A ture S election for DRS. ERASE initiates a unified and fair experimental framework, minimizing experimental discrepancies across various selection methods. Remarkably, ERASE pioneers as the first feature selection benchmark with a focus on DRS tasks, incorporating both prevalent DRS feature selection techniques and conventional methods. It introduces a novel taxonomy to classify these methods and unearth intrinsic patterns among groups of methods. By evaluating the performance on widely used public datasets and authentic industrial production datasets-through both offline comparison and online testing-ERASE furnishes strong empirical support, facilitating the generation of actionable insights. In its endeavor to provide a comprehensive assessment of feature selection methods, ERASE contrasts the optimal performance of compared selection methods alongside their outcomes under specific deployment prerequisites. Additionally, we introduce a novel metric, AUKC, specifically crafted for assessing the robustness and stability of feature selection methods across a range of feature quantities ùëò , thereby addressing the critical lack of such evaluative metrics. We summarize our major contributions as follows: ¬∑ We present ERASE, a comprehensive benchmark for DRS feature selection methods, providing a fair comparison for emerging selection techniques with various datasets and DRS backbones. To the best of our knowledge, we are the first to focus on benchmarking feature selection methods for recommendation tasks. ¬∑ We recognize the assessment shortfall linked to the substantial dependency of selection efficacy on the hyperparameter ùëò , and in response, we introduce a novel evaluation metric, AUKC. This metric is designed to evaluate the robustness and stability of feature selection methods, bridging the existing gap in assessment. ¬∑ We carry out thorough experiments across four widely used public datasets and real-world industrial production datasets, yielding insights from various angles. Notably, our experimental findings have guided optimizations in our online platform, achieving a 20% reduction in latency without compromising effectiveness, validating the practical utility of our benchmark.",
  "2 BECNCHMARK DESIGN": "In this section, we will give an overview of our benchmark. As shown in Figure 1, the benchmark consists of four components: dataset, feature selection methods, backbone models, and metrics.",
  "2.1 Datasets": "To comprehensively evaluate the effectiveness of different feature selection methods, we select four public datasets for our experiments. Avazu 2 and Criteo 3 are selected because they are frequently used dataset for studying feature selection in DRS [29, 35, 45]. To compare the performance of different methods with a small feature set, we choose the popular Movielens-1M 4 dataset in the recommendation field. Additionally, to compare the performance of different methods in scenarios closer to real-world recommender systems, we supplement with the AliCCP 5 dataset, which possesses user and item features and includes a rich set of 85,316,519 interaction samples. The statistics of datasets and the detailed introduction are illustrated in Appendix A.",
  "2.2 Feature Selection Methods": "Welist all feature selection methods in this work and their attributes in Table 1. There are three feasible dimensions to classify these 2 https://www.kaggle.com/competitions/avazu-ctr-prediction 3 https://ailab.criteo.com/ressources/ 4 https://grouplens.org/datasets/movielens/1m/ 5 https://tianchi.aliyun.com/dataset/408 ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems KDD '24, August 25-29, 2024, Barcelona, Spain Figure 1: Benchmark Overview. ERASE Datasets (¬ß2.1) Avazu Criteo Movielens-1M AliCCP Feature Selection Methods (¬ß2.2) Shallow Feature Selection Lasso [41] GBDT [13] RF [4] XGBoost [6] Gate-based Feature Selection AutoField [45] AdaFS [29] OptFS [35] LPFS [17] Sensitivity -based Feature Selection Permutation [11] SHARK [46] SFS [44] Backbone Models (¬ß2.3) Wide&Deep [7] DeepFM [16] DCN [43] FibiNet [20] Metrics (¬ß2.4) AUC Logloss AUKC methods. 1) Training strategy. Based on the training strategy, the methods can be single-stage or two-stage. Single-stage usually directly integrates the feature selection module into the original model without changing the training logic. In contrast, two-stage methods contain searching and retraining phases. Informative feature fields or feature values are selected in the searching phase, and the backbone model is trained with these selected features in the retraining phase. 2) Selection type. There are two types of selection in feature selection methods: soft selection and hard selection. The soft selection offers a mask to affect inputs. Features multiplied with 0 are considered filtered out. For the hard selection, features are removed directly from the inputs. 3) Selection technique. Depending on the techniques used for selection, as shown in Figure 2, we divide methods contained in our benchmark into three categories: shallow feature selection, gate-based selection, and sensitivity-based selection. Due to the unbalanced distribution of feature selection methods classifying on training strategy and selection type, we elaborate our work based on selection technique classification. Specifically, our benchmark contains the following methods:",
  "1. Shallow Feature Selection": "Table 1: Methods Overview. For the type column, \"Shallow\" represents shallow feature selection methods, \"Gate\" represents gate-based feature selection methods, and \"Sensitivity\" represents the sensitivity-based feature selection methods. For the single-stage, two-stage, soft selection, and hard selection columns, ‚úî represents applicable, ‚úò represents not applicable, and ‚úî ‚òÖ represents that this method can be applicable after appropriate modifications. ¬∑ Lasso [41]. The least absolute shrinkage and selection operator (Lasso) algorithm is a traditional and useful method in machine learning. It performs both variable selection and regularization to improve the model performance. ¬∑ GBDT [13]. Gradient-boosted decision tree (GBDT) achieves superior performance by continually adding trees to fit residuals. By aggregating the feature importance scores from each tree, it also serves as an effective method for feature selection. ¬∑ RandomForest [4] (RF). RandomForest derives the feature importance by measuring how much each feature decreases the impurity in a tree, commonly using Gini impurity or entropy. ¬∑ XGBoost [6]. Extreme Gradient Boosting (XGBoost) ranks the importance of features by calculating the improvement of every feature on the final performance.",
  "2. Gate-based Feature Selection": "¬∑ AutoField [45]. AutoField is the first gate-based feature selection method in DRS. It designs a novel controller network to generate a 2-dimensional vector determining whether to choose this feature field. It is a two-stage method and selects features on the field level. ¬∑ AdaFS [29]. AdaFS is an algorithm that can generate a gate for each feature field adaptively. It is a single-stage method and selects features on the field level. ¬∑ OptFS [35]. OptFS focuses on the feature value level selection. It trains a scalar for each feature value and continuously strengthens the constraints on sparsity as training progresses. ¬∑ LPFS [17]. LPFS argues that the conclusion that features with smaller weights are less important than those with larger weights may not be correct. It proposes a kind of smoothedùëô 0 function that can effectively select informative features. It is a single-stage method and focuses on the field level.",
  "3. Sensitivity-based Feature Selection": "¬∑ Permutation [11]. Permutation works by randomly permuting the feature values at a time and measuring how much this permutation affects the performance of the model. KDD '24, August 25-29, 2024, Barcelona, Spain Pengyue Jia, et al. Figure 2: Overview of three categories of feature selection methods in deep recommender systems. Shallow methods typically use statistical algorithms to assign feature importance to each field. Gate-based methods, on the other hand, assign gates to feature embeddings and consider the gate values as the importance of the corresponding features. Sensitivity-based methods derive parameter sensitivity from backward propagation steps and calculate feature importance accordingly. ... Forward ... Backward ... Sensitivity-Based Feature Selection ... ... ... Gate-Based Feature Selection ... ...",
  "Shallow Feature Selection": "Importance Score Feature Feature Embedding Forward Backward Sensitivity Score ¬∑ SHARK [46]. Shark takes the novel first-order component of Taylor expansion as the feature importance score for model prediction. It then prunes those features with lower scores from the embedding table to improve model performance and efficiency. with different ùêæ . Specifically, AUKC is formalized as follows: ¬∑ SFS [44]. SFS takes the gradients of the gate for each feature field as the feature importance score. It is a two-stage method and selects on the feature field level.",
  "2.3 Backbone Models": "To comprehensively evaluate the effectiveness of feature selection methods, we choose four popular DRS models as backbones: 1) Wide&Deep : A classical model contains shallow and deep networks to capture feature interactions. 2) DeepFM : A model with FM module to automatically learn feature interactions. 3) DCN : A model with cross layers to study feature interactions. 4) FibiNet : A model equipped with SENet and bilinear layer to adaptively learn feature importance and capture high-order feature interactions. The detailed introduction of these models is in Appendix B.",
  "2.4 Metrics": "In this paper, we focus on the Click-Through Rate (CTR) prediction task, so we take AUC and Logloss as the two main metrics in our benchmark. The detailed introduction is in Appendix C. Additionally, since there is currently no metric that comprehensively evaluates the performance of feature selection methods across varying numbers of selected features, we propose a new evaluation metric named 'AUKC' to address this challenge. AUKC. Current metrics cannot assess the robustness and stability of a specific feature selection method across different numbers of selections. Therefore, we introduce a new metric Area Under the ùêæ -performance Curve (AUKC) to fill this gap. AUKC measures the area under the performance curve of feature selection methods  where | ùêæ | denotes the number of all feature fields, ùê¥ùëàùê∂ ùëò is the AUC score when selecting ùëò feature fields follow a specific feature selecting method. If there are no input features, the model would make random predictions so that ùê¥ùëàùê∂ 0 = 0 . 5. In practice, due to resource constraints, it is generally not feasible to conduct experiments with every possible number of selections. Therefore, we further propose a more general form of AUKC to accommodate this change:  where | ùêæ | denotes the number of all feature fields and | ùëÅ | is the number of segments across the entire length of the feature fields set. ùê¥ùëàùê∂ ùëõ,ùëô is the AUC corresponding to the number of features selected at the left endpoint of the ùëõ -th segment, and ùê¥ùëàùê∂ ùëõ,ùëü denotes the AUC with the number of selections at the right endpoint of the ùëõ th segment. Œî ùëô ùëõ represents the length of the ùëõ -th segment. The detailed process of the formula derivation is in Appendix D. AUKC and AUC share a similar conceptual basis, representing the area under a curve, and both have a value range from 0 to 1. However, unlike AUC, the endpoint of AUKC does not necessarily equal 1, and the values in the middle of the curve can exceed the endpoint value. This occurs because eliminating features with less information can result in a model that performs better than one using all available features. The AUKC metric considers the effectiveness of feature selection at different numbers of selected features, ùëò , thereby providing a more comprehensive reflection of the efficacy of feature selection methods. ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems KDD '24, August 25-29, 2024, Barcelona, Spain",
  "3 EXPERIMENTS": "In this section, we will provide extensive experimental results to answer following research questions: ¬∑ RQ1: In a fair and unified environment, how do different feature selection methods perform? ¬∑ RQ2: How do feature selection methods perform in terms of robustness and stability across various numbers of selections? ¬∑ RQ3: How is the efficiency of feature selection methods? ¬∑ RQ4: Do the performances of feature selection methods align between large-scale industrial datasets and public datasets? ¬∑ RQ5: Do the performances of feature selection methods remain consistent online and offline? ¬∑ RQ6: What is the relationship between feature rankings of different feature selection methods?",
  "3.1 Experimental Details": "We implement the benchmark framework based on Pytorch 1.11. In the training phase, we utilize the Adam optimizer with ùõΩ 1 = 0 . 9, ùõΩ 2 = 0 . 999, and ùúñ = 1 √ó 10 -8 . We set the learning rate as 0.001, the batch size as 4,096, and the embedding size as 8. For the activation function, if the original papers do not emphasize a specific one, we use ReLU as the activation function. We release the repository of our benchmark online 6 . We conduct each experimental setting three times and record its average metrics to mitigate the impact of experimental fluctuations.",
  "3.2 Overall Performance (RQ1)": "In this subsection, we compare the performance of feature selection methods with different backbone models. The complete experimental results for all four backbone models are illustrated in Appendix E. For the two-stage approaches (Lasso, GBDT, RF, XGBoost, AutoField, Permutation, SHARK, and SFS), we experiment with different values of selected features during the retraining stage and adopt the best results. In the case of the single-stage approach, if it can also be modified to a two-stage method (OptFS and LPFS), we conduct experiments on both ways and record the optimal results; if it only supports single-stage (i.e., AdaFS), we directly document its results. From Table 2, we can observe the following points: ¬∑ In general, gate-based methods show better performance compared to shallow feature selection and sensitivity-based feature selection. The likely reason is that compared to using traditional machine learning methods or solely relying on gradient information, leveraging the powerful expression and learning capabilities of deep neural networks allows for a more thorough exploration of feature importance. ¬∑ The effectiveness of feature selection methods remains relatively consistent across different backbone models. This is because the information contained in feature combinations is objective and factual and is not affected by the backbone model. ¬∑ The validity of feature selection methods is distinct across different datasets. The shallow feature selection methods perform better in Criteo and Aliccp. The possible reason is the feature value distribution in the other two datasets is very imbalanced, making shallow models prone to overfitting. Gate-based feature 6 https://github.com/Applied-Machine-Learning-Lab/ERASE selection methods are good at dealing with limited data, while sensitivity-based feature selection methods achieve the best performance among all methods in datasets with rich samples. This finding can be attributed to the fact that in the data-scarce scenario, gradients are too sensitive to judge feature importance. However, with sufficient data support, the stability of gradient information will be greatly improved, allowing for more accurate feature importance rankings. ¬∑ The two-stage approach yields more stable results compared to the single-stage method across different backbone models. This is because the feature combinations filtered out by the twostage method can exist independently after the search phase, whereas the single-stage method requires retraining the feature importance scores with each training session.",
  "3.3 Stability Evaluation (RQ2)": "In this section, we conduct experiments on each backbone model with different number of selected features ùëò to investigate their robustness and stability. Specifically, we iterate a specific list for ùëò (e.g., [ 5 , 10 , 15 , 17 , 19 , 21 ] for Avazu) and record the corresponding performance of feature selection methods. Then, the stability of feature selection methods can be revealed from two perspectives, visual patterns of the performance curve (Figure 3) and AUKC values calculated as Equation (1) (Table 3). In Figrue 3, the x-axis represents the number of selected features ùëò and the y-axis represents the evaluation metrics AUC or Logloss. We only visualize the results of DeepFM, for we find that the trends for different backbone models remain consistent. The detailed experimental results for the other backbone models are listed in our released repository. In addition, it is worth noting that to make OptFS applicable in this experiment, we assign importance scores to features based on the drop rates of feature values in each feature field. From Figure 3, we can find: ¬∑ The effectiveness of features selected by different methods varies in ranking when the number of selections changes. Specifically, when the number of features selected is smaller (i.e., 5 or 10), Autofield, SHARK, and SFS perform better. They obtain information through the values or gradients of gate vectors, guiding the sorting of feature importance. They are good at selecting the most informative features from the candidate feature sets. OptFS doesn't initially perform well, as it is inherently a single-stage method. Modifying it to a two-stage method based on different feature value drop rates may not align with the goal of enhancing effectiveness through feature selection. When the number of features approaches the full set of features, the Permutation method performs well. This is because the permutation method calculates feature importance scores based on the loss of effect caused by dropping a feature from the full set. It tends to identify redundant features with less information. ¬∑ The XGBoost and GBDT methods show different trends on the Avazu and Criteo datasets. Specifically, they perform poorly on the Avazu dataset but well on the Criteo dataset. This is because the Avazu dataset's feature values are concentrated in a few feature fields, leading to an imbalance that makes XGBoost and GBDT prone to overfitting, which affects the assessment of feature importance [40]. KDD '24, August 25-29, 2024, Barcelona, Spain Pengyue Jia, et al. Table 2: Overall experimental results of feature selection for DeepFM and Wide&Deep backbones. Figure 3: Experimental results of feature selection methods with DeepFM backbone, evaluated by AUC and Logloss. 5 10 15 17 19 21 The number of selected features K 0.625 0.650 0.675 0.700 0.725 0.750 0.775 0.800 AUC 0.78576 AUC on Avazu 5 10 15 17 19 21 The number of selected features K 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44 Logloss 0.37680 Logloss on Avazu 5 10 15 20 25 303336 The number of selected features K 0.70 0.72 0.74 0.76 0.78 0.80 AUC 0.80024 AUC on Criteo 5 10 15 20 25 303336 The number of selected features K 0.45 0.46 0.47 0.48 0.49 0.50 0.51 0.52 0.53 Logloss 0.45321 Logloss on Criteo Baseline Lasso GBDT RF XGBoost AutoField OptFS LPFS Permutation SHARK SFS To offer a more direct comparison of the stability, we evaluate the selection results using the AUKC metric. Table 3 shows AUKC results on Avazu and Criteo. We can conclude: ¬∑ Overall, sensitivity-based feature selection methods outperform gate-based feature selection methods, both of which are superior to shallow feature selection methods. The possible reason is that gradient-based methods calculate feature importance based on partial batches, which helps to prevent overfitting. The inferior performance of shallow methods may be attributed to their excessive simplicity, which cannot effectively capture the complex relationships between features to determine feature importance. ¬∑ Regarding specific methods, AutoField, SHARK, and SFS exhibit the best performance, demonstrating leading effects across different datasets. It is noteworthy that AutoField significantly surpasses other gate-based feature selection methods. This may be attributed to AutoField being trained in a bi-level optimization manner, which reduces the risk of overfitting, making the selection results more stable and reliable.",
  "3.4 Efficiency Analysis (RQ3)": "In this section, we answer the RQ3 from two aspects: experiments with performance limitations and with memory limitations. ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems KDD '24, August 25-29, 2024, Barcelona, Spain Table 3: Experimental results of AUKC on Avazu and Criteo. Table 4: Experimental results with performance limitations. Table 5: Experimental results with memory limitations. 3.4.1 Experiments with performance limitations. In real-world scenarios, an important consideration is reducing the memory usage of a model as much as possible while ensuring its effectiveness. To test the memory-saving capabilities of different methods under the constraint of maintaining effectiveness, experiments were conducted on two classic DRS datasets, Avazu and Criteo, using DCN as the backbone model. Firstly, we calculate an acceptable threshold for effectiveness based on the baseline performance of the backbone model (without feature selection, using the full set of features), considering a 1% loss in AUC as acceptable. The threshold is AUC 0.77873 for Avazu and AUC 0.79245 for Criteo. Based on the feature importance ranking list obtained in the search phase by different feature selection methods, we select features from highest to lowest importance until the model's performance exceeds the threshold. We record the number of features and feature values required to reach this lower threshold of effectiveness for each feature selection method. Since most parameters in DRS models are concentrated in the embedding table, we only considered the memory usage of the embedding table when calculating the memory models required. From Table 4, we can conclude that: ¬∑ From the perspective of memory usage, methods like AutoField, LPFS, Permutation, SHARK, and SFS perform better. This indicates that even in DRS scenarios rich in high-dimensional sparse features, these methods do not rely solely on ID-type features (e.g., userid). Instead, they effectively select informative features. It's noteworthy that AdaFS shows 100% memory usage on both datasets. This is because AdaFS is an instance-level selection method (where gate weights vary with each input sample), and therefore cannot save memory usage. ¬∑ In terms of the number of features, RF, AutoField, LPFS, SHARK, and SFS only require half or fewer of the original feature set to achieve 99% of the backbone model's performance. On the other hand, GBDT and XGBoost need more features. A possible reason for this is that GBDT and XGBoost are prone to overfitting in high-dimensional sparse DRS datasets, which can affect the judgment of feature importance. ¬∑ From the perspective of the datasets, the memory-saving effectiveness of various methods is less pronounced on the Avazu dataset compared to the Criteo dataset. This is because most feature values in the Avazu dataset are concentrated in just a few feature fields, whereas in Criteo, the feature values are more evenly distributed. Therefore, when these particular feature fields are indispensable sources of information for CTR prediction, the memory-saving effect becomes less noticeable. 3.4.2 Experiments with memory limitations. Maximizing effectiveness within limited memory is also an important application scenario for feature selection. To test the capability of different methods in feature selection under memory constraints, experiments were conducted on the Criteo dataset using DCN as the backbone model. The memory limits were categorized into three levels: 25% memory, 50% memory, and 75% memory. Specifically, since the parameters in DRS are mostly concentrated in the embedding table, we used the memory usage of the full feature set's embedding table as a reference. We set thresholds at 25%, 50%, and 75% of the total memory to record the optimal performance achievable under memory constraints by different feature selection methods. Since AdaFS is incapable of performing a hard selection of partial feature fields, it KDD '24, August 25-29, 2024, Barcelona, Spain Pengyue Jia, et al. Table 6: Overall experimental results on industrial dataset Figure 4: K-performance figure on industrial dataset 17 37 57 77 97 117 137 155 The number of selected features K 0.927 0.928 0.929 0.930 0.931 0.932 0.933 0.934 0.935 AUC 0.93392 Baseline AutoField SHARK Permutation LPFS SFS is not included in this experiment. Based on the results presented in Table 5, we can draw the following conclusion: ¬∑ Overall, gate-based feature selection and sensitivity-based feature selection methods perform better. Even when restricted to using only 25% of the available memory, they can achieve results close to those obtained using the full set of features. This is because these methods rely on more powerful deep learning networks that can better model feature importance and are not dependent on high-dimensional sparse ID-type features in DRS. ¬∑ Specifically, AutoField achieves the best results because its designed controller effectively learns the importance of each field for the prediction outcome. The DARTS (Differentiable Architecture Search) [32] parameter updating manner made the learning of the gate vector more robust and capable of generalization. In contrast, Random Forest (RF) performs the worst. This is because, in the context of high-dimensional sparse DRS, RF tends to assign high weights to ID-type features with numerous feature values. Such feature fields offer more opportunities for splitting, which facilitates the training of RF. However, this approach increases the risk of model overfitting. This tendency of RF to favor features with many splitting points can lead to a bias towards complex, less generalizable models that don't necessarily capture the most predictive or relevant features for the task at hand.",
  "3.5 Offline Results on Industrial Dataset (RQ4)": "To find whether our conclusions for feature selection methods align between large-scale industrial datasets and public datasets, we conduct comparison experiments on large-scale industrial datasets. 3.5.1 Industrial Dataset. The dataset is constructed with sampled CVR records from 2023-11-29 to 2023-11-30 containing approximately seven hundred million samples. The feature used in this offline dataset is aligned with the online service model, which contains over 157 features. All of these features have been roughly proved useful in offline leave-one-out experiments. 3.5.2 Experiment. Considering the industry dataset is relatively huge, we only select the top 5 elite methods for comparison according to their performance on public datasets, including two gate-based methods (AutoField and LPFS) and three sensitivitybased methods (SHARK, Permutation, and SFS). The feature preprocessing is slightly different from the public dataset. Specifically, we utilize AutoDis [15] technique to directly convert dense features into embeddings with the same size as sparse features. As for multi-hot features, we adopt the reduce-mean operator to aggregate multiple embeddings into one feature embedding. Finally, the preprocessed features are fed into the FibiNet backbone. After training, we grab each method's feature importance and perform top-k experiments. The experiment result is demonstrated in Table 6 and Figure 4. From the redundant feature elimination perspective, SHARK, AutoField, and Permutation perform the best. They eliminate about half of the feature sets (80 features) without decreasing model accuracy. However, from an effective feature mining perspective, AutoField performs best for small feature sets, aligning with our observations from the public datasets. We can also find that AutoField achieves the highest AUKC in Table 6, demonstrating its robustness and stability across different numbers of selections. LPFS's performance is slightly misaligned with the one on the public dataset. We attribute this misalignment to the sensitive learnable polarization module.",
  "3.6 Online Experiments (RQ5)": "To validate the effectiveness of our benchmark in online environments, we reconfigure partial benchmarks to adapt to our feature factory, package them into a new toolkit, and conduct online tasks with this toolkit for redundant feature elimination tasks. Specifically, we create an experiment group that reduces 30% features with our toolkit in a latency-sensitive online recommendation platform. Then, we deploy the experimental group for the A/B test with 5% traffic (approximately 1 million users). After one week's observation, the inference latency was reduced by approximately 20% while the number of video views (vv) and average play duration remained the same. This experiment proves the effectiveness of our benchmark toolkits in the online environment.",
  "3.7 Similarity Visualization (RQ6)": "In this section, we visualize the similarity between feature rankings of different feature selection methods in DRS across 4 public datasets. Figure 5 shows the heatmap of pair-wise Spearman correlation similarities. In the heatmap, the x-axis and y-axis represent different feature selection methods. The color of each square indicates the level of similarity. The more the color leans towards dark green, the more similar between the feature rankings derived from the two feature selection methods. Conversely, the more the color leans towards light green, the greater the difference in the feature rankings selected by the two methods. We can discern the following information from the heatmap: 1) The heatmap displays three darker clusters in the upper left, middle, and lower right. This is because the features selected by ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems KDD '24, August 25-29, 2024, Barcelona, Spain Figure 5: Similarity between feature selection methods. Lasso GBDT RF XGBoost AutoField OptFS LPFS Permutation SHARK SFS Lasso GBDT RF XGBoost AutoField OptFS LPFS Permutation SHARK SFS Pair-wise Spearman Correlation Heatmap -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 the three different category methods are more similar within each category. 2) The rankings of LPFS and OptFS are highly similar. This is because they are both gate-based methods and use the L0 regularization term to control the sparsity of the gates. 3) The rankings of AutoField and Permutation are the most dissimilar. This aligns with our conclusion in Section 3.3. AutoField, as a gate-based method, determines feature importance by learning the gate weights of different feature fields during the model's training process, aiding in uncovering features rich in information. On the other hand, Permutation, as a sensitivity-based feature selection method, judges feature importance based on the loss of effect caused by shuffling a particular feature. This approach is more conducive to identifying features with less information. Therefore, their rankings show significant differences. 4) The rankings of SHARK and SFS are quite similar because they both determine feature importance based on the gradient of the features, sharing the same source of information.",
  "4 RELATED WORKS": "Feature Selection Methods. Machine learning has shown superior results in data mining [21-25, 28, 47]. The modeling performance relies heavily on input features, making feature selection a crucial part of feature engineering. Traditional feature selection methods fall into three categories: filter, wrapper, and embedded methods [18]. Filter methods utilize criteria to identify predictive feature fields, exemplified by the Chi2 score [31] and mutual information [42]. Wrapper methods employ black-box models to select predictive features, assessing the utility of feature subsets through, often, generic algorithms [14, 39]. Embedded methods, on the other hand, integrate the feature selection process within the prediction model, thereby evaluating the effectiveness of feature subsets in conjunction. Notable embedded methods include LASSO [41] and Gradient Boosting Machine [13]. The advent of deep learning has spurred novel approaches [5, 27, 30, 33, 48, 49, 51], such as gatebased methods, where learnable gates determine the significance of feature fields [26, 29, 35, 45], and sensitivity-based methods, leveraging gradient techniques to identify critical features by their sensitivity [11, 44, 46]. Furthermore, some studies explored reinforcement learning to automate feature selection, using agents to pinpoint predictive features [10, 34, 50]. Considering their appliability to DRS, our benchmark focus on traditional embedded methods (termed 'shallow methods'), alongside gate-based and sensitivity-based methods. Other techniques often fall short in terms of effectiveness or efficiency within the DRS context. Specifically, filter methods might overlook the DRS model's nuances by relying solely on data-intrinsic relationships, leading to suboptimal outcomes. Besides, wrapper and reinforcement learning methods, typically designed for smaller datasets, become impractically time-consuming for the expansive datasets characteristic of DRS, which can encompass millions of samples. Benchmarks for Feature Selection. Most existing benchmarks are tailored for classical downstream models [2, 3] and rely on synthetic or domain-specific datasets [1, 9, 12]. Due to their constrained relevance, these benchmarks frequently fall short in providing the necessary guidance to propel research forward in the domain of DRS feature selection. The most related work, DeepLasso [8], confines its evaluation to shallow methods and a narrow set of backbone models, primarily targeting tabular learning tasks. Despite utilizing real-world datasets, the scale of these datasets pales in comparison to those encountered in DRS scenarios, thus limiting the provision of actionable insights for DRS feature selection strategies. ERASE undertakes a thorough evaluation of shallow, gate-based, and sensitivity-based methods across an array of representative DRS backbones, leveraging both large-scale public datasets and private industrial datasets. Furthermore, ERASE proposes a novel metric designed to assess feature selection efficacy comprehensively.",
  "5 CONCLUSION": "In this study, we introduce ERASE , a comprehensive b E nchma R k for fe A ture SE lection for deep recommender systems (DRS). ERASE integrates a broad spectrum of feature selection methods pertinent to DRS and ensures fair and comprehensive experimentation across four public datasets as well as real-world industrial datasets. Furthermore, ERASE pioneers the adoption of the AUKC metric, devised to address the shortcomings of existing metrics by offering a thorough evaluation of the robustness and stability of feature selection methods over various numbers of selected features. The empirical validation provided by both offline and online analyses on industrial datasets underscores the applicability of our findings in practical settings, offering valuable perspectives on the deployment of feature selection methods within DRS environments.",
  "6 ACKNOWLEDGMENTS": "This research was partially supported by Huawei (Huawei Innovation Research Program), Research Impact Fund (No.R1015-23), APRC - CityU New Research Initiatives (No.9610565, Start-up Grant for New Faculty of CityU), CityU - HKIDS Early Career Research Grant (No.9360163), Hong Kong ITC Innovation and Technology Fund Midstream Research Programme for Universities Project (No. ITS/034/22MS), Hong Kong Environmental and Conservation Fund (No. 88/2022), and SIRG - CityU Strategic Interdisciplinary Research Grant (No.7020046, No.7020074). KDD '24, August 25-29, 2024, Barcelona, Spain Pengyue Jia, et al.",
  "REFERENCES": "[1] Ver√≥nica Bol√≥n-Canedo, Noelia S√°nchez-Maro√±o, and Amparo Alonso-Betanzos. 2013. A review of feature selection methods on synthetic data. Knowledge and information systems 34 (2013), 483-519. [2] Ver√≥nica Bol√≥n-Canedo, Noelia S√°nchez-Marono, Amparo Alonso-Betanzos, Jos√© Manuel Ben√≠tez, and Francisco Herrera. 2014. A review of microarray datasets and applied feature selection methods. Information sciences 282 (2014). [3] Andrea Bommert, Xudong Sun, Bernd Bischl, J√∂rg Rahnenf√ºhrer, and Michel Lang. 2020. Benchmark for filter methods for feature selection in high-dimensional classification data. Computational Statistics & Data Analysis 143 (2020), 106839. [4] Leo Breiman. 2001. Random forests. Machine learning 45 (2001), 5-32. [5] Bo Chen, Xiangyu Zhao, Yejing Wang, Wenqi Fan, Huifeng Guo, and Ruiming Tang. 2024. A comprehensive survey on automated machine learning for recommendations. ACM Transactions on Recommender Systems 2, 2 (2024), 1-38. [6] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining . 785-794. [7] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [8] Valeriia Cherepanova, Roman Levin, Gowthami Somepalli, Jonas Geiping, C. Bruss, Andrew Wilson, Tom Goldstein, and Micah Goldblum. 2023. A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning. In NeurIPS 2023 Second Table Representation Learning Workshop . [9] SL Shiva Darshan and CD Jaidhar. 2018. Performance evaluation of filter-based feature selection techniques in classifying portable executable files. Procedia Computer Science 125 (2018), 346-356. [10] Wei Fan, Kunpeng Liu, Hao Liu, Pengyang Wang, Yong Ge, and Yanjie Fu. 2020. AutoFS: Automated Feature selection via diversity-aware interactive reinforcement learning. In 2020 IEEE International Conference on Data Mining (ICDM) . IEEE, 1008-1013. [11] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2019. All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously. Journal of machine learning research: JMLR 20 (2019). [12] George Forman. 2003. An Extensive Empirical Study of Feature Selection Metrics for Text Classification. Journal of Machine Learning Research 3 (2003), 1289-1305. [13] Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting machine. Annals of statistics (2001), 1189-1232. [14] Pablo M Granitto, Cesare Furlanello, Franco Biasioli, and Flavia Gasperi. 2006. Recursive feature elimination with random forest for PTR-MS analysis of agroindustrial products. Chemometrics and intelligent laboratory systems (2006). [15] Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and Xiuqiang He. 2021. An embedding learning framework for numerical features in ctr prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 2910-2918. [16] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [17] Yi Guo, Zhaocheng Liu, Jianchao Tan, Chao Liao, Daqing Chang, Qiang Liu, Sen Yang, Ji Liu, Dongying Kong, Zhi Chen, et al. 2022. LPFS: Learnable Polarizing Feature Selection for Click-Through Rate Prediction. arXiv preprint arXiv:2206.00267 (2022). [18] Isabelle Guyon and Andr√© Elisseeff. 2003. An introduction to variable and feature selection. Journal of machine learning research 3, Mar (2003), 1157-1182. [19] Jeff Heaton. 2016. An empirical analysis of feature engineering for predictive modeling. In SoutheastCon 2016 . IEEE, 1-6. [20] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . 169-177. [21] Pengyue Jia, Ling Chen, and Dandan Lyu. 2024. Fine-Grained Population Mobility Data-Based Community-Level COVID-19 Prediction Model. Cybernetics and Systems 55, 1 (2024), 184-202. [22] Pengyue Jia, Yiding Liu, Xiaopeng Li, Xiangyu Zhao, Yuhao Wang, Yantong Du, Xiao Han, Xuetao Wei, Shuaiqiang Wang, and Dawei Yin. 2024. G3: An Effective and Adaptive Framework for Worldwide Geolocalization Using Large Multi-Modality Models. arXiv preprint arXiv:2405.14702 (2024). [23] Pengyue Jia, Yiding Liu, Xiangyu Zhao, Xiaopeng Li, Changying Hao, Shuaiqiang Wang, and Dawei Yin. 2023. Mill: Mutual verification with large language models for zero-shot query expansion. arXiv preprint arXiv:2310.19056 (2023). [24] Pengyue Jia, Yichao Wang, Shanru Lin, Xiaopeng Li, Xiangyu Zhao, Huifeng Guo, and Ruiming Tang. 2024. D3: A Methodological Exploration of Domain Division, Modeling, and Balance in Multi-Domain Recommendations. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 8553-8561. [25] Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition . 3128-3137. [26] Youngjune Lee, Yeongjong Jeong, Keunchan Park, and SeongKu Kang. 2023. MvFS: Multi-view Feature Selection for Recommender System. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . [27] Muyang Li, Zijian Zhang, Xiangyu Zhao, Wanyu Wang, Minghao Zhao, Runze Wu, and Ruocheng Guo. 2023. Automlp: Automated mlp for sequential recommendations. In Proceedings of the ACM Web Conference 2023 . 1190-1198. [28] Xiaopeng Li, Lixin Su, Pengyue Jia, Xiangyu Zhao, Suqi Cheng, Junfeng Wang, and Dawei Yin. 2023. Agent4Ranking: Semantic Robust Ranking via Personalized Query Rewriting Using Multi-agent LLM. arXiv preprint arXiv:2312.15450 (2023). [29] Weilin Lin, Xiangyu Zhao, Yejing Wang, Tong Xu, and Xian Wu. 2022. AdaFS: Adaptive Feature Selection in Deep Recommender System. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) . [30] Dugang Liu, Chaohua Yang, Xing Tang, Yejing Wang, Fuyuan Lyu, Weihong Luo, Xiuqiang He, Zhong Ming, and Xiangyu Zhao. 2024. MultiFS: Automated MultiScenario Feature Selection in Deep Recommender Systems. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining . 434-442. [31] Huan Liu and Rudy Setiono. 1995. Chi2: Feature selection and discretization of numeric attributes. In Proceedings of 7th IEEE international conference on tools with artificial intelligence . IEEE, 388-391. [32] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. DARTS: Differentiable Architecture Search. In International Conference on Learning Representations . [33] Haochen Liu, Xiangyu Zhao, Chong Wang, Xiaobing Liu, and Jiliang Tang. 2020. Automated embedding size search in deep recommender systems. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2307-2316. [34] Kunpeng Liu, Yanjie Fu, Pengfei Wang, Le Wu, Rui Bo, and Xiaolin Li. 2019. Automating feature subspace exploration via multi-agent reinforcement learning. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 207-215. [35] Fuyuan Lyu, Xing Tang, Dugang Liu, Liang Chen, Xiuqiang He, and Xue Liu. 2023. Optimizing Feature Set for Click-Through Rate Prediction. In Proceedings of the ACM Web Conference 2023 . 3386-3395. [36] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 1137-1140. [37] Steffen Rendle. 2010. Factorization Machines. In Proceedings of the 2010 IEEE International Conference on Data Mining . 995-1000. [38] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence . 452-461. [39] Shital C Shah and Andrew Kusiak. 2004. Data mining and genetic algorithm based gene/SNP selection. Artificial intelligence in medicine 31, 3 (2004), 183-196. [40] Carolin Strobl, Anne-Laure Boulesteix, Achim Zeileis, and Torsten Hothorn. 2007. Bias in random forest variable importance measures: Illustrations, sources and a solution. BMC bioinformatics 8, 1 (2007), 1-21. [41] Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological) (1996). [42] Jorge R Vergara and Pablo A Est√©vez. 2014. A review of feature selection methods based on mutual information. Neural computing and applications 24 (2014). [43] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [44] Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Bo Chen, Huifeng Guo, Ruiming Tang, and Zhenhua Dong. 2023. Single-shot Feature Selection for Multi-task Recommendations. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 341-351. [45] Yejing Wang, Xiangyu Zhao, Tong Xu, and Xian Wu. 2022. AutoField: Automating Feature Selection in Deep Recommender Systems. In Proceedings of the ACM Web Conference . [46] Beichuan Zhang, Chenggen Sun, Jianchao Tan, Xinjun Cai, Jun Zhao, Mengqi Miao, Kang Yin, Chengru Song, Na Mou, and Yang Song. 2023. SHARK: A Lightweight Model Compression Approach for Large-Scale Recommender Systems. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23) . [47] Zijian Zhang, Shuchang Liu, Jiaao Yu, Qingpeng Cai, Xiangyu Zhao, Chunxu Zhang, Ziru Liu, Qidong Liu, Hongwei Zhao, Lantao Hu, et al. 2024. M3oE: MultiDomain Multi-Task Mixture-of Experts Recommendation Framework. arXiv preprint arXiv:2404.18465 (2024). [48] Xiangyu Zhao, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, and Chong Wang. 2021. Autoloss: Automated loss function search in recommendations. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . [49] Xiangyu Zhao, Haochen Liu, Hui Liu, Jiliang Tang, Weiwei Guo, Jun Shi, Sida Wang, Huiji Gao, and Bo Long. 2021. Autodim: Field-aware embedding dimension searchin recommender systems. In Proceedings of the Web Conference 2021 . [50] Xiaosa Zhao, Kunpeng Liu, Wei Fan, Lu Jiang, Xiaowei Zhao, Minghao Yin, and Yanjie Fu. 2020. Simplifying reinforced feature selection via restructured choice strategy of single agent. In 2020 IEEE International Conference on Data Mining ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems KDD '24, August 25-29, 2024, Barcelona, Spain (ICDM) . IEEE, 871-880. [51] Xiangyu Zhaok, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, Chong Wang, Ming Chen, Xudong Zheng, Xiaobing Liu, and Xiwang Yang. 2021. Autoemb: Automated embedding dimensionality search in streaming recommendations. In 2021 IEEE International Conference on Data Mining (ICDM) . IEEE, 896-905. [52] Ruiqi Zheng, Liang Qu, Bin Cui, Yuhui Shi, and Hongzhi Yin. 2023. Automl for deep recommender systems: A survey. ACM Transactions on Information Systems 41, 4 (2023), 1-38. Table 7: Dataset statistics",
  "A DATASETS": "In this subsection, we introduce the datasets utilized in our benchmark. The statistics of these datasets are listed in Table 7. We select the following four datasets for use: ¬∑ Avazu. Avazu is a commonly used dataset in the field of feature selection for deep recommendation systems. It consists of 23 features and 40,428,967 interaction records. We divide the dataset into training, validation, and test sets in a 7:2:1 ratio. ¬∑ Criteo. Criteo is another frequently used dataset for studying feature selection in Deep Recommendation Systems. The Criteo dataset contains 45,850,617 samples and 39 features, offering a larger number of features. We also adopt a 7:2:1 ratio for dividing the data into training, validation, and test sets to train the model. ¬∑ Movielens-1m. The Movielens dataset is a well-known public movie dataset in the field of recommendation systems, featuring multiple variants with different data volumes to meet diverse research needs. We choose the Movielens-1M dataset, which comprises 9 features. The limited number of features presents a greater challenge for feature selection methods. In our experiments with Movielens, we also divide the dataset into training, validation, and test sets according to a 7:2:1 ratio. The interactions with a rating bigger than 3 are considered as positive samples. ¬∑ AliCCP. Alibaba Click and Conversion Prediction (AliCCP) dataset is extracted from the real-world e-commerce platform Taobao. It is a popular dataset used for Click-Through Rate (CTR) estimation. It consists of 23 features and 85,316,519 samples, making it an effective tool for evaluating feature selection methods in scenarios closely resembling real advertising contexts. We follow the original AliCCP splitting approach [36], allocating 50% of the data for training, with the remaining data split equally into validation and test sets in a 1:1 ratio.",
  "B BACKBONE MODELS": "we apply a variety of feature selection methods to the following four popular deep recommendation models in this work. ¬∑ Wide&Deep [7]. Wide and Deep (Wide&Deep) model is developed by Google to improve the recommender systems. It combines wide and deep models to fit specific feature combinations in large-scale datasets and previously unseen feature combinations through low-dimensional dense embeddings. ¬∑ DeepFM [16]. DeepFM is an effective model in recommender systems. It combines the strengths of FM models and deep neural networks to extract both low-order feature interactions and highorder feature interactions. ¬∑ DCN[43]. The Deep & Cross Network (DCN) model is proposed by Google to capture both explicit and implicit feature interactions for prediction tasks. The cross layers have cross operations that learn complex feature interactions to improve the prediction performance. ¬∑ FibiNet [20]. The Feature Importance and Bilinear feature Interaction NETwork (FibiNet) is an innovative model in CTR prediction. It introduces the SENET and bilinear layer. SENET learns feature importance adaptively and the bilinear layer captures high-order feature interactions.",
  "C METRICS": "In this section, we detail the two frequently used metrics in recommender systems: AUC and Logloss. 1) AUC. Area Under the ROC Curve (AUC) measures the area under the ROC curve. The ROC curve is one graph shows that the classification performance of the model under different thresholds. In addition, in random positive-negative pairs, AUC also represents the probability that the model ranks the positive sample with a higher score than the negative one. It can be formalized as follows:  where ùëÄ denotes the number of positive samples and ùëÅ is the number of negative samples. 2) Logloss. Logloss is also named Binary Cross-Entropy Loss, it is always used in binary classification tasks. The formula for Logloss is described as follows:  where ùëÅ is the number of samples, ùë¶ ùëñ denotes the predicted label for ùëñ th sample, and ÀÜ ùë¶ ùëñ denotes the ground truth for ùëñ th sample.",
  "D FORMULA DERIVATION OF AUKC": "In this section, we detail the formula derivation process of AUKC. AUKC is a comprehensive metric for assessing the AUC performance of a given feature selection method across different numbers of feature selections. Under ideal circumstances, experiments can be conducted for each number of selections to obtain the corresponding AUC metrics, and then derive AUKC:   where | ùêæ | denotes the number of all features, ùê¥ùëàùê∂ ùëò is the AUC metric with ùëò features selected. When there is no feature selected, the model will predict labels randomly (i.e., ùê¥ùëàùê∂ = 0 . 5). Therefore, we subtract 0.5 from ùê¥ùëàùê∂ ùëò to obtain the improvement. KDD '24, August 25-29, 2024, Barcelona, Spain Pengyue Jia, et al. Table 8: Overall experimental results of feature selection for deep recommender systems. When the number of selected features is unevenly distributed across the length of the set of features (e.g., in practice, the granularity of the feature selection number often becomes finer as it approaches the total number of features, and coarser when there are fewer features), AUKC has a more general format:   where | ùëÅ | represents the number of segments for the number of features selected across the entire length of the feature set. For example, with 6 features, if experiments are conducted at feature counts of 2,4,5, and 6, then | ùëÅ | = 4. ùê¥ùëàùê∂ ùëõ,ùëô is the AUC corresponding to the number of features selected at the left endpoint of the ùëõ th segment, and ùê¥ùëàùê∂ ùëõ,ùëü denotes the AUC with the number of selections at the right endpoint of the ùëõ th segment. Œî ùëô ùëõ represents the length of the ùëõ th segment.",
  "E COMPLETE OVERALL PERFORMANCE": "Due to page limitations, we can not include the complete overall experimental results in the main content. We list the complete results in Table 8 for reference.",
  "keywords_parsed": [
    "Benchmark",
    "Feature Selection",
    "Deep Recommender System"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "A review of feature selection methods on synthetic data"
    },
    {
      "ref_id": "b2",
      "title": "A review of microarray datasets and applied feature selection methods"
    },
    {
      "ref_id": "b3",
      "title": "Benchmark for filter methods for feature selection in high-dimensional classification data"
    },
    {
      "ref_id": "b4",
      "title": "Random forests"
    },
    {
      "ref_id": "b5",
      "title": "A comprehensive survey on automated machine learning for recommendations"
    },
    {
      "ref_id": "b6",
      "title": "Xgboost: A scalable tree boosting system"
    },
    {
      "ref_id": "b7",
      "title": "Wide & deep learning for recommender systems"
    },
    {
      "ref_id": "b8",
      "title": "A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning"
    },
    {
      "ref_id": "b9",
      "title": "Performance evaluation of filter-based feature selection techniques in classifying portable executable files"
    },
    {
      "ref_id": "b10",
      "title": "AutoFS: Automated Feature selection via diversity-aware interactive reinforcement learning"
    },
    {
      "ref_id": "b11",
      "title": "All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously"
    },
    {
      "ref_id": "b12",
      "title": "An Extensive Empirical Study of Feature Selection Metrics for Text Classification"
    },
    {
      "ref_id": "b13",
      "title": "Greedy function approximation: a gradient boosting machine"
    },
    {
      "ref_id": "b14",
      "title": "Recursive feature elimination with random forest for PTR-MS analysis of agroindustrial products"
    },
    {
      "ref_id": "b15",
      "title": "An embedding learning framework for numerical features in ctr prediction"
    },
    {
      "ref_id": "b16",
      "title": "DeepFM: a factorization-machine based neural network for CTR prediction"
    },
    {
      "ref_id": "b17",
      "title": "LPFS: Learnable Polarizing Feature Selection for Click-Through Rate Prediction"
    },
    {
      "ref_id": "b18",
      "title": "An introduction to variable and feature selection"
    },
    {
      "ref_id": "b19",
      "title": "An empirical analysis of feature engineering for predictive modeling"
    },
    {
      "ref_id": "b20",
      "title": "FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction"
    },
    {
      "ref_id": "b21",
      "title": "Fine-Grained Population Mobility Data-Based Community-Level COVID-19 Prediction Model"
    },
    {
      "ref_id": "b22",
      "title": "G3: An Effective and Adaptive Framework for Worldwide Geolocalization Using Large Multi-Modality Models"
    },
    {
      "ref_id": "b23",
      "title": "Mill: Mutual verification with large language models for zero-shot query expansion"
    },
    {
      "ref_id": "b24",
      "title": "D3: A Methodological Exploration of Domain Division, Modeling, and Balance in Multi-Domain Recommendations"
    },
    {
      "ref_id": "b25",
      "title": "Deep visual-semantic alignments for generating image descriptions"
    },
    {
      "ref_id": "b26",
      "title": "MvFS: Multi-view Feature Selection for Recommender System"
    },
    {
      "ref_id": "b27",
      "title": "Automlp: Automated mlp for sequential recommendations"
    },
    {
      "ref_id": "b28",
      "title": "Agent4Ranking: Semantic Robust Ranking via Personalized Query Rewriting Using Multi-agent LLM"
    },
    {
      "ref_id": "b29",
      "title": "AdaFS: Adaptive Feature Selection in Deep Recommender System"
    },
    {
      "ref_id": "b30",
      "title": "MultiFS: Automated MultiScenario Feature Selection in Deep Recommender Systems"
    },
    {
      "ref_id": "b31",
      "title": "Chi2: Feature selection and discretization of numeric attributes"
    },
    {
      "ref_id": "b32",
      "title": "DARTS: Differentiable Architecture Search"
    },
    {
      "ref_id": "b33",
      "title": "Automated embedding size search in deep recommender systems"
    },
    {
      "ref_id": "b34",
      "title": "Automating feature subspace exploration via multi-agent reinforcement learning"
    },
    {
      "ref_id": "b35",
      "title": "Optimizing Feature Set for Click-Through Rate Prediction"
    },
    {
      "ref_id": "b36",
      "title": "Entire space multi-task model: An effective approach for estimating post-click conversion rate"
    },
    {
      "ref_id": "b37",
      "title": "Factorization Machines"
    },
    {
      "ref_id": "b38",
      "title": "BPR: Bayesian personalized ranking from implicit feedback"
    },
    {
      "ref_id": "b39",
      "title": "Data mining and genetic algorithm based gene/SNP selection"
    },
    {
      "ref_id": "b40",
      "title": "Bias in random forest variable importance measures: Illustrations, sources and a solution"
    },
    {
      "ref_id": "b41",
      "title": "Regression shrinkage and selection via the lasso"
    },
    {
      "ref_id": "b42",
      "title": "A review of feature selection methods based on mutual information"
    },
    {
      "ref_id": "b43",
      "title": "Deep & cross network for ad click predictions"
    },
    {
      "ref_id": "b44",
      "title": "Single-shot Feature Selection for Multi-task Recommendations"
    },
    {
      "ref_id": "b45",
      "title": "AutoField: Automating Feature Selection in Deep Recommender Systems"
    },
    {
      "ref_id": "b46",
      "title": "SHARK: A Lightweight Model Compression Approach for Large-Scale Recommender Systems"
    },
    {
      "ref_id": "b47",
      "title": "M3oE: MultiDomain Multi-Task Mixture-of Experts Recommendation Framework"
    },
    {
      "ref_id": "b48",
      "title": "Autoloss: Automated loss function search in recommendations"
    },
    {
      "ref_id": "b49",
      "title": "Autodim: Field-aware embedding dimension search in recommender systems"
    },
    {
      "ref_id": "b50",
      "title": "Simplifying reinforced feature selection via restructured choice strategy of single agent"
    },
    {
      "ref_id": "b51",
      "title": "Autoemb: Automated embedding dimensionality search in streaming recommendations"
    },
    {
      "ref_id": "b52",
      "title": "Automl for deep recommender systems: A survey"
    }
  ]
}