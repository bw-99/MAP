{"Scaling Sequential Recommendation Models with Transformers": "Pablo Zivic Mercado Libre Inc. Buenos Aires, Argentina pablo.rzivic@mercadolibre.com Hernan Vazquez Mercado Libre Inc. Buenos Aires, Argentina hernan.vazquez@mercadolibre.com Jorge S\u00e1nchez Mercado Libre Inc. C\u00f3rdoba, Argentina jorge.sanchez@mercadolibre.com", "ABSTRACT": "", "CCS CONCEPTS": "Modeling user preferences has been mainly addressed by looking at users' interaction history with the different elements available in the system. Tailoring content to individual preferences based on historical data is the main goal of sequential recommendation. The nature of the problem, as well as the good performance observed across various domains, has motivated the use of the transformer architecture, which has proven effective in leveraging increasingly larger amounts of training data when accompanied by an increase in the number of model parameters. This scaling behavior has brought a great deal of attention, as it provides valuable guidance in the design and training of even larger models. Taking inspiration from the scaling laws observed in training large language models, we explore similar principles for sequential recommendation. Addressing scalability in this context requires special considerations as some particularities of the problem depart from the language modeling case. These particularities originate in the nature of the content catalogs, which are significantly larger than the vocabularies used for language and might change over time. In our case, we start from a well-known transformer-based model from the literature and make two crucial modifications. First, we pivot from the traditional representation of catalog items as trainable embeddings to representations computed with a trainable feature extractor, making the parameter count independent of the number of items in the catalog. Second, we propose a contrastive learning formulation that provides us with a better representation of the catalog diversity. We demonstrate that, under this setting, we can train our models effectively on increasingly larger datasets under a common experimental setup. We use the full Amazon Product Data dataset, which has only been partially explored in other studies, and reveal scaling behaviors similar to those found in language models. Computeoptimal training is possible but requires a careful analysis of the compute-performance trade-offs specific to the application. We also show that performance scaling translates to downstream tasks by fine-tuning larger pre-trained models on smaller task-specific domains. Our approach and findings provide a strategic roadmap for model training and deployment in real high-dimensional preference spaces, facilitating better training and inference efficiency. We hope this paper bridges the gap between the potential of transformers and the intrinsic complexities of high-dimensional sequential recommendation in real-world recommender systems. Code and models can be found at https://github.com/mercadolibre/srt. This work is licensed under a Creative Commons Attribution International 4.0 License. SIGIR '24, July 14-18, 2024, Washington, DC, USA \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0431-4/24/07 https://doi.org/10.1145/3626772.3657816 \u00b7 Information systems \u2192 Personalization ; Recommender systems ; Personalization ;", "KEYWORDS": "Sequential Recommendation, Scaling Laws, Transformers, Transfer Learning", "ACMReference Format:": "Pablo Zivic, Hernan Vazquez, and Jorge S\u00e1nchez. 2024. Scaling Sequential Recommendation Models with Transformers. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24), July 14-18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3626772.3657816", "1 INTRODUCTION": "A recommendation system aims to provide users with content that fits their preferences and interests. Classical methods have explored building static models based on past user interactions to predict future ones. Pattern mining and factorization-based methods are two classical methodologies that stand as the most popular in the literature [10, 47]. These models seek to capture static preferences in the interaction of users with items in a catalog. While the formulation largely simplifies the modeling of otherwise complex interaction patterns observed in the real world, the main drawback of these models relies on the assumption of static behavior patterns. In reality, user preferences are subject to a series of short- and long-term factors that are very hard to disentangle [4, 41]. From a modeling perspective, user preference dynamics can be seen as latent factors that govern the observed users' behavior as they interact with the system. These interactions are diverse and depend on the nature of the actual system. For instance, the types of events that can be registered in a music streaming platform differ from those observed in an e-commerce website. Despite the complexity of the task, the driving hypothesis of modern recommendation systems is that such behavioral patterns can be captured by models that can predict future interactions from historical sequential records. The nature and complexity of such models have been influenced to a great extent by the success of different machine-learning models in different fields, especially those from the natural language literature. We can find solutions based on simple Recurrent Neural Networks [16], convolutional architectures [51, 52], based on Attention mechanisms [29, 63] and, more recently, the Transformer [22, 48, 54]. Among them, transformer-based solutions are the most promising. This is not only due to the success of this architecture in fields beyond language modeling, such as computer vision [24], speech [27], and time-series forecasting [32], but also to their flexibility and good scaling behavior. Another important factor of the transformer architecture that led to its adoption as the model of choice in many applications is the availability of a pre-fitted version that can be adapted easily to more specific tasks using a fraction of the data required to train a similar model from scratch. This pre-training and fine-tuning strategy has not yet been widely adopted in the sequential recommendation literature. We believe that this obeys two main reasons: first, the data used to train recommendation models is specific to each application domain, i.e. the nature of the catalog and type of events are problem-specific, making it challenging to leverage prefitted models on domains that might be closely related but not the same as those on which the model has been trained (e.g. a model pre-trained on Amazon data being fine-tuned to a different catalog or e-commerce domain); second, there are particularities in the sequential recommendation problem that constraint the design of solutions that scale. While predicting the next token in a sentence and the next item in an interaction sequence share structural similarities, the sequential recommendation problem introduces some particularities that need special attention. For instance, in language modeling, it is common to cast the prediction task as a classification problem over a large set of tokens (subdivisions of words into finer sub-word units). Although large, the size of this vocabulary remains constrained to a manageable number (around 30K in most practical applications) that does not change over time. On the contrary, most real recommendation applications involve item sets (space of possible user preferences) that expand to massive scales, often reaching into the millions or even billions of different items [7]. Moreover, such collections may change over time as items are constantly added and removed from the catalog. These characteristics impose design constraints that must be satisfied if we are willing to take advantage of the flexibility and ease of adaptation observed by these architectures in other domains. Perhaps the most intriguing characteristic of these models relies on their ability to leverage increasingly large amounts of data by simply growing the number of parameters accordingly. From a system design perspective, this poses new challenges around how to scale the amount of data and compute required by these models to leverage their full potential. From a practical perspective, this choice is constrained not only by the desire to get the best possible performance but also to achieve such performance within the limits of a given computational budget. In light of recent discoveries regarding scaling laws in the language [1, 23] and other domains [15, 43, 62], recent research has provided new insights into how model performance scales with the number of parameters, the size of the datasets used for training, and the required computational budget [2, 17]. In this work, we explore the hypothesis that transformer-based sequential recommendation models exhibit scaling behaviors similar to those observed in other domains. Under such a hypothesis, we investigate how, within a given computational budget, optimizing the balance between model size and data size can yield improved results. To do so, we propose a generic yet scalable model that takes inspiration from other transformer-based models from the literature but lets us experiment with problems and models of different complexity. We run experiments on the full version of the widely used Amazon Product Data (APD) dataset [37]. Our findings confirm our hypothesis, and we show how such scaling behavior can be used in practice by training larger models that, when fine-tuned, achieve a performance that surpasses more complex approaches from the literature. Our main contributions are the following: \u00b7 We propose a generic transformer-based architecture that is both flexible and scalable. \u00b7 We show scaling laws similar to those observed in language modeling tasks. \u00b7 We show that it is possible to pre-train recommendation models at scale and fine-tune them to particular downstream tasks, improving performance w.r.t to similar models trained from scratch. The paper is organized as follows: Sec. 2 discusses some key aspects of sequential recommendation models in the context of scalability, Sec. 3 proposes a formulation that makes the model independent of the size of the catalog, Sec. 4 shows experimental results. In Sec. 4.4 we derive analytical laws that relate the target metric with the most relevant quantities of interest from a scaling perspective. In Sec. 4.5 we show we can use the pre-training and finetuning strategy for improving recommendations. Sec. 5 discusses related work. Finally, in Sec. 6 we draw some conclusions.", "2 SCALABILITY OF SEQUENTIAL RECOMMENDATION MODELS": "Sequential recommendation models seek to capture user interaction patterns and a possibly large collection of available items in a catalog. Users may perform many interaction types depending on the nature of such elements: an e-commerce site, a music streaming service, a social network, and others. For instance, users might play a song, skip it, or add it to a playlist in a music recommendation context, while they can add an article to a shopping cart, buy it, add it to a wish list, etc. Although such heterogeneity in the type of interactions can be handled accordingly [21, 38, 58], for the purpose of this study, we subsume all domain-specific cases into a more generic \"user-item\" interaction (i.e. a user interacted with an item in some way). With this in mind, let \ud835\udc48 denote the user base of a given platform and \ud835\udc3c the collection of items they can interact with. The behavior and preference dynamics of a user \ud835\udc62 \u2208 \ud835\udc48 can be considered as embedded into the sequence of items they interacted with for a given period. Let \ud835\udc46 \ud835\udc62 = { \ud835\udc56 \ud835\udc62 1 , \ud835\udc56 \ud835\udc62 2 , . . . \ud835\udc56 \ud835\udc62 \ud835\udc5b } be such a sequence, where \ud835\udc56 \ud835\udc62 \ud835\udc58 denotes the \ud835\udc58 -th item user \ud835\udc62 interacted with. In this context, a recommendation model can be thought of as a function \ud835\udc53 \ud835\udf03 , parameterized by \ud835\udf03 , that takes as input the interaction history encoded by \ud835\udc46 \ud835\udc62 and seeks to predict the item or items that user \ud835\udc62 will interact with in the future. Given the sequential nature of the problem, we consider models based on the transformer architecture [28]. These models, initially proposed in the context of language modeling tasks, have proven effective in various domains. From a scaling perspective, and similarly to what happens in the natural language case, we have two clear dimensions that affect their scaling behavior: the number of model parameters, \ud835\udc41 , and the number of user-item interactions seen during training. However, as we will see next, analyzing scalability in most recommender systems proposed in the literature would also require considering the number of available items in the catalog, | \ud835\udc3c | . This dependency originates in the way most transformer-based approaches cast the sequential prediction task. In a direct translation of the next-word prediction used to train language models, recommendation transformers treat the elements of the input sequence (items from the catalog) as \"tokens\", i.e. atomic elements whose cooccurrence patterns we try to learn from data. As in the language modeling case, the model is asked to learn the interaction patterns between sequences of such atoms and also a representation (embedding) that encodes some intrinsic aspect of each such element. This implies that, besides the actual number of parameters in the model devoted to sequential prediction, we also need to store a number of vector embeddings equal to the catalog size. While in the case of language the vocabulary size is relatively small, ranging between the tens to a few hundred thousand elements, the size of the catalog for a real-world recommendation model can grow dramatically, potentially reaching into the billions [7]. In such cases, the number of parameters associated with the matrix of trainable item embeddings quickly dominates the total parameter count. Such a dependency between model complexity and the size of the catalog is depicted in Fig. 1. In the bottom panel of the figure, we show an estimate of the 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 # of users 10 1 10 4 10 7 # of items 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 # of interactions 10 1 10 4 10 7 # of items 10 2 10 3 10 4 10 5 10 6 10 7 10 8 # items 10 4 10 7 10 10 # parameters 33K 4M 33M 1B 19B Figure 1: Increase in the catalog size induced by an increase in the number of active users (top) and interactions (middle). Increasing the number of items increases the total parameter count (bottom) in models that use trainable item embeddings for 64-dimensional embeddings. For smaller models (33K parameters), the catalog size dominates the total parameter count, while for larger models (19B parameters), the total number of trainable parameters remains stable across a wide spectrum of catalog sizes. total number of parameters for models of different sizes (expressed in terms of the total number of parameters, \ud835\udc41 , as a function of the size of the catalog, | \ud835\udc3c | . We observe that for models with a small complexity (33K and 4M parameters), the total parameter count grows linearly with the size of the catalog after a relatively short nearly flat initial regime. For larger models ( > 1 \ud835\udc35 ), on the other hand, the total parameter count remains stable across a wide range of | \ud835\udc3c | , growing only for extremely large values of | \ud835\udc3c | . This phenomenon makes the analysis difficult since, by increasing the number of training sequences, we would implicitly increase the number of items interacted with by users, which would, in turn, increase the number of parameters of the model. This growth is not deterministic and depends on the diversity of items observed in the pool of sequences used for training. This behavior is illustrated in the first two panels of Fig. 1, where we show the growth in the number of visited catalog items induced by an increase in the number of active users (top) and number of navigation data (middle). Moreover, building a solution based on learning item embeddings worsens the cold-start problem observed in real systems [11, 31]. In the next section, we reformulate the standard transformerbased approach to break this dependency. In this way, the complexity of the model becomes independent of | \ud835\udc3c | and we can analyze the scaling behavior w.r.t the variables of interest (parameter count and training set size) concisely.", "3 A SCALABLE RECOMMENDATION FRAMEWORK": "WetakeSASRec[22]asourreference model. SASRec is a transformerbased architecture that has shown competitive performance on several sequential recommendation tasks [30] and is regarded as a strong baseline in more recent evaluations [39]. In SASRec, the model takes as input a sequence of user-item interactions of length \ud835\udc5b and seeks to predict the item the user will interact with next. The sequence is fed into a transformer model of \ud835\udc3f layers to produce an output embedding that matches a representation of the following item in the sequence. The output might also include a classification layer over the items in the catalog that induces additional complexity [48]. Each item in the catalog is encoded as a trainable vector representation of size \ud835\udc37 , resulting in a total of | \ud835\udc3c | \u00d7 \ud835\udc37 trainable parameters. As mentioned above, this dependency between the number of trainable parameters and the catalog size makes the analysis of scaling behaviors difficult due to the interplay between \ud835\udc41 and | \ud835\udc3c | . Figure 2 (left) illustrates this scenario, where both the input and output sequences correspond (to indices) to items in the catalog. For large catalogs, the parameter count is dominated by the matrix of input embeddings, the output prediction layer(s), or both. Given these observations, instead of considering the learning problem as a classification one, we propose reformulating it as an embedding regression task [50], as follows. Given a user navigation sequence \ud835\udc46 \ud835\udc62 = { \ud835\udc56 \ud835\udc62 1 , \ud835\udc56 \ud835\udc62 2 , . . . \ud835\udc56 \ud835\udc62 \ud835\udc5b } , we assume we can compute a \ud835\udc37 -dimensional vector representation for each item in the sequence. To compute such representations, we rely on a parametric mapping \ud835\udf19 : \ud835\udc3c \u2192 R \ud835\udc37 . We denote as \ud835\udf19 \ud835\udc62 \ud835\udc58 \u2261 \ud835\udf19 ( \ud835\udc56 \ud835\udc62 \ud835\udc58 ) the representation of \ud835\udc58 -th item user \ud835\udc62 interacted with. These embeddings are user-independent in that the representation for a given item is the same irrespective of how the user might interact with it. We compute these representations i 4 ' Transformer encoder Classification layer i 1 i 2 i 3 i 4 input sequence { target Embeddings LUT output (item index) { i 4 ' Transformer encoder i 1 i 2 i 3 i 4 input sequence { Feature encoder \u03d5 1 \u03d5 2 \u03d5 3 \u03d5 4 output (embedding) target \u03d5 ( ) \u03d5 ' 4 { on the fly and train the full model (including the feature extraction model \ud835\udf19 ) to pick among (a subset of) them the one that corresponds to the target item for an input sequence \ud835\udc46 \ud835\udc62 . This is illustrated in Figure 2 (right), where we have replaced the embedding layer with a feature extractor that computes the item embeddings ( \ud835\udf19 \ud835\udc56 , \ud835\udc56 = 1 , .., 4) that feed the model. The output of this model ( \ud835\udf19 \u2032 4 ) is used for prediction (and learning) by comparing similarities with the items in the catalog (their embeddings). This feature-based approach has shown good performance in the literature [40, 64]. In this case, the number of parameters associated with the computation of item embeddings is given by the number of trainable parameters in the feature extraction module \ud835\udf19 , irrespective of the number of items in the catalog. We train our model autoregressively as follows. Given a training set of user navigation sequences of length \ud835\udc5b , we ask the model to predict each element of any given sequence based on the (sub)sequence of previous interactions. We optimize the following loss: Here, we omitted the superscript \ud835\udc62 for the sake of clarity 1 . Let S denote the set of all subsequences with a length of at least 2 interactions. Let us denote by \u02dc \ud835\udc46 = { \ud835\udc56 1 , . . . , \ud835\udc56 \ud835\udc5b -1 } the partial sequence containing the first \ud835\udc5b -1 items of \ud835\udc46 . Our goal is to train a model that, based on \u02dc \ud835\udc46 , can rank the target \ud835\udc56 \ud835\udc5b as high as possible when compared to other candidates from the catalog. We adopt sampled Softmax [57] as our choice for \u2113 : Here, \ud835\udc53 \ud835\udf03 denotes the model we are trying to fit, \ud835\udf19 \ud835\udc5b the representation of the target item \ud835\udc56 \ud835\udc5b computed by the feature extraction module, \ud835\udf0f is a temperature parameter that controls the softness of the positive and negative interactions, and N( \u02dc \ud835\udc46 ) \u2282 \ud835\udc3c is a set of negatives whose cardinality is to be set. In practice, we apply the logQ correction proposed in [60] to the logits in Eq. (2) to correct for the bias introduced by the negative sampling distribution. In the rest of the paper, we refer to our transformer-based model and learning formulation as Scalable Recommendation Transformer (SRT) . The framework introduced above is motivated by the need to set up a competitive yet simple baseline that scales well w.r.t the quantities we identified as the most relevant from a scaling perspective, namely the number of trainable parameters and the number of samples (or interactions) observed during training. Eq. (2) can be seen as an approximation to a cross-entropy loss over the items in the catalog, where we contrast against a subset of the possible items. This corresponds to a generalization of the loss used in SASRec or BERT4Rec, where N( \u02dc \ud835\udc46 ) is constrained to a single sample draw at random. Note that by drawing samples at random, we take the risk of contrasting against uninformative samples that are easily distinguishable from the positive ones. On the other hand, if we choose an elaborate negative sampling methodology, we might end up adding a non-negligible computation overhead to an otherwise simple model. Moreover, sampling hard negatives might induce biases that correlate with the catalog size [33], adding a degree of variability that is difficult to isolate. In our case, we opt to sample negatives from the item popularity distribution (i.e. items with which users interacted the most are sampled more frequently). This strategy is competitive and has a small footprint on the overall computations. From now on, we denote our models as SRT-X, where X is the number of negatives used to compute the loss in Eq. (2). Besides the advantages of the proposed formulation regarding scalability, an additional advantage of our model is the ability to work with non-static catalogs. Adding and deleting items dynamically from a catalog (due to policy infringements, product stockout, new trends, outdated information, etc) is commonplace in most practical applications. Building sequential recommendation models based on fixed item sets brings many concerns regarding the usability and maintainability of the system over time. These concerns might hinder a wider adoption of these types of approaches. Before delving deeper into scalability, which is the primary goal of our work, we first show that our formulation achieves competitive performance compared to other transformer-based formulations. Table 1 compares the performance of our model against other popular methods from the literature on the Beauty and Sports subsets of the Amazon Review Data [35] benchmark. Details of the dataset, metrics, and evaluation protocols are provided in Sec. 4. Weconsider different versions of our model trained using 10, 100, 300, and 1K negatives, respectively, and compare them against the popular SASRec [22] and BERT4Rec [48] models. For these models, we show the metrics reported by Chen et al. [5] for compatibility of evaluation methodology. We also report a variation of SASRec in which we replaced the binary cross-entropy with a full crossentropy loss over the items in the catalog. We name this variant as SASRec-CE. There are several observations to be made. First, we see that our best model outperforms all other alternatives, even when, at its core, the underlying models are very similar. Second, the negative sampling strategy is crucial in getting good performance. This is interesting since we can see the multiclass cross-entropy loss as a measure that contrasts each positive against all negatives (full catalog) and, in that sense, can be seen as a limiting case for our contrastive formulation. Increasing the number of samples beyond this value becomes impractical as it involves computing embeddings for additional \ud835\udc4b\ud835\udc35 samples, with \ud835\udc35 the size of the training batch. We believe these results validate the overall formulation and set a strong baseline model for scalability analysis.", "4 EXPERIMENTS": "This section discusses our experimental setup in the context of standard practices observed in the literature. We then show and discuss results on scalability and optimal compute allocation. Finally, we show fine-tuning results that compete favorably with other methods from the literature.", "4.1 Evaluation Protocol": "We ran experiments on the Amazon Product Data (APD) dataset [14, 35], a large dataset of product reviews crawled from Amazon between 1996 and 2014. The dataset consists of 82 . 7 million reviews over 9 . 9 million different products written by more than 21 million users. Reviews in this dataset correspond to a subset of all purchases made in the platform during the relevant time span. Due to its size, a common practice in the literature consists of using smaller subsets of the data. For instance, \"Amazon Beauty\" corresponds to the subset of samples where users bought (and reviewed) an item from the \"beauty\" category. To avoid issues related to cold-start [31], it is common to filter out users and products with less than five purchases. The remaining data is called a \"5-core\" dataset. These two procedures (per-category and 5-core filtering) distort or hide some of the intrinsic characteristics of real-world recommendation problems. For instance, if we consider the \"beauty\" category, only 8 . 7% of the interactions originate from items with at least 5 purchases/reviews. This is not only a matter of scale (with most data being discarded) but a problem of deceiving evaluation, as results reported on these datasets do not necessarily extrapolate to actual real systems. Table 2 provides dataset statistics for the entire dataset, two common subsets used in the literature, and their 5-core trimmed versions. There are an order of magnitude fewer interactions and items in the 5-core version of the dataset compared to their full counterpart. Figure 3 shows the distribution of reviews per item for the beauty subset of APD for both the full and 5-core versions. As the figure shows, trimming the dataset reshapes the original problem into modeling a long-tail phenomenon, disregarding the rich and more relevant aspects of real user interactions. 10 0 10 1 10 2 10 3 Number of reviews 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Relative frequency 5-core beauty Full beauty From the above and to analyze the scaling behavior of transformerbased recommendation models, we propose a simple strategy to generate datasets of different sizes with the same distributional information as the original (average sequence length, diversity, etc.). The strategy consists of sampling users at random and recording their interactions into a single dataset. Note that by increasing the number of active users, we account for a more extensive set of items they interact with (see Fig. 1). We follow standard practice and take the last item from each sequence as the target for evaluation (test), the item at position \ud835\udc5b -1 for validation, and leave the first \ud835\udc5b -2 elements for training. Due to the long-tail behavior of item categories, we expect larger datasets to account also for larger catalogs. This poses a challenge from an evaluation perspective since computing retrieval metrics over the whole catalog becomes infeasible. We sample a set of random negatives for each positive, as in [22]. In our case, however, we sample 10K negatives for each positive instead of only 100 as in [22]. By doing so, we increase the probability of sampling negatives, which are more challenging to discriminate w.r.t the ground truth, while providing us with a closer approximation to the full-catalog case. In our experiments, we chose the NDCG@5 score as a performance metric as it is one of the most common metrics reported in the literature. Following [17, 23], we use the number of FLOPs as a proxy for the amount of compute required to achieve a given performance for different choices of model complexity, number of training samples, and the catalog size.", "4.2 Model Design and Training Algorithm": "We adapt SASRec as outlined in Sec. 3, replacing the item embedding matrix with a trainable feature encoder whose complexity is independent of the size of the catalog. Concretely, we take the title and brand of each product and tokenize them into a vocabulary of 30 \ud835\udc58 tokens with the SentencePiece tokenizer [26]. This way, we replace the variable-sized item embedding matrix with a fixed matrix of token embeddings. As shown in Table 1, these changes lead to comparable performance in standard benchmarks. Based on this architecture, we consider different model complexities parameterized by the number of layers, \ud835\udc5b \ud835\udc3f , number of attention heads per layer, \ud835\udc5b \ud835\udc3b , and hidden embedding dimensionality, \ud835\udc51 . Table 3 details the different combinations of these parameters we used in our experiments. To train our models, we use the Adam optimizer and a one-cycle learning rate policy consisting of a linear warm-up stage and a cosine decay after one-third of the total iterations. We set the base learning rate to 1 \ud835\udc52 -4 and the number of epochs to 50. We use gradient clipping (set to 1) and a weight decay factor of 1 \ud835\udc52 -5. We base our implementations on the RecBole library [65].", "4.3 Scaling Model and Dataset Sizes for Optimal Compute": "In this section, we explore the relationship between the target metric and the compute resource requirements induced by different combinations of model sizes and number of training interactions. Our evaluation differs from similar studies [6, 17, 23] in two main aspects: first, we focus on a task-specific metric instead of a more generic loss; second, we train our models over multiple epochs, thus revisiting the same training sequences multiple times during the training process. These differences originate from the particularities of the sequential recommendation problem. We also focus on a performance metric that is closely tied to the actual recommendation task (NDCG vs loss as in the language modeling case) and which is more informative from a practical standpoint. Figure 4 shows the target metric as a function of the number of FLOPs for different training runs obtained with different combinations in the number of model parameters and size of the training set. The left and right plots show the same runs but use a different color encoding to highlight different aspects of these runs. On the left, the colors encode the number of interactions processed in each training subset. This value ranges from 80 \ud835\udc3e to 8 . 2 \ud835\udc40 . On the right, the colors encode the number of non-embedding parameters in each model. We decided to plot this number instead of the total parameter count for the following reasons: first, the number of token embeddings is constant across the different runs; second, non-embedding parameters (parameters of the transformer model) are responsible for capturing the sequential dependencies that are unique to our problem. The number of non-embedding parameters ranges between 10 . 2 \ud835\udc3e and 9 . 6 \ud835\udc40 . The figures show an improvement in the target metric as larger models or bigger training sets are used. In smaller models, increasing the amount of training data reaches a point where performance saturates. Such data regimens are only useful if they come accompanied by an increase in the number of parameters in the model. From these runs, we also extracted the envelope of maximal performance (i.e. the points that observe the best NDCG score) among all the configurations that require the same amount of FLOPs. This envelope is highlighted in blue in the left panel of Figure 4. From these points, we build scaling plots in Figure 5. The plot on the left shows the number of seen interactions as a function of the number of FLOPs. The number of seen interactions is the number of interactions in the dataset used to train the model, multiplied by the number of epochs required to reach the point of maximal performance by the current configuration. We chose this quantity since we work on a multi-epoch setting where the optimal point for each FLOP count results from a model trained by a given number of epochs using a dataset with sequences of varying lengths. The panel on the right shows the number of non-embedding parameters as a function of the number of FLOPs for the points in the envelope. In both cases, color encodes the value of the NDCG@5 score. From these plots, we see a trend in that increasing the number of parameters in the model or the size of the dataset led to higher performance scores. Smaller models do not exploit the more significant variability observed in larger datasets. Reaching good performance by adding more data requires models with the flexibility to deal with this added complexity. In this case, however, it is not easy to disentangle the effect of increasing either of these factors. If we look at the figure on the right, we observe that a subset of models achieves different degrees of performance according to the resources devoted to training them (larger datasets or more training iterations). We believe, however, that identifying the scaling behavior brings valuable insights that allow us to extrapolate to novel data and model complexity regimes. 10 12 10 13 10 14 10 15 10 16 10 17 FLOPs 0.00 0.02 0.04 0.06 0.08 0.10 0.12 NDCG @ 5 10 7 10 8 D 10 12 10 13 10 14 10 15 10 16 10 17 FLOPs 0.00 0.02 0.04 0.06 0.08 0.10 0.12 NDCG @ 5 10 7 N 10 10 10 12 10 14 10 16 10 18 FLOPs 10 5 10 6 10 7 10 8 10 9 # seen interactions 0.02 0.04 0.06 0.08 0.10 0.12 NDCG@5 10 10 10 12 10 14 10 16 10 18 FLOPs 10 3 10 4 10 5 10 6 10 7 # non-embedding parameters 0.02 0.04 0.06 0.08 0.10 0.12 NDCG@5", "4.4 Estimating Model Performance": "Based on the data obtained in our experiments, we present two formulations for estimating the expected performance in terms of the target metric for recommendation. These models aim at asking the following questions: a) for a fixed FLOP budget, is it possible to get an estimate of the maximum achievable performance? and b) for a given model and dataset size, is it possible to estimate the expected maximum NDCG for that configuration? In the first case, we assume there exists an \"oracle\" that selects the optimal model and dataset configuration. 4.4.1 Estimating NDCG from a fixed FLOPs budget. In the experiments, we recorded the maximum NDCG achieved for each FLOP budget. Figure 6 shows such points together with linear and sigmoidal fits. The figure shows a more complex relationship between NDCG and FLOPs in log space than the linear scaling behavior observed in other studies. In our case, we observe the beginning of an asymptotic trend for the maximum achievable NDCG. This behavior could be due to many factors, including the saturation of the target metric due to challenges intrinsic to the particular recommendation problem (recommendation over broad item categories, representation ambiguity in the item embeddings, etc). In this case, a sigmoidal fit appears more appropriate, in which case it corresponds to: This function reveals that as the FLOPs budget increases, the NDCG approaches an upper limit estimated at 0.149 (0.396-0.247), highlighting the diminishing returns of increasing the computational budget. We can identify the point where this diminishing return 10 12 10 13 10 14 10 15 10 16 FLOPS 0.00 0.02 0.04 0.06 0.08 0.10 0.12 NDCG@5 NDCG envelope Sigmoid fit Linear fit starts at log ( FLOPs ) = 30 . 7, which corresponds to approximately 2 . 15 \u00d7 10 -13 FLOPs. At this point, performance reaches a maximum estimated value of 0 . 0525 ( 0 . 155 / 2 ) . Identifying the point of diminishing returns is essential in optimizing resource allocation in real-world scenarios. 4.4.2 Estimating NDCG for a given model and dataset size. Here, we model the maximum achievable NDCG as a function of the total parameter count and the size of the dataset, as measured by the number of seen interactions. The goal is to find a parametric function that captures the underlying relationship between the model's complexity, data size, and final task performance. This involves identifying key parameters that influence the expected risk and then quantifying their impact on the model's effectiveness, as measured by the NDCG score. We follow a risk decomposition approach and propose the following functional form similar to [17]: Here, \ud835\udc41 denotes the total parameter count and \ud835\udc47 number of useritem interactions. We use a subtractive formulation to account for a target metric maximization law, instead of a loss minimization as in [17]. The chosen parametric form allows us to outline how changes in the number of parameters and the size of the dataset systematically affect the model's ability to rank items accurately. To fit the model, we use a non-linear least squares approach and constrain the model coefficients to be non-negatives to avoid nonsensical solutions. We obtain the following solution: From the above equation, we can interpret \ud835\udc38 as the maximum expected value for the NDCG@5 score, in which case reaches a value of 0 . 163. We also observe a similar value for the exponents for both \ud835\udc41 and \ud835\udc47 , suggesting that both data and parameters behave similarly from a scaling perspective. Figure 7 shows the predicted NDCG@5 score as a function of the number of model parameters, \ud835\udc41 , and number of seen interactions. 10 6 10 7 0.02 0.04 0.06 0.08 0.10 0.12 0.14 NDCG@5 80K 408K 813K 4M 8M # interactions N", "4.5 Transferability": "In this section, we evaluate the transfer ability of some of our larger pre-trained models by fine-tuning them in the Amazon beauty and sports subsets. This is a widely used strategy in the literature but has seen lesser popularity in the context of sequential recommendation. This is because, unlike the language and vision domains, the data used to train such models are particular to each recommendation domain (i.e. the nature of the catalog), and the type of events being recorded changes from case to case. Nevertheless, training more generic models at scale and fine-tuning them to different downstream tasks poses the same advantages observed in other domains, such as improvements in the final performance, shorter development cycles, improvements in the backbone model translate effortlessly to improvements in downstream performance, etc. We show that is it possible to pre-train and fine-tune recommendation models and that, by doing so, we obtain performance improvements that could not be achieved by training similar models from scratch. Our adaptation strategy is as follows. Given a pre-trained model 2 , we apply a progressive fine-tuning strategy that consisting of progressively unfreezing layer by layer, tuning them for 10 epochs using a learning rate of 1 \u00d7 10 -4 and a one-cycle cosine schedule. Once all transformer layers have been unfreeze, we unfreeze the token embedding layer and train for an additional 50 epochs. To avoid over-adaptation and catastrophic forgetting, we use the Elastic Weight Consolidation (EWC) formulation of [25] which has been proven to be effective in ranking contexts [34]. For EWC, we use \ud835\udf06 = 100. Table 4 compares the results of the fine-tuned model, a similar model trained from scratch, and the following models from the literature: MINCE [40], ICLRec [5], CoSeRec [33], and CL4SRec [59]. Results are reported on the beauty and sports subsets of APD. We report NDCG@5 and HIT@5 scores. Table 4 shows that our fine-tuned variants outperform all alternatives by a margin. Interestingly, training from scratch on these datasets competes favorably with more alternatives from the literature, showing that despite its simplicity, our approach serves as a strong baseline for evaluations. If we look at the NDCG@5 score, we see an improvement of more than 12 and 7% for the Beauty and Sports subsets, respectively, for the fine-tuned variant w.r.t to the models trained from scratch.", "5 RELATED WORK": "Sequential recommendation is a branch of recommendation systems, an area that recognizes the importance of sequential behavior in learning and discovering user preferences [56]. Initial models used the Markov Chain framework for anticipating user activities [12, 13, 42]. With advancements in deep learning, innovative approaches have emerged, such as employing Recurrent Neural Networks (RNN) [16], attention mechanisms [49], and Memory networks [20]. The disruption introduced by transformer architecture [54] led to significant progress, giving rise to well-known approaches like SASRec [22] and BERT4Rec [48].Despite their success, these methods face a substantial limitation in scaling. They rely on item IDs to represent the sequence of interactions, which presents several scalability issues [8]. First, the pure ID indexing of users and items is inherently discrete and fails to impart adequate semantic information to new items. Second, adding new items requires modifications to the model's vocabulary and parameters, causing transformer-based methods to scale poorly with an increase in the item count, which is crucial for many real-world recommendation systems. A viable solution to the constraints of ID-based recommender systems is to integrate textual information such as item titles, descriptions, and user reviews. The UniSRec model exemplifies this by deriving adaptable representations from item descriptions [19]. Text-based Collaborative Filtering (TCF) with Large Language Models like GPT-3 has demonstrated potential superiority over ID-based systems. Nevertheless, the overreliance on text prompted the development of VQ-Rec, which utilizes vector-quantized representations to temper the influence of text [18]. Additionally, approaches like ZSIR leverage Product Knowledge Graphs to augment item features without prior data [9], and ShopperBERT models user behavior via purchase histories [45]. IDA-SR advances this by using BERT to generate ID-agnostic representations from text [36]. On the contrary, MoRec illustrates that systems that combine IDs and text can surpass those dependent solely on IDs [61]. However, these advancements complicate existing architectures by adding computational demand and complicating scalability. To these intricate and parameter-intensive models, we must add the challenge that data in real-world applications is often noisy and sparse. Various methods have adopted contrastive learning [53] in new architectures, as seen with CoSeRec [33], ContraRec [55], and S3-Rec [66]. The success of these new contrastive learning-based methods motivates further investigation into the effectiveness of contrastive loss functions for item recommendation, particularly Sampled Softmax [57]. Regrettably, these studies typically focus on fixed item spaces and overlook the scaling issues of the functions. Scaling problems have been tackled through other methods. LSAN suggests aggressively compressing the original embedding matrix [30], introducing the concept of compositional embeddings, where each item embedding is composed by combining a selection of base embedding vectors. Recently, the concept of infinite recommendation networks [44] introduced two complementary ideas: \u221e-\ud835\udc34\ud835\udc38 , an infinite-width autoencoder to model recommendation data, and DISTILL-CF, which creates high-fidelity data summaries of extensive datasets for subsequent model training. Scaling issues are not unique to recommendation systems but are inherent in new transformer-based architectures. In the field of NLP, various studies have been carried out to discover scaling laws that predict the scaling of the model and inform decision-making [17]. To our knowledge, only two studies have attempted to find scaling laws in recommendation systems, yet none in SR. The first study [3] aimed to explore the scaling properties of recommendation models, characterizing scaling efficiency across three different axes (data, compute, parameters) and four scaling schemes (embedding table scaling vertically and horizontally, MLP and top layer scaling) in the context of CTR problems. Similarly, the second study [46] seeks to understand scaling laws in the pursuit of a general-purpose user representation that can be assessed across a variety of downstream tasks.", "6 CONCLUSIONS": "In this work, we studied the scaling behavior of the transformer architecture applied to real-world sequential recommendation problems. We introduced a simple and flexible architecture and learning formulation that allowed us to scale the recommendation problem and model complexity independently from each other. We showed there exist scaling laws similar to those observed in other sequential prediction domains, offering insights into the design of larger and more capable models. We also show that by pre-training larger recommendation transformers, we can fine-tune them for downstream tasks with significantly lesser data and obtain performance improvements compared to the same models trained from scratch.", "REFERENCES": "[1] Ibrahim M. Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. 2022. Revisiting Neural Scaling Laws in Language and Vision. (2022). [2] Ibrahim M. Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. 2023. Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. (2023). [3] Newsha Ardalani, Carole-Jean Wu, Zeliang Chen, Bhargav Bhushanam, and Adnan Aziz. 2022. Understanding Scaling Laws for Recommendation Models. CoRR (2022). arXiv:2208.08489 [4] Pedro G. Campos, Fernando D\u00edez, and Iv\u00e1n Cantador. 2014. Time-aware recommender systems: a comprehensive survey and analysis of existing evaluation protocols. User Model. User Adapt. Interact. 24, 1-2 (2014), 67-119. [5] Yongjun Chen, Zhiwei Liu, Jia Li, Julian J. McAuley, and Caiming Xiong. 2022. Intent Contrastive Learning for Sequential Recommendation. In WWW '22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022, Fr\u00e9d\u00e9rique Laforest, Rapha\u00ebl Troncy, Elena Simperl, Deepak Agarwal, Aristides Gionis, Ivan Herman, and Lionel M\u00e9dini (Eds.). ACM, 2172-2182. [6] Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake A. Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew J. Johnson, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Marc'Aurelio Ranzato, Jack W. Rae, Erich Elsen, Koray Kavukcuoglu, and Karen Simonyan. 2022. Unified Scaling Laws for Routed Language Models. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato (Eds.). PMLR, 4057-4086. [7] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems, Boston, MA, USA, September 15-19, 2016, Shilad Sen, Werner Geyer, Jill Freyne, and Pablo Castells (Eds.). ACM, 191-198. [8] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender Systems in the Era of Large Language Models (LLMs). CoRR (2023). arXiv:2307.02046 [9] Ziwei Fan, Zhiwei Liu, Shelby Heinecke, Jianguo Zhang, Huan Wang, Caiming Xiong, and Philip S. Yu. 2023. Zero-shot Item-based Recommendation via Multitask Product Knowledge Graph Pre-Training. (2023), 483-493. [10] Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020. Deep Learning for Sequential Recommendation: Algorithms, Influential Factors, and Evaluations. ACM Trans. Inf. Syst. 39, 1 (2020), 10:1-10:42. [11] Jyotirmoy Gope and Sanjay Kumar Jain. 2017. A survey on solving cold start problem in recommender systems. In 2017 International Conference on Computing, Communication and Automation (ICCCA). IEEE, 133-138. [12] Ruining He, Wang-Cheng Kang, and Julian J. McAuley. 2017. Translationbased Recommendation. In Proceedings of the Eleventh ACM Conference on Recommender Systems, RecSys 2017, Como, Italy, August 27-31, 2017, Paolo Cremonesi, Francesco Ricci, Shlomo Berkovsky, and Alexander Tuzhilin (Eds.). ACM, 161-169. [13] Ruining He and Julian J. McAuley. 2016. Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation. In IEEE 16th International Conference on Data Mining, ICDM 2016, December 12-15, 2016, Barcelona, Spain, Francesco Bonchi, Josep Domingo-Ferrer, Ricardo Baeza-Yates, Zhi-Hua Zhou, and Xindong Wu (Eds.). IEEE Computer Society, 191-200. [14] Ruining He and Julian J. McAuley. 2016. Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering. In Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Montreal, Canada, April 11 - 15, 2016, Jacqueline Bourdeau, Jim Hendler, Roger Nkambou, Ian Horrocks, and Ben Y. Zhao (Eds.). ACM, 507-517. [15] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. 2020. Scaling Laws for Autoregressive Generative Modeling. CoRR (2020). arXiv:2010.14701 [16] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. (2016). [17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training ComputeOptimal Large Language Models. CoRR (2022). arXiv:2203.15556 [18] Yupeng Hou, Zhankui He, Julian J. McAuley, and Wayne Xin Zhao. 2023. Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders. In Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023, Ying Ding, Jie Tang, Juan F. Sequeda, Lora Aroyo, Carlos Castillo, and Geert-Jan Houben (Eds.). ACM, 1162-1171. [35] Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-Based Recommendations on Styles and Substitutes. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015, Ricardo Baeza-Yates, Mounia Lalmas, Alistair Moffat, and Berthier A. Ribeiro-Neto (Eds.). ACM, 43-52. [36] Shanlei Mu, Yupeng Hou, Wayne Xin Zhao, Yaliang Li, and Bolin Ding. 2022. ID-Agnostic User Behavior Pre-training for Sequential Recommendation. In Information Retrieval -28th China Conference, CCIR 2022, Chongqing, China, September 16-18, 2022, Revised Selected Papers (Lecture Notes in Computer Science, Vol. 13819), Yi Chang and Xiaofei Zhu (Eds.). Springer, 16-27. [37] Jianmo Ni, Jiacheng Li, and Julian J. McAuley. 2019. Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 188-197. [38] Hao Peng, Renyu Yang, Zheng Wang, Jianxin Li, Lifang He, Philip S. Yu, Albert Y. Zomaya, and Rajiv Ranjan. 2022. Lime: Low-Cost and Incremental Learning for Dynamic Heterogeneous Information Networks. IEEE Trans. Computers 71, 3 (2022), 628-642. [39] Aleksandr V. Petrov and Craig Macdonald. 2022. A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation. In RecSys '22: Sixteenth ACM Conference on Recommender Systems, Seattle, WA, USA, September 18 - 23, 2022, Jennifer Golbeck, F. Maxwell Harper, Vanessa Murdock, Michael D. Ekstrand, Bracha Shapira, Justin Basilico, Keld T. Lundgaard, and Even Oldridge (Eds.). ACM, 436-447. [40] Ruihong Qiu, Zi Huang, and Hongzhi Yin. 2021. Memory Augmented MultiInstance Contrastive Predictive Coding for Sequential Recommendation. In IEEE International Conference on Data Mining, ICDM 2021, Auckland, New Zealand, December 7-10, 2021, James Bailey, Pauli Miettinen, Yun Sing Koh, Dacheng Tao, and Xindong Wu (Eds.). IEEE, 519-528. [41] Dimitrios Rafailidis and Alexandros Nanopoulos. 2016. Modeling Users Preference Dynamics and Side Information in Recommender Systems. IEEE Trans. Syst. Man Cybern. Syst. 46, 6 (2016), 782-792. [42] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized Markov chains for next-basket recommendation. In Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010, Michael Rappa, Paul Jones, Juliana Freire, and Soumen Chakrabarti (Eds.). ACM, 811-820. [43] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. 2020. A Constructive Prediction of the Generalization Error Across Scales. (2020). [44] Noveen Sachdeva, Mehak Preet Dhaliwal, Carole-Jean Wu, and Julian J. McAuley. 2022. Infinite Recommendation Networks: A Data-Centric Approach. (2022). [45] Kyuyong Shin, Hanock Kwak, Kyung-Min Kim, Minkyu Kim, Young-Jin Park, Jisu Jeong, and Seungjae Jung. 2021. One4all User Representation for Recommender Systems in E-commerce. CoRR (2021). arXiv:2106.00573 [46] Kyuyong Shin, Hanock Kwak, Su Young Kim, Max Nihl\u00e9n Ramstr\u00f6m, Jisu Jeong, Jung-Woo Ha, and Kyung-Min Kim. 2023. Scaling Law for Recommendation Models: Towards General-Purpose User Representations. In Thirty-Seventh AAAIConference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, Brian Williams, Yiling Chen, and Jennifer Neville (Eds.). AAAI Press, 4596-4604. [47] Xiaoyuan Su and Taghi M. Khoshgoftaar. 2009. A Survey of Collaborative Filtering Techniques. Adv. Artif. Intell. 2009 (2009), 421425:1-421425:19. [48] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019, Wenwu Zhu, Dacheng Tao, Xueqi Cheng, Peng Cui, Elke A. Rundensteiner, David Carmel, Qi He, and Jeffrey Xu Yu (Eds.). ACM, 1441-1450. [49] Ke Sun, Tieyun Qian, Tong Chen, Yile Liang, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2020. Where to Go Next: Modeling Long- and Short-Term User Preferences for Point-of-Interest Recommendation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 214-221. [50] Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved Recurrent Neural Networks for Session-based Recommendations. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, DLRS@RecSys 2016, Boston, MA, USA, September 15, 2016, Alexandros Karatzoglou, Bal\u00e1zs Hidasi, Domonkos Tikk, Oren Sar Shalom, Haggai Roitman, Bracha Shapira, and Lior Rokach (Eds.). ACM, 17-22. [51] Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018, Yi Chang, Chengxiang Zhai, Yan Liu, and Yoelle Maarek (Eds.). ACM, 565-573."}
