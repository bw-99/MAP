{
  "Continual Transfer Learning for Cross-Domain Click-Through Rate Prediction at Taobao": "Lixin Liu ‚àó Alibaba Group Beijing, China llx271805@alibabainc.com Yanling Wang ‚àó Renmin University of China Beijing, China wangyanling@ruc.edu.cn Tianming Wang Alibaba Group Hangzhou, China tianming.wtm@alibabainc.com Dong Guan Alibaba Group Beijing, China inc.com jiexing.gd@alibaba- Jiawei Wu Alibaba Group Hangzhou, China wujiawei@alibabainc.com Jingxu Chen Alibaba Group Hangzhou, China xianlu@alibabainc.com Rong Xiao Alibaba Group Hangzhou, China xiaorong.xr@alibaba- inc.com Wenxiang Zhu Alibaba Group Hangzhou, China wenxiang.zwx@alibabainc.com",
  "ABSTRACT": "We study the problem of cross-domain click-through rate (CTR) prediction for recommendation at Taobao. Cross-domain CTR prediction has been widely studied in recent years, while most attempts ignore the continual learning setting in industrial recommender systems. In light of this, we present a necessary but less-studied problem named Continual Transfer Learning (CTL), which transfers knowledge from a time-evolving source domain to a time-evolving target domain. We propose an effective and efficient model called CTNet to perform CTR prediction under the CTL setting. The core idea behind CTNet is to treat source domain representations as external knowledge for target domain CTR prediction, such that the continually well-trained source and target domain parameters can be preserved and reused during knowledge transfer. Extensive offline experiments and online A/B testing at Taobao demonstrate the efficiency and effectiveness of CTNet. CTNet is now fully deployed online at Taobao bringing significant improvements.",
  "CCS CONCEPTS": "¬∑ Information systems ‚Üí Information retrieval .",
  "KEYWORDS": "Continual Transfer Learning, CTR Prediction, Cross-Domain",
  "ACMReference Format:": "Lixin Liu, Yanling Wang, Tianming Wang, Dong Guan, Jiawei Wu, Jingxu Chen, Rong Xiao, Wenxiang Zhu, and Fei Fang. 2023. Continual Transfer Learning for Cross-Domain Click-Through Rate Prediction at Taobao. In Companion Proceedings of the ACM Web Conference 2023 (WWW '23 Companion), April 30-May 4, 2023, Austin, TX, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3543873.3584625 ‚àó The first two authors contribute equally to the work. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'23 Companion, April 30-May 4, 2023, Austin, TX, USA ¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9419-2/23/04...$15.00 https://doi.org/10.1145/3543873.3584625 Fei Fang Alibaba Group Hangzhou, China mingyi.ff@alibabainc.com",
  "1 INTRODUCTION": "Click-through rate (CTR) prediction [5, 18, 24, 29, 30] which estimates the probability of a user clicking on a candidate item is a crucial task for recommender systems (RSs). In practice, largescale platforms like Taobao contain multiple different recommendation domains, and each domain maintains its own CTR prediction model. Since some users and items are shared by different domains, cross-domain CTR prediction [10, 13, 14, 17] is desired to transfer knowledge from the larger-scale source domain to alleviate the data sparsity issue in the target domain. Previous efforts on cross-domain CTR prediction can be broadly categorized into joint learning methods [14, 16, 17] and pre-training & fine-tuning methods [1]. The former jointly optimizes the source domain and the target domain objectives. Shared model parameters like user/item embeddings establish the connection between different domains. However, such methods are inefficient to deploy online because large-scale source domain data is required for training. Besides, due to potential conflicts of different objectives [1, 26], joint training could induce negative impacts on the optimization of target domain models. Alternatively, pre-training & fine-tuning methods replace the target domain parameters with pre-trained source domain parameters and then fine-tune the target domain model. Since only target domain data and objective are used, pretraining & fine-tuning methods are more efficient and effective. In most real-world RSs, CTR prediction models are trained continually with user feedback data, such that the latest user interests can be captured in time. In view of this, it is necessary to study cross-domain CTR prediction under the setting of continual learning, named Continual Transfer Learning (CTL) . For this task, a straightforward solution is to use pre-training & fine-tuning methods every time the source domain model is updated. However, well-trained parameters of the target domain model are directly discarded and replaced by that of the source domain model. To recapture the lost historical knowledge, we need large-scale historical target domain data for fine-tuning, which makes CTL impractical. To better solve this problem, we propose a simple and effective model called Continual Transfer Network (CTNet) . The core idea is to preserve all the continually well-trained source domain and target domain parameters during CTL. To achieve this, we treat the latest source domain representations as external knowledge WWW'23 Companion, April 30-May 4, 2023, Austin, TX, USA Liu and Wang, et al. for target domain CTR prediction and continually train the target domain parameters. As shown in Figure 1, CTNet consists of a source tower, a target tower, and light-weighted adapters. The source tower is consistently initialized by the latest source domain model, while the target tower is continually trained with new target domain data. Adapters project the extracted source domain knowledge, so that it can be used by the target tower. Since CTNet preserves all the valuable well-trained target domain parameters, it only needs incremental target domain data to realize efficient CTL. To sum up, the main contributions of this work include: ¬∑ We study an important but less-studied problem - continual transfer learning (CTL), which can be applied to several real-world industrial applications. ¬∑ We present CTNet, an effective and efficient CTL model for continual cross-domain CTR prediction. We provide useful industrial experience in deploying CTNet in large-scale RSs. ¬∑ We conduct extensive offline and online experiments. CTNet shows high efficiency and superior performance compared with the SOTA methods. Since Dec. 2021, it has been fully deployed online at Taobao bringing significant improvements.",
  "2 METHODS": "",
  "2.1 Single-Domain Models at Taobao": "The large-scale e-commerce platform Taobao has multiple recommendation domains, and each of them maintains its own CTR prediction model. Each model aims to predict the probability of a user clicking on an item with the input of user-item features such as user features, item features, and cross features. Specifically, these features can be summarized into categorical features like user ID, user gender, item ID, category ID, etc, and numerical features like user's or item's historical statistics. Numerical features are pre-processed with discretization [8] to be categorical. In practice, these features are transformed into dense embeddings via the embedding lookup to be concatenated as the embedding e FEAT . Users' historical behavior sequences within all domains are introduced, which has been demonstrated crucial for CTR prediction [30]. Concretely, target attention (TA) [4, 30], and its variants SIM (hard) [18] and ETA [2] for long-term sequences are adopted to model users' behavior sequences. Finally, the feature embedding and the attention layer output are concatenated as e = [ e FEAT , e TA , e SIM , e ETA ] to be fed into the MLP-based CTR prediction model.",
  "2.2 CTNet: Continual Transfer Network": "Conventionally, during a time period ùë° ‚Üí ùë° +1, each single-domain model is independently trained with new user feedback data via offline incremental learning or online learning. In this work, we propose to perform continual transfer learning (CTL) and design a CTR prediction model called CTNet under the setting of CTL. Definition 1. Continual Transfer Learning (CTL) : Given a time-evolving source domain {D ùëÜ ùë° } ùëá ùë° = 1 and a time-evolving target domain {D ùë° } ùëá ùë° = 1 , continual transfer learning aims to improve prediction performance on target domain D ùëá + 1 using the historical and real-time knowledge from both the source and the target domains. Figure 1: Architecture of the proposed CTNet. Hidden Layer Hidden Layer Hidden Layer Hidden Layer Hidden Layer Hidden Layer CTR Score Adapter Adapter Adapter Input Features Input Features Source Tower Target Tower Trainable g 1 g 2 W S 1 W S 2 W S 3 W 3 W 2 W 1 z 0 z 1 z 2 z S 2 z S 1 z S 0 e S e e S FEAT e FEAT e TA e S TA e S SIM e SIM e ETA e S ETA g 0 Model Architecture of CTNet. CTNet extends the previous target domain model to be a two-tower architecture, including a source tower and a target tower. The two towers are connected by lightweighted adapters. Figure 1 shows an illustration of CTNet. Specifically, the source tower has the same model architecture as the source domain model, and the target tower has the same architecture as the previous target domain model. Every time we train the CTNet, all the parameters of the latest source domain model, including embedding layers, attention layers, and MLP layers, are built into the computational graph of the source tower. Since we use source domain knowledge as external information, parameters of the source tower are kept frozen during backpropagation. Light-weighted adapters are used as layer-wise connections of the two towers to enable domain adaptation. Each adapter ùëî (¬∑) projects the hidden representations of source tower to be external knowledge for the target tower. Formally, the hidden representation z ùëô output by the ùëô -th layer of target tower is computed by,   where ùúì denotes the activation function, W ùëô is the trainable weight matrix of the ùëô -th layer of the target tower, and z ùëÜ ùëô is the hidden representation output by the ùëô -th layer of source tower. We implement each adapter with a gated linear unit (GLU) [3] to adaptively control the information flow from the source tower to the target tower,  where ùúé denotes the sigmoid activation function, ‚äô denotes elementwise vector multiplication, and U 1 ùëô and U 2 ùëô are trainable matrices. Weuse the output from the target tower for prediction. The training objective is a binary cross-entropy loss with clicked samples as positives and viewed but not clicked samples as negatives.",
  "2.3 Deployment of CTNet": "In this section, we provide the experience of how to deploy CTNet. To facilitate understanding, we illustrate single-domain continual learning and cross-domain continual learning in Figure 2. Before deploying CTNet, each single-domain model is independently and Continual Transfer Learning for Cross-Domain Click-Through Rate Prediction at Taobao WWW'23 Companion, April 30-May 4, 2023, Austin, TX, USA continually trained with new feedback data. When we deploy CTNet at the time step ùë° , its target tower is initialized by the latest target domain model. More details in CTNet include, Two-towermodeldesignforderivingfine-grainedandrealtimesourcedomainknowledge. Touse the source domain knowledge, we can cache the source domain feature embeddings. However, such fixed knowledge cannot well fit the time-evolving target domain user behaviors and does not encode fine-grained knowledge of user-item interactions. Instead of caching embeddings, we present the entire online two-tower architecture, which enables to model real-time user behaviors with source domain parameters. This design will not bring extra inference latency with parallel computing, since the source domain model is lighter and runs faster than the target domain model 1 . Warm start for better preserving the well-trained target domain parameters. We initialize the target tower of CTNet with previous target domain model. First, the adapters integrate source domain hidden representations into target domain with addition operations instead of concatenation, so that the target tower and the previous target domain model have the same architecture. Second, the previous target domain model is highly optimized with billions of data to converge into a local optimum. Therefore, significant change of parameters makes the model deviate from this optimal solution thus hurting the performance. Therefore, we initialize the parameters of adapters with very small values, such that the initial output of the target tower is almost identical to that of the well-trained target domain model. Special designs for time efficiency. Weuselayer-wise adapters rather than cross-layer adapters, since this design is convenient for achieving parallel computing. To further reduce the computing cost, behavior sequence-based features are shared by both source tower and target tower during training and inference 2 . We observe that CTNet does not lead to extra online response time compared with previous target domain production models.",
  "3 EXPERIMENTS": "",
  "3.1 Experimental Details": "Datasets. Considering that there exists no suitable public benchmarks for evaluating continual cross-domain CTR prediction, we evaluate our approach on Taobao production data. Three differentsized recommendation domains - A, B, and C are used for evaluating models' transfer performances from A to B, and A to C. Domains share some users and items, whereas domain A has covered most users and items in Taobao. We collect and sample traffic logs of these domains to get user-item interaction data within 31 days. Data from the last day is used for testing, and data from the other days is used for training. We organize datasets into periods according to the time of interactions. Each period lasts for 6 days so each dataset is split into 5 periods. The sizes of datasets in domains A, B, and C are about 150 billion, 2 billion, and 1 billion, respectively. 1 Source domain model has fewer input features than target domain model. 2 User behavior sequences encoded by TA, SIM, and ETA are from all domains in Taobao. So we can set ( e TA , e SIM , e ETA ) = ( e S TA , e S SIM , e S ETA ) . TA, SIM, and ETA modules consume the most online computing costs in our production models. Setting ( e TA , e SIM , e ETA ) = ( e S TA , e S SIM , e S ETA ) will not bring negative impacts to model performances but reduce most extra computing costs. Figure 2: An illustration of CTNet deployment. Target domain model Source domain model Source domain model Target domain model Source domain model CTNet Source domain model CTNet Knowledge Transfer Knowledge Transfer √â √â √â √â t t+1 t-2 t-1 Single-Domain Continual Learning Cross-Domain Continual Learning Baselines. Various methods are compared, including pre-training & fine-tuning methods like Finetune (Embeddings) 3 , Finetune (All) 4 , and Extra Embedding 5 [28], and joint learning methods including MLP++ [10], Share Bottom [16], PLE [21], MiNet [17], DDTCDR [14], and DASL [13]. We also report the results of singledomain models including Base (w/o Transfer) and Source Model . Settings. All experiments are conducted on production models to ensure the fairness of the experiments. All the single-domain models have been continually trained on Taobao data for 3 years. For all the methods, we set the batch size to 512 and the hidden dimension of the 3-layer MLP to [ 1024 , 512 , 256 ] . Multi-head target attention with 8 heads and 256 hidden units is used in sequence modeling. The sub-sequence length of SIM and ETA is set to be 64. AdaGrad optimizer with an initial learning rate of 0 . 01 is used for optimization. Following previous work in CTR prediction, we apply AUC and Group AUC (GAUC) [31] for evaluation. GAUC is user group AUC weighted by the impressions of each user, which has been demonstrated to be consistent with online performance [30].",
  "3.2 Offline Experimental Results": "As shown in Table 1, CTNet outperforms all the baselines on the large-scale industrial dataset. More specifically, (1) CTNet performs better than all the single-domain models, which demonstrates the necessity of cross-domain CTR prediction. (2) Compared with pre-training & fine-tuning methods, CTNet re-uses all the well-trained target domain parameters to minimize the loss of information. Notably, Finetune (All) discards all the welltrained target domain parameters thereby performing worse than Finetune (Embeddings), which further demonstrates the importance of preserving well-trained target domain parameters. (3) CTNet outperforms Extra Embedding as CTNet captures more fine-grained and real-time source domain knowledge. (4) All the joint learning methods have relatively weak performance on the Taobao industrial dataset. These approaches transfer knowledge mainly by updating low-level features such as user/item embeddings, while the high-level representations of user-item interactions are ignored. Additionally, joint learning methods only leverage the limited 30-day source domain training data without 3 Update the target domain embeddings with pre-trained source embeddings and fine-tune the model on the incremental target domain data. 4 Fine-tune the entire source domain model on the incremental target domain data. 5 Embeddings from the latest source domain model are cached and added into the target domain model as auxiliary features. This method is similar to the online version of KEEP [28]. WWW'23 Companion, April 30-May 4, 2023, Austin, TX, USA Liu and Wang, et al. Table 1: Offline model evaluation results. inheriting the power of the pre-trained source domain model. Conversely, CTNet uses long-term knowledge from both the source domain and the target domain. (5) We conduct an ablation study to show that GLU-based adapters perform better than linear layer-based adapters (CTNet (w/o GLU)), verifying the effectiveness of feature selection of gated units. (6) We show the performance of CTNet under two different settings: continual transfer and one-time transfer. As shown in Table 2, continual transfer is consistently better than one-time transfer. And the performance gain of one-time transfer drops with time. That indicates the necessity of CTL.",
  "3.3 Production Deployment at Taobao": "We deploy CTNet on two large-scale RSs at Taobao (domain B and domain C). Compared with our previous production models, CTNet respectively yielded 1.0% and 3.6% GAUC improvement in domain B and C (as is shown in Table 1). Although domain B and C are smaller than domain A, they still have very large traffic with hundreds of millions of active users. For the production models, a gain of 0.1% on GAUC is considered a significant improvement. Therefore, the performance gains from CTNet are remarkable. Wealso observe significant performance gains of CTNet through online A/B testing. CTNet derives a 2.5% CTR gain and 7.7% Gross Merchandise Volume (GMV) gain in domain B, meanwhile derives 12.3% CTR gain and 31.9% GMV gain in domain C. Compared with previous production models, the offline training and online inference time does not increase with the same computational resources. Since December 2021, CTNet has been deployed fully online and serves the main traffic at Taobao.",
  "4 RELATED WORK": "Click-Through Rate (CTR) Prediction. In the era of deep learning, a variety of powerful models are proposed for CTR prediction [2, 4, 5, 9, 18, 24, 27, 29, 30]. This work studies cross-domain CTR prediction [14, 17] which improves the performance of CTR prediction by transferring knowledge from large domains to small domains. A concurrent work KEEP [28] shares similar architectures to CTNet. The main differences between KEEP and CTNet include: Table 2: The comparisons between one-time transfer and continual transfer. (1) KEEP pre-trains a concise model with data from source domain to extract the source domain knowledge. Instead, CTNet extracts knowledge directly based on the existing source domain model, so CTNet saves computation and storage resources of pre-training and can extract more expressive knowledge from the well-trained source domain model. (2) KEEP requests the cached source domain user-level features, item-level features, and coarse-grained interaction features (i.e., user-category interaction features) for online inference. Since the knowledge extraction and the online inference of KEEP are separated, a synchronization strategy is required to guarantee the consistency between them. Conversely, CTNet builds the parameters of the latest source domain model into the computational graph. Thus, CTNet does not cache the source domain knowledge, thereby avoiding extra synchronization strategy. To sum up, the deployment of CTNet is easier than KEEP, and CTNet will not bring extra online latency compared to the previous production model. Moreover, the online CTNet can extract the fine-grained and real-time knowledge of source domain user-item interactions. Continual Learning. Continual learning [6, 12, 19], also called lifelong learning, aims to learn from an infinite stream of data to gradually extend acquired knowledge for future learning [12]. Studies in this field mainly focus on solving the catastrophic forgetting problem, i.e., performance on old tasks significantly degrades with the training of new tasks. However, we do not focus on this problem, since RSs are designed to predict users' future interests. Continual learning for the cross-domain setting has not been fully studied in both academia and industry. Recent attempts such as continual pre-training [11, 20] and continual domain adaptation [7, 15, 22, 23, 25] seem similar to our work. However, these attempts set only one of the source domain and the target domain to be timeevolving. Conversely, both domains are time-evolving in continual transfer learning (CTL) described in this paper.",
  "5 CONCLUSIONS": "In this paper, we address the problem of cross-domain CTR prediction at Taobao, and present CTNet for the task. CTNet preserves all the valuable well-trained parameters, thereby only needing incremental data to realize efficient continual transfer learning. We deploy CTNet in recommendation systems at Taobao and witness significant improvements over the existing solutions in terms of system performance and online business metrics. WWW'23 Companion, April 30-May 4, 2023, Austin, TX, USA Continual Transfer Learning for Cross-Domain Click-Through Rate Prediction at Taobao",
  "REFERENCES": "[1] Lei Chen, Fajie Yuan, Jiaxi Yang, Xiangnan He, Chengming Li, and Min Yang. 2023. User-specific Adaptive Fine-tuning for Cross-domain Recommendations. IEEE Transactions on Knowledge and Data Engineering (2023). [2] Qiwei Chen, Yue Xu, Changhua Pei, Shanshan Lv, Tao Zhuang, and Junfeng Ge. 2022. Efficient Long Sequential User Data Modeling for Click-Through Rate Prediction. In 4th Workshop on Deep Learning Practice and Theory for HighDimensional Sparse and Imbalanced Data (DLP-KDD) . [3] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier. 2016. Language Modeling with Gated Convolutional Networks. In Proceedings of the 34th International Conference on Machine Learning (ICML) . [4] Chen et al. 2020. Behavior sequence transformer for e-commerce recommendation in alibaba. In International Workshop on Deep Learning Practice for HighDimensional Sparse Data (DLP-KDD) . [5] Heng-Tze Cheng et al. 2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS) . arXiv:1606.07792 [6] J. Kirkpatrick et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences (PNAS) (2017). [7] Rui Gong, Wen Li, Yuhua Chen, and Luc Van Gool. 2019. Dlow: Domain flow for adaptation and generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . [8] H. Guo, B. Chen, R. Tang, Z. Li, and X. He. 2021. AutoDis: Automatic Discretization for Embedding Numerical Features in CTR Prediction. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) . [9] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In International Joint Conference on Artificial Intelligence (IJCAI) . [10] Guangneng Hu, Yu Zhang, and Qiang Yang. 2018. Conet: Collaborative cross networks for cross-domain recommendation. In Proceedings of the ACM International Conference on Information & Knowledge Management (CIKM) . [11] Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, and Xiang Ren. 2021. Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora. arXiv preprint arXiv:2110.08534 (2021). [12] M. Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, and A. et al. Leonardis. 2021. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021). [13] Pan Li, Zhichao Jiang, Maofei Que, Yao Hu, and Alexander Tuzhilin. 2021. Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) . [14] Pan Li and Alexander Tuzhilin. 2020. Ddtcdr: Deep dual transfer cross domain recommendation. In Proceedings of the International Conference on Web Search and Data Mining (ICDM) . [15] Hong Liu, Mingsheng Long, Jianmin Wang, and Yu Wang. 2020. Learning to Adapt to Evolving Domains.. In Advances in Neural Information Processing Systems (NeurIPS) . [16] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD International Conference on",
  "keywords_parsed": [
    "Continual Transfer Learning",
    "CTR Prediction",
    "Cross-Domain"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "User-specific Adaptive Fine-tuning for Cross-domain Recommendations"
    },
    {
      "ref_id": "b2",
      "title": "Efficient Long Sequential User Data Modeling for Click-Through Rate Prediction"
    },
    {
      "ref_id": "b3",
      "title": "Language Modeling with Gated Convolutional Networks"
    },
    {
      "ref_id": "b4",
      "title": "Behavior sequence transformer for e-commerce recommendation in alibaba"
    },
    {
      "ref_id": "b5",
      "title": "Wide & Deep Learning for Recommender Systems"
    },
    {
      "ref_id": "b6",
      "title": "Overcoming catastrophic forgetting in neural networks"
    },
    {
      "ref_id": "b7",
      "title": "Dlow: Domain flow for adaptation and generalization"
    },
    {
      "ref_id": "b8",
      "title": "AutoDis: Automatic Discretization for Embedding Numerical Features in CTR Prediction"
    },
    {
      "ref_id": "b9",
      "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction"
    },
    {
      "ref_id": "b10",
      "title": "Conet: Collaborative cross networks for cross-domain recommendation"
    },
    {
      "ref_id": "b11",
      "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora"
    },
    {
      "ref_id": "b12",
      "title": "A continual learning survey: Defying forgetting in classification tasks"
    },
    {
      "ref_id": "b13",
      "title": "Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction"
    },
    {
      "ref_id": "b14",
      "title": "Ddtcdr: Deep dual transfer cross domain recommendation"
    },
    {
      "ref_id": "b15",
      "title": "Learning to Adapt to Evolving Domains"
    },
    {
      "ref_id": "b16",
      "title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts"
    }
  ]
}