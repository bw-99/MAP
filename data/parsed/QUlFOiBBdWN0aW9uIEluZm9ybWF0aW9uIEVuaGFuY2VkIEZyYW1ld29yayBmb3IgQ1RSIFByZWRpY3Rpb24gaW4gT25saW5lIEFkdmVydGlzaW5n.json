{
  "AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising": "",
  "Yang Yang âˆ—": "yangyang590@huawei.com Huawei Noah's Ark Lab China Bo Chen âˆ— chenbo116@huawei.com Huawei Noah's Ark Lab China Chenxu Zhu zhuchenxu1@huawei.com Huawei Noah's Ark Lab China Menghui Zhu zhumenghui1@huawei.com Huawei Noah's Ark Lab China Muyu Zhang zhangmuyu@huawei.com Huawei Technologies Co Ltd China Xinyi Dai daixinyi5@huawei.com Huawei Noah's Ark Lab China",
  "Zhenhua Dong": "dongzhenhua@huawei.com Huawei Noah's Ark Lab China",
  "ABSTRACT": "Huifeng Guo huifeng.guo@huawei.com Huawei Noah's Ark Lab China Ruiming Tang â€  tangruiming@huawei.com Huawei Noah Ark's Lab China",
  "KEYWORDS": "Click-Through Rate (CTR) prediction is a fundamental technique for online advertising recommendation and the complex online competitive auction process also brings many difficulties to CTR optimization. Recent studies have shown that introducing posterior auction information contributes to the performance of CTR prediction. However, existing work doesn't fully capitalize on the benefits of auction information and overlooks the data bias brought by the auction, leading to biased and suboptimal results. To address these limitations, we propose A uction I nformation E nhanced Framework (AIE) for CTR prediction in online advertising, which delves into the problem of insufficient utilization of auction signals and first reveals the auction bias. Specifically, AIE introduces two pluggable modules, namely Adaptive Market-price Auxiliary Module (AM2) and Bid Calibration Module (BCM), which work collaboratively to excavate the posterior auction signals better and enhance the performance of CTR prediction. Furthermore, the two proposed modules are lightweight, model-agnostic, and friendly to inference latency. Extensive experiments are conducted on a public dataset and an industrial dataset to demonstrate the effectiveness and compatibility of AIE. Besides, a one-month online A/B test in a large-scale advertising platform shows that AIE improves the base model by 5.76% and 2.44% in terms of eCPM and CTR, respectively.",
  "CCS CONCEPTS": "",
  "Â· Information systems â†’ Retrieval models and ranking .": "âˆ— Co-first authors with equal contributions. â€  Corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys '24, October 14-18, 2024, Bari, Italy Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0505-2/24/10...$15.00 https://doi.org/10.1145/3640457.3688136 Online Advertising, Click-through Rate Prediction, Posterior Feature Modeling",
  "ACMReference Format:": "Yang Yang, Bo Chen, Chenxu Zhu, Menghui Zhu, Xinyi Dai, Huifeng Guo, Muyu Zhang, Zhenhua Dong, and Ruiming Tang. 2024. AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising. In 18th ACM Conference on Recommender Systems (RecSys '24), October 14-18, 2024, Bari, Italy. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3640457.3688136",
  "1 INTRODUCTION": "Click-Through Rate (CTR) prediction [22, 37] holds a vital place in online advertising systems since CTR prediction performance directly influences the overall satisfaction of the users and the revenue generated by companies. The CTR is the probability that a user clicks the ad when it is shown. Advertisers submit cost-perclick (CPC) bids [10, 42] to show how much they are willing to pay if a user clicks. The common practice for advertisement platforms is combining predicted click-through rates (pCTR) and CPC bid to calculate effective cost per mille (eCPM) for rank to maximize the expected revenue [5, 18]. In this process, CTR prediction is very important as it directly influences the auction's outcomes. However, the complicated auction environment in online advertising systems poses great challenges to CTR prediction tasks. Firstly, the ranking criteria eCPM is determined by pCTR and CPC bid [12, 42], which means CPC bid will also directly determine the final ranking results. The CPC bid changes over time and it influences the ranking results and user clicks. However, when we model CTR, this information is not explicitly considered, which leads to a biased estimation. Secondly, the auction environment is highly dynamic with many third-party demand-side platforms (DSPs) participating in the auction competition, causing fluctuations and instability in advertisement auction results. RecSys '24, October 14-18, 2024, Bari, Italy Yang Yang et al. Typically, CTR prediction models highly rely on the offline training data collected and lack the ability to perceive bidding information and auction environment. Therefore, it is common in realworld industrial applications that a CTR model shows high accuracy offline but exhibits no significant improvement online. (a) CTR-Market Price relationship 0.0 0.2 0.4 0.6 0.8 1.0 Normalized Market Price 0.00 0.02 0.04 0.06 0.08 0.10 0.12 CTR Industry 1 Industry 2 Industry 3 Industry 4 \u0000 \u0000 \u0000 \u0000 Normalized Market Price 0 \u0000 (b) Market Price Distribution Figure 1: (a) CTRs against different market prices under four industries in an industrial dataset. (b) Market price distribution under four industries in an industrial dataset. To model the highly dynamic auction environment and mitigate the offline-online inconsistency, we need to take the auction environment and the bidding information into consideration in the phase of CTR prediction. CTR prediction and the market price modeling tasks have strong inherent interdependence [35]. Figure 1 (a) shows the CTR is positively correlated with the market price under four industries in our industrial dataset. Some work[35] considers introducing market price in the CTR prediction phase but lacks fine-grained modeling to capture the variance of market price distributions under different scenarios, leading to sub-optimal results. The distribution of market prices varies significantly across different scenarios as shown in Figure 1 (b). We depict the market price distribution for the four selected industries in a real-world advertising platform, where the vertical axis divides scenarios and the horizontal axis shows the market prices' relative values. Therefore, how to exploit the auction signals including market price and features that distinguish the market environment to improve CTR prediction is a vital issue, which we call it auction signals utilization problem . Recommender systems are always subject to various biases, including popularity bias [41, 43], position bias [26, 30], selection bias [31, 32] and so on. In the context of advertisement recommendations, CTR prediction is facing a certain bias brought by the auction. Suppose some advertisers bid very high for their ads, which causes these ads can win the auction more easily even if the respective click relevance of them are lower than other ads. Although these ads gained more exposure, they are recommended to less relevant users who are not inclined to click. The over-exposure leads to a lower CTR for those ads with high bids. These samples flow into our training data, causing the distribution of CTR of the ads with high bids to be shifted and biased. We call this kind of bias in training data as auction bias . Take Figure 2 as an example. Suppose we have a CTR model that can give basically accurate CTR predictions, it outputs three pCTRs 0.9, 0.5, and 0.1 under three different contexts for an advertisement. Here ğ‘’ğ¶ğ‘ƒğ‘€ = ğ‘ğ‘–ğ‘‘ âˆ— ğ‘ğ¶ğ‘‡ğ‘… and suppose the second highest eCPM from other competing DSPs Figure 2: Auction Bias Illustration. When the advertiser gives a normal bid for an ad as Figure (a) shows, the ad only wins the auction in Context 1 due to the high click relevance. When the advertiser gives a high bid for the same ad as Figure (b) shows, the ad wins more auctions in all three Contexts though the click relevance is low for Context 2 and 3. Therefore, the training data distribution is different from the original target distribution, leading to data bias, which is called auction bias due to high bidding. CTR Model Context 1 Context 2 Context 3 pCTR1 =0.9 pCTR2 =0.5 pCTR3 =0.1 (a) Normal Bids Case eCPM1 =0.9 eCPM2=0.5 eCPM3 =0.1 win lose lose (b) High Bids Case Context 1 Context 2 Context 3 eCPM1 =9 eCPM2=5 eCPM3 =1 Advertiser bid $1 bid $10 Suppose the second highest eCPM = 0.6 win win win Dataset Dataset Training Distribution Target Distribution for all contexts is 0.6. When the advertiser gives a normal bid as $1 for this ad, it can win the auction only under context 1 as illustrated in Figure 2 (a). However, when the advertiser gives a high bid as $10 for this ad, it can win under all three contexts and thus gain some unpreferable exposure since the click relevance for context 2 and context 3 are relatively low as Figure 2 (b) shows. This leads to the CTR of this ad in our collected data being lower than the unbiased situation as only exposure data can be collected in advertising recommendations. It is worth noting that the phenomenon of advertisers increasing their bids to gain exposure is widespread. Therefore, auction bias exists widely in the training data in the advertisement recommendation scenario, and dealing with it is meaningful and significant. To address the auction signals utilization problem and the auction bias mentioned above, we propose a novel framework named Auction Information Enhanced Framework for CTR prediction in online advertising (AIE) which is composed of two modules Adaptive Market-price Auxilary Module (AM2) and Bid Calibration Module (BCM). AM2 constructs an auxiliary task to make use of the market price with a dynamic network to capture the variance of market price across different scenarios. BCM alleviates the auction bias in our data by approaching the target distribution utilizing bid information reasonably. Our paper contributes to the literature in the following ways: Â· We pay attention to the challenges brought by the auction environment in online advertising systems and first reveal auction bias and its influence on CTR prediction. Â· We propose a novel framework AIE to take full advantage of the posterior auction signals and alleviate the auction bias. AIE can improve the performance of CTR prediction models with two lightweight and model-agnostic modules. Â· Comprehensive experiments on a public dataset and an industrial dataset validate the superiority of AIE. Results on a large-scale online advertising system further confirm the effectiveness and applicability of AIE. AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising RecSys '24, October 14-18, 2024, Bari, Italy",
  "2 METHOD": "In this section, we first describe the problem definition of the CTR prediction considering auction information under the advertisement recommendation scenario. Then we provide an overview of AIE, detail its key components and give a discussion about some interesting findings.",
  "2.1 Problem Formulation": "Considering a training dataset D = GLYPH<8> GLYPH<0> ğ’™ ğ‘— , ğ‘¦ ğ‘— GLYPH<1> GLYPH<9> | D| ğ‘— = 1 with |D| samples, where ğ’™ ğ‘— = { ğ‘ 1 , ...ğ‘ ğ‘– , ...ğ‘ ğ¼ } and ğ‘¦ ğ‘— represent the common features for CTRprediction and binary click label of the ğ‘— ğ‘¡â„ sample, respectively. The task of CTR prediction in the common online advertisement is to build a prediction model to estimate the probability of a user clicking a specific ad in a given context, which can be formulated as Ë† ğ‘¦ = ğ¶ğ‘‡ğ‘… _ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ ( ğ’™ ) where ğ’™ is the common feature about user, ad and context to predict relevance. However, CTR models normally ignore any posterior auction information and give predictions merely based on common features, causing severe problems as we can see in Figure 2. Hence we need to consider this posterior information in the training phase. Features ğ’‚ represents the posterior auction information including market price, CPC bid and so on. Features ğ’” represents scenario-related features that indicate auction environments. Auction information can only be used in offline training because they are posterior feature for CTR prediction models. Combined with the common feature ğ’™ , the task we defined here in advertisement recommendation can be formulated as Ë† ğ‘¦ = ğ¶ğ‘‡ğ‘… _ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ ( ğ’™ , ğ’‚ , ğ’” ) . Our goal is to enhance the CTR prediction model's performance by taking advantage of the extra auction information.",
  "2.2 Overview": "In this section, we present the overview architecture of AIE as illustrated in Figure 3. Since the auction information is posterior for the CTR prediction model, we only utilize it in the training phase and design two pluggable modules that do not take effect during the inference phase. The Adaptive Market-price Auxiliary Module (AM2) and the Bid Calibration Module (BCM) are deployed to leverage auction information to boost CTR prediction models' performance. The Adaptive Market-price Auxiliary Module(AM2) is deployed to let the model learn useful knowledge in auction information by a multi-objective structure.",
  "2.3 Adaptive Market-price Auxiliary Module": "The always-changing auction environment and the fierce competition in the market under advertisement recommendations pose great challenges to CTR prediction models. The rank eCPM can be influenced greatly by auction-related factors thus leading to results that fall short of expectations. A key problem we face in advertisement recommendation is how to utilize the various auction information to empower the CTR prediction model. Hereby, we propose AM2 to model the auction information at a fine-grained level. We observe the phenomenon that market price and the actual CTR are to some extent positively correlated in our industrial application as Fig 1(a) shows. By designing an auxiliary task to fit the market price as complementary to the main CTR prediction task based on a shared bottom structure, useful information can be learned through the shared embedding and the backbone model. Moreover, to capture the variance of market price distribution, the price prediction tower's weights and biases are generated by a dynamic weight network fed by scenario features that can differentiate auction environments [6, 36, 39]. Given the input of the price prediction tower h ( 0 ) , which is the last layer representation of the backbone model, the price prediction tower's MLP can be formulated as:   where ğœ is the activation function, ğ‘ is the depth of the MLP, E ğ‘  is the designated scenario embedding. The output of the last layer of the MLP h ( ğ‘ ) is Ë† yprice , which is the estimated value of the market price. In particular, the weight and bias parameters W ( k ) and b ( k ) are generated by the designated scenario embedding by reshaping and splitting. The number of layers and hidden units of each layer of the MLP are hyperparameters that need to be tuned. In our experience, a lightweight MLP is enough for the price prediction tower and the CTR prediction tower to gain a good overall performance. A visual illustration of this process is shown in the right part of Figure 3. After we get the estimated value of the market price, we compare it with the real value of the market price and calculate the regression loss, which can be expressed as:  Here, we use the MAE regression loss because it doesn't rely on prior knowledge like statistical bucketing of market prices and performs better than classification loss. Then, AIE can be jointly trained by optimizing the weighted sum of the ctr prediction loss and the price prediction loss. The final loss can be demonstrated as:  where ğ‘¤ is the hyperparameter that controls the importance of the price prediction auxiliary task. Because our goal is to enhance the CTR prediction's performance, the price prediction task's performance doesn't matter. So we tune the ğ‘¤ to ensure the ctr prediction task performs the best.",
  "2.4 Bid Calibration Module": "CTR prediction is one of the core algorithms in computational advertising. In the CPC billing model, the mechanism design can simply sort the advertisements by effective eCPM to maximize advertising revenue. CPC bids directly affect the final rank eCPM of the advertisement, thus affecting the advertisement's exposure. As illustrated in Figure 2, high bids given by advertisers can lead to more exposures, some of these are low-quality exposures and resulting in a lower CTR. Intuitively, we can reweight the training sample to calibrate the biased training distribution. Specifically, we assign higher weights for positive samples with higher CPC bids. Here we only reweight the positive samples because they can influence the positive ratios in training data and they are worth paying more attention to since users click on them. In this way, we raise the CTR in the training RecSys '24, October 14-18, 2024, Bari, Italy Yang Yang et al. Figure 3: Overall framework of AIE, which consists of two key modules: AM2 and BCM. AM2 uses market price and scenario features to construct an auxiliary task. BCM uses bid to impact the cross-entropy loss. Common Features (e.g., advertising id, user id, user behaviors, etc.) Scenario Features (e.g., advertising slot) ... Price Prediction Tower CTR Prediction Tower CPC bid Weights Backbone Model FC Layer FC Layer Sigmoid FC Layer FC Layer Reshape & Load Param Linear Activation Min-Max Scale Truncate BCM Weights & Bias AM2 data for those samples with high bids to approach the target distribution. There are always bids with very large values in some extreme cases and we need to clip the origin value to avoid the effects of extreme values on our training loss. Then the clipped values are transformed by Min-Max scaling [2] to a specific range as the final weighting factor. To control the prediction bias, the overall expectation of the positive ratio after reweighting remains the same as the original, which means some positive samples' weights with low bids would be decreased. The whole process can be divided into two steps. The first step, which is truncate can be formulated as: where ğ›¼ here is the final weighting term for each sample. By taking the CPC bids as auxiliary information into account, we calibrate the training CTR distribution towards the target distribution, thus making the CTR prediction more accurate. Our approach is more suitable for scenarios where advertisers do not adjust their bidding strategies frequently. In our industrial practice, advertisers adjust their bids daily, and our model can be updated on an hourly basis or even faster to make sure BCM captures the real-time auction information. More interesting perspectives on this method will be presented and discussed in Section 2.5.  where C represents the original CPC bid value at sample-level, ğ‘šğ‘–ğ‘› , ğ‘šğ‘ğ‘¥ means the statistical minimum value and maximum value of C after removing the outliers and C â€² represents the clipped CPC bid value. Afterward, C â€² needs to be transformed to a designated range, which can be expressed as:  where ğ‘ , ğ‘ means the lower and upper bound that specify the interval of the transformation and ğ›¼ means the reweighting factor we get. It's worth noting that in real industry applications, the bid's distribution varies under different scenes. In that case, we need to calculate ğ›¼ for each scene and perform fine-grained weighting. Then we take advantage of this additional information to reweight the positive samples, which can be reflected in the ctr loss as:",
  "2.5 Discussion": "Firstly, we would like to discuss the BCM's connections and differences with the previous debias methods like inverse-propensityscoring (IPS). IPS is a practical debias method for industry products, which can be regarded as a specific case of reweight learning. In our case, the propensity score can be defined as:  where ğ‘ƒ ( ğ‘¥,ğ‘¦ ) is the training distribution, ğ‘„ ( ğ‘¥,ğ‘¦ ) is the target distribution without the impact of the bidding-related factors. However, in advertisement recommendation, it's impossible to rule out the impact of bidding factors and observe the unbiased distribution. So here we deployed BCM to calibrate the training distribution as mentioned in Section 2.4. It's similar to the IPS format and the optimal function can be expressed as:  AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising RecSys '24, October 14-18, 2024, Bari, Italy where ğ‘§ ğ‘– = ( ğ‘¥ ğ‘– , ğ‘¦ ğ‘– ) is an observed sample drawn from training distribution, ğœƒ âˆˆ Î˜ is a model parameter, ğ‘™ ( ğ‘§ ğ‘– , ğœƒ ) is a loss function, ğ‘… ( ğœƒ ) is a regularizer. ğ‘ ğ‘– here is approximated by the inverse of the weighting term ğ›¼ for each sample. Secondly, we present another perspective on what BCM is doing. BCMraises the CTR for those positive samples with high bids. If we reckon that items with higher bids have higher value in the online advertising system, pCTR for those high-value items would also be raised. When competing with other third-party DSPs regarding high-value traffic, the platform would gain an advantage and have a better chance of winning. In this way, the revenue of the platform will be increased.",
  "3 EXPERIMENTS": "In this section, we present our experiments in detail, including experimental setup, model comparison, and the corresponding analysis. We conduct experiments on a public dataset and an industrial dataset to investigate the following questions: Â· RQ1: How much does our framework enhance the accuracy and revenue compared to competing methods? Â· RQ2: Is the proposed framework suitable for SOTA backbone CTR models? Â· RQ3: How effective are the two proposed modules (i.e., AM2 and BCM) for improving the performance? Â· RQ4: How are the AIE's training and inference efficiency?",
  "3.1 Experimental Setting": "3.1.1 Datasets. Â· iPinYou 1 The iPinYou dataset [16] is a real-world dataset for ad click logs over 10 days. After one-hot encoding, we get a dataset containing 19.50M instances with 937.67K input dimensions. We keep the original train/test splitting scheme, where for each advertiser the last 3-day data are used as the test dataset while the rest as the training dataset. We follow the previous data processing [21]. To the best of our knowledge, it is the most appropriate public dataset for AIE to valid its performance, which contains paying price (the highest bid from competitors, also called market price and auction winning price), bidding price (the bid price from iPinYou for this bid request) [40] information and enough other user, item features for CTR prediction. 3.1.2 Evaluation Metrics. Four evaluation metrics are tested in our experiments. The two major metrics are: Â· AUC Area under ROC curve is a widely used metric in evaluating classification problems. Specifically, a higher AUC value at the '0.001' level indicates significantly better performance [7]. Â· csAUC CPM-sensitive AUC (csAUC) [17] is a metric that takes bid into consideration in offline evaluation to better approach the online evaluation metric eCPM for advertising platforms. If a high-level sample ğ‘¥ â„ and a low-level sample ğ‘¥ ğ‘™ are randomly selected from dataset D . Given our revenue 1 https://contest.ipinyou.com/ of ( ğ‘¥ â„ , ğ‘¥ ğ‘™ ) as follows:   csAUC of dataset D is Ë ( ğ‘¥ â„ ,ğ‘¥ ğ‘™ ) âˆˆ ğ· ğ‘…ğ‘’ğ‘£ ( ğ‘¥ â„ ,ğ‘¥ ğ‘™ ) Ë ( ğ‘¥ â„ ,ğ‘¥ ğ‘™ ) âˆˆ ğ· ğ‘ğ‘–ğ‘‘ â„ . csAUC not only takes the ability of the model to distinguish between positive and negative samples into consideration but also evaluates if the model is able to rank the advertisement with a high value higher among the positive samples. Following the method proposed in [17], we can calculate csAUC on large-scale data in real-world data applications. Â· Rev We define a metric called Rev on iPinYou dataset to simulate the revenue that a prediction model would generate for each auction by ad slot granularity similar to the Rev defined in [33]. Because we only have exposure data, from which we need to simulate bidding and exposure process and then count revenue based on that. Following the previously proposed method [20], we group the testset logs by the following features: weekday , hour , slotid to simulate an auction. Then we rank each group's samples by the simulated eCPM ( ğ‘’ğ¶ğ‘ƒğ‘€ = ğ‘ğ¶ğ‘‡ğ‘… âˆ— ğ‘ğ‘–ğ‘‘ ). In the single-slot advertisement scene, only the advertisement with the highest eCPM wins the auction. If that top 1 ad was clicked in our test set, we would calculate the paying price , which can be formulated as:    where ğ‘” means one simulated auction across all groups ğº , ğ‘– means the item with the highest eCPM within that group ğ‘” , ğ‘…ğ‘’ğ‘£ ğ‘” means the simulated revenue for group ğ‘” for the top 1 ad recommendation. ğ‘…ğ‘’ğ‘£ is the sum of ğ‘…ğ‘’ğ‘£ ğ‘” across all groups in testset. This metric is a good representation of the platform's profit. Â· Rev NDCG Following the principles of the NDCG [15], we proposed Rev NDCG. Based on the Rev obtained, we divide it by the Maximum Possible Revenue gained by the platform to get Rev NDCG [33]. The Maximum Possible Revenue is defined as the sum of paying price of the clicked item with the highest paying price within each group, which can be formulated as:    where ğ‘…ğ‘’ğ‘£ ğ‘” ğ‘šğ‘ğ‘¥ is the Maximum Possible Revenue gained by the platform for one auction ğ‘” , ğ‘…ğ‘’ğ‘£ ğ‘šğ‘ğ‘¥ is the Maximum RecSys '24, October 14-18, 2024, Bari, Italy Yang Yang et al. Possible Revenue gained by the platform among all auction groups. 3.1.3 Competing Models. To verify the effectiveness of the proposed approach, we compare AIE with the following baselines. The backbone CTR prediction models we selected include DNN [25], DCN [28], DeepFM [7], AutoInt [23], FiBiNET [11], DCNv2 [11], DFFM [8], which includes the classical CTR prediction models that captures different orders of feature interactions like DCN, DeepFM and advanced multi-scenario model like DFFM. We also include Multi-task Advertising Estimator (MTAE) [35] as a competing model since it introduces market price information to jointly optimize CTR prediction. 3.1.4 Implementation Details. We use Adam [14] optimizer to optimize different models. For fair comparison, we fix the embedding size as 8, the batch size as 2000 for all models. The hyperparameters of the deep layer are tuned for each model and ensure that the capacity is about the same. For CTR prediction tower and price prediction tower's micro MLP, the number of layers is searched from 1 to 3, and neurons at each layer from {16, 32, 64}. The hyperparameter ğ‘¤ that controls the importance of the auxiliary price prediction loss is searched from 1e-5 to 1e-3. The lower bound ğ‘ for BCM is chosen from 0.5 to 1 and the upper bound is chosen from 1 to 2. The learning rate is searched from {1e-3,1e-4,1e-5,1e-6} and ğ¿ 2 regularization coefficient from {1e-4, 1e-5, 1e-6, 1e-7}. Besides, we run each experiment 5 times with the optimal parameters searched and report the average performance. Besides, RelaImpr [34] is applied to measure the relative improvement between the measured model and base model in terms of AUC and csAUC :  As for the Revenue metrics, the RelaImpr is defined as :  Thecodeis available based on MindSpore 2 . We thank MindSpore [1] for the partial support of this work.",
  "3.2 Overall Performance (RQ1&RQ2)": "This subsection gives an overall comparison between AIE and different baselines from the aspect of CTR prediction and revenue evaluation, whose results are depicted in Table 1. From this we can conclude that: Â· CTR prediction From Table 1,it can be observed that AIE achieves significant improvement over MTAE and different baseline models of AUC. AIE and MTAE can both be adapted to any baseline model. MTAE performs equal or better compared to the baseline models, which proves that the modeling of auction information is necessary and gainful. AIE consistently yields better performance in terms of AUC based on all backbone models. Specifically, AIE enhances the AUC of AutoInt the most with a RelaImpr of 2.01% and gains 1.01% RelaImpr on average for all base models. Compared to MTAE, the average RealImpr on 2 https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/AIE all base models increases from 0.32% to 1.01%, which is significant. This is due to our more fine-grained modeling of auction information. Â· Revenue Evaluation We can observe from Table 1 that the trend of csAUC is basically the same as AUC. MTAE beats the baseline in most cases, proving the effectiveness of setting of adding an auxiliary task to predict market price. However, MTAE's improvement is not stable on some baselines like DCN, which is probably because it adopts a multi-classification loss whose effect depends on the precise grasping of the way market prices are categorized. AIE adopts regression loss which can avoid bucketing the market price to multi-class by manual experience. AIE gains 1.00% RealImpr on csAUC since it achieves more comprehensive modeling of auction information. The presented Rev is divided by 1000 [33]. AIE performs the best for Rev and Rev NDCG on all backlines selected. The average RelaImprs of AIE on Rev and Rev NDCG are 3.95% and 3.97%, demonstrating a strong ability to optimize revenue. Â· Recommend Tendency Exploration To figure out what kind of ads AIE tends to recommend to users, we conduct an experiment. As Table 2 shown, we divide the advertisements in testset into three groups ranked by bidding price , where the top 1/3 are categorized as 'high', the lowest 1/3 as 'medium' and the remaining as 'low'. We choose DCN as the baseline and then calculate the proportion of ads that are being recommended for baseline and baseline+AIE. The following observations can be made: (i) the proportion of 'high' and 'medium' group items recommended to users significantly increases on average by AIE; (ii) in contrast, the proportion of 'low' group items recommended decreases by AIE. The overall revenue gain is positive because 'medium' and 'high' items are recommended more frequently and bring much more value to the platform. These results align with the intuition of the AIE, which pays more attention to the highbid samples at the training phase, thereby recommending highvalue items to more users. Combined with the AUC improvement illustrated in the CTR prediction , we can conclude that AIE not only enhances the sorting ability of CTR prediction model but also enables the CTR prediction model to perceive high-value traffic and thus improve the revenue. Â· Compatibility Analysis (RQ2) As Figure 1 shows, BCM and AM2 can be adapted to any CTR prediction models as plug-ins. We apply AIE to seven CTR prediction models, namely DNN [25], DCN [28], DeepFM [7], AutoInt [23], FiBiNET [11], DCNv2 [29], DFFM [8] on iPinYou dataset to test its transferability. As shown in Table 1, employing AIE to utilize auction information to assist the CTR prediction achieves much better performance on these seven models on iPinYou dataset. Therefore, we can conclude that AIE module has a strong compatibility with any CTR prediction models.",
  "3.3 Ablation Study (RQ3)": "WetakeAutoInt as the base model and verify the superiority of BCM and AM2 on this base model, respectively. The results are shown in Figure 4, from which we can draw the following conclusions. AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising RecSys '24, October 14-18, 2024, Bari, Italy Table 1: Overall performance comparison on different backbones on iPinYou dataset. Boldface denotes the highest score and * represents significance level ğ‘ -value < 0.05. Table 2: Distribution of items recommended to users across different groups of items divided by value on iPinYou testset. The basline here is DCN. Â· Comparing BCM with the base model, we can observe that it can improve the revenue significantly and improve prediction accuracy slightly. This phenomenon is consistent with our design intuition of BCM, which leverages bids to alleviate the auction bias and improve the prediction accuracy. The offline metrics including AUC and csAUC are not improved significantly because the testing data is also biased and hard to reflect the real effect. Â· AM2 achieved significant improvement in sorting ability compared to the base model, including the ability to rank high-value positive samples higher. The revenue increase of AM2 is also considerable, manifesting that the fine-grained modeling of extra auction information is profitable for increasing revenue. Â· BCM and AM2 both contribute substantially to the overall performance of AIE. AIE achieves the best performance over these variants, confirming that leveraging posterior auction information in online advertising can boost the performance of CTR prediction model.",
  "3.4 Application in Industry System": "The industrial dataset is collected from a large-scale industrial Advertisement System, which samples from user behavior logs in eight consecutive days. We select the first seven days as the training set, sample part of instances as the validation set, and collect the last day as the testing set. This dataset contains item features (e.g., Figure 4: Ablation study about different modules of AIE in terms of four metrics. Base +BCM +AM2 +AIE 0.770 0.772 0.774 0.776 0.778 0.780 0.782 AUC Base +BCM +AM2 +AIE 0.770 0.772 0.774 0.776 0.778 0.780 0.782 csAUC Base +BCM +AM2 +AIE 25.5 26.0 26.5 27.0 27.5 28.0 28.5 Rev Base +BCM +AM2 +AIE 0.122 0.124 0.126 0.128 0.130 0.132 0.134 0.136 RevNDCG creative ID, category), user features (e.g., user's behavior history), context features(e.g., slot ID) and auction information(e.g., market price, bid). The deploy scenario contains hundreds of sites and mobile applications, where millions of daily active users interact with ads and tens of millions of user logs are generated every day. 3.4.1 Offline Industrial Experiments. Our framework is conducted on an offline industrial dataset, which is a large-scale dataset sampled from the click logs of the online advertisement platform and has more than 600 million impressions. We split them into train/valid/test sets by timestamp with a 6:1:1 proportion. Apart from common features ğ’™ as defined in Section 2.1, we also collected auction information ğ’‚ including market price and CPC bid at sample level in our offline training data. In our scenario, the platform will bid and adjust bids on behalf of the advertiser based on pctr or pcvr, which leads to the fact that the bid can not be used during CTR prediction. The market price is only available after the auction stage, which is more posterior. Considering that, the bid and market price can RecSys '24, October 14-18, 2024, Bari, Italy Yang Yang et al. not directly used as a feature for CTR prediction in most online ads recommendation system. The scenario features ğ’” can be multiple features that reflect the auction environment like slot ID, app category, hour of day and so on. Weuse AUC and csAUC here to evaluate the offline performance of AIE. The baseline we compared with includes FiBiNET [11], DCN [28], EDCN [4], DFFM [8] and HierRec [6]. The performance concerning revenue is evaluated on an online A/B test and will be presented later. From Table 3, we summarize the observations from two dimensions. From the horizontal dimension of Table 3, we can see that scenario-aware dynamic network is useful as DFFM and HierRec perform well. From the vertical dimension of Table 3, the conclusion that AIE outperforms each baseline significantly can be easily drawn. The improvement in AUC shows AIE enhances the CTR model's sorting ability to distinguish the positive and negative samples. Moreover, AIE empowers the CTR model to better perceive high-value positive samples which can be reflected by the increase in csAUC. Table 3: Overall performance comparison on the industrial dataset. 3.4.2 Online Industrial A/B Test. To evaluate the performance of our framework (i.e., AM2 and BCM) in the real industry application, we conduct an online A/B test in our online advertising platform for one month. AM2 was implemented in the first two weeks. Based on that, we added BCM and the complete framework AIE was deployed in the last two weeks. The compared baseline is a highly optimized CTR model. Each model is trained over the latest exposure log, where an identical data process procedure is performed to ensure comparability. For online serving, all the three models (Base, AM2, AIE) are allocated 5% of the overall traffic. Our models perform streaming incremental training hourly to capture real-time changes in auction information. We compare the performance according to four metrics: RPM (Revenue Per Mille), eCPM (effective Cost Per Mille), CTR (Click Through Rate), and predicted bias, which are all widely used metrics for online advertising. Among them, CTR measures the relevance between the user and the ad. RPM and eCPM are the core metrics for an advertising platform to measure revenue. The predicted bias is calculated by ( ğ‘ğ¶ğ‘‡ğ‘… -ğ¶ğ‘‡ğ‘… ) ğ¶ğ‘‡ğ‘… Ã— 100%, which needs to be kept within reasonable limits because excessive bias will infringe on advertisers' interests. Table 4 shows the results of the three models, among which AIE achieves 6.14% improvements in RPM, 5.76% improvements in eCPM and 2.44% improvements in CTR while the predicted bias decreased by 5.38%. The full volume of AIE went from 5%, 10%, 20%, 50% to 100% of the flow, with each step being observed for 2-3 days and within each observation interval the enhancement of AIE is confident. In Table 4, the confidence intervals for the improvement rate of AM2 and AIE regarding eCPM are [2.36%, 5.14%] and [4.75%, 6.51%]. Both the upper and lower bounds of the confidence interval are positive, indicating that the experimental results are statistically significant. These results sufficiently validate that our framework can enhance the CTR prediction model's accuracy and revenue-optimizationoriented capabilities. Furthermore, the predicted bias of AIE decreased greatly compared to the base model, which proves the effectiveness of BCM in alleviating auction bias. The AM2 also outperforms the base model 1.83%, 3.94%, 1.21% on RPM, eCPM, CTR respectively, proving the effectiveness of the elaborated way to use auction information. After one month of evaluation, the AIE has become the main model in this scenario to carry all of the online traffic. Table 4: Online A/B testing results of AM2 and BCM modules compared to the base model.",
  "3.5 Efficiency Analysis (RQ4)": "In practical applications, the inference efficiency of the CTR prediction model is important since the recommender system has a high demand for real-time response. The training efficiency is also important because it affects how long it takes to update our model. Therefore, to answer RQ4, this subsection presents a comparison of training and inference time between AIE and other baselines on the industrial dataset, whose results are summarized in Table 5. Based on the results, it can be concluded that AIE's training time is essentially comparable to the baseline. Due to the lightweight design of the CTR prediction tower and the price prediction tower, the increase in training time is negligible compared to the base figure. Likewise, the inference time barely grows, thanks to the plug-in design of the AIE whose two auxiliary modules do not take effect at the inference phase. Table 5: Training time and inference time (whole test set) comparison on industrial dataset",
  "4 RELATED WORK": "Our proposed AIE framework utilizes posterior auction information to enhance the CTR prediction model's performance. Therefore we would provide a brief overview of the literature related to the following two aspects. The related work of the methods involved like multi-task and multi-scenario learning will be covered briefly. AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising RecSys '24, October 14-18, 2024, Bari, Italy CTR prediction CTR prediction models learn the user and item's relevance and hold a crucial place in recommender systems. Due to the importance of feature co-occurrence relation, feature interaction is vital to perform accurate CTR prediction. DeepFM [7] uses a FM component and places it parallel with the DNN to model feature interactions. Deep & Cross Network (DCN) [28] captures different orders' feature interaction by deploying layerwise feature crossing recursively and DCN V2 [29] upgrades the feature crossing vector to a matrix for enhancing representing ability. EDCN [4] further enhances the performance by facilitating the information sharing between the parallel structures. FiBiNET [11] combines the attention mechanism based on SENET [9] with a bi-linear interaction layer to dynamically learn feature weights and achieves fine-grained feature interactions. AutoInt [23] achieves superior performance and good interpretability with a self-attention architecture to learn feature interactions. Besides, multi-scenario recommendations [13, 38] are widely adopted to depict the differences in data distribution among different scenarios. To solve this problem, the Multi-Task Learning (MTL) paradigm [3] is widely used by constructing the shared and specific experts and applying adaptive gates to select relevant information for prediction. Shared Bottom [3], MMOE [19] and PLE [24] are representative models in MTL. To achieve better modeling of multiscenario, Dynamic Weight models [36, 39] are proposed by generating scenario-specific dynamic parameters adaptively. DFFM [8] incorporates scenario-related information into the parameters of the feature interaction and user behavior modules, allowing for fine-grained scenario-specific learning. Moreover, HierRec [6] conducts explicit and implicit scenario modeling simultaneously by a scenario-aware hierarchical dynamic network. Auction Information Utilization As elaborated above, the CTR prediction problem is comprehensively optimized from the aspects of feature interaction, multi-task and multi-scenario. However, considering posterior auction information to enhance the CTR prediction is crucial in online advertising. MTAE [35] proposed a framework to leverage posterior market price to ancillary CTR prediction. In the field of utility optimization, there is some related literature [18, 27, 33] that utilizes auction information to optimize revenue. Although market price is considered for CTR prediction in online advertising, the exploitation of other auction information including bid is not enough as it can also provide extra posterior signals and impact the final online display exposure. Moreover, the fine-grained modeling of the market price is necessary to capture the distribution variance of the market price. Based on these considerations, we propose AIE to better utilize posterior auction signals to enhance the CTR prediction's performance.",
  "5 CONCLUSION": "In this paper, we delve into the problem of insufficient utilization of auction signals and first reveal the auction bias for CTR prediction in online advertising. Besides, a novel framework called AIE is proposed to better utilize the posterior auction information, which is lightweight, model-agnostic and latency-friendly. Specifically, AM2 constructs an auxiliary task to learn extra market price information while realizing a fine-grained perception of market prices in different auction scenarios. BCM performs a delicate reweighting method by using posterior bidding information to ease the auction bias and improve the prediction accuracy. Offline experiments are conducted on a public dataset and an industrial dataset to demonstrate its effectiveness and compatibility. Besides, a one-month online A/B test in a large-scale advertising platform shows that AIE improves the base model by 5.76% and 2.44% in terms of eCPM and CTR.",
  "REFERENCES": "[1] 2020. Mindspore. https://www.mindspore.cn. [2] Md Manjurul Ahsan, MA Parvez Mahmud, Pritom Kumar Saha, Kishor Datta Gupta, and Zahed Siddique. 2021. Effect of data scaling methods on machine learning algorithms and model performance. Technologies 9, 3 (2021), 52. [3] Rich Caruana. 1997. Multitask learning. Machine learning 28, 1 (1997), 41-75. [4] Bo Chen, Yichao Wang, Zhirong Liu, Ruiming Tang, Wei Guo, Hongkun Zheng, Weiwei Yao, Muyu Zhang, and Xiuqiang He. 2021. Enhancing explicit and implicit feature interactions via information sharing for parallel deep CTR models. In Proceedings of the 30th ACM international conference on information & knowledge management . 3757-3766. [5] Kristin Fridgeirsdottir and Sami Najafi-Asadolahi. 2018. Cost-per-impression pricing for display advertising. Operations Research 66, 3 (2018), 653-672. [6] Jingtong Gao, Bo Chen, Menghui Zhu, Xiangyu Zhao, Xiaopeng Li, Yuhao Wang, Yichao Wang, Huifeng Guo, and Ruiming Tang. 2023. Scenario-Aware Hierarchical Dynamic Network for Multi-Scenario Recommendation. arXiv preprint arXiv:2309.02061 (2023). [7] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [8] Wei Guo, Chenxu Zhu, Fan Yan, Bo Chen, Weiwen Liu, Huifeng Guo, Hongkun Zheng, Yong Liu, and Ruiming Tang. 2023. DFFM: Domain Facilitated Feature Modeling for CTR Prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 4602-4608. [9] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition . 7132-7141. [10] Yu Hu, Jiwoong Shin, and Zhulei Tang. 2016. Incentive problems in performancebased online advertising pricing: Cost per click vs. cost per action. Management Science 62, 7 (2016), 2022-2038. [11] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM conference on recommender systems . 169-177. [12] Patrick Hummel and R Preston McAfee. 2017. Loss functions for predicted clickthrough rates in auctions for online advertising. Journal of Applied Econometrics 32, 7 (2017), 1314-1328. [13] Yuchen Jiang, Qi Li, Han Zhu, Jinbei Yu, Jin Li, Ziru Xu, Huihui Dong, and Bo Zheng. 2022. Adaptive Domain Interest Network for Multi-domain Recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 3212-3221. [14] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [15] Walid Krichene and Steffen Rendle. 2020. On sampled metrics for item recommendation. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining . 1748-1757. [16] Hairen Liao, Lingxiao Peng, Zhenchuan Liu, and Xuehua Shen. 2014. iPinYou global rtb bidding algorithm competition dataset. In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising . 1-6. [17] Zhaocheng Liu and Guangxue Yin. 2019. CPM-sensitive AUC for CTR prediction. arXiv preprint arXiv:1904.10272 (2019). [18] Boxiang Lyu, Zhe Feng, Zachary Robertson, and Sanmi Koyejo. 2023. Pairwise ranking losses of click-through rates prediction for welfare maximization in ad auctions. In International Conference on Machine Learning . PMLR, 23239-23263. [19] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In SIGKDD . 1930-1939. [20] Weitong Ou, Bo Chen, Yingxuan Yang, Xinyi Dai, Weiwen Liu, Weinan Zhang, Ruiming Tang, and Yong Yu. 2023. Deep landscape forecasting in multi-slot realtime bidding. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4685-4695. [21] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In 2016 IEEE 16th international conference on data mining (ICDM) . IEEE, 1149-1154. [22] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting clicks: estimating the click-through rate for new ads. In Proceedings of the 16th international conference on World Wide Web . 521-530. [23] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM international conference RecSys '24, October 14-18, 2024, Bari, Italy Yang Yang et al. on information and knowledge management . 1161-1170. [24] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In RecSys . 269-278. [25] Hind Taud and Jean-Franccois Mas. 2018. Multilayer perceptron (MLP). Geomatic approaches for modeling land change scenarios (2018), 451-455. [26] Ali Vardasbi, Maarten de Rijke, and Ilya Markov. 2020. Cascade model-based propensity estimation for counterfactual learning to rank. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2089-2092. [27] Flavian Vasile, Damien Lefortier, and Olivier Chapelle. 2017. Cost-sensitive learning for utility optimization in online advertising auctions. In Proceedings of the ADKDD'17 . 1-6. [28] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [29] Ruoxi Wang, Rakesh Shivanna, Derek Zhiyuan Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In Proceedings of WWW , Jure Leskovec, Marko Grobelnik, Marc Najork, Jie Tang, and Leila Zia (Eds.). ACM / IW3C2, 1785-1797. [30] Xuanhui Wang, Michael Bendersky, Donald Metzler, and Marc Najork. 2016. Learning to rank with selection bias in personal search. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval . 115-124. [31] Xiaojie Wang, Rui Zhang, Yu Sun, and Jianzhong Qi. 2021. Combating selection biases in recommender systems with a few unbiased ratings. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining . 427-435. [32] Yixin Wang, Dawen Liang, Laurent Charlin, and David M Blei. 2018. The deconfounded recommender: A causal inference approach to recommendation. arXiv preprint arXiv:1808.06581 (2018). [33] Liang Wu, Diane Hu, Liangjie Hong, and Huan Liu. 2018. Turning clicks into purchases: Revenue optimization for product search in e-commerce. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 365-374. [34] Ling Yan, Wu-Jun Li, Gui-Rong Xue, and Dingyi Han. 2014. Coupled group lasso for web-scale ctr prediction in display advertising. In International conference on machine learning . PMLR, 802-810. [35] Haizhi Yang, Tengyun Wang, Xiaoli Tang, Qianyu Li, Yueyue Shi, Siyu Jiang, Han Yu, and Hengjie Song. 2021. Multi-task learning for bias-free joint ctr prediction and market price modeling in online advertising. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 2291-2300. [36] Xuanhua Yang, Xiaoyu Peng, Penghui Wei, Shaoguo Liu, Liang Wang, and Bo Zheng. 2022. Adasparse: Learning adaptively sparse structures for multi-domain click-through rate prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 4635-4639. [37] Yanwu Yang and Panyu Zhai. 2022. Click-through rate prediction in online advertising: A literature review. Information Processing & Management 59, 2 (2022), 102853. [38] Tianzi Zang, Yanmin Zhu, Haobing Liu, Ruohan Zhang, and Jiadi Yu. 2022. A survey on cross-domain recommendation: taxonomies, methods, and future directions. ACM Transactions on Information Systems 41, 2 (2022), 1-39. [39] Qianqian Zhang, Xinru Liao, Quan Liu, Jian Xu, and Bo Zheng. 2022. Leaving no one behind: A multi-scenario multi-task meta learning approach for advertiser modeling. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 1368-1376. [40] Weinan Zhang, Shuai Yuan, Jun Wang, and Xuehua Shen. 2014. Real-time bidding benchmarking with ipinyou dataset. arXiv preprint arXiv:1407.7073 (2014). [41] Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Depeng Jin. 2021. Disentangling user interest and conformity for recommendation with causal embedding. In Proceedings of the Web Conference 2021 . 2980-2991. [42] Han Zhu, Junqi Jin, Chang Tan, Fei Pan, Yifan Zeng, Han Li, and Kun Gai. 2017. Optimized cost per click in taobao display advertising. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining . 2191-2200. [43] Ziwei Zhu, Yun He, Xing Zhao, Yin Zhang, Jianling Wang, and James Caverlee. 2021. Popularity-opportunity bias in collaborative filtering. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining . 85-93.",
  "keywords_parsed": [
    "Online Advertising",
    "Click-through Rate Prediction",
    "Posterior Feature Modeling"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Mindspore"
    },
    {
      "ref_id": "b2",
      "title": "Effect of data scaling methods on machine learning algorithms and model performance"
    },
    {
      "ref_id": "b3",
      "title": "Multitask learning"
    },
    {
      "ref_id": "b4",
      "title": "Enhancing explicit and implicit feature interactions via information sharing for parallel deep CTR models"
    },
    {
      "ref_id": "b5",
      "title": "Cost-per-impression pricing for display advertising"
    },
    {
      "ref_id": "b6",
      "title": "Scenario-Aware Hierarchical Dynamic Network for Multi-Scenario Recommendation"
    },
    {
      "ref_id": "b7",
      "title": "DeepFM: a factorization-machine based neural network for CTR prediction"
    },
    {
      "ref_id": "b8",
      "title": "DFFM: Domain Facilitated Feature Modeling for CTR Prediction"
    },
    {
      "ref_id": "b9",
      "title": "Squeeze-and-excitation networks"
    },
    {
      "ref_id": "b10",
      "title": "Incentive problems in performance-based online advertising pricing: Cost per click vs. cost per action"
    },
    {
      "ref_id": "b11",
      "title": "FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction"
    },
    {
      "ref_id": "b12",
      "title": "Loss functions for predicted clickthrough rates in auctions for online advertising"
    },
    {
      "ref_id": "b13",
      "title": "Adaptive Domain Interest Network for Multi-domain Recommendation"
    },
    {
      "ref_id": "b14",
      "title": "Adam: A method for stochastic optimization"
    },
    {
      "ref_id": "b15",
      "title": "On sampled metrics for item recommendation"
    },
    {
      "ref_id": "b16",
      "title": "iPinYou global rtb bidding algorithm competition dataset"
    },
    {
      "ref_id": "b17",
      "title": "CPM-sensitive AUC for CTR prediction"
    },
    {
      "ref_id": "b18",
      "title": "Pairwise ranking losses of click-through rates prediction for welfare maximization in ad auctions"
    },
    {
      "ref_id": "b19",
      "title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts"
    },
    {
      "ref_id": "b20",
      "title": "Deep landscape forecasting in multi-slot realtime bidding"
    },
    {
      "ref_id": "b21",
      "title": "Product-based neural networks for user response prediction"
    },
    {
      "ref_id": "b22",
      "title": "Predicting clicks: estimating the click-through rate for new ads"
    },
    {
      "ref_id": "b23",
      "title": "Autoint: Automatic feature interaction learning via self-attentive neural networks"
    },
    {
      "ref_id": "b24",
      "title": "Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations"
    },
    {
      "ref_id": "b25",
      "title": "Multilayer perceptron (MLP)"
    },
    {
      "ref_id": "b26",
      "title": "Cascade model-based propensity estimation for counterfactual learning to rank"
    },
    {
      "ref_id": "b27",
      "title": "Cost-sensitive learning for utility optimization in online advertising auctions"
    },
    {
      "ref_id": "b28",
      "title": "Deep & cross network for ad click predictions"
    },
    {
      "ref_id": "b29",
      "title": "DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems"
    },
    {
      "ref_id": "b30",
      "title": "Learning to rank with selection bias in personal search"
    },
    {
      "ref_id": "b31",
      "title": "Combating selection biases in recommender systems with a few unbiased ratings"
    },
    {
      "ref_id": "b32",
      "title": "The deconfounded recommender: A causal inference approach to recommendation"
    },
    {
      "ref_id": "b33",
      "title": "Turning clicks into purchases: Revenue optimization for product search in e-commerce"
    },
    {
      "ref_id": "b34",
      "title": "Coupled group lasso for web-scale ctr prediction in display advertising"
    },
    {
      "ref_id": "b35",
      "title": "Multi-task learning for bias-free joint ctr prediction and market price modeling in online advertising"
    },
    {
      "ref_id": "b36",
      "title": "Adasparse: Learning adaptively sparse structures for multi-domain click-through rate prediction"
    },
    {
      "ref_id": "b37",
      "title": "Click-through rate prediction in online advertising: A literature review"
    },
    {
      "ref_id": "b38",
      "title": "A survey on cross-domain recommendation: taxonomies, methods, and future directions"
    },
    {
      "ref_id": "b39",
      "title": "Leaving no one behind: A multi-scenario multi-task meta learning approach for advertiser modeling"
    },
    {
      "ref_id": "b40",
      "title": "Real-time bidding benchmarking with ipinyou dataset"
    },
    {
      "ref_id": "b41",
      "title": "Disentangling user interest and conformity for recommendation with causal embedding"
    },
    {
      "ref_id": "b42",
      "title": "Optimized cost per click in taobao display advertising"
    },
    {
      "ref_id": "b43",
      "title": "Popularity-opportunity bias in collaborative filtering"
    }
  ]
}