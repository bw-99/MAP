{"title": "CAB: Continuous Adaptive Blending for Policy Evaluation and Learning", "authors": "Yi Su; Lequn Wang; Michele Santacatterina; Thorsten Joachims", "pub_date": "2019-08-28", "abstract": "The ability to perform offline A/B-testing and off-policy learning using logged contextual bandit feedback is highly desirable in a broad range of applications, including recommender systems, search engines, ad placement, and personalized health care. Both offline A/B-testing and offpolicy learning require a counterfactual estimator that evaluates how some new policy would have performed, if it had been used instead of the logging policy. In this paper, we present and analyze a family of counterfactual estimators which subsumes most estimators proposed to date. Most importantly, this analysis identifies a new estimator -called Continuous Adaptive Blending (CAB) -which enjoys many advantageous theoretical and practical properties. In particular, it can be substantially less biased than clipped Inverse Propensity Score (IPS) weighting and the Direct Method, and it can have less variance than Doubly Robust and IPS estimators. In addition, it is subdifferentiable such that it can be used for learning, unlike the SWITCH estimator. Experimental results show that CAB provides excellent evaluation accuracy and outperforms other counterfactual estimators in terms of learning performance.", "sections": [{"heading": "Introduction", "text": "Contextual bandit feedback is ubiquitous in a wide range of intelligent systems that interact with their users through the following process. The system observes a context, takes an action, and then observes feedback under the chosen action. The logs of search engines, recommender systems, ad-placement systems, and many other systems contain terabytes of this partial-information feedback data, and it is highly desirable to use this historic data for offline evaluation and learning. What makes evaluation and learning challenging, however, is that we only get to see the feedback for the chosen action, but we do not observe how the user would have responded if the system had taken a different action. This means the feedback is both partial (e.g. only observed for the selected action) and biased (e.g. by the choices of the policy that logged the data), which makes batch learning from contextual bandit feedback substantially different from typical supervised learning, where the correct label and a loss function provide full-information feedback.\nBoth learning and evaluation can be viewed as examples of counterfactual reasoning, where we need to estimate from the historic log data how well some other policy would have performed, if we had used it instead of the policy that logged the data. Three main approaches have been proposed for this counterfactual or off-policy evaluation problem. First, the Direct Method (DM) (Schafer, 1997;Rubin, 2004;Dud\u00edk et al., 2011;Little & Rubin, 2019) uses regression to learn a model of the reward and imputes the missing feedback. Second, inverse propensity score (IPS) weighting (Horvitz & Thompson, 1952;Strehl et al., 2011) models the selection bias in the assignment mechanism and directly provides an unbiased estimate of the quality of a policy under suitable common support conditions. Both approaches are complementary and have different strengths and drawbacks. On the one hand, DM typically has low variance but can lead to highly biased results due to model misspecification. On the other hand, since we are typically controlling the logging policy and can log propensities, IPSbased methods can be provably unbiased but often suffer from large variance when the IPS weights are large. The third class is a hybrid of them. The most prominent one is the Doubly Robust (DR) estimator (Robins & Rotnitzky, 1995;Kang et al., 2007;Dud\u00edk et al., 2011), which is based on DM but also uses IPS weighting and an additive control variate to reduce variance.\nGeneralizing these existing counterfactual estimators, we present a parametric family of estimators for off-policy evaluation that covers most of the off-policy estimators proposed to date -including IPS (Horvitz & Thompson, 1952), clipped IPS (Strehl et al., 2011), DM (Dud\u00edk et al., 2011), DR (Dud\u00edk et al., 2011) and its MRDR variant (Farajtabar et al., 2018), SWITCH (Wang et al., 2017), and Static Blend-ing (SB) (Thomas & Brunskill, 2016). Providing a general bias-variance analysis for this family of estimators, we find that there is a particular new estimator in this family that has many desirable properties. We call this new estimator Continuous Adaptive Blending (CAB) and show how it blends the complementary strengths of DM and IPS, thus providing an effective tool for optimizing the bias of DM against the variance of IPS. Compared to existing estimators, we find that CAB can be substantially less biased than clipped IPS and DM while having lower variance compared to IPS and DR estimators. Furthermore, compared to estimators that perform static blending (SB) (Thomas & Brunskill, 2016), CAB is adaptive to the IPS weights and handles violations of the support condition gracefully. Unlike SWITCH (Wang et al., 2017), CAB is sub-differentiable which allows its use as the training objective in off-policy learning algorithms like POEM (Swaminathan & Joachims, 2015a) and BanditNet (Joachims et al., 2018). Finally, unlike the DR estimator, CAB can be used in off-policy Learning to Rank (LTR) algorithms like (Joachims et al., 2017), and CAB is specifically designed to control the bias/variance trade-off. We evaluate CAB both theoretically and empirically. In particular, we present theoretical results that characterize the bias and variance of CAB. Furthermore, we provide an extensive empirical evaluation of CAB on both contextualbandit problems and partial-information ranking problems, including real-world data from Amazon Music.", "publication_ref": ["b20", "b19", "b6", "b15", "b9", "b21", "b18", "b14", "b6", "b9", "b21", "b6", "b6", "b8", "b26", "b24", "b24", "b26", "b13", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Off-policy Evaluation in Contextual Bandits", "text": "Before presenting our general family of counterfactual estimators, we begin with a formal definition of off-policy evaluation and learning in the contextual-bandit setting.\nTo keep notation and exposure simple, we focus on the contextual bandit setting and do not explicitly consider other partial-information settings like counterfactual LTR (Joachims et al., 2017). However, most estimators can be translated into that setting as well, and we discuss further details in the context of the ranking experiments in Section 4 and in Appendix A.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Contextual-Bandit Setting and Learning", "text": "In the contextual-bandit setting, a context x \u2208 X (e.g., user profile, query) is drawn i.i.d. from some unknown P (X ), the deployed (stochastic) policy \u03c0 0 (y|x) then selects an action y \u2208 Y, and the system receives feedback r \u223c D(r|x, y) for this particular (context, action) pair. However, we do not observe feedback for any of the other actions. The logged contextual bandit data we get from logging policy \u03c0 0 is of the form\nS = {(x i , y i , r i , \u03c0 0 (\u2022|x i ))} n i=1 ,(1)\nwhere r i := r(x i , y i ) is the observed reward. If the logging policy \u03c0 0 is unknown, an estimate \u03c00 of the logging policy is used. Off-policy evaluation refers to the problem of using S for estimating the expected reward (or loss) R of a new policy \u03c0\nR(\u03c0) = E x\u223cP (x) E \u0233\u223c\u03c0(\u0233|x) E r\u223cD(r|x,\u0233) [r].(2)\nOff-policy learning uses S for finding an optimal policy \u03c0 * \u2208 \u03a0 that maximizes the expected reward (or minimizes the expected loss)\n\u03c0 * = argmax \u03c0\u2208\u03a0 R(\u03c0) .(3)\nThe expected reward R(\u03c0) cannot be computed directly, and it is typically replaced with a counterfactual estimate R(\u03c0) that can be computed from the logged bandit feedback S (Bottou et al., 2013). This enables Empirical Risk Minimization (ERM) for batch learning from bandit feedback (BLBF) (Zadrozny et al., 2003;Beygelzimer & Langford, 2009;Strehl et al., 2011;Swaminathan & Joachims, 2015a;b), where the algorithm finds the policy in \u03a0 that maximizes the estimated expected reward\n\u03c0 * = argmax \u03c0\u2208\u03a0 R(\u03c0) ,(4)\npossibly subject to capacity and variance regularization (Swaminathan & Joachims, 2015a). Since the counterfactual estimator R(\u03c0) is at the core of ERM learning, it is expected that an improved estimator will also lead to improved learning performance (Strehl et al., 2011). In particular, unlike in supervised learning, the counterfactual estimator can have vastly different bias and variance for different policies in \u03a0, such that trading off bias and variance of the estimator becomes important for effective learning (Swaminathan & Joachims, 2015b).", "publication_ref": ["b5", "b27", "b4", "b21", "b19", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Interpolated Counterfactual Estimator Family (ICE Family)", "text": "In this section, we present a general family of estimators for off-policy evaluation in the contextual bandit setting. An analogous family of estimators for the setting of partialinformation learning-to-rank (Joachims et al., 2017) is described in Appendix A.\nLet \u03b4(x, \u0233) be the estimated reward for action \u0233 given context x, and let \u03c00 be the estimated (or known) logging policy.\nThen our family of off-policy estimators is defined as follows, where w = (w \u03b1 , w \u03b2 , w \u03b3 ) is a triplet of weighting functions that parameterizes the family.\nDefinition (Interpolated Counterfactual Estimator Family). Estimator\nw \u03b1 i (\u0233) w \u03b2 i w \u03b3 i DM 1 0 0 IPS 0 1 0 cIPS 0 min M \u03c00(yi|xi) \u03c0(yi|xi) ,1 0 DR 1 1 -1 SB 1 -\u03c4 \u03c4 0 SWITCH 1 \u03c0(\u0233|xi) \u03c00(\u0233|xi) > M 1 \u03c0(yi|xi) \u03c00(yi|xi) \u2264 M 0\nGiven a triplet w = (w \u03b1 , w \u03b2 , w \u03b3 ) of weighting functions,\nRw (\u03c0) = 1 n n i=1 \u0233\u2208Y \u03c0(\u0233|x i ) w \u03b1 i\u0233 \u03b1 i\u0233 + 1 n n i=1 \u03c0(y i |x i ) w \u03b2 i \u03b2 i + 1 n n i=1 \u03c0(y i |x i ) w \u03b3 i \u03b3 i with \u03b1 i\u0233 := \u03b1(x i , \u0233) := \u03b4(x i , \u0233), \u03b2 i := \u03b2(x i , y i ) := r(x i , y i ) \u03c00 (y i |x i ) , \u03b3 i := \u03b3(x i , y i ) := \u03b4(x i , y i ) \u03c00 (y i |x i ) .\nWe will see in the following that different choices of the weighting functions w = (w \u03b1 , w \u03b2 , w \u03b3 ) recover a wide variety of existing estimators (see Table 1), and that our bias-variance analysis of Rw (\u03c0) suggests new estimators with desirable properties. This class of counterfactual estimators builds upon three basic estimators for off-policy evaluation, and we will now introduce the motivation for the construction of the (context, action) level functions \u03b1(x i , \u0233), \u03b2(x i , y i ) and \u03b3(x i , y i ).\nThe first component \u03b1(x i , \u0233) follows a \"Model the World\" approach (Schafer, 1997;Rubin, 2004;Dud\u00edk et al., 2011), which exploits a model \u03b4(x, \u0233) of the reward. The estimator that purely relies on this component is the DM estimator that is a special case of Rw (\u03c0) with static weights w = (1, 0, 0) for all (x i , y i , \u0233) (see Table 1). The reward model \u03b4(x, \u0233) is typically learned via regression, and it serves as an estimate of E r [r|x, \u0233] in (2) to be imputed in place of the unobserved rewards. The reward model typically has low variability, so the variance of DM is typically small. However, due to often unavoidable misspecification of the reward model, DM is often statistically inconsistent and can have a large bias.\nThe second component \u03b2(x i , y i ) follows a \"Model the Bias\" approach for the assignment mechanism, which is particularly attractive in many applications where we control the assignment mechanism by design (Horvitz & Thompson, 1952;Swaminathan & Joachims, 2015a;Nikos Vlassis, 2018). The estimator fully based on this term is the widely used inverse propensity score (IPS) weighting estimator (Horvitz & Thompson, 1952;Strehl et al., 2011;Bottou et al., 2013;Swaminathan & Joachims, 2015a), which is the special case of Rw (\u03c0) with static weights w = (0, 1, 0) for all (x i , y i , \u0233) (see Table 1). If \u03c00 (y i |x i ) is known and the logging policy has sufficient support w.r.t. the new policy \u03c0, then the IPS estimator is unbiased (see Section 2.3). However, it can have large variance if the IPS weights \u03c0(y i |x i )/\u03c0 0 (y i |x i ) are large.\nThe third component \u03b3(x i , y i ) combines the \"Model the World\" approach and \"Model the Bias\" approach by debiasing the estimated reward term. This component is not necessarily an attractive estimator itself, but can be used as part of a control variate for variance reduction as in the DR estimator (Robins & Rotnitzky, 1995;Bang & Robins, 2005;Kang et al., 2007;Dud\u00edk et al., 2011). DR treats DM as a baseline while correcting the baseline when data is available, and it is a special case of Rw (\u03c0) with static weights w = (1, 1, -1) for all (x i , y i , \u0233). It is unbiased when either the reward model or the propensity model is correct. However, by maintaining unbiasedness, we will discuss in Section 3.1 that it can still suffer from excessive variance. Furthermore, due to the non-zero weight put on the control variate term \u03b3(x i , y i ), DR cannot be used for LTR from implicit feedback, since the analog of y i is only partially observable in that setting (see Appendix A).\nAnother hybrid estimator was proposed by (Thomas & Brunskill, 2016) for off-policy evaluation in the more general setting of reinforcement learning. When we translate the key idea behind their MAGIC estimator to the contextual bandit setting, we see that it is a special case of Rw (\u03c0) with tunable weights w = (1 -\u03c4, \u03c4, 0) for all (x i , y i , \u0233), where \u03c4 \u2208 [0, 1] is a parameter. We call this estimator Static Blending (SB), since \u03c4 is static and does not depend on the importance weights.\nThe SWITCH Estimator (Wang et al., 2017), in contrast, is more adaptive. As the name implies, it switches between \u03b1(x i , \u0233) and \u03b2(x i , y i ) depending on a hard threshold M on the IPS weights. It is a special case of Rw (\u03c0) using the weights given in Table 1. A drawback of SWITCH is that its hard switching makes it discontinuous with respect to any parameters of the target policy \u03c0. This not only creates more erratic behavior when the threshold M is changed, but it also means that SWITCH is not differentiable and thus cannot be used in gradient-based learning algorithms like POEM (Swaminathan & Joachims, 2015a) or BanditNet (Joachims et al., 2018) for BLBF.", "publication_ref": ["b12", "b20", "b19", "b6", "b9", "b16", "b9", "b21", "b5", "b18", "b3", "b14", "b6", "b24", "b26", "b13"], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_0", "tab_0"]}, {"heading": "Theoretical Analysis", "text": "We will now provide a general characterization of the bias and variance for the Interpolated Counterfactual Estimator Family, where both the propensity model and the reward model may be misspecified. Following Dud\u00edk et al. (2011), let \u03b6(x, y) denote the multiplicative deviation of the propensity estimates from the true propensity model, and \u2206(x, y) be the additive deviation of the reward estimates from the true expected reward.\n\u03b6 := \u03b6(x, y) = 1 - \u03c0 0 (y|x) \u03c00 (y|x)(5)\n\u2206 := \u2206(x, y) = \u03b4(x, y) -\u03b4(x, y).(6)\nMoreover, let \u03c3 2 r (x, y) denote the randomness of the reward.\n\u03c3 2 r := \u03c3 2 r (x, y) = V r (r(x, y)|x, y)(7)\nNote that \u03b6(x, y) is zero when the logging policy \u03c0 0 is known. For brevity, we denote the true IPS weight as c(x, y) := \u03c0(y|x) \u03c00(y|x) and its estimated version as \u0109(x, y) := \u03c0(y|x) \u03c00(y|x) . For known propensities, \u0109(x, y) = c(x, y). As usual, we posit that the following support/positivity condition holds.\nCondition 1 (Common Support). The logging policy \u03c0 0 has full support for the target policy \u03c0, which means \u03c0(y|x) > 0 \u2192 \u03c0 0 (y|x) > 0 for all x and y.\nTheorem 1 (Bias of the Interpolated Counterfactual Estimator Family). For contexts x 1 , x 2 , \u2022 \u2022 \u2022 , x n drawn i.i.d from some distribution P (X ) and for actions y i \u223c \u03c0 0 (Y|x i ), under Condition 1 the bias of Rw (\u03c0) with weighting functions w = (w \u03b1 , w \u03b3 , w \u03b3 ) is\nE x E y\u223c\u03c0 w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4 -\u03b4 (8) Proof: See Appendix B.1.\nTheorem 2 (Variance of the Interpolated Counterfactual Estimator Family). Under the same conditions as in Theorem 1, the variance of Rw (\u03c0) with weighting functions w = (w \u03b1 , w \u03b2 , w \u03b3 ) is\n1 n V x E \u03c0 [w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4] + E x E \u03c0 (w \u03b2 ) 2 c(1 -\u03b6) 2 \u03c3 2 r + E x V \u03c00 (w \u03b2 c(1 -\u03b6)\u03b4 + w \u03b3 c(1 -\u03b6)(\u03b4 + \u2206)) (9) Proof: See Appendix B.2.\nThroughout the rest of the paper, these general results will guide the design of new estimators, and they will allow us to compare different estimators within the family with respect to their bias/variance trade-offs.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Continuous Adaptive Blending Estimator (CAB)", "text": "In this section, we identify a new estimator within this family that has many desirable properties. In particular, the estimator arises naturally by filtering our family of estimators according to these properties. First, we would like the estimator to be unbiased if the reward model and the propensity model are correct, which can be achieved through constraining the weights (w \u03b1 i\u0233 , w \u03b2 i , w \u03b3 i ) to sum to 1 for each context-action pair. Second, the estimator should be applicable to a wide range of partial information settings, including learning to rank, which requires w \u03b3 i = 0. Third, we would like to achieve low MSE, which argues for data-dependent weights that allow an instance dependent trade-off between bias and variance. And, fourth, we would like to use the estimator for gradient-based learning, which implies that the weighting functions need to be (sub-)differentiable.\nThese desiderata and constraints lead us to the following new Continuous Adaptive Blending estimator (CAB).\nRCAB (\u03c0) = Rw (\u03c0) with \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 w \u03b1 i\u0233 = 1 -min M \u03c00(\u0233|xi) \u03c0(\u0233|xi) , 1 w \u03b2 i = min M \u03c00(yi|xi) \u03c0(yi|xi) , 1 w \u03b3 i = 0\nIt is easy to see that CAB interpolates between DM and IPS in an example-dependent way, which allows trade-off between bias and variance by controlling M . In particular, CAB inherits the idea that clipping is a data-dependent way to achieve smaller MSE. However, CAB imputes a regression estimate \u03b4(x i , \u0233) proportional to the clipped-off portion of the IPS weight -unlike clipped IPS (see Table 1) that implicitly imputes zero. Note that the particular choice of weights (w \u03b1 i\u0233 , w \u03b2 i , w \u03b3 i ) makes RCAB (\u03c0) continuous and subdifferentiable with respect to the parameters of the policy \u03c0.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Bias and Variance Analysis", "text": "We now analyze the bias and variance of CAB as an instance of the counterfactual estimator family, and we will compare them to those of IPS, cIPS, DR, and DM. Theorem 3 (Bias of CAB). For contexts x 1 , x 2 , \u2022 \u2022 \u2022 , x n drawn i.i.d from some distribution P (X ) and for actions\ny i \u223c \u03c0 0 (Y |x i ), under Condition 1 the bias of RCAB (\u03c0) is E x E \u03c0 -\u03b4\u03b61{\u0109 \u2264 M } (10) +{\u2206(1 - M c(1-\u03b6) ) - M c(1-\u03b6) \u03b4\u03b6}1{\u0109 > M } Proof: See Appendix B.3\nNote that the first part of the bias results from the use of IPS when the IPS weight is small, while the second part results from the convex combination of IPS and DM when the IPS weight is large.\nTheorem 4 (Variance of CAB). Under the same conditions as in Theorem 3, the variance of RCAB (\u03c0) is\n1 n V x E \u03c0 [\u03b4 -\u03b4\u03b61{\u0109 \u2264 M } + (\u2206(1 - M c(1 -\u03b6) ) - M c(1 -\u03b6) \u03b4\u03b6)1{\u0109 > M }] + E x E \u03c0 c(1 -\u03b6) 2 \u03c3 2 r 1{\u0109 \u2264 M } + M 2 c \u03c3 2 r 1{\u0109 > M } + E x V \u03c00 (c(1 -\u03b6)\u03b41{\u0109 \u2264 M } + M \u03b41{\u0109 > M }) (11) Proof: See Appendix B.4\nThe first term of the variance is due to the randomness in context x, the second term results from the randomness in the rewards compounded with bounded IPS weights. The third term is, in expectation, the variability in the expected reward compounded with bounded IPS weights.\nBias improvements over cIPS and DM. We can now compare the bias of CAB to that of cIPS, which we can derive as a special case of Theorem 3 with \u03b4(x i , \u0233) = 0 for all (x i , \u0233) pair. Focusing on the case of logged propensities for conciseness and its realworld prevalence in online systems, this reduces to\nBias( RcIP S (\u03c0)) = E x E \u03c0 [-\u03b4(1 -M c )1{c > M }] for cIPS and Bias( RCAB (\u03c0)) = E x E \u03c0 [\u2206(1-M c )1{c > M }] for CAB.\nIt can be seen that if we have a moderately good predictor of the expected reward \u03b4(x, \u0233), CAB will have an advantage as long as the predictor is better than imputing the constant 0 everywhere. In practice, it is sensible to assume that the reward estimation error \u2206(x, \u0233) is substantially smaller than \u03b4(x, \u0233), such that CAB enjoys a substantial amount of bias reduction.\nIn comparison to DM, CAB can also enjoy smaller bias when the propensity is known, since Bias(\nRDM (\u03c0)) = E x E \u03c0 [\u2206] while Bias( RCAB (\u03c0)) = E x E \u03c0 [\u2206(1 - M c )1{c > M }]\nwhich reflects that CAB incurs bias only on the clipped portion of the importance sample weights.\nVariance improvements over IPS and DR. Comparing the variance of CAB to that of IPS and DR, we again focus on the case of logged propensities. From Theorem 2 we can deduce that the variance of IPS is\n1 n V x E \u03c0 [\u03b4] + E x E \u03c0 c\u03c3 2 r + E x V \u03c00 (c\u03b4)(12)\nwhile the variance for CAB is\n1 n V x E \u03c0 [\u03b4 + \u2206(1 - M c )1{c > M }] + E x E \u03c0 c\u03c3 2 r 1{c \u2264 M } + M 2 c \u03c3 2 r 1{c > M } + E x V \u03c00 [(c\u03b4)1{c \u2264 M } + M \u03b41{c > M }] .(13)\nThe first term is similar for both estimators since \u03b4 can be expected to dominate \u2206. The second and the third terms, which are the variance of the reward r(x, \u0233) and the expected reward \u03b4(x i , \u0233) compounded with the IPS weights c, can be very large for IPS when the logging policy \u03c0 0 and the target policy \u03c0 are very different. In contrast, for CAB these two terms are bounded by\nM E \u03c0 [\u03c3 2 r ] + M 2 E x [V \u03c00 (\u03b4)]\n, and thus will be smaller than those for IPS.\nComparing CAB to DR, note that DR intends to reduce the variance of IPS by putting weight 1 on the observed loss term \u03b2(x i , y i ) and -1 on the estimated loss term \u03b3(x i , y i ). However, this \"residual term\" is still compounded with the IPS weights c and can blow up the variance either when we have a poor estimate \u2206 or when the target policy is very different from the logging policy. This is apparent in the second and third terms of the variance of DR as derived from Theorem 2.\n1 n V x E \u03c0 [\u03b4] + E x E \u03c0 c\u03c3 2 r + E x V \u03c00 (c\u2206)(14)\nThis is again different from CAB, where the IPS weights are all bounded by M .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CAB-DR", "text": "Finally, we present a variant of CAB that incorporates the DR estimator, called CAB-DR. While this leads to an estimator that cannot be used for ranking, we investigate whether the control variate of DR leads to even better estimates than CAB. The key idea is to substitute the IPS part of CAB with the DR estimator, leading to\nRCABDR (\u03c0) = Rw (\u03c0) with \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 w \u03b1 i\u0233 = 1 w \u03b2 i = min M \u03c00(yi|xi) \u03c0(yi|xi) , 1 w \u03b3 i = -min M \u03c00(\u0233|xi) \u03c0(\u0233|xi) ,1\nas a clipped version of DR. Using Theorem 1, it is easy to derive the bias of CAB-DR to be\nE x E \u03c0 \u03b6\u22061{\u0109 \u2264 M } + \u2206(1 - M c )1{\u0109 > M } .\nIf the propensities are logged, then the bias terms for CAB and CAB-DR are identical. However, if the propensities are only approximates, CAB will suffer from more bias from the term\nE x E \u03c0 [\u03b6\u03b41{\u0109 \u2264 M }] compared to E x E \u03c0 [\u03b6\u22061{\u0109 \u2264 M }] for CAB-DR.\nThe variance of CAB-DR is given in Appendix B.5. Given logged propensities, the variance of CAB-DR differs from that of CAB in only a single term. For CAB-DR we have\nE x V \u03c00 c(-\u2206)1{c \u2264 M } + M (-\u2206)1{c > M } ,\nwhile for CAB we have\nE x V \u03c00 c(\u03b4)1{c \u2264 M } + M (\u03b4)1{c > M } .\nIn general cases, by adopting the idea of choosing opposite weights for \u03b2(x i , y i ) and \u03b3(x i , y i ) from DR, the third variance term for CAB-DR becomes\nE x [V \u03c0 (-w \u03b2 c(1 -\u03b6)\u2206)],\nwhich is no longer on the order of\nE x [V \u03c0 (w \u03b2 c(1 -\u03b6)\u03b4)].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We empirically examine the evaluation accuracy and learning performance of CAB in two different partial-information settings. In the BLBF setting, we conduct the experiments on bandit feedback data for multi-class classification. This setting is extensively used in the off-policy evaluation literature (Dud\u00edk et al., 2011;Wang et al., 2017). In the LTR setting, the experiments are based on user feedback with position bias for ranking (Joachims et al., 2017). In both cases, we use real datasets from which we sample synthetic bandit or click data. This increases the external validity of the experiments, while at the same time providing ground truth for a bias/variance analysis. Furthermore, it allows us to vary the properties of both data and logging policy \u03c0 0 to explore the robustness of the estimators.", "publication_ref": ["b6", "b26", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Experiment Setup", "text": "For the BLBF setting, our experiment setup follows Dud\u00edk et al. (2011) and Wang et al. (2017) using the standard supervised \u2192 bandit conversion (Agarwal et al., 2014) for several multiclass classification datasets from the UCI repository (Asuncion & Newman, 2007). In the LTR setting, we follow the experiment setup of Joachims et al. (2017) and conduct experiments on the YAHOO! LTR Challenge corpus (set 1), which comes with a train/validation/test split. More details are given in Appendix A.\nIn both settings, we use a small amount of the fullinformation training data to train a logger \u03c0 0 and a regression model \u03b4. The policy \u03c0 to be evaluated is trained on the whole training set. The partial feedback data is generated from the full-information test set. We evaluate the policy \u03c0 with different estimators on the partial feedback data of different sizes and treat the performance of the fullinformation test set as the ground truth. The performance is measured by the expected test error and the average rank of positive results for BLBF and LTR respectively. We repeat the experiments 500 times for BLBF and 100 times for LTR to calculate bias, variance and MSE.\nFor the BLBF learning experiments, we use POEM (Swaminathan & Joachims, 2015a) to learn stochastic linear policies. For LTR, Appendix C.2 derives a generalized version of propensity SVM-Rank (Joachims et al., 2017) that enables the use of CAB and other estimators with w \u03b3 i = 0 from our family. As input to the learning algorithms, different amounts of partial feedback data are simulated from the full-information training data. To avoid biases from the regression model, we adopt 90 percentile cIPS to conduct hyperparameter selection for M (or \u03c4 for SB) and regularization parameter on the partial feedback data simulated from the validation set. The experiments are run for 10 and 5 times on BLBF and LTR respectively and the average is reported. Details are shown in Appendix C.", "publication_ref": ["b6", "b26", "b0", "b12", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Experiment Results", "text": "Can CAB achieve improved estimation accuracy by trading bias for variance through M ? We first verify that CAB can indeed achieve improved MSE by adjusting the bias-variance trade-off. Figure 1 shows how the choice of M affects bias and variance of CAB on the SATIM-AGE and YAHOO! LTR datasets using different amounts of data (qualitatively similar results are obtained for the other datasets). For each dataset, the bias decreases as we increase M as expected, since CAB moves towards the unbiased IPS estimator. However, the variance increases as more data points rely on IPS weighting. For all data set sizes, the best MSE always falls in the middle of the range of M which confirms that CAB can effectively trade-off between bias and variance through controlling M for a range of data-set sizes. Moreover, the MSE curve suggests that the performance of CAB is pretty robust to the choice of M .\nHow does CAB compare to other off-policy estimators? Figure 2 compares different off-policy estimators on the 4 UCI datasets (selecting those with dataset size larger than 5000) and the YAHOO! LTR dataset. For each UCI dataset, we keep the test data size as 2,000. For the LTR dataset, we present the results with 0.5 sweeps of the test set. Notice that CAB , CAB-DR, cIPS and SWITCH all have a clipping parameter M \u2208 [0, \u221e), while for SB the blending is achieved through the static weight parameter \u03c4 \u2208 [0, 1]. To be able to plot SB together with the other estimators, we rescale \u03c4 in the plot for comparison. DM and DR do not have any hyperparameter, so we use two horizontal lines to represent them.\nFor both SB and CAB (CAB-DR), and across all datasets, we observe a U -shape curve for MSE with the optimum value in the middle. However, on most datasets CAB (and CAB-DR) substantially outperforms SB. Furthermore, CAB outperforms cIPS in the full range of M on all datasets, indicating that imputing a reasonably good regression estimate is indeed consistently better than naively imputing  zero. For the SWITCH estimator, the MSE curve is somewhat more erratic than that of CAB especially on the UCI datasets, which we conjecture is due to the hard switch it makes and the discontinuities this implies. While SWITCH can be used in LTR algorithms like Propensity SVM-Rank (Joachims et al., 2017) (details are shown in Appendix A), we show in the next section that this behavior may make model selection during learning more stable for CAB than for SWITCH. Furthermore, CAB performs at least comparable to SWITCH across all datasets, and on some it can be substantially more accurate than SWITCH. For all datasets, DR outperforms IPS and DM as expected. However, CAB (also CAB-DR) still outperforms DR on most datasets, which validates the idea that estimators outside the class of unbiased estimators can have advantages on this problem. Furthermore, when used in learning, one already faces a bias/variance trade-off due to the capacity of the policy space, such that it seems unjustified to insist on the unbiasedness of the empirical risk to begin with.\nHow robust is CAB on real-world data? We evaluated CAB on data from a contextual bandit problem at Amazon Music. Both the logging policy and the target policy are a Thompson sampling contextual bandit algorithm for which we estimated the respective policy \u03c0 t 0 (y|x) and \u03c0 t target (y|x) at each time step t through Monte Carlo sampling. When analyzing the logging distribution, we found that the logging policy does not provide full support as the Thompson sampler converges, and on average only 33% of the available actions have non-zero support. We used the model Figure 3 shows the error of the estimates depending on the clipping constant M , where we use the average reward of \u03c0 t target (y|x) measured during online A/B testing as the gold standard. For the majority of the values of the clipping constant M , CAB and SWITCH show a higher level of accuracy than DM, IPS and SB. Both IPS and SB perform poorly. This is due to the fact that IPS has no mechanism for detecting and correcting for the missing support of the logging policy, while SB has a static blending constant. For a large range of M , CAB and SWITCH also outperform DR and CAB-DR. Overall, we find that both CAB and SWITCH outperform the other methods and can provide robust solutions to the contextual bandit problem at Amazon Music.\nHow effective is learning with CAB as empirical risk across datasets? Table 2 shows the learning performance when the various estimators are used as empirical risk in POEM and Propensity SVM-Rank. We use the same datasets as in the evaluation experiments. For the 4 UCI datasets (with 5000 training data for each) we report the average test set error, while for YAHOO! LTR (with 1 sweep of data) we report the average rank of the relevant results for the test queries. For both lower is better. Across all datasets, learning with CAB performs very well, providing one of the best prediction performances on all datasets which validates that learning with an improved estimator will also lead to improved learning performance. Comparing CAB with CAB-DR, we don't see a real benefit in using the DR model in CAB-DR instead of using IPS in CAB. We conjecture that it is due to the over-reliance on the regression model, as DR relies more on it than IPS.  2) in Figure 4 show test set performance when we increase the amount of training data for PENDIGITS and YAHOO! LTR. We find that performance improves for all estimators as the amount of data increases. However, for all training data sizes, we observe that CAB and CAB-DR perform very well compared to the other estimators, with a substantial improvement over IPS, cIPS and DR especially in the small sample setting. Furthermore, CAB performs better than SWITCH in the ranking setting, which we attribute to the more erratic behavior of SWITCH during model selection. How does learning performance change when we vary the quality of the logging policy? Plot (4) of Figure 4 shows the results when changing the logger quality. To do so, we used different fractions of the full-information data to train the logging policy, from 0.01 to 0.5. Since the DM estimator is independent of the logging policy, it results in a flat line (up to variance). IPS and cIPS are heavily affected by the logger quality, due to their dependency on the IPS weights, while CAB, CAB-DR and SB are only moderately affected. Overall, CAB and CAB-DR perform well across the whole range.", "publication_ref": ["b12"], "figure_ref": ["fig_0", "fig_1", "fig_2", "fig_3", "fig_3"], "table_ref": ["tab_1"]}, {"heading": "Conclusion", "text": "This paper proposed a parametric family of estimators for off-policy evaluation, which unifies and characterizes a number of popular off-policy estimators. The theoretical analysis also motivates the CAB estimator, which not only provides a controllable bias/variance trade-off for off-policy evaluation, but is also continuous with respect to the target policy to enable gradient-based learning. We argue theoretically that CAB can be less biased than cIPS and DM and often enjoys smaller variance than IPS and DR. Experiment results on two different partial-information settings -contextual bandit and partial-information LTR -confirm that CAB can consistently achieve improved evaluation performance over other counterfactual estimators, and that it also leads to excellent learning performance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Appendix: Counterfactual Learning to Rank", "text": "We also consider another important partial information setting: ranking evaluation and learning to rank based on implicit feedback (e.g. clicks, dwell time). Here the selection bias on the feedback signal is strongly influenced by position bias, since items lower in the ranking are less likely to be discovered by the user. However, it can be shown that this bias can be estimated (Joachims et al., 2017;Wang et al., 2018;Agarwal et al., 2019), and that the resulting estimates can serve as propensities in IPS-style estimators.\nTo connect the ranking setting with the contextual bandit setting more formally, now each context x \u223c P (X ) represents a query and/or user profile. Given ranking function \u03c0, we use \u03c0(x) to represent the ranking for query x. However, in the LTR setting, we no longer consider the actions atomic, but instead treat rankings as combinatorial actions where the reward decomposes as a weighted sum of component rewards. Formally speaking, the reward for rankings \u03c0(x) is denoted as \u2206(\u03c0(x)|x, r) := d\u2208d \u03bb(rank(d|\u03c0(x)))r(x, d), where \u03bb(\u2022) is a function that maps a rank to a score, d is the candidate set for query x and rank(d|\u03c0(x)) represents the rank of document d in the candidate set d under the ranking policy \u03c0 given context x, and r(x, d) \u2208 {0, 1} is the relevance indicator. Then we can define the overall reward for a ranking policy \u03c0 as\nR(\u03c0) = \u2206(\u03c0(x)|x, r)dP (x)(15)\nNote that in this partial information setting we typically do not observe rewards for all (query, document) pairs. The only observable reward per component may be whether the user clicks the document or not, c(x, \u03c0 0 (x), d) \u2208 {0, 1}, and there is inherent ambiguity whether the lack of a click means lack of relevance or lack of discovery. Here we use a latent variable o(x, \u03c0 0 (x), d) \u2208 {0, 1} to represent whether the user x observes document d under the logging policy \u03c0 0 , which then leads to the following click model: a user clicks a document when the user observes it and the document is relevant\n, c(x, \u03c0 0 (x), d) = r(x, d) \u2022 o(x, \u03c0 0 (x), d).\nNote that the examination o(x, \u03c0 0 (x), d) is not observed by the system, but one can estimate a missingness model (Joachims et al., 2017), and use p(x, \u03c0 0 (x), d) be the (estimated) probability of 1{o(x, \u03c0 0 (x), d) = 1}. We denote this probability value as the propensity of the observation. In practice one could estimate the propensities as outlined in (Agarwal et al., 2019;Fang et al., 2019). The logged data we get is in the format of\nS = {{(x i , d ij , p ij , c ij )} mi j=1 } n i=1 where m i is the number of candidates for context x i , p ij is p(x i , \u03c0 0 (x i ), d ij ) and c ij is c(x i , \u03c0 0 (x i ), d ij ).\nFor additive ranking metrics, various estimators in the Interpolated Counterfactual Estimator Family can be written in the form\nRw (\u03c0) = 1 n n i=1 mi j=1 w \u03b1 ij \u03b1 ij + o ij w \u03b2 ij \u03b2 ij + o ij w \u03b3 ij \u03b3 ij \u2022 \u03bb(rank(d ij |x i , \u03c0(x i )))\nwith\n\u03b1 ij := \u03b4(x i , d ij ), \u03b2 ij := r ij p ij , \u03b3 ij := \u03b4(x i , d ij ) p ij .\nwhere w \u03b1 ij , w \u03b2 ij and w \u03b3 ij are the weight functions of the three components of the Interpolated Counterfactual Estimator Family. Given a perfect reward model and logged propensities, we can get unbiased estimate of additive ranking metrics if the weight functions sum to 1. The weights of various estimators in this setting are shown in Table 3. Estimator\nw \u03b1 ij w \u03b2 ij w \u03b3 ij DM 1 0 0 IPS 0 1 0 cIPS 0 min{M p ij ,1} 0 SB 1 -\u03c4 \u03c4 0 SWITCH 1 1 pij > M 1 1 pij \u2264 M 0 CAB 1 -min{M p ij ,1} min{M p ij ,1}0\nNote that estimators with w \u03b3 ij = 0 (DR, CAB-DR) are not applicable in this setting since the third term o ij w \u03b3 ij \u03b3 ij depends on o ij , which is not observed nor fully captured by c ij . However, the second term is computable since the unobserved o ij and r ij are captured by\nc ij through o ij \u03b2 ij = oij rij pij = cij pij .\nThe SWITCH estimator is applicable for learning in this setting since the weights of the estimator do not depend on the ranking policy to be learned.", "publication_ref": ["b12", "b25", "b1", "b12", "b1", "b7"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "B. Appendix: Proofs", "text": "In this appendix, we provide proofs of the main theorems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1. Proof of Theorem 1", "text": "Theorem 1 (Bias of the Interpolated Counterfactual Estimator Family). For contexts x 1 , x 2 , \u2022 \u2022 \u2022 , x n drawn i.i.d from some distribution P (X ) and for actions y i \u223c \u03c0 0 (Y|x i ), under Condition 1 the bias of Rw (\u03c0) with weighting functions w = (w \u03b1 , w \u03b3 , w \u03b3 ) is\nE x E y\u223c\u03c0 w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4 -\u03b4 (8)\nProof. For simplicity, we make the following notations throughout the proof. We let \u03b6 := \u03b6(x, y) denote the multiplicative deviation of the propensity estimate from the true propensity model, and \u2206 := \u2206(x, y) be the additive deviation of the reward model from the true reward. Recall\n\u03b6(x, y) = 1 - \u03c0 0 (y|x) \u03c00 (y|x)(16)\n\u2206(x, y) = \u03b4(x, y) -\u03b4(x, y).\nMoreover, the \u03c3 2 r := \u03c3 2 r (x, y) is used to denote the randomness in reward r(x, y) with \u03c3 2 r (x, y) = V r (r(x, y)|x, y). Moreover, we denote the true IPS weight \u03c0(y|x) \u03c00(y|x) as c(x, y) with the estimated version being \u0109(x, y). Also, let w \u03b1 := w \u03b1 xy , w \u03b2 := w \u03b2 xy and w \u03b3 := w \u03b3 xy be the abbreviation for the weighting functions.\nWe will start the proof by calculating the expectation of three different components of Rw (\u03c0). For the \u03b1 i\u0233 component, this term is independent of the distribution of y i , and we have:\nE 1 n n i=1 \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 i\u0233 \u03b1 i\u0233 = E x \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b4(x, \u0233) = E x E y\u223c\u03c0 w \u03b1 xy (\u03b4 + \u2206)(18)\nFor the IPS term \u03b2 i , with x i \u223c P (X ) and\ny i \u223c \u03c0 0 (Y |x). E 1 n n i=1 \u03c0(y i |x i ) w \u03b2 i \u03b2 i = E x E y\u223c\u03c00 E r w \u03b2 xy \u03c0(y|x) \u03c00 (y|x) r(x, y) = E x E y\u223c\u03c00 w \u03b2 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) \u03b4 = E x E y\u223c\u03c00 c w \u03b2 xy (1 -\u03b6)\u03b4 = E x E y\u223c\u03c0 w \u03b2 xy (1 -\u03b6)\u03b4(19)\nwhere the second equation follows from the fact that conditioning on (x, y), E r [r(x, y)|x, y] = \u03b4(x, y). For the third term \u03b3 i , we have\nE 1 n n i=1 \u03c0(y i |x i ) w \u03b3 i \u03b3 i = E x E y\u223c\u03c00 w \u03b3 xy \u03c0(y|x) \u03c00 (y|x) \u03b4(x, y) = E x E y\u223c\u03c00 w \u03b3 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) (\u03b4 + \u2206) = E x E y\u223c\u03c00 c w \u03b3 xy (1 -\u03b6)(\u03b4 + \u2206) = E x E y\u223c\u03c0 w \u03b3 xy (1 -\u03b6)(\u03b4 + \u2206)(20)\nCombining these three terms and using the formula that Bias(\nRw (\u03c0)) = E[ Rw (\u03c0)] -E x E y\u223c\u03c0 E r [r], we have Bias( Rw (\u03c0)) = E x E y\u223c\u03c0 w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4 -\u03b4 (21) B.2. Proof of Theorem 2\nTheorem 2 (Variance of the Interpolated Counterfactual Estimator Family). Under the same conditions as in Theorem 1, the variance of Rw (\u03c0) with weighting functions w = (w \u03b1 , w \u03b2 , w \u03b3 ) is\n1 n V x E \u03c0 [w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4] + E x E \u03c0 (w \u03b2 ) 2 c(1 -\u03b6) 2 \u03c3 2 r + E x V \u03c00 (w \u03b2 c(1 -\u03b6)\u03b4 + w \u03b3 c(1 -\u03b6)(\u03b4 + \u2206))(9)\nProof. We follow the same notation as in Appendix B.1. Let Rw\ni (\u03c0) := \u0233\u2208Y \u03c0(\u0233|x i ) w \u03b1 i\u0233 \u03b1 i\u0233 + \u03c0(y i |x i ) w \u03b2 i \u03b2 i + \u03c0(y i |x i ) w \u03b3 i \u03b3 i\nwith the abbreviated version defined as R w xy (\u03c0), and it is easy to see that\nV( Rw (\u03c0)) = 1 n V( Rw i (\u03c0)). V( Rw i (\u03c0)) = V x E y\u223c\u03c00,r [R w xy (\u03c0) |x] + E x V y\u223c\u03c00,r (R w xy (\u03c0) |x) = V x E y\u223c\u03c00,r [R w xy (\u03c0) |x] + E x E y\u223c\u03c00 [V r (R w xy (\u03c0) |x, y)|x] + E x V y\u223c\u03c00 (E r [R w xy (\u03c0) |x, y]|x)(22)\nFor the first term, using the bias formula in Appendix B.1, it is easy to see that\nV x E y\u223c\u03c00,r [R w xy (\u03c0) |x] = V x E y\u223c\u03c0 [w \u03b1 xy \u2206 -w \u03b2 xy \u03b6\u03b4 + w \u03b3 xy (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 x\u0233 + w \u03b2 xy + w \u03b3 xy )\u03b4|x](23)\nFor the second term, we will calculate\nV r (R w xy (\u03c0) |x, y) first. V r (R w xy (\u03c0) |x, y) = V r w \u03b2 xy \u03c0(y|x) \u03c00 (y|x) r(x, y)|x, y = V r w \u03b2 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) r|x, y = c 2 (w \u03b2 xy ) 2 (1 -\u03b6) 2 V r (r|x, y) = c 2 (w \u03b2 xy ) 2 (1 -\u03b6) 2 \u03c3 2 r (24\n)\nwhere the first equality follows from the fact that conditioning on (x, y), \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b1 x\u0233 + \u03c0(y|x) w \u03b3 xy \u03b3 xy is just a constant, and we use the formula V(a + X) = V(X) for any constant a, random variable X.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Then for the term E", "text": "x E y\u223c\u03c00 [V r (R w xy (\u03c0) |x, y)|x] , we have E x E y\u223c\u03c00 [V r (R w xy (\u03c0) |x, y)|x] = E x E y\u223c\u03c00 c 2 (w \u03b2 xy ) 2 (1 -\u03b6) 2 \u03c3 2 r = E x E y\u223c\u03c0 c(w \u03b2 xy ) 2 (1 -\u03b6) 2 \u03c3 2 r (25)\nSimilarly, for the third term, we will calculate\nE r [R w xy (\u03c0) |x, y] first. E r [R w xy (\u03c0) |x, y] = E r \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b1 i\u0233 + \u03c0(y|x) w \u03b2 xy \u03b2 xy + \u03c0(y|x) w \u03b3 xy \u03b3 xy |x, y = \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b4(x, \u0233) + E r w \u03b2 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) r|x, y + w \u03b3 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) (\u03b4 + \u2206) = \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b4(x, \u0233) + w \u03b2 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) \u03b4 + w \u03b3 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) (\u03b4 + \u2206) = \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b4(x, \u0233) + w \u03b2 xy c(1 -\u03b6)\u03b4 + w \u03b3 xy c(1 -\u03b6)(\u03b4 + \u2206)(26)\nFor the term V y\u223c\u03c00 E r [R w xy (\u03c0) |x, y]|x , since the first term \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b4(x, \u0233) is independent of y, then we have\nV y\u223c\u03c00 E r [R w xy (\u03c0) |x, y]|x = V y\u223c\u03c00 \u0233\u2208Y \u03c0(\u0233|x i ) w \u03b1 x\u0233 \u03b4(x i , \u0233) + w \u03b2 xy c(1 -\u03b6)\u03b4 + w \u03b3 xy c(1 -\u03b6)(\u03b4 + \u2206)|x = V y\u223c\u03c00 w \u03b2 xy c(1 -\u03b6)\u03b4 + w \u03b3 xy c(1 -\u03b6)(\u03b4 + \u2206)|x(27)\nThen taking the outer expectation over x, we have:\nE x V y\u223c\u03c00 (E r [R w xy (\u03c0) |x, y]|x) = E x V y\u223c\u03c00 (w \u03b2 xy c(1 -\u03b6)\u03b4 + w \u03b3 xy c(1 -\u03b6)(\u03b4 + \u2206)|x)(28)\nSumming all the three terms together, and using the formula\nV( Rw (\u03c0)) = 1 n V( Rw i (\u03c0)) for i.i.d Rw i , we have: V( Rw (\u03c0)) = 1 n V x E \u03c0 [w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4] + E x E \u03c0 (w \u03b2 ) 2 c(1 -\u03b6) 2 \u03c3 2 r + E x V \u03c00 (w \u03b2 c(1 -\u03b6)\u03b4 + w \u03b3 c(1 -\u03b6)(\u03b4 + \u2206))(29)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3. Proof of Theorem 3", "text": "Theorem 3 (Bias of CAB). For contexts x 1 , x 2 , \u2022 \u2022 \u2022 , x n drawn i.i.d from some distribution P (X ) and for actions y i \u223c \u03c0 0 (Y |x i ), under Condition 1 the bias of RCAB (\u03c0) is\nE x E \u03c0 [-\u03b4\u03b61{\u0109 \u2264 M } + {\u2206(1 - M c(1 -\u03b6) ) - M c(1 -\u03b6) \u03b4\u03b6}1{\u0109 > M }](30)\nProof. Note CAB falls into the class of counterfactual estimator with the weighting functions w\n\u03b1 i\u0233 = 1 - min M \u03c00(\u0233|xi) \u03c0(\u0233|xi) , 1 , w \u03b2 i = min M \u03c00(yi|xi) \u03c0(yi|xi) , 1 , w \u03b3 i = 0.\nUsing Theorem 1, the bias for RCAB (\u03c0) is:\nBias( RCAB (\u03c0)) = E x E y\u223c\u03c0 w \u03b1 xy \u2206 -w \u03b2 xy \u03b6\u03b4 + w \u03b3 xy (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 x\u0233 + w \u03b2 xy + w \u03b3 xy )\u03b4 -\u03b4 = E x E y\u223c\u03c0 (1 -min{ M \u0109(x, y) , 1})\u2206 -min{ M \u0109(x, y) , 1}\u03b6\u03b4 = E x E y\u223c\u03c0 -\u03b6\u03b4 1{\u0109 \u2264 M} +{(1 - M \u0109(x, y) )\u2206 - M \u0109(x, y) \u03b6\u03b4} 1{\u0109 > M} = E x E y\u223c\u03c0 -\u03b6\u03b4 1{\u0109 \u2264 M} +{(1 - M c(1 -\u03b6) )\u2206 - M c(1 -\u03b6) \u03b6\u03b4} 1{\u0109 > M}(31)\nwhile the last equality follows from the fact that \u0109(x, y)\n:= \u03c0(y|x) \u03c00(y|x) = \u03c0(y|x) \u03c00(y|x) \u03c00(y|x) \u03c00(y|x) = c(x, y)(1 -\u03b6(x, y))", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C. Experiment Details", "text": "In this section, we provide experiment details for both the BLBF and LTR settings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1. BLBF", "text": "In the BLBF experiment, specifically, given a supervised dataset {(x i , y * i )} n i=1 , where x is i.i.d drawn from a certain fixed distribution P (X ) and y * \u2208 {1, 2, \u2022 \u2022 \u2022 , k} denotes the true class label. For a particular logging policy \u03c0 0 , the logged bandit data is simulated by sampling y i \u223c \u03c0 0 (Y|x i ) and a deterministic loss r(x i , y i ) is revealed. In our experiments, the loss is defined as r(x i , y i ) = 1{y i = y * i } -1. The resulting logged contextual bandit data S = {x i , y i , r(x i , y i ), \u03c0 0 (y i |x i )} is then used to evaluate the performance of different estimators.\nFor evaluation, we split each dataset equally into train and test sets. For the train set, we use 10% of the full-information data to train the logger \u03c0 0 and loss predictor \u03c0 r (x), with loss estimates defined by \u03b4(x i , y) = 1{\u03c0 r (x i ) = y} -1. The policy \u03c0 we want to evaluate is a multiclass logistic regression trained on the whole train set. Finally, we use the full-information test set to generate the contextual bandit datasets S for off-policy evaluation of sizes n = 200, 500, 2000. We evaluate the policy \u03c0 with different estimators on the logged bandit feedback of different sizes and treat the performance on the full-information test set as ground truth R(\u03c0). The performance is measured by MSE. We repeat each experiment 500 times and calculate the bias, variance and MSE.\nFor learning, we first split the original dataset into training (48%), validation (32%) and test sets (20%). Following (Swaminathan & Joachims, 2015a), the policy we want to learn lies on the space F := {\u03c0 w : w \u2208 R p } with \u03c0 w as the stochastic linear rules defined by: \u03c0 w (y|x) = exp(w T \u03c6(x, y))\nZ(x)(42)\nHere, \u03c6(x, y) denotes the joint feature map between context x and action y, and Z(x) is a normalization factor. The training objective is defined by \u03c0 est = argmin \u03c0w\u2208F Rest (\u03c0 w ) + \u03bb||w|| 2 , where \u03bb is selected through the lowest RIP S (\u03c0) on the validation set. To avoid local minimum, the objective is optimized via L-BFGS using scikit-learn with 10 random starts. The performance of the learned policy \u03c0 est is measured via expected error on the test set, defined as:\n1 ntest ntest i=1 E yi\u223c\u03c0 est (Y|xi) [1{y i = y * i }].\nSimilar to evaluation, we use 20% of the training data to train the multiclass logistics regression as logging policy with default hyperparameter. While for the estimated loss \u03b4(x, y), we train the logistic regression using 10% training data with tuned hyperparameter selected from the validation set. All the result is averaged over 10 runs with n = 5000.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2. LTR", "text": "In the LTR experiment, we use 10% of the training set for learning a DM, which reflects that we typically have a small amount of manual relevance judgements. The DM is a binary Gradient Boosted Decision Tree Classifier calibrated by Sigmoid Calibration (Platt et al., 1999). We use \u03bb(rank) = rank as the performance metric which can be interpreted as the average rank of the relevant results. The examination probability (propensity) that we use is p(x, \u03c0 0 (x), d) = 1 rank(d|\u03c00(x)) . For the evaluation experiments, to get a ranking policy for evaluation, we train a ranking SVM (Joachims, 2002) on the remaining 90% training data. As input to the estimators, different amounts of click data are generated from the test set. For each experiment, we generate the log data 100 times and report the bias, variance, and MSE with respect to the estimated ground truth from the full-information test set.\nFor the learning experiments, we derived a concrete learning algorithm based on propensity SVM-Rank that conducts learning from biased user feedback using different estimators. The SVM-style algorithm (Joachims, 2002;2006;Joachims et al., 2017) optimizes an upper bound on different estimators and details are in Appendix C.3. We compare the performance of different estimators using different amounts of simulated user feedback with the proposed learning algorithm. As input to the propensity SVM-Rank, different amount of click data is simulated from the 90% training data. Specifically, we present the performance using 1 sweep of the data in Table 2. We grid search C for propensity SVM-Rank and M for different estimators and conduct hyperparameter selection with 90 percentile cIPS on user feedback data simulated from the validation set for 5 sweeps. All the experiments are run for 5 times and the average is presented.", "publication_ref": ["b17", "b10", "b10", "b11", "b12"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "B.4. Proof of Theorem 4", "text": "Theorem 4 (Variance of CAB). Under the same conditions as in Theorem 3, the variance of RCAB (\u03c0)\nProof. The result follows by plugging in the weighting function for CAB with\nFor the term\n, using the result from Theorem 3, we have:\nFor the term\nFor the last term\nCombining all, we have\nB.5. Proof of Bias and Variance of CAB-DR Theorem 5 (Bias of CAB-DR). For contexts x 1 , x 2 , \u2022 \u2022 \u2022 , x n drawn i.i.d from some distribution P (X ) and for actions\nProof. CAB-DR is also an instance in the Interpolated Counterfactual Estimator Family with the weighting function:\nUsing Theorem 1, the bias for CAB-DR is:\nTheorem 6 (Variance of CAB-DR). Under the same conditions as in Theorem 3, the variance of RCABDR (\u03c0)\nProof. The proof follows by using Theorem 2 with the weights w \u03b1 i\u0233 = 1, w\nFor the first term, following directly from Appendix B.5, it is easy to see\nFor the second term\nsince CAB and CAB-DR has the same weighting function w \u03b2 , this term is exactly the same for CAB and CAB-DR.\nFor the third term\nCombining all the three terms will give us the variance for RCABDR (\u03c0).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3. Generalized Propensity SVM-Rank", "text": "We now derive a concrete learning algorithm that conducts learning from biased user feedback using different estimators from the Interpolated Counterfactual Estimator Family. It is based on SVM-Rank (Joachims, 2002;2006;Joachims et al., 2017) but we expect other learning to rank methods can also be adapted to the estimators.\nThe generalized propensity SVM-Rank learns a linear scoring function f (x, d) = w \u2022 \u03c6(x, d) with \u03c6(x, d) describing how context x and document d interact. It optimizes the following objective\nwhere w is the parameter of the generalized propensity SVM-Rank and C is a regularization parameter. The training objective optimizes an upper bound on the estimator with average rank of positive examples metric(\u03bb(rank) = rank) since i j", "publication_ref": ["b10", "b11", "b12"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Taming the monster: A fast and simple algorithm for contextual bandits", "journal": "", "year": "2014", "authors": "A Agarwal; D Hsu; S Kale; J Langford; L Li; R Schapire"}, {"ref_id": "b1", "title": "Estimating position bias without intrusive interventions", "journal": "ACM", "year": "2019", "authors": "A Agarwal; I Zaitsev; X Wang; C Li; M Najork; T Joachims"}, {"ref_id": "b2", "title": "Uci machine learning repository", "journal": "", "year": "2007", "authors": "Asuncion ; A Newman; D "}, {"ref_id": "b3", "title": "Doubly robust estimation in missing data and causal inference models", "journal": "Biometrics", "year": "2005", "authors": "H Bang; J M Robins"}, {"ref_id": "b4", "title": "The offset tree for learning with partial labels", "journal": "ACM", "year": "2009", "authors": "A Beygelzimer; J Langford"}, {"ref_id": "b5", "title": "Counterfactual reasoning and learning systems: The example of computational advertising", "journal": "The Journal of Machine Learning Research", "year": "2013", "authors": "L Bottou; J Peters; J Qui\u00f1onero-Candela; D X Charles; D M Chickering; E Portugaly; D Ray; P Simard; E Snelson"}, {"ref_id": "b6", "title": "Doubly robust policy evaluation and learning", "journal": "", "year": "2011", "authors": "M Dud\u00edk; J Langford; L Li"}, {"ref_id": "b7", "title": "Intervention harvesting for context-dependent examination-bias estimation", "journal": "", "year": "2019", "authors": "Z Fang; A Agarwal; T Joachims"}, {"ref_id": "b8", "title": "More robust doubly robust off-policy evaluation", "journal": "", "year": "2018", "authors": "M Farajtabar; Y Chow; M Ghavamzadeh"}, {"ref_id": "b9", "title": "A generalization of sampling without replacement from a finite universe", "journal": "Journal of the American statistical Association", "year": "1952", "authors": "D G Horvitz; D J Thompson"}, {"ref_id": "b10", "title": "Optimizing search engines using clickthrough data", "journal": "", "year": "2002", "authors": "T Joachims"}, {"ref_id": "b11", "title": "Training linear SVMs in linear time", "journal": "", "year": "2006", "authors": "T Joachims"}, {"ref_id": "b12", "title": "Unbiased learning-to-rank with biased feedback", "journal": "", "year": "2017", "authors": "T Joachims; A Swaminathan; T Schnabel"}, {"ref_id": "b13", "title": "Deep learning with logged bandit feedback", "journal": "", "year": "2018", "authors": "T Joachims; A Swaminathan; M De Rijke"}, {"ref_id": "b14", "title": "Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data", "journal": "Statistical science", "year": "2007", "authors": "J D Kang; J L Schafer"}, {"ref_id": "b15", "title": "Statistical analysis with missing data", "journal": "Wiley", "year": "2019", "authors": "R J Little; D B Rubin"}, {"ref_id": "b16", "title": "On the design of estimators for off-policy evaluation", "journal": "", "year": "2018", "authors": "Nikos Vlassis; Aurelien Bibaut; T J "}, {"ref_id": "b17", "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "journal": "Advances in Large Margin Classifiers", "year": "1999", "authors": "J Platt"}, {"ref_id": "b18", "title": "Semiparametric efficiency in multivariate regression models with missing data", "journal": "Journal of the American Statistical Association", "year": "1995", "authors": "J M Robins; A Rotnitzky"}, {"ref_id": "b19", "title": "Multiple imputation for nonresponse in surveys", "journal": "John Wiley and Sons", "year": "2004", "authors": "D B Rubin"}, {"ref_id": "b20", "title": "Analysis of incomplete multivariate data", "journal": "Chapman and Hall/CRC", "year": "1997", "authors": "J L Schafer"}, {"ref_id": "b21", "title": "Learning from logged implicit exploration data", "journal": "", "year": "2011", "authors": "A Strehl; J Langford; L Li; S M Kakade"}, {"ref_id": "b22", "title": "Batch learning from logged bandit feedback through counterfactual risk minimization", "journal": "Journal of Machine Learning Research (JMLR)", "year": "2015-09", "authors": "A Swaminathan; T Joachims"}, {"ref_id": "b23", "title": "The self-normalized estimator for counterfactual learning", "journal": "", "year": "2015", "authors": "A Swaminathan; T Joachims"}, {"ref_id": "b24", "title": "Data-efficient off-policy policy evaluation for reinforcement learning", "journal": "", "year": "2016", "authors": "P Thomas; E Brunskill"}, {"ref_id": "b25", "title": "Position bias estimation for unbiased learning to rank in personal search", "journal": "ACM", "year": "2018", "authors": "X Wang; N Golbandi; M Bendersky; D Metzler; M Najork"}, {"ref_id": "b26", "title": "Optimal and adaptive off-policy evaluation in contextual bandits", "journal": "", "year": "2017", "authors": "Y.-X Wang; A Agarwal; M Dudik"}, {"ref_id": "b27", "title": "Cost-sensitive learning by cost-proportionate example weighting", "journal": "IEEE", "year": "2003", "authors": "B Zadrozny; J Langford; N Abe"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. The Bias, Variance and MSE graph for CAB on the SATIMAGE and YAHOO! LTR dataset. From left to right: (a) The bias, variance and MSE curves for the SATIMAGE dataset. (b) MSE curves for SATIMAGE when we vary the amount of log data. (c) The bias, variance and MSE curves for the YAHOO! LTR dataset. (d) MSE curves for YAHOO! LTR when we vary the amount of log data.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. MSE comparison of various off-policy estimators for different datasets", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. Error of the estimates on the Amazon Music contextual bandit problem.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Test set learning performance under various scenarios. (1)-(2) Performance vs. amount of data for PENDIGITS and YAHOO! LTR. (3) Performance vs. regression model quality for PENDIGITS. (4) Performance vs. logger quality for PENDIGITS.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "How does learning performance change when we vary the quality of the estimated regression model? In order to vary the quality of the regression model used by DM, DR, CAB, CAB-DR, SWITCH and SB, we vary the fraction of full-information data that is used to learn the regression model. The results are shown in plot (3) of Figure 4. IPS and cIPS result in two flat lines (up to variance), as they do not rely on the regression model. The other methods improve with the quality of the regression model, and both CAB and CAB-DR do well over the whole range.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Overview of how the family of estimators Rw (\u03c0) subsumes existing off-policy estimators.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Test set learning performance on various datasets.", "figure_data": "DATALETTER OPTDIGITS SATIMAGE PENDIGITS YAHOO! LTRDM0.63720.06490.30830.113311.25DR0.68520.04710.27620.1191-IPS0.89690.06950.32660.274811.17CIPS0.85040.04470.24150.122811.39SB0.60910.04600.24810.094910.98SWITCH----10.95CAB0.57400.04450.24420.091710.83CAB-DR0.58770.04610.27620.0946-How does learning performance change when we in-crease the amount of training data? Plots (1) and ("}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The weight functions for different estimators of the family Rw (\u03c0) in the ranking setting.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "S = {(x i , y i , r i , \u03c0 0 (\u2022|x i ))} n i=1 ,(1)", "formula_coordinates": [2.0, 110.09, 709.36, 179.35, 12.69]}, {"formula_id": "formula_1", "formula_text": "R(\u03c0) = E x\u223cP (x) E \u0233\u223c\u03c0(\u0233|x) E r\u223cD(r|x,\u0233) [r].(2)", "formula_coordinates": [2.0, 337.67, 146.55, 203.77, 9.96]}, {"formula_id": "formula_2", "formula_text": "\u03c0 * = argmax \u03c0\u2208\u03a0 R(\u03c0) .(3)", "formula_coordinates": [2.0, 379.21, 225.42, 162.23, 18.59]}, {"formula_id": "formula_3", "formula_text": "\u03c0 * = argmax \u03c0\u2208\u03a0 R(\u03c0) ,(4)", "formula_coordinates": [2.0, 378.35, 381.02, 163.09, 19.04]}, {"formula_id": "formula_4", "formula_text": "w \u03b1 i (\u0233) w \u03b2 i w \u03b3 i DM 1 0 0 IPS 0 1 0 cIPS 0 min M \u03c00(yi|xi) \u03c0(yi|xi) ,1 0 DR 1 1 -1 SB 1 -\u03c4 \u03c4 0 SWITCH 1 \u03c0(\u0233|xi) \u03c00(\u0233|xi) > M 1 \u03c0(yi|xi) \u03c00(yi|xi) \u2264 M 0", "formula_coordinates": [3.0, 63.03, 99.89, 217.9, 95.53]}, {"formula_id": "formula_5", "formula_text": "Rw (\u03c0) = 1 n n i=1 \u0233\u2208Y \u03c0(\u0233|x i ) w \u03b1 i\u0233 \u03b1 i\u0233 + 1 n n i=1 \u03c0(y i |x i ) w \u03b2 i \u03b2 i + 1 n n i=1 \u03c0(y i |x i ) w \u03b3 i \u03b3 i with \u03b1 i\u0233 := \u03b1(x i , \u0233) := \u03b4(x i , \u0233), \u03b2 i := \u03b2(x i , y i ) := r(x i , y i ) \u03c00 (y i |x i ) , \u03b3 i := \u03b3(x i , y i ) := \u03b4(x i , y i ) \u03c00 (y i |x i ) .", "formula_coordinates": [3.0, 56.62, 238.73, 239.2, 145.48]}, {"formula_id": "formula_6", "formula_text": "\u03b6 := \u03b6(x, y) = 1 - \u03c0 0 (y|x) \u03c00 (y|x)(5)", "formula_coordinates": [4.0, 115.55, 148.24, 173.89, 22.31]}, {"formula_id": "formula_7", "formula_text": "\u2206 := \u2206(x, y) = \u03b4(x, y) -\u03b4(x, y).(6)", "formula_coordinates": [4.0, 102.53, 179.84, 186.91, 11.59]}, {"formula_id": "formula_8", "formula_text": "\u03c3 2 r := \u03c3 2 r (x, y) = V r (r(x, y)|x, y)(7)", "formula_coordinates": [4.0, 103.01, 216.5, 186.43, 12.69]}, {"formula_id": "formula_9", "formula_text": "E x E y\u223c\u03c0 w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4 -\u03b4 (8) Proof: See Appendix B.1.", "formula_coordinates": [4.0, 55.13, 442.4, 234.31, 60.84]}, {"formula_id": "formula_10", "formula_text": "1 n V x E \u03c0 [w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4] + E x E \u03c0 (w \u03b2 ) 2 c(1 -\u03b6) 2 \u03c3 2 r + E x V \u03c00 (w \u03b2 c(1 -\u03b6)\u03b4 + w \u03b3 c(1 -\u03b6)(\u03b4 + \u2206)) (9) Proof: See Appendix B.2.", "formula_coordinates": [4.0, 55.13, 562.68, 234.31, 100.73]}, {"formula_id": "formula_11", "formula_text": "RCAB (\u03c0) = Rw (\u03c0) with \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 w \u03b1 i\u0233 = 1 -min M \u03c00(\u0233|xi) \u03c0(\u0233|xi) , 1 w \u03b2 i = min M \u03c00(yi|xi) \u03c0(yi|xi) , 1 w \u03b3 i = 0", "formula_coordinates": [4.0, 309.6, 318.0, 230.07, 48.84]}, {"formula_id": "formula_12", "formula_text": "y i \u223c \u03c0 0 (Y |x i ), under Condition 1 the bias of RCAB (\u03c0) is E x E \u03c0 -\u03b4\u03b61{\u0109 \u2264 M } (10) +{\u2206(1 - M c(1-\u03b6) ) - M c(1-\u03b6) \u03b4\u03b6}1{\u0109 > M } Proof: See Appendix B.3", "formula_coordinates": [4.0, 307.13, 598.5, 234.31, 88.83]}, {"formula_id": "formula_13", "formula_text": "1 n V x E \u03c0 [\u03b4 -\u03b4\u03b61{\u0109 \u2264 M } + (\u2206(1 - M c(1 -\u03b6) ) - M c(1 -\u03b6) \u03b4\u03b6)1{\u0109 > M }] + E x E \u03c0 c(1 -\u03b6) 2 \u03c3 2 r 1{\u0109 \u2264 M } + M 2 c \u03c3 2 r 1{\u0109 > M } + E x V \u03c00 (c(1 -\u03b6)\u03b41{\u0109 \u2264 M } + M \u03b41{\u0109 > M }) (11) Proof: See Appendix B.4", "formula_coordinates": [5.0, 55.13, 140.81, 234.31, 140.93]}, {"formula_id": "formula_14", "formula_text": "Bias( RcIP S (\u03c0)) = E x E \u03c0 [-\u03b4(1 -M c )1{c > M }] for cIPS and Bias( RCAB (\u03c0)) = E x E \u03c0 [\u2206(1-M c )1{c > M }] for CAB.", "formula_coordinates": [5.0, 55.44, 440.57, 234.17, 37.3]}, {"formula_id": "formula_15", "formula_text": "RDM (\u03c0)) = E x E \u03c0 [\u2206] while Bias( RCAB (\u03c0)) = E x E \u03c0 [\u2206(1 - M c )1{c > M }]", "formula_coordinates": [5.0, 55.44, 568.01, 234.0, 38.02]}, {"formula_id": "formula_16", "formula_text": "1 n V x E \u03c0 [\u03b4] + E x E \u03c0 c\u03c3 2 r + E x V \u03c00 (c\u03b4)(12)", "formula_coordinates": [5.0, 68.02, 699.66, 221.42, 22.31]}, {"formula_id": "formula_17", "formula_text": "1 n V x E \u03c0 [\u03b4 + \u2206(1 - M c )1{c > M }] + E x E \u03c0 c\u03c3 2 r 1{c \u2264 M } + M 2 c \u03c3 2 r 1{c > M } + E x V \u03c00 [(c\u03b4)1{c \u2264 M } + M \u03b41{c > M }] .(13)", "formula_coordinates": [5.0, 314.24, 87.11, 227.2, 64.33]}, {"formula_id": "formula_18", "formula_text": "M E \u03c0 [\u03c3 2 r ] + M 2 E x [V \u03c00 (\u03b4)]", "formula_coordinates": [5.0, 410.96, 235.29, 110.41, 12.2]}, {"formula_id": "formula_19", "formula_text": "1 n V x E \u03c0 [\u03b4] + E x E \u03c0 c\u03c3 2 r + E x V \u03c00 (c\u2206)(14)", "formula_coordinates": [5.0, 318.28, 378.72, 223.17, 22.31]}, {"formula_id": "formula_20", "formula_text": "RCABDR (\u03c0) = Rw (\u03c0) with \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 w \u03b1 i\u0233 = 1 w \u03b2 i = min M \u03c00(yi|xi) \u03c0(yi|xi) , 1 w \u03b3 i = -min M \u03c00(\u0233|xi) \u03c0(\u0233|xi) ,1", "formula_coordinates": [5.0, 309.6, 543.27, 224.98, 46.75]}, {"formula_id": "formula_21", "formula_text": "E x E \u03c0 \u03b6\u22061{\u0109 \u2264 M } + \u2206(1 - M c )1{\u0109 > M } .", "formula_coordinates": [5.0, 324.48, 630.12, 199.92, 22.31]}, {"formula_id": "formula_22", "formula_text": "E x E \u03c0 [\u03b6\u03b41{\u0109 \u2264 M }] compared to E x E \u03c0 [\u03b6\u22061{\u0109 \u2264 M }] for CAB-DR.", "formula_coordinates": [5.0, 307.44, 696.3, 234.0, 21.61]}, {"formula_id": "formula_23", "formula_text": "E x V \u03c00 c(-\u2206)1{c \u2264 M } + M (-\u2206)1{c > M } ,", "formula_coordinates": [6.0, 62.15, 116.47, 220.59, 9.65]}, {"formula_id": "formula_24", "formula_text": "E x V \u03c00 c(\u03b4)1{c \u2264 M } + M (\u03b4)1{c > M } .", "formula_coordinates": [6.0, 73.39, 158.17, 198.1, 9.65]}, {"formula_id": "formula_25", "formula_text": "E x [V \u03c0 (-w \u03b2 c(1 -\u03b6)\u2206)],", "formula_coordinates": [6.0, 185.38, 202.51, 105.3, 11.23]}, {"formula_id": "formula_26", "formula_text": "E x [V \u03c0 (w \u03b2 c(1 -\u03b6)\u03b4)].", "formula_coordinates": [6.0, 192.87, 214.46, 93.52, 11.22]}, {"formula_id": "formula_27", "formula_text": "R(\u03c0) = \u2206(\u03c0(x)|x, r)dP (x)(15)", "formula_coordinates": [11.0, 235.47, 252.58, 305.97, 8.96]}, {"formula_id": "formula_28", "formula_text": ", c(x, \u03c0 0 (x), d) = r(x, d) \u2022 o(x, \u03c0 0 (x), d).", "formula_coordinates": [11.0, 85.95, 337.19, 168.37, 9.65]}, {"formula_id": "formula_29", "formula_text": "S = {{(x i , d ij , p ij , c ij )} mi j=1 } n i=1 where m i is the number of candidates for context x i , p ij is p(x i , \u03c0 0 (x i ), d ij ) and c ij is c(x i , \u03c0 0 (x i ), d ij ).", "formula_coordinates": [11.0, 55.44, 382.62, 486.0, 23.99]}, {"formula_id": "formula_30", "formula_text": "Rw (\u03c0) = 1 n n i=1 mi j=1 w \u03b1 ij \u03b1 ij + o ij w \u03b2 ij \u03b2 ij + o ij w \u03b3 ij \u03b3 ij \u2022 \u03bb(rank(d ij |x i , \u03c0(x i )))", "formula_coordinates": [11.0, 138.4, 424.55, 321.27, 30.44]}, {"formula_id": "formula_31", "formula_text": "\u03b1 ij := \u03b4(x i , d ij ), \u03b2 ij := r ij p ij , \u03b3 ij := \u03b4(x i , d ij ) p ij .", "formula_coordinates": [11.0, 192.54, 460.08, 188.42, 25.85]}, {"formula_id": "formula_32", "formula_text": "w \u03b1 ij w \u03b2 ij w \u03b3 ij DM 1 0 0 IPS 0 1 0 cIPS 0 min{M p ij ,1} 0 SB 1 -\u03c4 \u03c4 0 SWITCH 1 1 pij > M 1 1 pij \u2264 M 0 CAB 1 -min{M p ij ,1} min{M p ij ,1}0", "formula_coordinates": [11.0, 197.68, 564.62, 200.61, 91.05]}, {"formula_id": "formula_33", "formula_text": "c ij through o ij \u03b2 ij = oij rij pij = cij pij .", "formula_coordinates": [11.0, 149.76, 692.05, 136.3, 14.61]}, {"formula_id": "formula_34", "formula_text": "E x E y\u223c\u03c0 w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4 -\u03b4 (8)", "formula_coordinates": [12.0, 201.29, 181.28, 340.15, 32.73]}, {"formula_id": "formula_35", "formula_text": "\u03b6(x, y) = 1 - \u03c0 0 (y|x) \u03c00 (y|x)(16)", "formula_coordinates": [12.0, 252.12, 271.21, 289.32, 22.31]}, {"formula_id": "formula_37", "formula_text": "E 1 n n i=1 \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 i\u0233 \u03b1 i\u0233 = E x \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b4(x, \u0233) = E x E y\u223c\u03c0 w \u03b1 xy (\u03b4 + \u2206)(18)", "formula_coordinates": [12.0, 117.96, 407.64, 423.48, 30.47]}, {"formula_id": "formula_38", "formula_text": "y i \u223c \u03c0 0 (Y |x). E 1 n n i=1 \u03c0(y i |x i ) w \u03b2 i \u03b2 i = E x E y\u223c\u03c00 E r w \u03b2 xy \u03c0(y|x) \u03c00 (y|x) r(x, y) = E x E y\u223c\u03c00 w \u03b2 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) \u03b4 = E x E y\u223c\u03c00 c w \u03b2 xy (1 -\u03b6)\u03b4 = E x E y\u223c\u03c0 w \u03b2 xy (1 -\u03b6)\u03b4(19)", "formula_coordinates": [12.0, 168.19, 451.9, 373.25, 119.32]}, {"formula_id": "formula_39", "formula_text": "E 1 n n i=1 \u03c0(y i |x i ) w \u03b3 i \u03b3 i = E x E y\u223c\u03c00 w \u03b3 xy \u03c0(y|x) \u03c00 (y|x) \u03b4(x, y) = E x E y\u223c\u03c00 w \u03b3 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) (\u03b4 + \u2206) = E x E y\u223c\u03c00 c w \u03b3 xy (1 -\u03b6)(\u03b4 + \u2206) = E x E y\u223c\u03c0 w \u03b3 xy (1 -\u03b6)(\u03b4 + \u2206)(20)", "formula_coordinates": [12.0, 155.35, 618.72, 386.09, 98.66]}, {"formula_id": "formula_40", "formula_text": "Rw (\u03c0)) = E[ Rw (\u03c0)] -E x E y\u223c\u03c0 E r [r], we have Bias( Rw (\u03c0)) = E x E y\u223c\u03c0 w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4 -\u03b4 (21) B.2. Proof of Theorem 2", "formula_coordinates": [13.0, 55.44, 67.7, 486.0, 99.28]}, {"formula_id": "formula_41", "formula_text": "1 n V x E \u03c0 [w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4] + E x E \u03c0 (w \u03b2 ) 2 c(1 -\u03b6) 2 \u03c3 2 r + E x V \u03c00 (w \u03b2 c(1 -\u03b6)\u03b4 + w \u03b3 c(1 -\u03b6)(\u03b4 + \u2206))(9)", "formula_coordinates": [13.0, 193.0, 215.6, 348.44, 60.58]}, {"formula_id": "formula_42", "formula_text": "i (\u03c0) := \u0233\u2208Y \u03c0(\u0233|x i ) w \u03b1 i\u0233 \u03b1 i\u0233 + \u03c0(y i |x i ) w \u03b2 i \u03b2 i + \u03c0(y i |x i ) w \u03b3 i \u03b3 i", "formula_coordinates": [13.0, 55.44, 316.69, 487.93, 28.46]}, {"formula_id": "formula_43", "formula_text": "V( Rw (\u03c0)) = 1 n V( Rw i (\u03c0)). V( Rw i (\u03c0)) = V x E y\u223c\u03c00,r [R w xy (\u03c0) |x] + E x V y\u223c\u03c00,r (R w xy (\u03c0) |x) = V x E y\u223c\u03c00,r [R w xy (\u03c0) |x] + E x E y\u223c\u03c00 [V r (R w xy (\u03c0) |x, y)|x] + E x V y\u223c\u03c00 (E r [R w xy (\u03c0) |x, y]|x)(22)", "formula_coordinates": [13.0, 63.07, 331.7, 478.37, 68.32]}, {"formula_id": "formula_44", "formula_text": "V x E y\u223c\u03c00,r [R w xy (\u03c0) |x] = V x E y\u223c\u03c0 [w \u03b1 xy \u2206 -w \u03b2 xy \u03b6\u03b4 + w \u03b3 xy (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 x\u0233 + w \u03b2 xy + w \u03b3 xy )\u03b4|x](23)", "formula_coordinates": [13.0, 71.43, 450.17, 470.01, 12.71]}, {"formula_id": "formula_45", "formula_text": "V r (R w xy (\u03c0) |x, y) first. V r (R w xy (\u03c0) |x, y) = V r w \u03b2 xy \u03c0(y|x) \u03c00 (y|x) r(x, y)|x, y = V r w \u03b2 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) r|x, y = c 2 (w \u03b2 xy ) 2 (1 -\u03b6) 2 V r (r|x, y) = c 2 (w \u03b2 xy ) 2 (1 -\u03b6) 2 \u03c3 2 r (24", "formula_coordinates": [13.0, 189.41, 482.6, 347.89, 114.78]}, {"formula_id": "formula_46", "formula_text": ")", "formula_coordinates": [13.0, 537.29, 551.1, 4.15, 8.64]}, {"formula_id": "formula_47", "formula_text": "x E y\u223c\u03c00 [V r (R w xy (\u03c0) |x, y)|x] , we have E x E y\u223c\u03c00 [V r (R w xy (\u03c0) |x, y)|x] = E x E y\u223c\u03c00 c 2 (w \u03b2 xy ) 2 (1 -\u03b6) 2 \u03c3 2 r = E x E y\u223c\u03c0 c(w \u03b2 xy ) 2 (1 -\u03b6) 2 \u03c3 2 r (25)", "formula_coordinates": [13.0, 134.26, 647.33, 407.18, 70.05]}, {"formula_id": "formula_48", "formula_text": "E r [R w xy (\u03c0) |x, y] first. E r [R w xy (\u03c0) |x, y] = E r \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b1 i\u0233 + \u03c0(y|x) w \u03b2 xy \u03b2 xy + \u03c0(y|x) w \u03b3 xy \u03b3 xy |x, y = \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b4(x, \u0233) + E r w \u03b2 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) r|x, y + w \u03b3 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) (\u03b4 + \u2206) = \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b4(x, \u0233) + w \u03b2 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) \u03b4 + w \u03b3 xy \u03c0(y|x) \u03c0 0 (y|x) \u03c0 0 (y|x) \u03c00 (y|x) (\u03b4 + \u2206) = \u0233\u2208Y \u03c0(\u0233|x) w \u03b1 x\u0233 \u03b4(x, \u0233) + w \u03b2 xy c(1 -\u03b6)\u03b4 + w \u03b3 xy c(1 -\u03b6)(\u03b4 + \u2206)(26)", "formula_coordinates": [14.0, 66.35, 68.62, 475.1, 140.41]}, {"formula_id": "formula_49", "formula_text": "V y\u223c\u03c00 E r [R w xy (\u03c0) |x, y]|x = V y\u223c\u03c00 \u0233\u2208Y \u03c0(\u0233|x i ) w \u03b1 x\u0233 \u03b4(x i , \u0233) + w \u03b2 xy c(1 -\u03b6)\u03b4 + w \u03b3 xy c(1 -\u03b6)(\u03b4 + \u2206)|x = V y\u223c\u03c00 w \u03b2 xy c(1 -\u03b6)\u03b4 + w \u03b3 xy c(1 -\u03b6)(\u03b4 + \u2206)|x(27)", "formula_coordinates": [14.0, 67.2, 249.73, 474.24, 42.95]}, {"formula_id": "formula_50", "formula_text": "E x V y\u223c\u03c00 (E r [R w xy (\u03c0) |x, y]|x) = E x V y\u223c\u03c00 (w \u03b2 xy c(1 -\u03b6)\u03b4 + w \u03b3 xy c(1 -\u03b6)(\u03b4 + \u2206)|x)(28)", "formula_coordinates": [14.0, 114.89, 326.59, 426.56, 12.71]}, {"formula_id": "formula_51", "formula_text": "V( Rw (\u03c0)) = 1 n V( Rw i (\u03c0)) for i.i.d Rw i , we have: V( Rw (\u03c0)) = 1 n V x E \u03c0 [w \u03b1 \u2206 -w \u03b2 \u03b6\u03b4 + w \u03b3 (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 + w \u03b2 + w \u03b3 )\u03b4] + E x E \u03c0 (w \u03b2 ) 2 c(1 -\u03b6) 2 \u03c3 2 r + E x V \u03c00 (w \u03b2 c(1 -\u03b6)\u03b4 + w \u03b3 c(1 -\u03b6)(\u03b4 + \u2206))(29)", "formula_coordinates": [14.0, 106.34, 351.94, 435.1, 61.88]}, {"formula_id": "formula_52", "formula_text": "E x E \u03c0 [-\u03b4\u03b61{\u0109 \u2264 M } + {\u2206(1 - M c(1 -\u03b6) ) - M c(1 -\u03b6) \u03b4\u03b6}1{\u0109 > M }](30)", "formula_coordinates": [14.0, 155.18, 501.18, 386.26, 22.31]}, {"formula_id": "formula_53", "formula_text": "\u03b1 i\u0233 = 1 - min M \u03c00(\u0233|xi) \u03c0(\u0233|xi) , 1 , w \u03b2 i = min M \u03c00(yi|xi) \u03c0(yi|xi) , 1 , w \u03b3 i = 0.", "formula_coordinates": [14.0, 55.44, 536.33, 486.0, 29.64]}, {"formula_id": "formula_54", "formula_text": "Bias( RCAB (\u03c0)) = E x E y\u223c\u03c0 w \u03b1 xy \u2206 -w \u03b2 xy \u03b6\u03b4 + w \u03b3 xy (\u2206 -\u03b6(\u03b4 + \u2206)) + (w \u03b1 x\u0233 + w \u03b2 xy + w \u03b3 xy )\u03b4 -\u03b4 = E x E y\u223c\u03c0 (1 -min{ M \u0109(x, y) , 1})\u2206 -min{ M \u0109(x, y) , 1}\u03b6\u03b4 = E x E y\u223c\u03c0 -\u03b6\u03b4 1{\u0109 \u2264 M} +{(1 - M \u0109(x, y) )\u2206 - M \u0109(x, y) \u03b6\u03b4} 1{\u0109 > M} = E x E y\u223c\u03c0 -\u03b6\u03b4 1{\u0109 \u2264 M} +{(1 - M c(1 -\u03b6) )\u2206 - M c(1 -\u03b6) \u03b6\u03b4} 1{\u0109 > M}(31)", "formula_coordinates": [14.0, 96.15, 598.56, 445.29, 95.81]}, {"formula_id": "formula_55", "formula_text": ":= \u03c0(y|x) \u03c00(y|x) = \u03c0(y|x) \u03c00(y|x) \u03c00(y|x) \u03c00(y|x) = c(x, y)(1 -\u03b6(x, y))", "formula_coordinates": [14.0, 278.9, 705.27, 205.4, 14.38]}, {"formula_id": "formula_56", "formula_text": "Z(x)(42)", "formula_coordinates": [17.0, 311.83, 349.65, 229.61, 15.81]}, {"formula_id": "formula_57", "formula_text": "1 ntest ntest i=1 E yi\u223c\u03c0 est (Y|xi) [1{y i = y * i }].", "formula_coordinates": [17.0, 56.64, 436.3, 164.13, 14.56]}], "doi": ""}
