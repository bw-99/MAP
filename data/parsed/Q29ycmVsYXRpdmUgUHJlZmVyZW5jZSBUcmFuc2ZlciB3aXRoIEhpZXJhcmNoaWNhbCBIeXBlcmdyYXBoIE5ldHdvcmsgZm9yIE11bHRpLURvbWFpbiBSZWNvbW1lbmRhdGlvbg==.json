{"Correlative Preference Transfer with Hierarchical Hypergraph Network for Multi-Domain Recommendation": "Zixuan Xu \u2021 , Penghui Wei \u2021 , Shaoguo Liu \u2217 , Weimin Zhang, Liang Wang and Bo Zheng Alibaba Group Beijing, China {xuzixuan.xzx,wph242967,shaoguo.lsg,dutan.zwm,liangbo.wl,bozheng}@alibaba-inc.com", "ABSTRACT": "Advanced recommender systems usually involve multiple domains (such as scenarios or categories) for various marketing strategies, and users interact with them to satisfy diverse demands. The goal of multi-domain recommendation (MDR) is to improve the recommendation performance of all domains simultaneously. Conventional graph neural network based methods usually deal with each domain separately, or train a shared model to serve all domains. The former fails to leverage users' cross-domain behaviors, making the behavior sparseness issue a great obstacle. The latter learns shared user representation with respect to all domains, which neglects users' domain-specific preferences. In this paper we propose H 3 Trans , a h ierarchical h ypergrap h network based correlative preference trans fer framework for MDR, which represents multi-domain user-item interactions into a unified graph to help preference transfer. H 3 Trans incorporates two hyperedge-based modules, namely dynamic item transfer (Hyper-I) and adaptive user aggregation (Hyper-U). Hyper-I extracts correlative information from multi-domain user-item feedbacks for eliminating domain discrepancy of item representations. Hyper-U aggregates users' scattered preferences in multiple domains and further exploits the high-order (not only pair-wise) connections to improve user representations. Experiments on both public and production datasets verify the superiority of H 3 Trans for MDR.", "CCS CONCEPTS": "\u00b7 Informationsystems \u2192 Personalization ; Recommendersystems ; \u00b7 Computing methodologies \u2192 Neural networks .", "KEYWORDS": "Multi-domain Recommendation, Preference Transfer, Hypergraph Learning, Behavior Sparseness \u2020 Co-first authorship. \u2217 Correspondence to: S. Liu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'23, April 30-May 4, 2023, Austin, Texas, USA ACM ISBN 978-1-nnnn-nnnn-n/nn/nn...$15.00 \u00a9 2023 Association for Computing Machinery. Figure 1: Multi-domain recommendation: the definition of domain can be recommendation scenario or item categories . \u2026 \u2026 \u2026 User A User B \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 Cate 1: Clothes Cate 2: Shoes Cate 3: Food Scenarios as domains Categories as domains Scenario 1: Homepage Scenario 3: Live Broadcast Scenario 2: Banner M", "1 INTRODUCTION": "Personalized recommender systems aim to make effective and satisfying choices for users. They usually involve multiple recommendation scenarios or domains, and each scenario contains a set of items that is related to the scenario's topic and marketing strategy. Users interact with these scenarios to satisfy diverse demands. For example, the E-commerce platform Taobao 1 provides diversified shopping spots including product search, homepage feed, banner, live broadcast and so on, as shown in the left part of Fig. 1. Baidu 2 serves as a comprehensive website where users can read news, watch videos and more. Broadly speaking, different item categories can also be regarded as multiple domains. As in the right part of Fig. 1, users usually interact with various categories such as clothes, food and more for their different demands. Multi-domain recommendation (MDR) has attracted increasing research attention, the goal of which is to improve the recommendation performance of all domains simultaneously. There are both commonality and diversity among domains. For the commonality, multiple domains usually have common users and overlapped items, and a user may have similar behavior patterns across domains (for example, preferring ordinary or fashionable goods). The users' domain-invariant preference and items' static information can be shared across domains. For the diversity, the domains have different topics with specific items, thus attract different audiences and cause discrepant data distributions. 1 https://www.taobao.com/ 2 https://www.baidu.com/ https://doi.org/10.1145/nnnnnnn.nnnnnnn WWW'23, April 30-May 4, 2023, Austin, Texas, USA Xu and Wei et al. Graph neural networks (GNNs) have proven to be powerful for recommendations because user-item interactions are naturally suitable for modeling as a graph. Conventional GNN-based methods for MDR can be divided into two types. The first type deals with domains separately. That is, for each domain we construct useritem interaction graph and train model independently, which learns separate representations for different domains to characterize users' domain-specific preferences. However, the sparseness of interaction behaviors in emerging domains [3, 5] is a crucial obstacle. The second type alternatively constructs a unified interaction graph using multi-domain data and train a shared model to serve all domains [31]. Considering the intrinsic difference among domains' data distributions, the shared model neglects domain-specific characteristics which results in limited performance. For effective MDR, the key is to learn from the interactions in all domains and acquire transferable knowledge to obtain better user representations that characterize their domain-specific preferences. In this paper we propose H 3 Trans , a h ierarchical h ypergrap h network based correlative preference trans fer framework to improve MDR. As a general topological structure, a hyperedge can connect an arbitrary number of nodes, and thus hypergraph provides a means for modeling high-order connections in multiple domains. We integrate users' multi-domain behaviors into a unified graph and incorporate hyperedges to help preference transfer. Specifically, each user is viewed as multiple nodes w.r.t. to different domains, where the representation of each user node characterizes the domain-specific preference. For item nodes, because items' properties are relatively static than users, we view each item as a single node shared by all domains. Researchers have proposed some advanced methods [1, 13, 19, 20, 32] that exert the prominent feature extracting ability of GNN and incorporate knowledge transfer to alleviate the sparseness. For example, pretrain-finetune diagram which transfers a pre-trained graph encoder to initialize the node embedding on the target domain is a widely used way [20]. Considering the pretrain-finetune paradigm only improves the recommendation accuracy on a single target domain, some works exploit to improve the recommendation accuracy on both domains simultaneously [13, 32]. Despite their effectiveness, these methods focus on knowledge transfer between only two domains. When employed in more than two domains, they only capture pair-wise relations between domains and dismiss the high-order connections. The core of the hypergraph structure constructed by H 3 Trans is two novel types of hyperedges for improving user and item representation learning. We first design a dynamic item transfer module named Hyper-I. For a given domain, we dynamically seek out related items from user-item interactions of other domains, and construct a hyperedge (named hyperedge-i) to connect them as cross-domain item relations. Hyperedge-i helps build relations between the items of different domains and capture users' correlative preference from the cross-domain behaviors without interference information. Moreover, we propose a structure-aware aggregator with attention mechanism to model the message passing procedure through hyperedge-i, which adjusts item representations much more correlative to the target domain and thus improves the recommendation performance in multiple domains. Wefurther introduce an adaptive user aggregation module named Hyper-U. Each user is viewed as a separate node per domain, that is, for a given user we can acquire separate user representations in multiple domains. We utilize a hyperedge (named hyperedge-u) to connect these separate user nodes of a given user, which aggregates the scattered user preferences among multiple domains. To effectively model the high-order connections among domains, we propose to employ attention mechanism into the message propagation within such hyperedges. Hyperedge-u contributes to transferring correlative preferences from source domains and capturing the commonality among multiple domains. Note that each domain can be viewed as the target domain (and the others as the sources), thus our proposed H 3 Trans can improve the quality of user representation for all domains simultaneously. The contributions are as follows: \u00b7 We propose H 3 Trans , a hierarchical hypergraph network based correlative preference transfer framework for MDR. To our knowledge, this is the first work that investigates hypergraph-based preference transfer in MDR. \u00b7 To model the high-order connections among users' multidomain behaviors, Hyper-U aggregates users' scattered preferences in multiple domains and exploits the high-order connections with an attention based propagation layer. \u00b7 To improve item representations for cross-domain transfer, Hyper-I performs dynamic item transfer which helps extract correlative preference from the cross-domain behaviors without interference information. \u00b7 Extensive experiments on large-scale production datasets and public datasets are conducted to analyze our proposed H 3 Trans , and the results demonstrate the superiority.", "2 PRELIMINARY": "", "2.1 Definition of Hypergraph": "Compared to an ordinary graph, a hypergraph is a more general topological structure where a hyperedge can connect an arbitrary number of nodes. Formally, a hypergraph is composed of a node set and a hyperedge set. The connectivity of a hypergraph can be represented by an incidence matrix \ud835\udc3b , where \u210e \ud835\udc63\ud835\udc52 = 1 if the hyperedge \ud835\udc52 contains the node \ud835\udc63 , otherwise \u210e \ud835\udc63\ud835\udc52 = 0 . Besides, we use \ud835\udc38 \ud835\udc63 to denote a set of hyperedges that connect to node \ud835\udc63 , and use \ud835\udc49 \ud835\udc52 to denote a set of nodes connected to hyperedge \ud835\udc52 . Also, we can define the neighbors N \ud835\udc63 of node \ud835\udc63 as a set of nodes that share at least one hyperedge with node \ud835\udc63 .", "2.2 Problem Definition": "Given domains {D \ud835\udc5a } \ud835\udc47 \ud835\udc5a = 1 , where \ud835\udc47 denotes the number of domains. For domain D \ud835\udc5a , we utilize \ud835\udc7c \ud835\udc5a and \ud835\udc70 \ud835\udc5a to denote its user ID set and item ID set respectively. Let R \ud835\udc5a \u2208 R | \ud835\udc7c \ud835\udc5a |\u00d7| \ud835\udc70 \ud835\udc5a | denotes the user-item interaction matrix of domain D \ud835\udc5a . If its entry \ud835\udc5f \ud835\udc5a \ud835\udc62\ud835\udc56 = 1 , it means that the user \ud835\udc62 interacted with the item \ud835\udc56 under domain \ud835\udc5a . In this work, we consider click behavior as the interaction type.  Given a specific domain D \ud835\udc5a , the problem of single-domain recommendation is to estimate the scores of unobserved entries in one interaction matrix R \ud835\udc5a , and we compute the score between a user and an item as: Correlative Preference Transfer with Hierarchical Hypergraph Network for Multi-Domain Recommendation WWW'23, April 30-May 4, 2023, Austin, Texas, USA < l a t e x i s h 1 _ b 6 4 = \" Q w n R 3 M c j J o f X A u W 0 2 8 > B 7 V N S E U r q / 9 L G P K g D T m v H 5 + F d Z k z C p I O Y y < l a t e x i s h 1 _ b 6 4 = \" V p N P Z 9 v I S A O W R u o 0 > B 7 n c 8 E J 3 X K k j r G C / Y 2 d M L y + z w H q g Q m f 5 D F T U < l a t e x i s h 1 _ b 6 4 = \" 2 + / H A O D Y r p j f I Z q 3 u k Q J P R > B 7 n c V N S 8 E W X K o m y d F g T 0 9 G L w U C 5 v M z < l a t e x i s h 1 _ b 6 4 = \" y O S 8 V Z R p d 9 T w M U P g > A B n c N E J 3 r q / L 0 G K D m 7 Q v H 5 2 + u f F W o k j C z Y I X < l a t e x i s h 1 _ b 6 4 = \" V X N q T 9 8 v S z B + n > A 7 H c E J 3 U r / L 0 G P R K p D w m g Y y 5 2 Q u M f d W o Z F I j O k C \u00c9 \u00c9 \u00c9 \u00c9 \u00c9 Graphormer Op Multi-Head Attention Op Shared Graphormer Op \u00c9 \u00c9 \u00c9 Readout Module: Embedding Module: Message Passing Module: Prediction Module: \u00c9 Layer 1 Layer 2 Layer 3 \u00c9 z i x u 1 x u 2 x u 3 x i Hyper-I Hyper-U Domain 1 Domain 2 Domain T (Target) (Sources) z u 1 Figure 2: Overall architecture of H 3 Trans . It contains two hyperedge-based modules: adaptive user aggregation (Hyper-U) and dynamic item transfer module (Hyper-I). These two modules compose a hierarchical hypergraph neural network. Different colors refer to different domains. Here we regard the first domain D 1 as target domain and the others are sources. Graphormer Operation Linear MatMul Linear Linear Scale SoftMax MatMul + Spatial Encoding i 1 i 1 i 2 i 2 i 3 i 3 i 4 i 4 i 5 i 5 i 6 i 6 i 1 i 2 i 3 i 4 i 5 i 6 Q K V user node item node hyperedge-u hyperedge-i Node Representation edge Here \ud835\udc67 \ud835\udc62 and \ud835\udc67 \ud835\udc56 denote the learned representations of user \ud835\udc62 \u2208 \ud835\udc7c \ud835\udc5a and item \ud835\udc56 \u2208 \ud835\udc70 \ud835\udc5a for domain D , and \ud835\udc53 (\u00b7) is the similarity function. \ud835\udc5a The problem of multi-domain recommendation is to estimate the unobserved scores for all interaction matrices {R \ud835\udc5a } \ud835\udc47 \ud835\udc5a = 1 . Specifically, the user set \ud835\udc7c is shared among all \ud835\udc47 domains, i.e., \ud835\udc7c = \ud835\udc7c 1 = \ud835\udc7c 2 = \u00b7 \u00b7 \u00b7 = \ud835\udc7c \ud835\udc47 , because each user may actively interact with all domains. For the item set \ud835\udc70 , each domain has its own set and we denote the total item candidate pool as \ud835\udc70 = \ud835\udc70 1 \u222a \ud835\udc70 2 \u222a\u00b7 \u00b7 \u00b7 \u222a \ud835\udc70 \ud835\udc47 . Note that different domains may have overlapped items. user node representation characterizes user's preference under a specific domain. Then for item nodes , items' properties are relatively static than users. Thus we treat each item \ud835\udc56 \u2208 \ud835\udc70 as a single node across various domains. In other words, each item \ud835\udc56 only corresponds to one node in the graph. The item \ud835\udc56 's node is also denoted as \ud835\udc56 .", "3 METHODOLOGY": "Fig. 2 shows the overall architecture of H 3 Trans . We introduce the construction of multi-domain graph, and basic graph neural network in \u00a7 3.1 and 3.2. Two core modules, namely dynamic item transfer and adaptive user aggregation, compose a hierarchical hypergraph neural network, are introduced in \u00a7 3.3.1 and 3.3.2. Finally, \u00a7 3.4 gives training procedure and optimization.", "3.1 Unified Multi-domain Graph": "To improve recommendation performance in all domains, instead of constructing individual graph for each domain, we integrate users' multi-domain behaviors into a unified graph G = (V , E) . In details, the node set V consists of user nodes and item nodes, i.e., V = U\u222aI . For user nodes , considering the domain discrepancy and the diversity of users' multi-domain behaviors, it is necessary to acquire separate representations for different domains. Thus we regard each user as separate nodes positioned in different domains. Specifically, for a given user \ud835\udc62 \u2208 \ud835\udc7c , it corresponds to \ud835\udc47 nodes ( \ud835\udc62 1 , \ud835\udc62 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc62 \ud835\udc47 ) , thus the relation between user node set size |U| and user ID set size | \ud835\udc7c | meets the condition of |U| = | \ud835\udc7c | \u00b7 \ud835\udc47 . Each The basic edge set collects the user-item history interactions from all domain, i.e. , R = (R 1 , R 2 , \u00b7 \u00b7 \u00b7 , R \ud835\udc47 ) , where R \ud835\udc5a denotes the user-item interaction matrix of domain D \ud835\udc5a . This work considers click behavior as the interaction type. For an entry \ud835\udc5f \ud835\udc5a \ud835\udc62\ud835\udc56 = 1 , it means that the user \ud835\udc62 has interacted with the item \ud835\udc56 under domain D \ud835\udc5a , and we build an interaction edge between the corresponding user node \ud835\udc62 \ud835\udc5a and item node \ud835\udc56 , denoted as \ud835\udc52 ( \ud835\udc62 \ud835\udc5a , \ud835\udc56 ) . To clarify which domain the edges belong to, we utilize distinct edge types for different domains. For domain D \ud835\udc5a , the edge subset is denoted as E \ud835\udc5a , and the whole edge set is the union of all domains as well as hyperedges, i.e., E = E 1 \u222a E 2 \u222a \u00b7 \u00b7 \u00b7 \u222a E \ud835\udc47 \u222a E hyper . We detail the construction of hyperedge set in \u00a7 3.3. With access to user-item interactions in any domain, it is convenient to leverage hyperedges to build cross-domain relations and capture correlative knowledge.", "3.2 Basic Graph Neural Network": "Wefirst introduce a base GNN that learns node representations without considering multi-domain relationships. The base GNN includes four modules: (1) embedding module that transforms nodes' sparse attribute features into low-dimensional embeddings; (2) messagepassing module with several layers that learn node representations by aggregating information from neighbors; (3) readout module that generates nodes' final representation; (4) prediction module that produces prediction score. < l a t e x i s h 1 _ b 6 4 = \" o + d v K 3 W 7 p E g r I z G Q k > A B n c V N S 8 J U q / 9 L 0 F y 2 m Z P C j X M Y H O T D R u 5 f w < l a t e x i s h 1 _ b 6 4 = \" m g I G c B Q u p n X D Y R f H o 3 > A V N S 8 E J U r q / 9 L 0 K M F 7 W y 2 k Z P C j z + O d w T 5 v < l a t e x i s h 1 _ b 6 4 = \" 5 v W o J X r 3 L u G m A c H + p j 0 > B n V N S 8 E U q / 9 P R K g D T w 7 Q y 2 M f F d Z k C O z Y I < l a t e x i s h 1 _ b 6 4 = \" V u M J f r 8 2 S 5 G p j q Q U > A B n c N E 3 / 9 L 0 m k F 7 o W y Z I P C z X Y + O d g H D w T K R v < l a t e x i s h 1 _ b 6 4 = \" 8 3 R W V / Z u Y J 0 G L B w r O T j > A n c D S g N E y f U 9 o P K 2 Q I 5 H m d p z 7 F v k q C + M X < l a t e x i s h 1 _ b 6 4 = \" / K W R 2 Q + P C G o v B c Z y > A n V N S 8 E J 3 U r q 9 L 0 F 7 k p I j z X M Y u O d g f H D w T 5 m < l a t e x i s h 1 _ b 6 4 = \" o + d v K 3 W 7 p E g r I z G Q k > A B n c V N S 8 J U q / 9 L 0 F y 2 m Z P C j X M Y H O T D R u 5 f w < l a t e x i s h 1 _ b 6 4 = \" m g I G c B Q u p n X D Y R f H o 3 > A V N S 8 E J U r q / 9 L 0 K M F 7 W y 2 k Z P C j z + O d w T 5 v < l a t e x i s h 1 _ b 6 4 = \" m g I G c B Q u p n X D Y R f H o 3 > A V N S 8 E J U r q / 9 L 0 K M F 7 W y 2 k Z P C j z + O d w T 5 v < l a t e x i s h 1 _ b 6 4 = \" 5 v W o J X r 3 L u G m A c H + p j 0 > B n V N S 8 E U q / 9 P R K g D T w 7 Q y 2 M f F d Z k C O z Y I < l a t e x i s h 1 _ b 6 4 = \" 8 3 R W V / Z u Y J 0 G L B w r O T j > A n c D S g N E y f U 9 o P K 2 Q I 5 H m d p z 7 F v k q C + M X < l a t e x i s h 1 _ b 6 4 = \" V u M J f r 8 2 S 5 G p j q Q U > A B n c N E 3 / 9 L 0 m k F 7 o W y Z I P C z X Y + O d g H D w T K R v < l a t e x i s h 1 _ b 6 4 = \" 5 v W o J X r 3 L u G m A c H + p j 0 > B n V N S 8 E U q / 9 P R K g D T w 7 Q y 2 M f F d Z k C O z Y I < l a t e x i s h 1 _ b 6 4 = \" o + d v K 3 W 7 p E g r I z G Q k > A B n c V N S 8 J U q / 9 L 0 F y 2 m Z P C j X M Y H O T D R u 5 f w < l a t e x i s h 1 _ b 6 4 = \" 8 3 R W V / Z u Y J 0 G L B w r O T j > A n c D S g N E y f U 9 o P K 2 Q I 5 H m d p z 7 F v k q C + M X < l a t e x i s h 1 _ b 6 4 = \" / K W R 2 Q + P C G o v B c Z y > A n V N S 8 E J 3 U r q 9 L 0 F 7 k p I j z X M Y u O d g f H D w T 5 m < l a t e x i s h 1 _ b 6 4 = \" / K W R 2 Q + P C G o v B c Z y > A n V N S 8 E J 3 U r q 9 L 0 F 7 k p I j z X M Y u O d g f H D w T 5 m < l a t e x i s h 1 _ b 6 4 = \" V u M J f r 8 2 S 5 G p j q Q U > A B n c N E 3 / 9 L 0 m k F 7 o W y Z I P C z X Y + O d g H D w T K R v WWW'23, April 30-May 4, 2023, Austin, Texas, USA Xu and Wei et al. 3.2.1 Embedding Module . This module maps each node into a \ud835\udc51 -dimensional embedding vector \ud835\udc65 \ud835\udc62 \ud835\udc5a (or \ud835\udc65 \ud835\udc56 ). For each user node \ud835\udc62 \ud835\udc5a \u2208 U (or item node \ud835\udc56 \u2208 I ), we acquire its embedding \ud835\udc65 \ud835\udc62 \ud835\udc5a (or \ud835\udc65 \ud835\udc56 ) from a learnable look-up table \ud835\udc4b \u2208 R ( | U |+|I |)\u00d7 \ud835\udc51 . Each user corresponds to \ud835\udc47 nodes, and these nodes share the same initial embedding. Note that each user has attribute features, and the corresponding nodes share the same attribute embedding. 3.2.2 Message Passing Module . The message-passing module consists of several layers that follow the neighborhood aggregation scheme. It can be taken as a two-stage process to learn node representations by aggregating information from neighbors. The two stages are neighbor aggregation and node update: Neighbor aggregation :  Node update :  where \ud835\udc59 denotes the \ud835\udc59 -th message passing layer. \u210e ( \ud835\udc59 ) \ud835\udc62 \ud835\udc5a and \u210e ( \ud835\udc59 ) \ud835\udc56 refer to the hidden representation of user node \ud835\udc62 \ud835\udc5a and item node \ud835\udc56 respectively. AGG U and AGG I are the aggregation functions for user and item nodes. The same is to the node update function UP U and UP I . There are a lot of designs for aggregate and update function. Here we use mean pooling for the aggregator and linear transforming for node update. Noted that the initial representation is acquired from embedding module, i.e., \u210e ( 0 ) \ud835\udc62 \ud835\udc5a = \ud835\udc65 \ud835\udc62 \ud835\udc5a , \u210e ( 0 ) \ud835\udc56 = \ud835\udc65 \ud835\udc56 . 3.2.3 Readout Module . After obtaining \ud835\udc3f layers representations, we utilize a readout layer to generate the final representation:  where the subscript \ud835\udc63 can denote user node \ud835\udc62 \ud835\udc5a or item node \ud835\udc56 . Common designs for the readout function include last-layer only, concatenation, and weighted sum. Here we adopt last-layer only. 3.2.4 Prediction Module . The prediction module produces the prediction score that how likely a user \ud835\udc62 would interact with item \ud835\udc56 under domain D \ud835\udc5a . It is formulated as:  where \ud835\udc53 is the score function and we usually adopt similarity function such as inner product and cosine function.", "3.3 Hierarchical Hypergraph Network": "The base GNN cannot model the multi-domain relation well. We propose H 3 Trans which utilizes hyperedge to exploit high-order connections among users' multi-domain behaviors with two hyperedgebased modules: dynamic item transfer module (Hyper-I) & adaptive user aggregation module (Hyper-U). These two modules have a hierarchical connection structure and compose a hierarchical hypergraph neural network. 3.3.1 Hyper-I: Dynamic Item Transfer Module . In MDR, each domain contains a set of items that is related to the domain's topic and marketing strategy. Due to the intrinsic difference, directly transferring users' cross-domain behaviors from multiple sources to the target is not a good approach. It will introduce interference information and degenerate user representations. To extract correlative preference from users' cross-domain behaviors, we design a dynamic item transfer module, namely Hyper-I. It dynamically adjusts source item representations during transfer to be more relevant to a given target domain, that contributes to capturing correlative user preferences from sources. Take domain D \ud835\udc61 as target domain, and the others as source domains. For each source domain D \ud835\udc60 , before feeding item node hidden representation \u210e ( \ud835\udc59 ) \ud835\udc56 into message passing layers that acquire user node representation by aggregating information from neighboring items, we adjust the item representations to eliminate domain discrepancy. Specifically, for each user's interacted item under source domain D \ud835\udc60 , we seek out similar items from the target domain D \ud835\udc61 , and then construct a hyperedge (named hyperedge-i ) to connect these item nodes. This hyperedge contains a two-level relationship. The first level is that the interacted source item is related to the picked target items. The second level is that the picked target items are also related to each other. We first introduce the method to seek out the related target items, and then we design a structure-aware hypergraph layer to adjust item representations. Hyperedge Construction. For a given interacted item \ud835\udc56 in a source domain D \ud835\udc60 , we seek out a similar item set S \ud835\udc61 \ud835\udc56 from the target domain D \ud835\udc61 , and construct a hyperedge to connect the source item node \ud835\udc56 and the item nodes of picked item set S \ud835\udc61 \ud835\udc56 . We offer two ways to get similar items: path-based and embedding-based. \u00b7 Path-based: Utilize co-occurrence relation among items. We assume that: if there is a user \ud835\udc62 that clicked on both items \ud835\udc56 and \ud835\udc57 , then the two items are similar. We design a walk path ( \ud835\udc56 \u2192 \ud835\udc62 \ud835\udc60 \u2192 \ud835\udc62 \ud835\udc61 \u2192 \ud835\udc57 ) and sample \ud835\udc58 items from the item set I \ud835\udc61 of target domain as similar items. To avoid noisy paths, we restrict that the click timestamp of each item in S \ud835\udc61 \ud835\udc56 should lie in a range of the source item \ud835\udc56 's click timestamp. \u00b7 Embedding-based: Path-based method is an intuitive way but it seriously relies on the interaction history of users. Embedding-based method makes use of the hidden representation of items \u210e ( \ud835\udc59 -1 ) \ud835\udc56 . It leverages the appropriate nearest neighbor algorithm to find the top\ud835\udc58 similar items from the target domain, where the source item node \ud835\udc56 is query and I \ud835\udc61 is candidate set. Graphormer Layer. To perform message passing within the hyperedge, UniGNN [8] and AllSet [4] propose a message passing paradigm on the hypergraph. UniGNN rethinks the messagepassing layer of the basic GNN as a two-stage aggregation process. In the first stage, for each hyperedge, use a permutation-invariant function to aggregate the information of the nodes within it. In the second stage, update each node with its incident hyperedges using another aggregating function. The method of AllSet is similar. Weclaim that the above message-passing paradigm fails to model the two-level relationship within hyperedge-i. Instead, we employ Correlative Preference Transfer with Hierarchical Hypergraph Network for Multi-Domain Recommendation WWW'23, April 30-May 4, 2023, Austin, Texas, USA attention [18] to adjust the item representation. Moreover, to effectively exploit the topology structure within the hyperedge-i, we introduce the distance matrix of the shortest path among the picked nodes (denoted as \ud835\udc69 ) into the attention layers, as introduced in [25]. Fig. 2 illustrates the details of this module. Specifically, it refines \u210e ( \ud835\udc59 -1 ) \ud835\udc56 before neighbor aggregation:  where GH HyperI (\u00b7) is the graphormer layer for Hyper-I module: GH HyperI ( \ud835\udc3b I ) = Concat GLYPH<0> Attn I , 1 ( \ud835\udc3b I ) , \u00b7 \u00b7 \u00b7 , Attn I ,\ud835\udc43 ( \ud835\udc3b I ) GLYPH<1> \ud835\udc4a \ud835\udc42 I ,  here \u03a6 () is a learnable function shared across all layers that maps the distance between every paired nodes to a scalar. \ud835\udc4a \ud835\udc44 \u2217 , \ud835\udc4a \ud835\udc3e \u2217 , \ud835\udc4a \ud835\udc49 \u2217 , and \ud835\udc4a \ud835\udc42 \u2217 are training parameters. 3.3.2 Hyper-U:AdaptiveUserAggregationModule . After adjusting item representations with Hyper-I, we acquire the representations of separate user nodes by aggregating adjusted representation of their neighbor items. Each user corresponds with multiple nodes that characterize the user's domain-specific preference. Next step is to transfer correlative user preferences from source domains to the target and refine the user representation of target domain. Noted that the preference transfer in MDR involves more than one source. The key point is how to aggregate users' scattered preferences in multiple domains and adequately exploit the highorder connections among them. Here we integrate a hyperedgebased module: Hyper-U, to realize adaptive user aggregation. Hyperedge Construction . Weutilize hyperedge to connect nodes that belong to the same user, and we name this hyperedge as hyperedge-u . Within hyperedge-u, each separate node representation characterizes user's interest preference under a specific domain. The hyperedge-u connects these separate user nodes and bridges the information propagation across domains, thus realizing adaptive preference transfer. Moreover, benefiting from that hyperedge connects plural nodes, hyperedge-u can further exploit the highorder (more than pairwise) connections among multiple domains. Multi-head Attention Layer. We design a new message passing layer for the hyperedge-u to replace the original layer. For the \ud835\udc59 -th layer, we first acquire user's separate representations under multiple domains, denoted as [ \u210e ( \ud835\udc59 ) \ud835\udc62 1 , \u210e ( \ud835\udc59 ) \ud835\udc62 2 , \u00b7 \u00b7 \u00b7 , \u210e ( \ud835\udc59 ) \ud835\udc62 \ud835\udc47 ] . Hyper-U module take these separate representations as input, and then refine these representations by aggregating users' scattered preferences and transferring knowledge from other domains. Considering the domain discrepancy and diversity of users' multi-domain behaviors, we employ self-attention mechanism in the Hyper-U module to adaptively fuse users' cross-domain interest representations. Specifically, it refines representations after node update:  where MHA HyperU (\u00b7) denotes the multi-head attention layer: MHA HyperU ( \ud835\udc3b U ) = Concat GLYPH<0> Attn U , 1 ( \ud835\udc3b U ) , \u00b7 \u00b7 \u00b7 , Attn I ,\ud835\udc43 ( \ud835\udc3b U ) GLYPH<1> \ud835\udc4a \ud835\udc42 U ,  here \ud835\udc4a \ud835\udc44 \u2217 , \ud835\udc4a \ud835\udc3e \u2217 , \ud835\udc4a \ud835\udc49 \u2217 , and \ud835\udc4a \ud835\udc42 \u2217 are trainable parameters. The multihead attention layer takes users' separate nodes representations as input and exploits the high-order connections with the selfattention mechanism. For each domain, the corresponding node can adaptively refine its preference representation by extracting the correlative information from other domains.", "3.4 Model Optimization and Time Complexity": "These two hyperedge-base modules: dynamic item transfer module (Hyper-I) and adaptive user aggregation module (Hyper-U), compose a hierarchical hypergraph neural network. It realizes correlative preference transfer and exploits the high-order connection among users' multi-domain behaviors. For model optimization, we mix the multi-domain data and randomly select a sample ( \ud835\udc62 \ud835\udc5a , \ud835\udc56 ) from domain D \ud835\udc5a for each training step. Domain D \ud835\udc5a is taken as the target domain and the others are source domains. We employ a contrastive loss InfoNCE [17] to optimize the model, which maximizes the agreements between positive pairs. Formally,  where \ud835\udc60\ud835\udc56\ud835\udc5a (\u00b7) stands for similarity measure function and we use inner product. ( \ud835\udc62 \ud835\udc5a , \ud835\udc56 -) is a randomly sampled negative pair that \ud835\udc5f \ud835\udc5a \ud835\udc62,\ud835\udc56 -= 0 , and \ud835\udf0f is the temperature hyperparameter. The time complexity of Hyper-U module is O( \ud835\udc47 2 \ud835\udc51 + \ud835\udc47\ud835\udc51 2 ) , where \ud835\udc47 is domain number and \ud835\udc51 is embedding dim. For Hyper-I module, the time complexity is O( \ud835\udc47\ud835\udc5b ( \ud835\udc58 2 \ud835\udc51 + \ud835\udc58\ud835\udc51 2 )) , where \ud835\udc5b is the number of sampled neighbors and \ud835\udc58 is the size of similar item set. The main limitation of H 3 Trans is computation cost and memory cost (incorporating hyperedges). Compared to the baselines that trains models for multiple domains in parallel, H 3 Trans unifies all domain data and training time increases. In future work, we shall focus on efficient algorithms, i.e., reducing memory cost via hyperedge dropout and reducing time complexity via accelerating self-attention.", "4 EXPERIMENTS": "In this section, we conduct both offline and online experiments to validate the effectiveness of our method. And the experiments are intended to answer the following research questions: \u00b7 RQ1 : How does our proposed method perform when compared with other state-of-the-art GNN-based methods? \u00b7 RQ3 : Does our method help alleviate the behavior sparseness issue and improve recommendation performance for the relatively inactive users (with fewer interaction items)? \u00b7 RQ2 : How do the different components (e.g., unified multidomain graph, adaptive user aggregation module, dynamic item transfer module) contribute to the model performance? WWW'23, April 30-May 4, 2023, Austin, Texas, USA Xu and Wei et al. Table 1: Dataset Statistics \u00b7 RQ4 : Does H 3 Trans achieve improvement when deployed to our advertising system?", "4.1 Experimental Settings": "4.1.1 Datasets . Weconduct extensive offline experiments on both the public dataset and the product dataset. Product Dataset: The product dataset is collected from four real-world scenarios from an industry advertising platform, named MDR-A, MDR-B, MDR-C, and MDR-D. These four sub-datasets share the same user set and have overlapped items. Each subset consists of users' interacted items. We additionally filter the datasets to retain users/items with at least 5 interactions. Table 1 lists the statistics of both the product dataset and the public amazon dataset. Public Dataset: Amazon dataset [16] is a popular dataset to conduct experiments of multi-domain recommendation. The dataset provides dozens of domains and the frequently-used domains are Books, Movies and TV (Movie), and CDS and Vinyl (Music). Following existing research, we take binarize the ratings to 1 and 0 (the ratings higher or equal to 4 as positive and others as negative.) Besides, we filter the users and items with less than 5 interactions. 4.1.2 Compared methods . We compare H 3 Trans with following strong baselines. Except for the base model, all baselines attempt to transfer information from other domains in different ways. \u00b7 Base . Base method constructs a user-item bipartite graph and trains models individually for each domain with its user behavior data. \u00b7 MGNN . MGNN [29] integrates users' multi-domain behaviors and constructs the unified multi-domain graph. Nodes belonging to the same user share the same attribute. MGNN learns domain-specific representation for user nodes. \u00b7 PPGN . PPGN [31] fuses the interaction information of multiple domains into a graph and shares the features of users learned from the joint interaction graph. Notes that one user only has one node within the joint graph. \u00b7 PCRec . PCRec [20] adopts a pre-training and fine-tuning diagram to transfer knowledge from the source domain to the target. Here we first pre-train a graph model on the joint graph and then fine-tune it on each domain. \u00b7 BiTGCF+ . BiTGCF+ is an extended version of BiTGCF. Here we modify the feature transfer layer and extend it to multidomain recommendation. \u00b7 BiTGCF . BiTGCF [13] is proposed for dual-target recommendation. It connects common users of both domains as bridge and designs a feature transfer layer to realize the twoway transfer of knowledge across two domains. Here we randomly pick two domains to realize the combination layer. 4.1.3 Evaluation Protocol . We adopt the widely used leave-oneout evaluation method. Specifically, we take the last interaction from each user's interaction history as the test set, and the remaining are utilized for training. For users in the test set, we follow the all-ranking protocol [22] to evaluate the top-K recommendation performance. For product dataset, we report the average HitRate@K (HR@K) and Mean Reciprocal Rank (MRR) on each domain. For public dataset, we report the HR@K and NDCG@K as these two metrics are more popular of public experiments. 4.1.4 Implementation Details . We provide the implementation details of our proposed model and baselines. For fair comparison, each of graph neural network models has two layers, and the hidden embedding dimensions are set as [128, 64]. We sample \ud835\udc58 = 20 related items to build hyperedge-i in Hyper-I module. For model training, we set batch size \ud835\udc41 = 512 and adopt adam optimizer [11], where the learning rate is set to 0 . 01 .", "4.2 Performance Comparison (RQ1)": "Table 2 and Table 3 present the experimental results of H 3 Trans compared with other baselines. From these two tables, we have the following observations. \u00b7 Base method performs poorly on all domains, which indicates that individually training model for each domain limits the recommendation performance in multi-domain recommendation. \u00b7 MGNNtakesaccountofboththecommonfeatureandthedomainspecific feature for different domains. which brings improvement to the recommendation service. Note that common feature is only acquired by the shared node attributes. The information transfer among domains is limited. \u00b7 PPGN mixes the multi-domain data and constructs a joint graph for model training. As a result, it achieves large improvement in most domains. But it still has negative effects on some domains such as MDR-B, because different domains share the same user representation and neglect the user's domain-specific preferences. The user representation is dominated by the data-rich domain. \u00b7 PCRec performs transfer learning by adopting the pre-training and fine-tuning diagram. Pre-training on the joint graph helps learn users' common preferences among domains. Then finetuning on domain's individual graph make the user node representation more preferable for each domain. However, fine-tuning is more time- and space-consuming for multi-domain recommendation. \u00b7 H 3 Trans achieves the best performance with significant improvement on all metrics of all domains. This indicates that H 3 Trans benefits from learning the high-order connections among multiple domains extracted by Hyper-U module and transferring correlative information via Hyper-I. The high-quality representations learned from the hypergraph enhance the recommendation performance in all domains. \u00b7 BiTGCF and BiTGCF+ are two competitive baselines in our experiments. BiTGCF leverages a combination layer to realize the two-way transfer across domains. Here we extend the feature transfer layer of BiTGCF to multiple domains as BiTGCF+. We can see that BitGCF+ achieves larger improvement than BitGCF because it introduces more domains to perform multi-domain recommendation. But the improvement is still limited because we just simply sum user's multi-domain representations and neglect the high-order connections among them. Correlative Preference Transfer with Hierarchical Hypergraph Network for Multi-Domain Recommendation WWW'23, April 30-May 4, 2023, Austin, Texas, USA Table 2: Main results on product dataset Table 3: Main results on public amazon dataset", "4.3 Ablation Study (RQ2)": "For further analysis, we compare different variants of H 3 Trans on the product dataset for ablation study, and the results are listed in table 4. Vanilla is a basic graph model trained on the unified multi-domain graph. User nodes learn the common interest only through the shared node attributes. 4.3.1 Effect of Hyper-U module : HUadds the Hyper-U module but without the attention mechanism based layer, which is equivalent to BiTGCF+. It only utilizes a vanilla combination layer to combine users' separate representations from multiple domains. HU+ integrates our self-attention mechanism based message passing layer into HU. From the table, we can see that aggregating users' scattered preferences and modeling the high-order connections among multiple domains could help refine the user representation for each separate domain. And the self-attention mechanism contributes to further improving the representation quality, because the attention layer adaptively extracts correlative knowledge from source domains. 4.3.2 Effect of Hyper-I module : PHI and EHI are two models that additionally integrate the Hyper-I module, and equipped with path-based or embed-based method to seek out similar items respectively. Table 4 shows that these two methods perform better than HU+, which indicates that the dynamic item transfer module could eliminate the domain discrepancy and adjust the latent item representation more correlative to the target domain without interference information. Besides, EHI achieves a marginal improvement than PHI, that shows embed-based method is a little better than path-based method. EHI+ is the best variant of our model, which further employs the graphormer layer to exploit the structure information within the hyperedge-i. It consistently shows around 1% on HR@20 and 2% on HR@50. Figure 3: Performance comparison over different number of domains in MDR 1 2 3 4 Num of domains 2.5 5.0 7.5 10.0 12.5 15.0 17.5 HitRate@20(%) MDR-A MDR-B MDR-C MDR-D 1 2 3 4 Num of domains 10 15 20 25 30 35 HitRate@50(%) MDR-A MDR-B MDR-C MDR-D The itemset size of each domain ranges from tens-of-thousands to millions, while the size of selected correlative itemset is \ud835\udc3e . The value of \ud835\udc3e is a key hyperparameter: A too small value brings unstable training. A too large value increases computation cost, and different source items usually retrieve similar itemsets that lacks of discriminatory information. 4.3.3 Effect of multiple domains : Multi-domain recommendation jointly optimizes the recommendation performance of all domains. Intuitively, with more domains, we can access more users' behaviors to better characterize users' interest. Here we analyze the effect when introducing different numbers of domains to perform multi-domain recommendation. The results are reported in figure 3. We can see that it indeed achieves better performance when introducing more domains, because we can transfer knowledge from more source domains, and H 3 Trans help exploit the high-order connections among them. Additionally, the marginal improvement decreases as more domains are introduced.", "4.4 Alleviating Behavior Sparseness (RQ3)": "As stated before, GNN-based methods suffer from the behavior sparseness issue, and here we conduct a detailed analysis to test the improvement on behavior-sparse users. Specifically, we split the users into four groups G1, G2, G3, G4, and G5 in the order of increasing number of interactions. The larger the GroupID is, the more behaviors the users have collected. Figure 4 reports the percentage increase compared with the Base model. We can find that the improvement achieved in the first three groups is more significant than that of the last two. We conclude that H 3 Trans help improve more for relatively inactive users (with fewer user-item interactions), indicating that H 3 Trans alleviates the sparseness of user behaviors by transferring knowledge from other domains. WWW'23, April 30-May 4, 2023, Austin, Texas, USA Xu and Wei et al. Table 4: Ablation study on product dataset. Methods refer to different variants of H 3 Trans . Figure 4: Performance comparison over different user groups (percentage increase relative to Base model) G1 G2 G3 G4 G5 GroupID 0 50 100 150 200 HitRate@50(%) (a) MDR-A Mix MGNN PCRec BitGCF BitGCF+ H 3 Trans G1 G2 G3 G4 G5 GroupID 0 50 100 150 200 HitRate@50(%) (b) MDR-B Mix MGNN PCRec BitGCF BitGCF+ H 3 Trans G1 G2 G3 G4 G5 GroupID 0 50 100 150 200 HitRate@50(%) (c) MDR-C Mix MGNN PCRec BitGCF BitGCF+ H 3 Trans G1 G2 G3 G4 G5 GroupID 0 50 100 150 200 HitRate@50(%) (d) MDR-D Mix MGNN PCRec BitGCF BitGCF+ H 3 Trans", "4.5 Online Experiment (RQ4)": "We have deployed H 3 Trans online to the retrieval module of our advertising system for an emerging scenario, and conducted online A/B test for one week. For fair comparison, we follow the same configuration with the best retrieval model deployed online [20]. The online metrics include CTR, conversion rate (CVR), gross merchandise volume (GMV) and return on investment (ROI). We observe that H 3 Trans achieves +2.8% lift on CTR, +10.9% lift on CVR, +6.7% lift on GMV and +7.3% lift on ROI, and the daily improvement over baseline is stable. The uplift is mainly from users having lowest activity level, verifying that H 3 Trans learns highquality embeddings for inactive users through preference transfer. Therefore H 3 Trans improves the important online metrics and promotes the performance to our system.", "5 RELATED WORK": "", "5.1 Multi-domain Recommendation": "Multi-domain recommendation aims to improve recommendations performance of all domains by transferring knowledge from related domains. MCF [30] and ICAN [24] consider multiple collaborative filtering tasks in different domains simultaneously and exploit the relationships between domains. Ma et al. [15] further introduce cross-media content information. Some works focus on the users' multiple behaviors. MBGCN [10] and MGNN [29] propose a multi-behavior graph convolutional network to capture behaviors' different influences on target behavior. Furthermore, by considering each domain as a task, multi-task approaches can be directly applied in MDR. For general MDR, MMoE [14] models the tradeoffs between domain-specific objectives and inter-domain relationships with a new multi-gate expert strategy.", "5.2 GNNs for Cross-domain Recommendation": "Inspired by the success of graph neural networks[6, 12], researchers have taken efforts to exploit the user-item interaction behavior graph. GNN-based methods [7, 22, 26] suffer from the sparseness of user behaviors, and some researchers have exploited to alleviate it by transferring information from other domains [13, 20, 31]. PPGN [31] fuses the interaction information of two domains into a graph and learns shared features for users. Wang et al. [20] propose a pre-training and fine-tuning diagram to transfer information to the target domain. Liu et al. [13] realizes the two-way transfer of knowledge across two domains with a bi-directional feature transfer module. Zhu et al. [32] propose a graphical and attentional model to combine the embeddings of common users from both domains, thus enhancing the quality of user embeddings and improving the recommendation performance on each domain. However, they fail to model high-order connections among more domains.", "5.3 Hypergraph Learning for Recommendation": "Hypergraph, as a more general topological structure to model highorder connections, has been exploited in recommendation [2, 9, 21, 23, 27, 28]. Xia et al. [23] models session-based data as a hypergraph and then propose a hypergraph convolutional network for session-based recommendation. Yu et al. [27] propose a multichannel hypergraph convolutional network to enhance social recommendation by leveraging high-order user connections. Zhang et al. [28] incorporate the complex tuple-wise correlations into a hypergraph and propose a self-supervised hypergraph learning framework for group recommendation. Our work is the first to investigate hypergraph learning in multi-domain recommendation, which can exploit the high-order connections among multiple domain and realize correlative preference transfer.", "6 CONCLUSION": "In this paper, we propose an correlative preference transfer framework with hierarchical hypergraph network ( H 3 Trans ) to improve multi-domain recommendations. H 3 Trans constructs a unified multidomain graph and integrates two hyperedge-based module: adaptive user aggregation and dynamic item transfer. H 3 Trans not only exploits high-order connections among users' scattered preferences in multiple domain, but also transfers correlative user preference to alleviate the behavior sparseness of each single domain. Extensive experiments demonstrate the superiority of our method. Correlative Preference Transfer with Hierarchical Hypergraph Network for Multi-Domain Recommendation WWW'23, April 30-May 4, 2023, Austin, Texas, USA", "REFERENCES": "[1] Fengwen Chen, Shirui Pan, Jing Jiang, Huan Huo, and Guodong Long. 2019. DAGCN: dual attention graph convolutional networks. In 2019 International Joint Conference on Neural Networks (IJCNN) . IEEE, 1-8. [3] Yuting Chen, Yanshi Wang, Yabo Ni, An-Xiang Zeng, and Lanfen Lin. 2020. Scenario-aware and Mutual-based approach for Multi-scenario Recommendation in E-Commerce. In 2020 International Conference on Data Mining Workshops (ICDMW) . IEEE, 127-135. [2] Xu Chen, Kun Xiong, Yongfeng Zhang, Long Xia, Dawei Yin, and Jimmy Xiangji Huang. 2020. Neural feature-aware recommendation with signed hypergraph convolutional network. ACM Transactions on Information Systems (TOIS) (2020). [4] Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. 2022. You are AllSet: AMultiset Function Framework for Hypergraph Neural Networks. ArXiv (2022). [6] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances in neural information processing systems 30 (2017). [5] Yulong Gu, Wentian Bao, Dan Ou, Xiang Li, Baoliang Cui, Biyu Ma, Haikuan Huang, Qingwen Liu, and Xiaoyi Zeng. 2021. Self-Supervised Learning on Users' Spontaneous Behaviors for Multi-Scenario Ranking in E-commerce. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . [7] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . [9] Shuyi Ji, Yifan Feng, Rongrong Ji, Xibin Zhao, Wanwan Tang, and Yue Gao. 2020. Dual channel hypergraph collaborative filtering. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . [8] Jing Huang and Jie Yang. 2021. UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21) . [10] Bowen Jin, Chen Gao, Xiangnan He, Depeng Jin, and Yong Li. 2020. Multibehavior recommendation with graph convolutional networks. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 659-668. [12] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [11] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [13] Meng Liu, Jianjun Li, Guohui Li, and Peng Pan. 2020. Cross Domain Recommendation via Bi-Directional Transfer Graph Collaborative Filtering Networks. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . [15] Weizhi Ma, Min Zhang, Chenyang Wang, Cheng Luo, Yiqun Liu, and Shaoping Ma. 2018. Your Tweets Reveal What You Like: Introducing Cross-media Content Information into Multi-domain Recommendation.. In IJCAI . 3484-3490. [14] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [16] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP) . 188-197. [18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [17] Aaron Van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv e-prints (2018). [19] Bei Wang, Chenrui Zhang, Hao Zhang, Xiaoqing Lyu, and Zhi Tang. 2020. Dual Autoencoder Network with Swap Reconstruction for Cold-Start Recommendation. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2249-2252. [21] Jianling Wang, Kaize Ding, Liangjie Hong, Huan Liu, and James Caverlee. 2020. Next-item recommendation with sequential hypergraphs. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval . [20] Chen Wang, Yueqing Liang, Zhiwei Liu, Tao Zhang, and Philip S. Yu. 2021. Pretraining Graph Neural Network for Cross Domain Recommendation. CoRR abs/2111.08268 (2021). arXiv:2111.08268 [22] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval . 165-174. [23] Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Lizhen Cui, and Xiangliang Zhang. 2021. Self-supervised hypergraph convolutional networks for sessionbased recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence . [25] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform badly for graph representation? Advances in Neural Information Processing Systems 34 (2021), 28877-28888. [24] Ruobing Xie, Zhijie Qiu, Jun Rao, Yi Liu, Bo Zhang, and Leyu Lin. 2020. Internal and Contextual Attention Network for Cold-start Multi-channel Matching in Recommendation.. In IJCAI . 2732-2738. [26] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 974-983. [28] Junwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, and Hongzhi Yin. 2021. Double-Scale Self-Supervised Hypergraph Learning for Group Recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . [27] Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Self-supervised multi-channel hypergraph convolutional network for social recommendation. In Proceedings of the Web Conference 2021 . [29] Weifeng Zhang, Jingwen Mao, Yi Cao, and Congfu Xu. 2020. Multiplex Graph Neural Networks for Multi-Behavior Recommendation. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . [31] Cheng Zhao, Chenliang Li, and Cong Fu. 2019. Cross-Domain Recommendation via Preference Propagation GraphNet. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . [30] Yu Zhang, Bin Cao, and Dit-Yan Yeung. 2010. Multi-domain collaborative filtering. In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence . 725-732. [32] Feng Zhu, Yan Wang, Chaochao Chen, Guanfeng Liu, and Xiaolin Zheng. 2020. A Graphical and Attentional Framework for Dual-Target Cross-Domain Recommendation.. In IJCAI . 3001-3008."}
