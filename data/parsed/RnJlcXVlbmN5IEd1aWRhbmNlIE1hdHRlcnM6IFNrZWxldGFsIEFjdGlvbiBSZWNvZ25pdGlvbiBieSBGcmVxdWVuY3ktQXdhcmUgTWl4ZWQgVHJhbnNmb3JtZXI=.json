{"Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer": "Wenhan Wu University of North Carolina at Charlotte Department of Computer Science Charlotte, NC, USA wwu25@uncc.edu Ce Zheng Carnegie Mellon University Robotics Institute Pittsburgh, PA, USA cezheng@andrew.cmu.edu Zihao Yang Microsoft Corporation One Inventory Organization Redmond, WA, USA mattyang@microsoft.com Chen Chen University of Central Florida Center for Research in Computer Vision Orlando, Florida, USA chen.chen@crcv.ucf.edu Srijan Das University of North Carolina at Charlotte Department of Computer Science Charlotte, NC, USA sdas24@uncc.edu", "Abstract": "Aidong Lu University of North Carolina at Charlotte Department of Computer Science Charlotte, NC, USA aidong.lu@uncc.edu", "Keywords": "Recently, transformers have demonstrated great potential for modeling long-term dependencies from skeleton sequences and thereby gained ever-increasing attention in skeleton action recognition. However, the existing transformer-based approaches heavily rely on the naive attention mechanism for capturing the spatiotemporal features, which falls short in learning discriminative representations that exhibit similar motion patterns. To address this challenge, we introduce the Freq uency-aware Mix ed Trans former (FreqMixFormer), specifically designed for recognizing similar skeletal actions with subtle discriminative motions. First, we introduce a frequency-aware attention module to unweave skeleton frequency representations by embedding joint features into frequency attention maps, aiming to distinguish the discriminative movements based on their frequency coefficients. Subsequently, we develop a mixed transformer architecture to incorporate spatial features with frequency features to model the comprehensive frequency-spatial patterns. Additionally, a temporal transformer is proposed to extract the global correlations across frames. Extensive experiments show that FreqMiXFormer outperforms SOTA on 3 popular skeleton action recognition datasets, including NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets. Our project is publicly available at: https://github.com/wenhanwu95/FreqMixFormer.", "CCS Concepts": "\u00b7 Computing methodologies \u2192 Artificial intelligence; Computer vision . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Skeleton Action Recognition, Frequency, Transformer", "ACMReference Format:": "Wenhan Wu, Ce Zheng, Zihao Yang, Chen Chen, Srijan Das, and Aidong Lu. 2024. Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer. In Proceedings of Proceedings of the 32nd ACM International Conference on Multimedia (MM '24). ACM, New York, NY, USA, 16 pages. https://doi.org/XXXXXXX.XXXXXXX", "1 Introduction": "Reading Writing Time Frequency-aware Mixed Transformer Fuse High-Frequency Feature Low-Frequency Feature Spatial Feature Frequency-Spatial Feature General Transformer Spatial Feature Previous methods Our methods Drink water Figure 1: The overall design of our Frequency-aware Mixed Transformer. Our FreqMixFormer model overcomes the limitations of traditional transformer-based methods, which cannot effectively recognize confusing actions such as reading and writing due to the straightforward process of skeleton sequences. As highlighted with the colored boxes, the FreqMixFormer introduces the frequency domain and extracts high-frequency features, which often indicate subtle and dynamic movements (red), and low-frequency features, which are associated with slow and steady movements (blue). These features are then fused with spatial features. Our results demonstrate that the integrated frequency-spatial features significantly improve the model's capability to discern discriminative joint correlations. Human action recognition, a vital research topic in computer vision, is widely applied in various applications, including visual surveillance [52, 53], human-computer interaction [19, 40], and autonomous driving systems [29, 33]. Particularly, skeleton sequences represent the motion trajectories of human body joints to characterize distinctive human movements with 3D structural pose information, which is robust to surface textures and backgrounds. Consequently, skeletal action recognition stands out as an effective approach for recognizing human actions compared to RGB-based [56, 70] or Depth-based [66, 82] methods. Early skeleton-based works typically represent the human skeleton as a sequence of 3D joint coordinates or a pseudo-image, then adapted Convolutional Neural Networks (CNNs) [15, 20, 21] or Recurrent Neural Networks (RNNs) [12, 22, 73] to model spatial features among joints. However, unlike static images, skeleton data embodies dynamic and complex spatiotemporal topological correlations that CNNs and RNNs fail to capture. Therefore, to effectively model the skeletal information encapsulated within the topological graph structure that reflects human anatomy, Graph Convolutional Networks (GCNs) [8-10, 23, 24, 26, 50, 68, 71] are utilized. Nevertheless, as graph information progresses through deeper layers, the model may lose vital joint correlations and direct propagation, diminishing its ability to capture long-range interactions between distant frames. Recently, Transformer [57] has acquired promising results in human action recognition in various data modalities such as RGB [30, 47], depth [61, 62], point cloud [36, 65], and skeleton [10, 60]. The Transformer's ability to model the intrinsic representation of human joints and sequential frame correlations makes it a suitable backbone for skeleton-based action recognition. Despite the notable achievements of several transformer-based studies [4, 14, 34, 44, 51, 67, 69, 75, 81], they have yet to surpass the accuracy benchmarks set by GCNs. We hypothesize that the primary issue contributing to this gap is - GCNs, through their localized graph convolutions, effectively capture the human spatial configuration essential for action recognition. In contrast, traditional transformers, utilizing global self-attention operations, lack the inductive bias to inherently grasp the skeleton's topology. Although these global operations model overall motion patterns within a skeleton sequence, the self-attention mechanism in transformers may dilute the subtle local interactions among joints. Additionally, as attention scores are normalized across the entire sequence, subtle yet crucial discrimination in action sequences might be ignored if they do not substantially impact the overall attention landscape. To bridge this gap, we focus on improving transformers' capability to learn discriminative representation of subtle motion patterns. In this work, we aim to aggregate the frequency-spatial features by introducing Discrete Cosine Transform (DCT) [1] into a mixed attention transformer framework [11, 69] to encode joint correlations in the frequency domain to explore the frequency-based joint representations. The motivation behind the representations is straightforward and intuitive: the frequency components can represent the entire joint sequence [39] and are sensitive to subtle movements. As a result, we introduce a novel Frequency-aware Mixed Transformer (FreqMixFormer) for skeleton action recognition to capture the discriminative correlations among joints. The key steps of our approach are outlined as follows: Firstly , we formulate a Frequency-aware Attention module for transferring spatial joint information to the frequency domain, where the skeletal movement dependencies (similarity score between Queries \ud835\udc44 and Keys \ud835\udc3e ) are embedded in the spectrum domain with a distinct representation based on their energy. As illustrated in Fig.1, discriminative skeleton features with similar patterns (e.g., confusing actions like reading and writing) can be effectively learned by leveraging their physical correlations. In the context of these skeleton sequences, minor movements that contain subtle variations and exhibit rapid spatial changes are effectively compressed into high-frequency components with lower energy (highlighted with the red box). Conversely, actions that constitute a larger portion of the sequence and change slowly over time in the temporal domain are compressed into low-frequency components with higher energy (shown in the blue box). Subsequently, a frequency operator is applied to accentuate the high-frequency coefficients while diminishing the low-frequency coefficients, thereby enabling selective amplification and attenuation for fine-tuning within the frequency domain. Secondly , we propose a transformerbased model that utilizes mixed attention mechanism to extract spatial and frequency features separately with self-attention (SA) and cross-attention (CA) operations, where SA and CA extract joint dependencies and contextual joint correlations respectively. An integration module subsequently fuses the features from both the frequency and spatial domains, resulting in frequency-spatial features. These features are then fed into a temporal transformer, which globally learns the inter-frame joint correlations (e.g., from the first to the last frame), effectively capturing the discriminating frequency-spatial features temporally. Our contributions are summarized as follows: \u00b7 We propose a Frequency-aware Attention Block (FAB) to investigate frequency features within skeletal sequences. A frequency operator is specifically designed to improve the learning of frequency coefficients, thereby enhancing the ability to capture discriminative correlations among joints. \u00b7 Consequently, we introduce the Frequency-awareMixedTransformer (FreqMixFormer) to extract frequency-spatial joint correlations. The model incorporates a temporal transformer designed to enhance its ability to capture temporal features across frames. \u00b7 Our proposed FreqMixFormer outperforms state-of-the-art performance on three benchmarks, including NTU RGB+D [48], NTU RGB+D 120 [35], and Northwestern-UCLA [58].", "2 Related Work": "", "FrequencyRepresentationLearningforSkeleton-based Tasks.": "Traditional pose-based methods aim to extract motion patterns directly from the poses for trajectory-prediction [28, 54], pose estimation [6, 79], action recognition [10, 71]. The representations derived from pose space naturally reflect physical characteristics (spatial dependency of structure information) and motion patterns (temporal dependency of motion information), making it challenging to encode poses in a spatiotemporal way. Motivated by a strong ability to encode temporal information in the frequency domain smoothly and compactly [3], several recent works [13, 27, 39, 59] utilize discrete cosine transform (DCT) to convert the temporal motion to frequency domain for frequency-specific representation learning. In skeleton action recognition, only a few works [5, 7, 16, 45] have considered frequency representations so far. [16] proposed a multi-feature branches framework to extract subtle frequency features with fast Fourier transform (FFT) and spatial-temporal joint dependencies, aiming to build a multi-task framework in skeleton action recognition. [7] adopts discrete wavelet transform with a GCN-based decoupling framework to decouple salient and subtle motion features, aiming for fine-grained skeleton action recognition. While our interest aligns with frequency-based modeling, we opt for a DCT-based approach since its frequency coefficients are welldistributed in the frequency domain, benefiting the discriminative motion representation learning. Transformer-based Skeleton Action Recognition. Many recent works adopt transformers for human pose estimation [76, 78] and skeleton action recognition [38, 46] to explore joint correlations via attention mechanism. ST-TR [44] is the first to introduce the transformer to process skeleton data with spatial transformer and temporal transformer, proving its effectiveness in action recognition. Many follow-up works [14, 38, 46, 69, 75] keep employing this spatial-temporal structure for skeleton recognition with different configurations. STTFormer [46] proposed a tuple self-attention mechanism for capturing the joint relationships among frames. FG-STFormer [14] was developed to understand the connections between local joints and contextual global information across spatial and temporal dimensions. SkeMixFormer [69] introduced mixed attention method [11] and channel grouping techniques into spatiotemporal structure, enabling the model to learn the dynamic multivariate topological relationships. Besides these methods that focus on model configurations, [41] designed a partitioning strategy with the self-attention mechanism to learn the semantic representations of the interactive body parts. [34] presented an efficient transformer with a temporal partitioning aggregation strategy and topology-aware spatial correlation modeling module. Most of the transformer-based methods mentioned above mainly focus on configuration improvement and spatiotemporal correlation learning without exploiting the skeletal motion patterns in the frequency domain. In this work, we propose a frequency-based transformer with a frequency-spatial mixed attention mechanism, leveraging joint representation learning.", "3 Methodology": "", "3.1 Preliminaries": "Transformer. Self-Attention is the core mechanism of the transformer [57]. Given the input \ud835\udc4b \u2208 R \ud835\udc36 \u00d7 \ud835\udc37 , where \ud835\udc36 is the number of patches and \ud835\udc37 is the embedding dimension, \ud835\udc4b is first mapped to three matrices: Query matrix \ud835\udc44 , Key matrix \ud835\udc3e and Value matrix \ud835\udc49 by three linear transformation: where \ud835\udc4a \ud835\udc44 , \ud835\udc4a \ud835\udc3e and \ud835\udc4a \ud835\udc49 \u2208 R \ud835\udc37 \u00d7 \ud835\udc37 are the learnable weight matrices. The self-attention score can be described as the following mapping function: where \ud835\udc44\ud835\udc3e \u22a4 is the similarity score, 1 \u221a \ud835\udc51 is the scaling factor that prevents the softmax function from entering regions where gradients are too small. Next, the Multi-Head Self-Attention (MHSA) function is introduced to process information from different representation subspaces in different positions. The MHSA score is expressed as: where \ud835\udc3b \ud835\udc56 = \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b ( \ud835\udc44 \ud835\udc56 , \ud835\udc3e \ud835\udc56 , \ud835\udc49 \ud835\udc56 ) , \ud835\udc56 \u2208 { 1 , 2 , . . . , \u210e } is the single attention head, \ud835\udc4a \ud835\udc5c\ud835\udc62\ud835\udc61 is a linear projection \u2208 R \ud835\udc37 \u00d7 \ud835\udc37 . Baseline. The existing transformer-based skeleton action recognition methods rely heavily on plain self-attention blocks mentioned above to capture spatiotemporal correlations, ignoring the contextual information among different blocks. Thus, we simply adopt off-the-shelf SkeMixformer [69] as our baseline for capturing spatial skeletal features, where the contextual information can be extracted based on a mixed way: 1) Cross-attention, an asymmetric attention scheme of mixing Query matrix \ud835\udc44 and Key matrix \ud835\udc3e , leveraging asymmetric information integration. 2) Channel grouping, a strategy that divides the input into unit groups to capture multivariate interaction characteristics, preserving the inherent features of the skeleton data by avoiding the full self-attention's dependency on global complete channels. However, SkeMixFormer falls short of modeling discriminative motion patterns, thereby not fully leveraging its representational potential. In light of the baseline's limitations, we introduce our proposed FreqMixFormer to verify the effectiveness of frequencyspatial features over purely spatial ones. The detailed components of our model are elaborated in the following sections.", "3.2 Overview of FreqMixFormer": "The overall architecture of FreqMixFormer is illustrated in Fig. 2. Given the input \ud835\udc4b \u2208 R \ud835\udc3d \u00d7 \ud835\udc36 \u00d7 \ud835\udc39 is embedded by joint and positional embedding layers to represent a skeleton sequence with a consistent frame count of \ud835\udc39 , where \ud835\udc36 denotes the dimensionality of the joint, and \ud835\udc3d represents the number of joints in each frame. Then a partition block is proposed for capturing multivariate interaction association characteristics, where \ud835\udc4b is divided into \ud835\udc5b unit groups ( \ud835\udc5b = 3 in Fig. 2 for example) by channel splitting to facilitate interpretable learning of joint adjacency. The split unit is expressed as \ud835\udc65 \ud835\udc56 \u2208 R \ud835\udc3d \u00d7( \ud835\udc36 / \ud835\udc5b ) \u00d7 \ud835\udc39 and \ud835\udc4b \u2190 Concat [ \ud835\udc65 1 , \ud835\udc65 2 , . . . , \ud835\udc65 \ud835\udc56 ] , where \ud835\udc56 = 1 , 2 , . . . , \ud835\udc5b . Next, we feed unit inputs \ud835\udc65 \ud835\udc56 to the Frequency-aware Mixed Transformer based on self-attention and cross-attention mechanisms among Spatial Attention Blocks and Frequency-aware Attention Blocks. Afterward, frequency-spatial mixed features are processed with a Temporal Attention Block to learn inter-frame correlations. The final outputs are further reshaped and passed to an FC layer for classification. \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfcf \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc8e\ud835\udc8a\ud835\udc99 \ud835\udfcf \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfd0 \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc8e\ud835\udc8a\ud835\udc99 \ud835\udfd0 \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfd1 \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfcf \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfd0 \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfd1 \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc8e\ud835\udc8a\ud835\udc99 \ud835\udfcf \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc8e\ud835\udc8a\ud835\udc99 \ud835\udfd0 Concat \ud835\udc15 TAB Reshape Action Class \ud835\udc99\ud835\udfcf \ud835\udc99\ud835\udfd0 \ud835\udc99\ud835\udfd0 \ud835\udc99\ud835\udfd1 \ud835\udc99\ud835\udfd1 \ud835\udc99\ud835\udfd0 \ud835\udc99\ud835\udfd0 \ud835\udc99\ud835\udfd1 \ud835\udc99\ud835\udfd1 \ud835\udc99\ud835\udfcf Partition Joint Embedding Positional Embedding FAB FAB FAB FAB FAB SAB SAB SAB SAB SAB \ud835\udc721 \ud835\udc722 \ud835\udc721 \ud835\udc722 Spatial Conv \ud835\udc7f Mixed Spatial Attention Map ( \ud835\udc74\ud835\udc7a\ud835\udc8a ) Frequency-aware Mixed Transformer Skeleton Data Processing (e.g., \ud835\udc8f = \ud835\udfd1 in Partition) Head Mixed Frequency Attention Map ( \ud835\udc74\ud835\udc6d\ud835\udc8a ) Mixed Frequency- Spatial Attention Map  ( \ud835\udc74\ud835\udc6d\ud835\udc7a\ud835\udc8a ) FAB Frequency-aware Attention Block SAB Spatial Attention Block Temporal Attention Block TAB Add MatMul \ud835\udc74\ud835\udc6d\ud835\udfcf \ud835\udc74\ud835\udc6d\ud835\udfd0 \ud835\udc74\ud835\udc6d\ud835\udfd1 \ud835\udc74\ud835\udc7a\ud835\udfcf \ud835\udc74\ud835\udc7a\ud835\udfd0 \ud835\udc74\ud835\udc7a\ud835\udfd1 \ud835\udc74\ud835\udc6d\ud835\udc7a\ud835\udfcf \ud835\udc74\ud835\udc6d\ud835\udc7a\ud835\udfd0 \ud835\udc74\ud835\udc6d\ud835\udc7a\ud835\udfd1 \ud835\udc74 \ud835\udc99\ud835\udc95 \ud835\udc7f\ud835\udc90\ud835\udc96\ud835\udc95 FC-Layer", "3.3 Discrete Cosine Transform (DCT) for Joint Sequence Encoding": "coefficients of the transformation, meaning it is well-distributed for amplifying subtle motion features. Let \ud835\udc65 \u2208 R \ud835\udc3d \u00d7 \ud835\udc36 \u00d7 \ud835\udc39 denotes the input joint sequence, the trajectory of the \ud835\udc57 -th joint across \ud835\udc39 frames is denoted as \ud835\udc4b \ud835\udc57 = ( \ud835\udc65 \ud835\udc57, 0 , \ud835\udc65 \ud835\udc57, 1 , ..., \ud835\udc65 \ud835\udc57,\ud835\udc39 ) . While existing transformer-based skeleton action recognition methods only use this \ud835\udc4b \ud835\udc57 as an input sequence for skeletal correlation representation learning in the spatial domain, we propose to adopt a frequency representation based on Discrete Cosine Transform (DCT). Different from the previous DCT-based trajectory representation learning methods [26, 39, 77], which discard some of the high-frequency coefficients for providing a more compact representation, we not only keep all the DCT coefficients but also enhance the high-frequency parts and reduce the low-frequency parts. The main motivations behind this are: (i) High-frequency DCT components are more sensitive to those subtle discrepancies that are difficult to discriminate in the spatial domain (e.g., the hand movements in reading and writing, which are illustrated in Fig. 1). (ii) Low-frequency DCT coefficients reflect the movements with steady or static motion patterns, which are not discriminative enough in recognition (e.g., the lower body movements in reading and writing, which are also illustrated in Fig. 1). (iii) The cosine transform exhibits excellent energy compaction properties to concentrate the majority of the energy (low-frequency coefficients) into the first few Thus, we apply DCT to each trajectory individually. For trajectory \ud835\udc4b \ud835\udc57 , the \ud835\udc56 -th DCT coefficient is calculated as: where the Kronecker \ud835\udeff \ud835\udc56 \ud835\udc57 = 1 if \ud835\udc56 = \ud835\udc57 , otherwise \ud835\udeff \ud835\udc56 \ud835\udc57 = 0. In particular, \ud835\udc56 \u2208 { 1 , 2 , . . . , \ud835\udc39 } , the larger \ud835\udc56 , the higher frequency coefficient. These coefficients enable us to represent skeleton motion within the frequency domain effectively. Besides, the original input sequence in the time domain can be restored using Inverse Discrete Cosine Transform (IDCT), which is given by: where \ud835\udc57 \u2208 { 1 , 2 , . . . , \ud835\udc39 } . To use DCT coefficients in the transformer, we further introduce a Frequency-aware Mixed Transformer for extracting mixed frequency-spatial features in the next section.", "3.4 Frequency-aware Mixed Transformer": "Mixed Spatial Attention. Given a split input \ud835\udc65 \ud835\udc56 \u2208 R \ud835\udc3d \u00d7( \ud835\udc36 / \ud835\udc5b ) \u00d7 \ud835\udc39 mentioned in Section 3.2, the basic Query matrix and Key matrix for each sequence are extracted along the spatial dimension: where \ud835\udc56 = 1 , 2 , . . . , \ud835\udc5b . In Eq. 6, \ud835\udc34\ud835\udc63\ud835\udc54\ud835\udc43\ud835\udc5c\ud835\udc5c\ud835\udc59 denotes adaptive average pooling for smoothing the joint weight and minimizing the impact of noisy or less relevant variations within the skeletal data, and an FC-layer with a ReLU activation operation is applied to ensure \ud835\udc44 \ud835\udc56 and \ud835\udc3e \ud835\udc56 are globally integrated. Then, the self-attention is expressed as: In order to enable richer contextual integration across different unit groups, inspired by [11], a cross-attention strategy is proposed, where \ud835\udc3e \ud835\udc56 is shared between adjacent attention blocks. The cross attention is expressed as: Each mixed attention map is formulated as: where the number of this association mixed-attention maps is based on the number of unit groups (e.g., \ud835\udc5b = 3 in Fig. 2). These mixedattention maps are extracted by several SABs (Spatial Attention Blocks, illustrated in Fig. 3 (a)) for spatial representation learning. Mixed Frequency-Spatial Attention. We apply DCT to obtain the corresponding frequency coefficients from the split joint sequence \ud835\udc65 \ud835\udc56 , and then the inputs to FABs (Frequency-aware Attention Blocks, see in Fig. 3) can be denoted as \ud835\udc37\ud835\udc36\ud835\udc47 ( \ud835\udc65 \ud835\udc56 ) , where \ud835\udc37\ud835\udc36\ud835\udc47 (\u00b7) denotes the transform expressed in Eq. 4. Similar to the mixed spatial attention, we obtain the Query and Key values along the frequency domain: Thecorresponding frequency-based self-attention and mixed-attention maps are: Thus, the mixed frequency attention maps are expressed as: Subsequently, a Frequency Operator (FO) \ud835\udf13 (\u00b7) is adopted to mixed frequency attention maps: \ud835\udf13 ( \ud835\udc40\ud835\udc39 \ud835\udc56 ) . Given a frequency operator coefficient \ud835\udf11 , where \ud835\udf11 \u2208 ( 0 , 1 ) , the high-frequency coefficients in \ud835\udc40\ud835\udc39 \ud835\udc56 are enhanced by ( 1 + \ud835\udf11 ) , making minimal and subtle actions more pronounced. On the other hand, the low-frequency coefficients are reduced by \ud835\udf11 , appropriately diminishing the focus on salient actions while preserving the integrity of overall action representations. The search for the best \ud835\udf11 is discussed in Section 4.5. Afterward, an IDCT module is employed to restore the transformed skeleton sequence: \ud835\udc40\ud835\udc39 \ud835\udc56 = \ud835\udc3c \ud835\udc37\ud835\udc36\ud835\udc47 ( \ud835\udf13 ( \ud835\udc40\ud835\udc39 \ud835\udc56 )) . All the \ud835\udc40 \ud835\udc56 are extracted by Frequency-aware Attention Blocks (FABs), as depicted in Fig. 3 (b). Thus the output is: \ud835\udc40\ud835\udc39\ud835\udc46 \ud835\udc56 = \ud835\udc40\ud835\udc39 \ud835\udc56 + \ud835\udc40\ud835\udc46 \ud835\udc56 , and the final output of the mixed frequency-spatial attention map can be expressed as: Weobtain Value \ud835\udc49 from the initial input \ud835\udc4b with unified computation via adding one spatial 1 \u00d7 1 convolutional layer along the spatial dimension. Consequently, the input of the Temporal Attention Block is expressed as: Temporal Attention Block. Given the temporal input \ud835\udc65 \ud835\udc61 based on the mixed frequency-spatial attention method, some tricky strategies in [69] are adopted to transform the input channel and acquire more multivariate information alone the temporal dimension: \ud835\udc4b \ud835\udc61 = \ud835\udc36\ud835\udc47 ( \ud835\udc65 \ud835\udc61 ) (the channel transformation \ud835\udc36\ud835\udc47 (\u00b7) is detailed in the Appendix). Then the transformed input \ud835\udc4b \ud835\udc61 is processed with a temporal attention block (Fig. 3 (c)) to obtain the corresponding Query and Key matrices: And the Value in temporal attention block \ud835\udc49 \ud835\udc61 is obtained from the temporal input after a 1 \u00d7 1 convolutional layer along the temporal dimension. Finally, the temporal attention is expressed as: and the final output for the classification head is defined as: MatMul Scale Softmax \ud835\udc4b ReLU Linear AvgPool ReLU Linear AvgPool \ud835\udc65\ud835\udc56 \ud835\udc44\ud835\udc56 \ud835\udc3e\ud835\udc56 (a) Spatial Attention Block (c) Temporal Attention Block MatMul Scale Sigmoid MatMul Conv \ud835\udc4b ReLU Linear AvgPool ReLU Linear MaxPool \ud835\udc44\ud835\udc61 \ud835\udc3e\ud835\udc61 \ud835\udc49\ud835\udc61 \ud835\udc65\ud835\udc61 Channel Transformation MatMul Scale Softmax DCT IDCT (b)  Frequency-aware Attention Block ReLU Linear AvgPool ReLU Linear AvgPool Frequency Operator \ud835\udc44\ud835\udc56 \u0d25 \ud835\udc3e\ud835\udc56 \ud835\udc65\ud835\udc56", "4 Experiments": "", "4.1 Datasets": "NTU RGB+D (NTU-60) [48] is one of the most widely used largescale datasets for action recognition, containing 56,880 skeleton action samples from 40 subjects across 155 camera viewpoints. Each 3D skeleton data consists of 25 joints. The data is classified into 60 classes with two benchmarks. 1) Cross-Subject (X-Sub): half of the subjects are set for training, and the rest are used for testing. 2) Cross-View (X-View): training and test sets are split based on different camera views (2, 3 views for training, 1 for testing). NTU RGB+D 120 (NTU-120) [35] is an expansion dataset of NTU RGB+D, containing 113,945 samples with 120 action classes performed by 106 subjects. There are two benchmarks. 1) CrossSubject (X-Sub): 53 actions are used for training, and the rest are used for testing. 2) Cross-Setup (X-Set): samples with even setup IDs are set as training sets, and samples with odd setup IDs are used for testing. Northwestern-UCLA (NW-UCLA) [58] is a 10-classes action recognition dataset containing 1494 video clips. Three Kinect cameras capture the actions with different camera views. We adopt the commonly used evaluation protocols: the first two camera views are used for training, and the testing set comes from the other camera.", "4.2 Implementation Details": "We follow the standard data processing method from [8] to preprocess the skeleton data. The proposed method is implemented on Pytorch [42] with two NVIDIA RTX A6000 GPUs. The model is trained with 100 epochs and 128 batch size for all datasets mentioned above and a warm-up at the first 5 epochs. The weight decay is 0.0005, and the learning rate is initialized to 0.1 in the NTU RGB+D and NTU RGB+D 120 datasets (with a 0.1 reduction at the 35th, 55th, and 75th rounds) and 0.2 in the Northwestern-UCLA dataset (with a 0.1 reduction at the 50th round). A commonly used multi-stream ensemble method [10] is implemented for 4-stream fusion and 6-stream fusion. The experimental results are shown in Table 1 and Table 2.", "4.3 Comparison with the State-of-the-Art": "In this section, we conduct a comprehensive performance comparison with the state-of-the-art (SOTA) methods on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets to demonstrate the competitive ability of our FreqMixFormer. The comparison is made with three ensembles of different modalities and the details are provided in the appendix. Comparisons for NTU datasets are shown in Table 1. We compare our model with the recent SOTA methods based on their frameworks (GCN and Transformer). The recognition accuracy of our FreqMixFormer has outperformed all the transformerbased methods. It is noted that even our 4-stream ensemble results on both NTU-60 (93.4% in X-Sub and 97.3% in X-View) and NTU120 datasets (90.2% in X-Sub and 91.5% in X-Set) have exceeded all SOTA approaches. Despite the predominant role of GCN-based methods in the skeleton-based action recognition, as we mentioned in Section 1, FreqMixFormer still surpasses the recent methods, such as InfoGCN [10] and HD-GCN [23]. Moreover, our method achieves better performance compared with all the methods that also focus on recognizing discriminative subtle actions, including FR-Head [80] (outperformed by 0.8% on NTU-60 X-Sub and 0.6% on X-View) and WDCE-Net [7] (outperformed by 0.6% on NTU-60 X-Sub and 0.2% on X-View) methods. It is worth noting that our method not only surpasses the existing SOTA GCN-based methods but also enhances the transformer's ability to learn discriminative representations among subtle actions. In addition to experiments on large-scale datasets like NTU-60 and NTU-120, we extend our research to the small-scale dataset NW-UCLA to further validate our model's performance across different data scales. Table 2 shows results on the NW-UCLA dataset. FreqMixFormer achieves the best results (97.7 %) in comparison to SOTA methods based on GCNs and transformers. Our method outperforms the HD-GCN by 0.8% and SkeMixFormer by 0.3%.", "4.4 Comparison of Complexity with Other Models": "Table 3 shows the complexity comparison with other models. For a fair comparison, we conduct the experiments under the same settings. Although our FreqMixFormer is less efficient (GCNs typically require fewer parameters and incur lower computational costs than Transformers since GCNs leverage the inherent structure of graph data, which allows them to model node dependencies directly with minimal parameters. Additionally, GCN operations are confined to the edges of a graph, significantly reducing the GFLOPs. In contrast, transformers process all pairwise element interactions in a sequence, leading to a rapid increase in computational complexity and parameter count, especially for long sequences) than GCN-based methods, we achieve a very competitive result on the NTU-120 dataset X-Sub, which outperforms HD-GCN by 2.2 % and SKeMixFormer by 0.8 % with fewer parameters than SkeMixformer.", "4.5 Ablation Study": "In this section, we first evaluate the role of key modules in FreqMixFormer, including FAB, FO, and TAB, to analyze the effectiveness of each block. Then we propose to search for the best variants within the model, including the number of unit group \ud835\udc5b for input partition and the best frequency operator coefficient \ud835\udf11 . Additionally, we provide the visualization of the attention maps to show the effectiveness of the mixed frequency-spatial attention mechanism. The Design of Frequency-aware Mixed Transformer. As the results shown in Table 4, the baseline only contains the basic Spatial MixFormer module from [69], which only achieves 89.8% accurate. Then we propose 3 modules for analysis: 1) Frequency-aware Attention Block (FAB): the key part in our proposed method, extracting frequency-based attention maps from the joint sequence, leading to a 1.2% improvement in our baseline. 2) Frequency Operator (FO): an extra module within FABs enhances the high-frequency coefficients and reduces the low-frequency coefficients based on the DCT operator coefficient, resulting in a 1.4% improvement in the baseline. 3) Temporal Attention Block (TAB): a module utilized to learn joint correlations across frames, leading to a 0.9% over the baseline performance. As we can see, each of these modules can enhance the baseline's performance, where the main contribution comes from the utilization of Frequency-aware Attention Blocks with a proper frequency operator coefficient. The best result comes from the combination of all these modules with baseline, which achieves 91.5% accuracy in NTU-60 X-Sub with joint modality. It is speculated that the proposed frequency-aware attention mechanism (see in Section 3.4) plays a significant role in enhancing the action recognition performance. The experiment results on confusing actions with subtle motions are presented in Section 4.6. Search for the best frequency operator coefficient \ud835\udf11 . Moreover, we also conduct analysis regarding the best number of the unit \ud835\udc5b in Table 5. It can be seen in Table 5 that the increasing \ud835\udc5b does improve the results ( \ud835\udc5b = 2 , 3 , 4) with a peak accuracy of 91.5% in NTU-60 X-Sub and 96.0% in NTU-60 X-View. However, further increments do not lead to better outcomes, only to a higher computational cost (the model parameters keep increasing from 1.26M to 2.83M). Given the trade-off between cost and performance, we opt for a splitting number of \ud835\udc5b = 4 for subsequent experiments. Search For the Best Frequency Operator Coefficient \ud835\udf11 . In Table 6, we investigate the impact of frequency operator coefficient \ud835\udf11 . As we discussed on Section 3.4, the high-frequency coefficients will be amplified by ( 1 + \ud835\udf11 ) , and the low-frequency coefficients will be diminished by \ud835\udf11 . As \ud835\udf11 increases from 0.1 to 0.5, there is a general trend of improved performance, reaching a peak at \ud835\udf11 = 0 . 5, which achieves the highest accuracy of 91.5% on NTU-60 X-Sub and 96.0% on X-View. However, further increasing \ud835\udf11 from 0.6 to 0.9 does not lead to improvements in performance. In fact, the accuracy slightly declines. This suggests that enhancing high-frequency components too much or reducing low-frequency components too aggressively may lead to loss of motion patterns learning. Effectiveness of the Mixed Frequency-aware Attention. Fig. 4 presents the visualization of the attention matrices learned by FreqMixFormer. The skeleton configuration is generated from the NTU-60 dataset (Fig. 4 (a)). We take \"eat meal\" as an example (Fig. 4 (b)). In the correlation matrix, a more saturated yellow represents a large weight, indicating a stronger correlation among joints. And the numbers denote different joints. Note that the Mixed Spatial Attention Map (Fig. 4 (c), learned by SAB) represents the spatial relationships among joints. The Mixed Frequency Attention Map (Fig. 4 (d), learned by FAB) suggests the frequency aspects of motion. Based on these two attention maps, a mixed frequency-spatial attention map is proposed (Fig. 4 (e)) for capturing both spatial correlations and frequency dependencies, integrating the spatial and frequency skeleton features. As we see in the figures, the model focuses on the correlations with the spine and right-hand tip in the spatial domain. As for the frequency domain, more correlation areas are concerned (joint connections with the spine, left arm, and the interactions between head and hands), which indicates the model is analyzing more discriminative movements overlooked in the spatial domain. Meanwhile, the mixed frequency-spatial attention map contains not only the strong attention areas learned from spatial space but also the concerned correlations in frequency space. This demonstrates that our FreqMixFormer model advances this by extracting minimal and subtle joint representations (highlighted with the red box in Fig. 4 (b)) from both spatial and frequency domains. The effectiveness of the mixed frequency attention is also verified in Table 4. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 0- base of the spine 1-middle of the spine 2-neck 3-head 4-left shoulder 5-left elbow 6-left wrist 7-hand 8-right shoulder 9-right elbow 10-right wrist 11-right hand 12-left hip 13-left knee 14-left ankle 15-left foot 16-right hip 17-right knee 18-right ankle 19-right foot 20-spine 21-tip of the left hand 22-left thumb 23-tip of the right hand 24-right thumb (a) (c) (d) (e) (b) Joint Index Joint Index Time", "4.6 Comparison Results on Confusing Actions": "To validate our model's capability in discerning discriminative actions, similar to [7, 80], we categorize certain actions from the NTU-60 dataset (only joint stream with X-Sub protocol) into three sets based on the classification results of Hyperformer [81]: the actions with accuracy lower than 80% as Hard set, between 80% and 90% as Medium set, and higher than 90% as Easy set. All the confusing actions are classified into Hard and Medium sets. For example, \"writing,\" \"reading,\" and \"playing with a phone\" are categorized as Hard action sets due to their subtle differences, which are limited to small upper-body movements involving only a few joint correlations, leading to low recognition results. We compare our results with the recent transformer-based models Hyperformer [81] and SkeMixFormer [69]. The results of different difficult-level actions are displayed in Table 7, showcasing that our model outperforms the recent SOTA methods across these three subsets. Furthermore, the detailed results of Hard and Medium actions are also provided in Fig. 5 and Fig. 6. The results indicate that our method significantly enhances performance on both hard-level and medium-level confusing actions, demonstrating its capability to differentiate ambiguous movements. Hard Set 80,0% 60.0% 50.0% eat meal reading writing take off ashoe play with tpe sneerelcough phoneltablet keyboard FreqMixFormer SkeMixFormer Hyperformer 87.0% 86.0% 85,0% 4 84.0% 4 drink water brush teeth clapping put 0n shoe reach into headache /vomiting pocket FreqMixFormer SkeMixFormer Hyperformer Medium Set", "5 Conclusion and Discussion": "In this work, we introduce Frequency-aware Mixed Transformer (FreqMixFormer), a novel transformer architecture designed to discern discriminative movements among similar skeletal actions by leveraging a frequency-aware attention mechanism. This model enhances skeleton action recognition by integrating spatial and frequency features to capture comprehensive intra-class frequencyspatial patterns. Our extensive experiments across diverse datasets, including NTU RGB+D, NTU RGB+D 120, and NW-UCLA, establish FreqMixFormer's state-of-the-art performance. The proposed model demonstrates superior accuracy in general and significant advancements in recognizing confusing actions. Our research advances the field by presenting a method that integrates frequency domain analysis with current transformer models, paving the way for more precise and efficient action recognition systems. This work is anticipated to inspire future research on precision-targeted skeletal action recognition.", "Acknowledgment": "This work was supported in part by the NSF grant of 1840080.", "A OVERVIEW OF SUPPLEMENTARY MATERIAL": "In this supplementary material, we provide the following items: \u00b7 Partial DCT vs full DCT algorithms. \u00b7 Evaluation of the number of DCT coefficients. \u00b7 Evaluation on UAV-Human dataset. \u00b7 Additional results. \u00b7 Implementation details. \u00b7 More visualizations. \u00b7 Limitations and future work.", "B Partial DCT vs Full DCT Algorithms": "In FreqMixFormer, we utilize DCT in Frequency-aware Attention Block (FAB) to extract skeletal frequency features. As illustrated in Fig. 2 and 3 in the main paper, only Query matrix \ud835\udc44 and Key matrix \ud835\udc3e are processed with DCT and IDCT modules for attention score, Value matrix \ud835\udc49 is only processed with linear transformation, the methodology can be found in Algorithm 1 as Partial DCT Algorithm. Moreover, we also investigate the Full DCT Algorithm, where DCT and IDCT process \ud835\udc49 , and the methodology is shown in Algorithm 2. However, the full DCT algorithm performs poorly in the experiment: the full DCT algorithm only achieves 87.7% on the NTU-60 X-Sub setting, while the partial DCT algorithm achieves 91.5% accuracy. The overview of the FreqMixFormer with full DCT algorithm is illustrated in Fig. 8. The Spatial Attention Block (SAB) in this experiment is shown in Fig. 7 (a) and the Frequency-aware Mixed Former (FAB) with full DCT algorithm is shown in Fig. 7 (b). We hypothesize that the primary issues contributing to this gap are: 1) Applying DCT to \ud835\udc44 and \ud835\udc3e can effectively highlight key frequency features and improve the model accuracy by matching relevant features during the computation of attention scores. 2) By excluding \ud835\udc49 from the frequency domain, the original temporalspatial information is retained. This retention may help preserve more detailed and dynamic information in the final representation, enhancing the model's ability to utilize these details for action recognition. 3) Recognizing actions relies not only on the frequency characteristics of movements (such as the speed and rhythm) but also on the specifics of how the actions are performed (like the swinging of an arm). Processing \ud835\udc44 , \ud835\udc3e , and \ud835\udc49 in different domains may allow the model to balance these needs. (a) SAB MatMul Scale Softmax \ud835\udc4b ReLU Linear AvgPool ReLU Linear AvgPool \ud835\udc65\ud835\udc56 \ud835\udc44\ud835\udc56 \ud835\udc3e\ud835\udc56 Conv MatMul \ud835\udc49\ud835\udc56 MatMul Scale Softmax DCT IDCT ReLU Linear AvgPool ReLU Linear AvgPool Frequency Operator \ud835\udc44\ud835\udc56 MatMul \u0d25 \ud835\udc49\ud835\udc56 (b) FAB with full DCT algorithm Conv \ud835\udc65\ud835\udc56 \u0d25 \ud835\udc3e\ud835\udc56", "C Evaluation of the number of DCT coefficients": "In order to explore the frequency operator in-depth, we conduct an evaluation of the number of enhanced DCT coefficients. Table 9 shows the extra ablation study on the number of DCT coefficients \ud835\udc41 \ud835\udc50 that we set as high-frequency coefficients (the rest are set as low-frequency coefficients). The high-frequency coefficients are enhanced by a frequency operator coefficient \ud835\udf11 discussed in Section 3.4 of the main paper. For a fair comparison, we keep \ud835\udf11 = 0 . 5 during the experiments. As shown in the table, with the number of the enhanced DCT coefficient \ud835\udc41 \ud835\udc50 = 12, the model achieves the best performance on NTU-60 (91.5% in X-Sub and 96.0% in X-View) dataset, and further increasing does not result in improvements.", "Algorithm 1 Partial DCT": "Input : the skeleton sequence is processed with joint embedding \ud835\udc4b \ud835\udc4b \u2208 R \ud835\udc36 \u00d7 \ud835\udc39 \u00d7 \ud835\udc3d and positional embedding as the initial input , where . Init : \ud835\udc4a \ud835\udc44 , \ud835\udc4a \ud835\udc3e and \ud835\udc4a \ud835\udc49 are the learnable weight matrices. Output : the partial DCT attention score (1) \ud835\udc4b = \ud835\udc37\ud835\udc36\ud835\udc47 ( \ud835\udc4b ) (2) \ud835\udc44 = \ud835\udc4b\ud835\udc4a \ud835\udc5e , \ud835\udc3e = \ud835\udc4b\ud835\udc4a \ud835\udc58 , \ud835\udc49 = \ud835\udc4b\ud835\udc4a \ud835\udc63 Return : \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b 1", "Algorithm 2 Full DCT": "Input : the skeleton sequence is processed with joint embedding \ud835\udc4b \ud835\udc4b \u2208 R \ud835\udc36 \u00d7 \ud835\udc39 \u00d7 \ud835\udc3d and positional embedding as the initial input , where . Init : \ud835\udc4a \ud835\udc44 , \ud835\udc4a \ud835\udc3e are the learnable weight matrices. Output : the full DCT attention score (1) \ud835\udc4b = \ud835\udc37\ud835\udc36\ud835\udc47 ( \ud835\udc4b ) (2) \ud835\udc44 = \ud835\udc4b\ud835\udc4a \ud835\udc5e , \ud835\udc3e = \ud835\udc4b\ud835\udc4a \ud835\udc58 , \ud835\udc49 = \ud835\udc4b\ud835\udc4a \ud835\udc63 \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfcf \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc8e\ud835\udc8a\ud835\udc99 \ud835\udfcf \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfd0 \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc8e\ud835\udc8a\ud835\udc99 \ud835\udfd0 \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfd1 \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfcf \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfd0 \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc94\ud835\udc86\ud835\udc8d\ud835\udc87 \ud835\udfd1 \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc8e\ud835\udc8a\ud835\udc99 \ud835\udfcf \ud835\udc68\ud835\udc95\ud835\udc95\ud835\udc86\ud835\udc8f \ud835\udc8e\ud835\udc8a\ud835\udc99 \ud835\udfd0 Concat TAB Reshape Action Class \ud835\udc99\ud835\udfcf \ud835\udc99\ud835\udfd0 \ud835\udc99\ud835\udfd0 \ud835\udc99\ud835\udfd1 \ud835\udc99\ud835\udfd1 \ud835\udc99\ud835\udfd0 \ud835\udc99\ud835\udfd0 \ud835\udc99\ud835\udfd1 \ud835\udc99\ud835\udfd1 \ud835\udc99\ud835\udfcf Partition Joint Embedding Positional Embedding FAB FAB FAB FAB FAB SAB SAB SAB SAB SAB \ud835\udc721 \ud835\udc722 \ud835\udc721 \ud835\udc722 \ud835\udc7f Mixed Spatial Attention Map ( \ud835\udc74\ud835\udc7a\ud835\udc8a ) Frequency-aware Mixed Transformer Skeleton Data Processing (e.g., \ud835\udc8f = \ud835\udfd1 in Partition) Head Mixed Frequency Attention Map ( \ud835\udc74\ud835\udc6d\ud835\udc8a ) Mixed Frequency- Spatial Attention Map  ( \ud835\udc74\ud835\udc6d\ud835\udc7a\ud835\udc8a ) FAB Frequency-aware Attention Block (with full DCT algorithm) SAB Spatial Attention Block Temporal Attention Block TAB Add \ud835\udc74\ud835\udc6d\ud835\udfcf \ud835\udc74\ud835\udc6d\ud835\udfd0 \ud835\udc74\ud835\udc6d\ud835\udfd1 \ud835\udc74\ud835\udc7a\ud835\udfcf \ud835\udc74\ud835\udc7a\ud835\udfd0 \ud835\udc74\ud835\udc7a\ud835\udfd1 \ud835\udc74\ud835\udc6d\ud835\udc7a\ud835\udfcf \ud835\udc74\ud835\udc6d\ud835\udc7a\ud835\udfd0 \ud835\udc74\ud835\udc6d\ud835\udc7a\ud835\udfd1 \ud835\udc7f\ud835\udc90\ud835\udc96\ud835\udc95 FC-Layer \ud835\udc99\ud835\udc95 outperforms all the existing methods and achieves the new stateof-the-art results on this benchmark. Rreturn : \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b 2", "D Evaluation on UAV-Human Dataset": "", "D.1 UAV-Human Dataset": "UAV-Human [32] is an action recognition dataset comprising 22,476 video clips with 155 classes. The dataset was collected via a UAV across various urban and rural settings, both during daytime and nighttime. It extracts action data from 119 distinct subjects engaged in 155 different activities across 45 diverse environmental locations. For evaluation (X-Sub, 17 joints in each subject), 89 subjects are selected for training and 30 for testing.", "D.2 Experiment Settings": "The hardware configurations are the same as the experiments reported in the main paper. The model is trained with 100 epochs, and the batch size is 128. We set a warm-up at the first 5 epochs. The weight decay is set as 0.0005, and the basic learning rate is 0.2. There is a 0.1 reduction at the 50th epoch.", "D.3 Comparison Results": "As Table 10 shows, we compare our performance with the state-ofthe-art methods on the UAV-Human dataset. Our FreqMixFormer", "E Additional Results": "", "E.1 Accuracy Difference Results": "We further analyze the Top-1 Accuracy Difference (%) between the proposed FreqMixFormer and the baseline method SkeMixFormer[69] with the joint input modality on NTU RGB+D 120 X-Sub. As illustrated in Fig. 12, the most significant improvements typically appear in confusing actions with subtle movements. For instance, our model achieves an improvement of 35.09% for \" make OK sign \", 21.55% for \" make victory sign \", and 18.56% for \" counting money \". These results underscore FreqMixFormer's performance in recognizing actions that are visually confusing by extracting the frequencyspatial features.", "E.2 Comparison with Frequency-based Results": "Weprovide an extra comparison with the previous frequency-based methods in skeleton action recognition. As shown in Table 8, our FreqMixFormer outperforms all the existing methods utilizing frequency analysis on the NTU-60 X-Sub dataset. Moreover, our model also plays a significant role in efficiency, as it has the least parameters (2.04M) and the best GFLOPs (2.40) among the frequency-based methods. performance. However, a higher batch size (256) is not better because it requires more memory and leads to convergence issues. Thus, we choose 128 as our default batch size.", "F Implementation Details": "", "F.1 Multi-stram Fusion Strategy": "The comparison is made with three ensembles of different modalities (joint only, 4-stream ensemble, 6-stream ensemble. We denote the stream as S for convenience) following the setting of InfoGCN [10]: S1: k = 1, motion = False; S2: k = 2, motion = False; S3: k = 8 (k = 6 for NW-UCLA and UAV-Human datasets), motion = False; S4: k = 1, motion = True; S5: k = 2, motion = True; S6: k = 8 (k = 6 for NW-UCLA and UAV-Human datasets), motion = True, where k indicates k value of k-th mode representation of the skeleton. And 4-stream = S1+S2+S4+S5, 6-stream = S1+S2+S3+S4+S5+S6. For a fair comparison, experiments using the baseline method are also conducted with this emsemble strategy.", "F.2 Evaluations of the Batch Size": "Fig. 9 illustrates the impact of batch size during the training. We take the experimental results on the NTU-60 dataset as an example. As we can see, increasing the batch size from 32 to 128 enhances 96.0 95.6 95.0 94.9 95 94 93 92 91.1 90.8 90.2 128 256 Batch Size", "F.3 Channel Transformation in Temporal Attention Block": "As we mentioned in Section 3.4 of the main paper, we adopt some tricky strategies in the baseline method [69] as our temporal channel transformation \ud835\udc36\ud835\udc47 (\u00b7) , which is stacked with two modules: 1) Channel Reforming Model. An improving model derived from SEnet[17], which enhances the feature separation between groups and reduces noise, it is essential to reorganize the channel relationships within each group. 2) Multiscale Convolution Module. The first part of the Temporal MixFormer in [69], which is a simple optimization from MS-G3D [37] of maintaining a fixed filter while adjusting dilation, enabling the acquisition of more diverse multiscale temporal information and reducing computational costs. We simply adopt this combination as the \ud835\udc36\ud835\udc47 (\u00b7) operation.", "G More Visualizations": "In this section, we exhibit more attention maps, the same as the visualization results illustrated in Section 4.5 of the main paper. Since we have provided the action \"eat a meal\" from the Hard set, we give more visualization results from the Medium set (headache) and Easy set (kicking) as examples. All the skeletons and attention maps are generated by the NTU-60 dataset. As shown in Fig. 10 and Fig .11, our proposed Frequency-aware Mixed attention maps (extracted by FAB modules) contain more detailed information and joint correlations compared with the spatial maps (extracted by SAB).", "Time": "(b) Mixed spatial attention map Joint Index Joint Index (c) Mixed frequency-aware attention map (d) Mixed frequency-spatial attention map Time (b) Mixed spatial attention map Joint Index Joint Index (c) Mixed frequency-aware attention map (d) Mixed frequency-spatial attention map", "H Limitations and Future Work": "Despite the high accuracy of our model, it still has some limitations. Firstly, our model is still not efficient and lightweight enough. As we discussed in the ablation study from the main paper, there is a gap between our method and the recent GCN-based methods such as HD-GCN [23] (1.68M parameters vs 2.04M, 1.60 GFLIOPS vs 2.40 GFLOPS), and we have no remarkable advantages of efficiency over the recent transformer-based methods. Secondly, we keep all the high-frequency coefficients during the training, which is not robust to noisy joint information. The more efficient way is to enhance the high-frequency coefficients selectively instead of the whole coefficients. Our future work will focus on finding the best trade-off point between efficiency and accuracy. 1 make victory sign counting Money thumh up play with phoncltablet 1 fold paper shoot with gun point something shoot basket reach into pocket open writing ball up paper nausealvomiting play magic cuhe wield knile hand waving palms together throw up cap taking front cross toe touch cutting nails Dulon headphone kesboard somcbody reading hush photo punchslap drink water hopping exchange things giving object paper jacket stand up butt kicks nlick hair kicking juccle table tennis ball 1 check timenodone adabch) kicking something hcadache apply shake hcud phone call capitulate cutting paper pushing walking apart pain cheers and drink puton hatcap couph heavy objects shaking hands self brush hair high-five rock-paper-scissors walking towards salute take olf a hatlcap pocket carry ohjccr abject into pick bacl squat dov put on jump fallins stcp bae", "References": "[1] Nasir Ahmed, T_ Natarajan, and Kamisetty R Rao. 1974. Discrete cosine transform. IEEE transactions on Computers 100, 1 (1974), 90-93. [2] Dasom Ahn, Sangwon Kim, Hyunsu Hong, and Byoung Chul Ko. 2023. Startransformer: A spatio-temporal cross attention transformer for human action recognition. In Proceedings of the IEEE/CVF winter conference on applications of computer vision . 3330-3339. [3] Ijaz Akhter, Yaser Sheikh, Sohaib Khan, and Takeo Kanade. 2008. Nonrigid structure from motion in trajectory space. Advances in neural information processing systems 21 (2008). [4] Ruwen Bai, Min Li, Bo Meng, Fengfa Li, Miao Jiang, Junxing Ren, and Degang Sun. 2022. Hierarchical graph convolutional skeleton transformer for action recognition. In 2022 IEEE International Conference on Multimedia and Expo (ICME) . IEEE, 01-06. [5] Chaitanya Bandi and Ulrike Thomas. 2021. Skeleton-based action recognition for human-robot interaction using self-attention mechanism. In 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021) . IEEE, 1-8. [6] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017. Realtime multiperson 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition . 7291-7299. [7] Haochen Chang, Jing Chen, Yilin Li, Jixiang Chen, and Xiaofeng Zhang. 2024. Wavelet-Decoupling Contrastive Enhancement Network for Fine-Grained Skeleton-Based Action Recognition. arXiv preprint arXiv:2402.02210 (2024). [8] Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying Deng, and Weiming Hu. 2021. Channel-wise topology refinement graph convolution for skeleton-based action recognition. In Proceedings of the IEEE/CVF international conference on computer vision . 13359-13368. [9] Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian Cheng, and Hanqing Lu. 2020. Skeleton-based action recognition with shift graph convolutional network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 183-192. [10] Hyung-gun Chi, Myoung Hoon Ha, Seunggeun Chi, Sang Wan Lee, Qixing Huang, and Karthik Ramani. 2022. Infogcn: Representation learning for human skeletonbased action recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 20186-20196. [11] Yutao Cui, Cheng Jiang, Limin Wang, and Gangshan Wu. 2022. Mixformer: Endto-end tracking with iterative mixed attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 13608-13618. [12] Yong Du, Wei Wang, and Liang Wang. 2015. Hierarchical recurrent neural network for skeleton based action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition . 1110-1118. [13] Xuehao Gao, Shaoyi Du, Yang Wu, and Yang Yang. 2023. Decompose more and aggregate better: Two closer looks at frequency representation learning for human motion prediction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 6451-6460. [14] Zhimin Gao, Peitao Wang, Pei Lv, Xiaoheng Jiang, Qidong Liu, Pichao Wang, Mingliang Xu, and Wanqing Li. 2022. Focal and global spatial-temporal transformer for skeleton-based action recognition. In Proceedings of the Asian Conference on Computer Vision . 382-398. [15] Yonghong Hou, Zhaoyang Li, Pichao Wang, and Wanqing Li. 2016. Skeleton optical spectra-based action recognition using convolutional neural networks. IEEE Transactions on Circuits and Systems for Video Technology 28, 3 (2016), 807811. [16] Guyue Hu, Bo Cui, and Shan Yu. 2019. Joint learning in the spatio-temporal and frequency domains for skeleton-based action recognition. IEEE Transactions on Multimedia 22, 9 (2019), 2207-2220. [17] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition . 7132-7141. [18] Yujian Jiang, Zhaoneng Sun, Saisai Yu, Shuang Wang, and Yang Song. 2022. A graph skeleton transformer network for action recognition. Symmetry 14, 8 (2022), 1547. [19] Mohamad Kashef, Anna Visvizi, and Orlando Troisi. 2021. Smart city as a smart service system: Human-computer interaction and smart city surveillance systems. Computers in Human Behavior 124 (2021), 106923. [20] Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, and Farid Boussaid. 2017. A new representation of skeleton sequences for 3d action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition . 3288-3297. [21] Tae Soo Kim and Austin Reiter. 2017. Interpretable 3D Human Action Analysis with Temporal Convolutional Networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) . https://doi.org/10.1109/ cvprw.2017.207 [22] Inwoong Lee, Doyoung Kim, Seoungyoon Kang, and Sanghoon Lee. 2017. Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks. In Proceedings of the IEEE international conference on computer vision . 1012-1020. [44] Chiara Plizzari, Marco Cannici, and Matteo Matteucci. 2021. Skeleton-based action recognition via spatial and temporal transformer networks. Computer [64] Xinghan Wang, Xin Xu, and Yadong Mu. 2023. Neural Koopman pooling: Controlinspired temporal dynamics encoding for skeleton-based action recognition. In"}
