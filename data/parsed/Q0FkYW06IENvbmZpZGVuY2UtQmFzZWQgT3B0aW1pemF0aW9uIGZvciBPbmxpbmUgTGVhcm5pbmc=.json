{"title": "CADAM: CONFIDENCE-BASED OPTIMIZATION FOR ONLINE LEARNING", "authors": "Shaowen Wang; Anan Liu; Jian Xiao; Huan Liu; Yuekui Yang; Cong Xu; Qianqian Pu; Suncong Zheng; Wei Zhang; Jian Li", "pub_date": "2024-11-29", "abstract": "Modern recommendation systems frequently employ online learning to dynamically update their models with freshly collected data. The most commonly used optimizer for updating neural networks in these contexts is the Adam optimizer, which integrates momentum (m t ) and adaptive learning rate (v t ). However, the volatile nature of online learning data, characterized by its frequent distribution shifts and presence of noises, poses significant challenges to Adam's standard optimization process: (1) Adam may use outdated momentum and the average of squared gradients, resulting in slower adaptation to distribution changes, and (2) Adam's performance is adversely affected by data noise. To mitigate these issues, we introduce CAdam, a confidence-based optimization strategy that assesses the consistence between the momentum and the gradient for each parameter dimension before deciding on updates. If momentum and gradient are in sync, CAdam proceeds with parameter updates according to Adam's original formulation; if not, it temporarily withholds updates and monitors potential shifts in data distribution in subsequent iterations. This method allows CAdam to distinguish between the true distributional shifts and mere noise, and adapt more quickly to new data distributions. Our experiments with both synthetic and real-world datasets demonstrate that CAdam surpasses other well-known optimizers, including the original Adam, in efficiency and noise robustness. Furthermore, in large-scale A/B testing within a live recommendation system, CAdam significantly enhances model performance compared to Adam, leading to substantial increases in the system's gross merchandise volume (GMV).", "sections": [{"heading": "INTRODUCTION", "text": "Modern recommendation systems, such as those used in online advertising platforms, rely on online learning to update real-time models with freshly collected data batches (Ko et al., 2022). In online learning, models continuously adapt to users' interests and preferences based on immediate user interactions like clicks or conversions. Unlike traditional offline training-where data is pre-collected and static-online learning deals with streaming data that is often noisy and subject to frequent distribution changes. This streaming nature makes it challenging to effectively denoise and reorganize training samples (Su et al., 2024;Zhang et al., 2021).\nA widely adopted optimizer in these systems is the Adam optimizer (Kingma & Ba, 2015), which combines the strengths of parameter-adaptive methods and momentum-based methods. Adam adjusts learning rates based on the averaged gradient square norm (v t ) and incorporates momentum (m t ) for faster convergence. Its ability to maintain stable and efficient convergence by dynamically adjusting learning rates based on the first and second moments of gradients has made it a reliable Preprint choice for optimizing deep learning models across diverse applications, including image recognition (Alexey, 2020), natural language processing (Vaswani, 2017), and reinforcement learning (Schulman et al., 2017). However, Adam faces significant challenges in online learning environments. Specifically, it treats all incoming data equally, regardless of whether it originates from the original distribution, a new one, or is merely noise. This indiscriminate treatment leads to two key problems:\n1. Outdated Momentum and Averaged Squared Gradients: When the data distribution shifts-a common occurrence in online systems due to factors such as daily cycles in shopping habits, rapidly changing trends on social media, seasonal changes, promotional events, and sudden market dynamics-Adam continues to use momentum and averaged squared gradients computed from previous data (Lu et al., 2018;Viniski et al., 2021). These outdated statistics can misguide the optimizer, resulting in slower adaptation to the new data distributions.\n2. Sensitivity to Noise: Online learning data often contains noisy labels (Yang et al., 2023). For example, in advertisement systems, users might click ads by mistake (false positives) or ignore ads they are interested in (false negatives) (Wang et al., 2021). Sensitivity to such noise can affect convergence speed and may cause parameters to deviate from the correct optimization direction, especially in scenarios where noisy data constitutes a large proportion.\nTo address these issues inherent in online learning with Adam, we propose Confidence Adaptive Moment Estimation (CAdam), a novel optimization strategy that enhances Adam's robustness and adaptability. CAdam introduces a confidence metric that evaluates whether updating a specific parameter will be beneficial for the system. This metric is calculated by assessing the alignment between the current momentum and the gradient for each parameter dimension.\nSpecifically, if the momentum and the gradient point in the same direction, indicating consistency in the optimization path, CAdam proceeds with the parameter update following Adam's rule. Otherwise, if they point in opposite directions, CAdam pauses the update for that parameter to observe potential distribution changes in subsequent iterations. This strategy hinges on the idea that persistent opposite gradients suggest a distributional shift, as the momentum (an exponential moving average of past gradients) represents the recent trend. If the opposite gradients do not persist, it it likely to be noise, and the model resumes normal updates, effectively filtering out the noise.\nBy incorporating this simple, plug-and-play mechanism, CAdam retains the advantages of momentum-based optimization while enhancing robustness to noise and improving adaptability to meaningful distribution changes in online learning scenarios.\nOur contribution can be summarized as follows:\n1. We introduce CAdam, a confidence-based optimization algorithm that improves upon the standard Adam optimizer by addressing its limitations in handling noisy data and adapting to distribution shifts in real-time online learning.\n2. Through extensive experiments on both synthetic and public datasets, we demonstrate that CAdam consistently outperforms popular optimizers in online recommendation settings.\n3. We validate the real-world applicability of CAdam by conducting large-scale online A/B tests in a live system, proving its effectiveness in boosting system performance and achieving significant improvements in gross merchandise volume (GMV) worth millions of dollars.", "publication_ref": ["b18", "b33", "b43", "b17", "b2", "b34", "b29", "b22", "b35", "b40", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "Adam Extensions Adam is one of the most widely used optimizers, and researchers have proposed various modifications to address its limitations. AMSGrad (Reddi et al., 2018) addresses Adam's non-convergence issue by introducing a maximum operation in the denominator of the update rule. RAdam (Liu et al., 2019) incorporates a rectification term to reduce the variance caused by adaptive learning rates in the early stages of training, effectively combining the benefits of both adaptive and non-adaptive methods. AdamW (Loshchilov, 2017) separates weight decay from the gradient update, improving regularization. Yogi (Zaheer et al., 2018) modifies the learning rate using a different update rule for the second moment to enhance stability. AdaBelief (Zhuang et al., 2020) refines the second-moment estimation by focusing on the deviation of the gradient from its exponential moving average rather than the squared gradient. This allows the step size to adapt based on the \"belief\" in the current gradient direction, resulting in faster convergence and improved generalization. Our method, CAdam, similarly leverages the consistency between the gradient and momentum for adjustments. However, it preserves the original update structure of Adam and considers the sign (directional consistency) between momentum and gradient, rather than their value deviation, leading to better performance under distribution shifts and in noisy environments.\nAdapting to Distributional Changes in Online Learning In online learning scenarios, models encounter data streams where the underlying distribution can shift over time, a phenomenon known as concept drift (Lu et al., 2018). Adapting to these changes is essential for maintaining model performance. One common strategy is to use sliding windows or forgetting mechanisms (Bifet & Gavalda, 2007), which focus updates on the most recent data. Ensemble methods (Street & Kim, 2001) maintains a collection of models trained on different time segments and combine their predictions to adapt to emerging patterns. Adaptive learning algorithms, such as Online Gradient Descent (Zinkevich, 2003), dynamically adjust the learning rate or model parameters based on environmental feedback. Meta-learning approaches (Finn et al., 2017) aim to develop models that can quickly adapt to new tasks or distributions with minimal updates. Additionally, (Viniski et al., 2021) demonstrated that streaming-based recommender systems outperform batch methods in supermarket data, particularly in handling concept drifts and cold start scenarios.\nRobustness to Noisy Data General methods for noise robustness include robust loss functions (Ghosh et al., 2017), which modify the objective function to reduce sensitivity to mislabeled or corrupted data; regularization techniques (Srivastava et al., 2014), which prevent overfitting by introducing noise during training; and noise-aware algorithms (Gutmann & Hyv\u00e4rinen, 2010), which explicitly model noise distributions to improve learning. In recommendation systems, enhancing robustness against noisy data is crucial and is typically addressed through two main strategies: detect and correct and detect and remove. Detect and correct methods, such as AutoDenoise (Ge et al., 2023) and Dual Training Error-based Correction (DTEC) (Panagiotakis et al., 2021), identify noisy inputs and adjust them to improve model accuracy by leveraging mechanisms like validation sets or dual error perspectives. Conversely, detect and remove approaches eliminate unreliable data using techniques such as outlier detection with statistical models (Xu et al., 2022) or semantic coherence assessments (Saia et al., 2016) to cleanse user profiles. While these strategies can effectively enhance recommendation quality, they often require explicit design and customization for specific models or tasks, limiting their general applicability.", "publication_ref": ["b20", "b21", "b41", "b47", "b22", "b3", "b32", "b48", "b8", "b35", "b11", "b31", "b13", "b10", "b23", "b39", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "DETAILS OF CADAM OPTIMIZER", "text": "Notations We use the following notations for the CAdam optimizer:\n\u2022 f (\u03b8) \u2208 R, \u03b8 \u2208 R d : f is the stochastic objective function to minimize, where \u03b8 is the parameter vector in R d .\n\u2022 g t : the gradient at step t, g t = \u2207 \u03b8 f t (\u03b8 t-1 ).\n\u2022 m t : exponential moving average (EMA) of g t , calculated as\nm t = \u03b2 1 \u2022 m t-1 + (1 -\u03b2 1 ) \u2022 g t .\n\u2022 v t : EMA of the squared gradients, given by\nv t = \u03b2 2 \u2022 v t-1 + (1 -\u03b2 2 ) \u2022 g 2 t .\n\u2022 mt , vt : bias-corrected estimates of m t and v t , respectively, where mt = mt\n1-\u03b2 t 1 and vt = vt 1-\u03b2 t 2 .\n\u2022 \u03b1, \u03f5: \u03b1 is the learning rate, typically set to 10 -3 , and \u03f5 is a small constant to prevent division by zero, typically set to 10 -8 .\n\u2022 \u03b2 1 , \u03b2 2 : smoothing parameters, commonly set as \u03b2 1 = 0.9, \u03b2 2 = 0.999.\n\u2022 \u03b8 t : the parameter vector at step t.\n\u2022 \u03b8 0 : the initial parameter vector.\nAlgorithm 1 Confidence Adaptive Moment Estimation (CAdam) 1: m 0 \u2190 0, v 0 \u2190 0, vmax,0 \u2190 0, t \u2190 0, \u03b8 t = \u03b8 0 2: while \u03b8 t not converged do 3:\nt \u2190 t + 1 4: g t \u2190 \u2207 \u03b8 f t (\u03b8 t-1 ) 5: m t \u2190 \u03b2 1 \u2022 m t-1 + (1 -\u03b2 1 ) \u2022 g t 6: v t \u2190 \u03b2 2 \u2022 v t-1 + (1 -\u03b2 2 ) \u2022 g 2 t 7: mt \u2190 m t /(1 -\u03b2 t 1 ) 8: vt \u2190 v t /(1 -\u03b2 t 2 ) 9:\nif AMSGrad then 10:\nvmax,t \u2190 max(v max,t-1 , vt ) 11: else 12: vmax,t \u2190 vt 13: end if 14: mt \u2190 mt \u2299 I(m t \u2299 g t > 0) \u25b7 Element-wise mask out elements where m i t \u2022 g i t \u2264 0 15: \u03b8 t \u2190 \u03b8 t-1 -\u03b1 \u2022 mt /( vmax,t + \u03f5) 16: end while 17: return \u03b8 t\nComparison with Adam CAdam (Algorithm 1) and Adam both use the first and second moments of gradients to adapt learning rates. The main difference between CAdam and Adam is that CAdam introduces the alignment between the momentum and the gradient as a confidence metric to address two common problems in real-world online learning: distribution shifts and noise.\nIn Adam, the update direction is determined by m t , the exponential moving average (EMA) of the gradient g t , and v t , the EMA of the squared gradients g 2 t . This method assumes a relatively stable data distribution, where m t serves as a good estimator of the optimal update direction. However, if the data distribution changes, m t may no longer point in the correct direction. Adam will continue to update using the outdated m t for several iterations until it eventually aligns with the new gradient direction, leading to poor performance during this adaptation period. Additionally, when encountering noisy examples, Adam blindly updates using m t , which can be problematic as it equivalently increases the learning rate, especially when the proportion of noisy data is high.\nIn contrast, CAdam dynamically checks the alignment between the current gradient g t and the momentum m t before proceeding with an update. If g t and m t point in the same direction, indicating that the momentum aligns with the current gradient, CAdam performs the update using m t / \u221a v t .\nHowever, if g t and m t point in opposite directions, CAdam pauses the update for that parameter to observe subsequent gradients. This pause allows CAdam to distinguish between a potential distribution shift and noise.\nIf the reverse gradient signs persist in subsequent steps, it signals a distribution shift, and m t will gradually change direction to reflect the new data pattern, while CAdam doesn't update in these iterations, avoiding incorrect updates. Conversely, if the gradient signs realign in the following steps, it indicates that the previous opposite gradient was caused by noise. In this case, CAdam resumes normal updates, effectively filtering out noisy gradients without making unnecessary updates in the process.\nIn addition, CAdam also has an AMSGrad (Reddi et al., 2018) variant as described in 1 when AMSGrad option is enabled.\nConvergence Analysis Given a stream of functions f t : R d \u2192 R, t = 1, 2, . . . , T , an online learning algorithm chooses \u03b8 t in each time step t and aims to minimize the T -step regret w.r.t. the optimum, where the regret is defined as\nR T := T t=1 f t (\u03b8 t ) - T t=1 f t (\u03b8 * ), \u03b8 * = argmin \u03b8 T t=1 f t (\u03b8).(1)\nThe online learning setting has been widely used to model real-world recommendation scenarios. We show that CAdam has the same O( \u221a T ) regret as Adam/AMSGrad under the same assumptions made in Reddi et al. (2018). The detailed proofs can be found in the appendix.  2024)). We leave the analysis of C-Adam under these more general settings as an interesting future direction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENT", "text": "In this section, we systematically evaluate the performance of CAdam across various scenarios, starting with synthetic image data, followed by tests on a public advertisement dataset, and concluding with A/B tests in a real-world recommendation system. We first examine CAdam's behaviour under distribution shift, and noisy conditions using the CIFAR-10 dataset (Krizhevsky et al., 2009) with the VGG network (Simonyan & Zisserman, 2014). Next, we test CAdam against other popular optimizers on the Criteo dataset(Jean-Baptiste Tien, 2014), focusing on different models and scenarios. Finally, we conduct A/B tests with millions of users in a real-world recommendation system to validate CAdam's effectiveness in large-scale, production-level environments. The results demonstrate that CAdam consistently outperforms Adam and other optimizers across different tasks, distribution shifts, and noise conditions.", "publication_ref": ["b19", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "NUMERICAL EXPERIMENT", "text": "Distribution Change To illustrate the different behaviours of Adam and CAdam under distribution shifts, we designed three types of distribution changes for both L1 and L2 loss functions: (1) Sudden change, where the minimum shifts abruptly at regular intervals; (2) Linear change, where the minimum moves at a constant speed; and (3) Sinusoidal change, where the minimum oscillates following a sine function, resulting in variable speed over time.\nThe loss functions are defined as:\nL(x, t) = |x -x * (t)|, L1 loss, (x -x * (t)) 2 , L2 loss,\nwhere x * (t) represents the position of the minimum at time t and is defined based on the type of distribution change: (1) separable L1 loss, (2) inseparable L1 loss, (3) inseparable L2 loss, and (4) Rosenbrock function. These landscapes are defined as follows:\nx * (t) = \uf8f1 \uf8f2 \uf8f3 \u230a t T \u230b\n1. Separable L1 Loss:\nf 1 (x, y) = |x| + |y|. 2. Inseparable L1 Loss: f 2 (x, y) = |x + y| + |x-y| 10 . 3. Inseparable L2 Loss: f 3 (x, y) = (x + y) 2 + (x-y) 2 10 . 4. Rosenbrock Function: f 4 (x, y) = (a -x) 2 + b(y -x 2 ) 2\n, where a = 1 and b = 100.\nTo simulate noise in the gradients, we applied a random mask to each dimension of the gradient with a 50% probability using the same random seed across different optimizers. Specifically, the gradient , where the alignment ratio decreases, resulting in fewer parameter updates. This is followed by a gradual recovery in both the alignment ratio and accuracy, and a decline in loss. CAdam's accuracy drop is slower, and its recovery is faster than Adam's, illustrating its enhanced ability to adapt to distribution shifts. components were multiplied by a uniformly distributed random value from the range [-1, 1] to introduce noise:\n\u2207 noisy (x, y) = \u2207f (x, y) \u2022 U (-1, 1), with probability p = 0.5, \u2207f (x, y), otherwise,\nThe results of these experiments are shown in Figure 2. For comparison, the results without noise are provided in Figure 5 in the appendix. The trajectory of CAdam exhibits fewer random perturbations and lower regret, indicating its ability to resist noise interference.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "CNN ON IMAGE CLASSIFICATION", "text": "We perform experiments using the VGG network on the CIFAR-10 dataset to evaluate the effectiveness of CAdam in handling distribution shifts and noise. We synthesize three experimental conditions: (1) sudden distribution changes, (2) continuous distribution shifts, and (3) added noise to the samples. The hyperparameters for these experiments are provided in Section B.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sudden Distribution Shift", "text": "To simulate sudden changes in data distribution, we rotate the images by a specific angle at the start of each epoch, relative to the previous epoch, as illustrated in Figure 3. CAdam consistently outperforms Adam across varying rotation speeds, with a more significant performance gap compared to the non-rotated condition.\nWe define the alignment ratio as:\nAlignment Ratio = Number of parameters where m t \u2022 g t > 0 Total number of parameters A closer inspection in Figure 3 reveals that, during the rotation (indicated by the red dashed line), the alignment ratio decreases, resulting in fewer parameters being updated, followed by a gradual recovery. Correspondingly, the accuracy declines and subsequently improves, while the loss increases before decreasing. Notably, during these shifts, CAdam's accuracy drops more slowly and recovers faster than Adam's, indicating its superior adaptability to new data distributions.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Continuous Distribution Shifts", "text": "In contrast to sudden distribution changes, we also tested the scenario where the data distribution changes continuously. Specifically, we simulated this by rotating the data distribution at each iteration by an angle. The results, shown in Figure 4, indicate that as the rotation speed increases, the advantage of CAdam over Adam becomes more pronounced.\nNoisy Samples To evaluate the optimizer's robustness to noise, we introduced noise into the dataset by randomly selecting a certain number of batches in each epoch (resampling for each epoch) and replacing the labels of these batches with random values. The results are presented in Figure 4. We observed that as the proportion of noisy labels increases, the consistency of CAdam decreases, causing it to update fewer parameters in each iteration. Despite this, both CAdam and Adam experience a performance decline in test set accuracy as noise increases. Nevertheless, CAdam consistently outperforms Adam, maintaining accuracy even with 40% noise, comparable to Adam's performance in a noise-free setting by the end of training.", "publication_ref": [], "figure_ref": ["fig_4", "fig_4"], "table_ref": []}, {"heading": "PUBLIC ADVERTISEMENT DATASET", "text": "Experiment Setting To evaluate the effectiveness of the proposed CAdam optimizer, we conducted experiments using various models on the Criteo-x4-001 dataset(Jean-Baptiste Tien, 2014). This dataset contains feature values and click feedback for millions of display ads and is commonly used to benchmark algorithms for click-through rate (CTR) prediction (Zhu et al., 2021). To simulate a real-world online learning scenario, we trained the models on data up to each timestamp in a single epoch (Fukushima et al., 2020). This setup replicates the environment where new data arrives continuously, requiring the model to adapt quickly. Furthermore, for sparse parameters (e.g., embeddings), we update the optimizer's state only when there is a non-zero gradient for this parameter in the current batch using SparseAdam implementation in Pytorch (Paszke et al., 2019). This approach ensures that the optimizer's state reflects the parameters influenced by recent data changes. The hyperparameters are provided in Appendix B.3.\nWe benchmarked CAdam and other popular optimizers, including SGD, SGDM(Qian, 1999), AdaGrad (Duchi et al., 2011), AdaDelta(Zeiler, 2012), RMSProp, Adam(Kingma & Ba, 2015), AMSGrad (Reddi et al., 2018), and AdaBelief (Zhuang et al., 2020), on various models such as DeepFM(77M) (Guo et al., 2017), WideDeep(77M) (Cheng et al., 2016), DNN(74M) (Covington et al., 2016), PNN(79M) (Qu et al., 2016), and DCN(74M) (Wang et al., 2017). The performance of these optimizers was evaluated using the Area Under the Curve (AUC) metric.", "publication_ref": ["b45", "b9", "b24", "b7", "b47", "b12", "b4", "b5", "b26", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "The results in Table 1 show that CAdam and its AMSGrad variants outperform other optimizers across different models. While the AMSGrad variants perform better on certain datasets, they do not consistently outperform standard CAdam. Both versions of CAdam generally achieve higher AUC scores than other optimizers, demonstrating their effectiveness in the online learning setting.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Robustness under Noise", "text": "To simulate a noisier environment, we introduced noise into the Criteo x4-001 dataset by flipping 1% of the negative training samples to positive. All other settings remained unchanged. The results in Table 2 show that CAdam consistently outperforms Adam in terms of both AUC and the extent of performance drop. This demonstrates CAdam's robustness in handling noisy data. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "EXPERIMENT ON REAL-WORLD RECOMMENDATION SYSTEM", "text": "In real-world recommendation scenarios, the differences from the Criteo dataset experiments are quite significant. First, both data volume and model sizes are much larger, with models used in Preprint the following experiments ranging from 8.3 billion to 330 billion parameters-100 to 10,000 times larger. Second, as these are online experiments, unlike offline experiments with a fixed dataset, the model's output directly influences user behaviour. To test the effectiveness of CAdam in this setting, we conducted A/B tests on internal models serving millions of users across seven different scenarios (2 pre-ranking, 4 recall, and 1 ranking).\nDuring these online experiments, we used a batch size of B = 4096 The evaluation metric was the Generalized Area Under the Curve (GAUC). Due to limited resources, we compared only Adam and CAdam, running the experiments for 48 hours.\nThe results, shown in Table 3, indicate that CAdam consistently outperformed Adam across all test scenarios, demonstrating its superiority in real-world applications. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "CONCLUSION", "text": "In this paper, we addressed the inherent limitations of the Adam optimizer in online learning environments, particularly its sluggish adaptation to distributional shifts and heightened sensitivity to noisy data. To overcome these challenges, we introduced CAdam (Confidence Adaptive Moment Estimation), a novel optimization strategy that enhances Adam by incorporating a confidence-based mechanism. This mechanism evaluates the alignment between momentum and gradients for each parameter dimension, ensuring that updates are performed judiciously. When momentum and gradients are aligned, CAdam updates the parameters following Adam's original formulation; otherwise, it temporarily withholds updates to discern between true distribution shifts and transient noise.\nOur extensive experiments across synthetic benchmarks, public advertisement datasets, and largescale real-world recommendation systems consistently demonstrated that CAdam outperforms Adam and other well-established optimizers in both adaptability and robustness. Specifically, CAdam showed superior performance in scenarios with sudden and continuous distribution shifts, as well as in environments with significant noise, achieving higher accuracy and lower regret. Moreover, in live A/B testing within a production recommendation system, CAdam led to substantial improvements in model performance and gross merchandise volume (GMV), underscoring its practical effectiveness.\nFuture work may explore further refinements of the confidence assessment mechanism, its integration with other optimization frameworks, and its application to a broader range of machine learning models and real-time systems. Ultimately, CAdam represents a promising advancement in the development of more resilient and adaptive optimization algorithms for dynamic learning environments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preprint", "text": "A PROOFS OF THEOREM 1\nGiven a stream of objectives f t : R d \u2192 R, t = 1, 2, . . . , T , online learning aims to minimize the regret w.r.t. the optimum; that is,\nR T := T t=1 f t (x t ) - T t=1 f t (x * ), x * = argmin x T t=1 f t (x).(3)\nRecall that each update in CAdam can be characterized as followsfoot_0 :\nm t = \u03b2 1,t m t-1 + (1 -\u03b2 1,t )g t ,(4)\nv t = \u03b2 2 v t-1 + (1 -\u03b2 2 )g 2 t ,(5)\nm t,\u039et = m t,i , i \u2208 \u039e t 0, else ,(6)\nvt = max(v t-1 , v t ),(7)\nx t+1 = x t -\u03b1 t m t,\u039et /v t . (8\n)\nwhere\n\u039e t := {i \u2208 [d] : m t,i \u2022 g t,i \u2265 0}\nindicates the set of active entries at step t. For notation clarity, let x t,\u039e be the vector of which the entries not belonging to \u039e are masked. Following the AMSGrad (Reddi et al., 2018), we are to prove that the sequence of points obtained by CAdam satisfies R T /T \u2192 0 as T increases.\nWe first introduce three standard assumptions: Assumption 1. Let f t : R d \u2192 R, t = 1, 2, . . . , T be a sequence of convex and differentiable functions with\n\u2225\u2207f t (x)\u2225 \u221e \u2264 G \u221e for all t \u2208 [T ].\nAssumption 2. Let {m t }, {v t } be the sequences used in CAdam,\n\u03b1 t = \u03b1/ \u221a t, \u03b2 1,t = \u03b2 1 \u03bb t-1 < 1, \u03b3 = \u03b2 1 / \u221a \u03b2 2 < 1 for all t \u2208 [T ]\n. Assumption 3. The points involved are within a bounded diameter D \u221e ; that is, for the optimal point x * and any points x t generated by CAdam, it holds \u2225x\nt -x * \u2225 \u221e \u2264 D \u221e /2.\nWe present several essential lemmas in the following. Given that some of these lemmas have been partially established in prior works (Kingma & Ba, 2015;Reddi et al., 2018), we include them here for the sake of completeness. Lemma 1. For a convex and differentiable function f : R d \u2192 R, we have\nf (x) -f (y) \u2264 \u27e8\u2207f (x), x -y\u27e9.\n(9) Lemma 2. Under Assumption 1 and 2, we have\ng t,\u039et , x t,\u039et -x * \u039et \u2264 1 2\u03b1t(1-\u03b21,t) \u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 -\u2225V 1/4 t (x t+1,\u039et -x * \u039et )\u2225 2 + \u03b21,t 2\u03b1t(1-\u03b21,t) \u2225V 1/4 t (x t -x * )\u2225 2 + \u03b1t 2(1-\u03b21,t) \u2225V -1/4 t m t \u2225 2 + \u03b1t\u03b21,t 2(1-\u03b21,t) \u2225V -1/4 t m t-1 \u2225 2 , (10\n) where V t := diag(v t ).\nProof. CAdam updates the parameters as follows\nx t+1,\u039et = x t,\u039et -\u03b1 t m t,\u039et / vt = x t,\u039et -\u03b1 t V -1/2 t \u03b2 1,t m t-1,\u039et + (1 -\u03b2 1,t )g t,\u039et . Subtracting x * from both sides yields \u2225V 1/4 t (x t+1,\u039et -x * \u039et )\u2225 2 2 =\u2225V 1/4 t (x t,\u039et -x * \u039et ) -\u03b1 t V -1/4 t m t,\u039et \u2225 2 2 =\u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 2 -2\u27e8\u03b1 t V -1/4 t m t,\u039et , V 1/4 t (x t,\u039et -x * \u039et )\u27e9 + \u2225\u03b1 t V -1/4 t m t,\u039et \u2225 2 2 =\u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 2 -2\u03b1 t \u27e8\u03b2 1,t m t-1,\u039et + (1 -\u03b2 1,t )g t,\u039et , x t,\u039et -x * \u039et \u27e9 + \u2225\u03b1 t V -1/4 t m t,\u039et \u2225 2 2 .", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Preprint", "text": "Rearranging the equation gives\ng t,\u039et , x t,\u039et -x * \u039et = 1 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 2 -\u2225V 1/4 t (x t+1,\u039et -x * \u039et )\u2225 2 2 - \u03b2 1,t 1 -\u03b2 1,t m t-1,\u039et , x t,\u039et -x * \u039et + \u03b1 t 2(1 -\u03b2 1,t ) \u2225V -1/4 t m t,\u039et \u2225 2 2 .\nThe results follow from the Cauchy-Schwarz inequality and Young's inequality:\n-\n\u03b2 1,t 1 -\u03b2 1,t m t-1,\u039et , x t,\u039et -x * \u039et = \u03b2 1,t 1 -\u03b2 1,t m t-1,\u039et , x * \u039et -x t,\u039et = \u03b2 1,t 1 -\u03b2 1,t \u221a \u03b1 t V -1/4 t m t-1,\u039et , 1 \u221a \u03b1 t V 1/4 t (x * \u039et -x t,\u039et ) \u2264 \u03b2 1,t 1 -\u03b2 1,t \u221a \u03b1 t \u2225V -1/4 t m t-1,\u039et \u2225 \u2022 1 \u221a \u03b1 t \u2225V 1/4 t (x * \u039et -x t,\u039et )\u2225 \u2264 \u03b2 1,t 1 -\u03b2 1,t \u03b1 t 2 \u2225V -1/4 t m t-1,\u039et \u2225 2 + 1 2\u03b1 t \u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 \u2264 \u03b2 1,t 1 -\u03b2 1,t \u03b1 t 2 \u2225V -1/4 t m t-1 \u2225 2 + 1 2\u03b1 t \u2225V 1/4 t (x t -x * )\u2225 2 ,\nand the fact that \u2225V\n-1/4 t m t,\u039et \u2225 2 2 \u2264 \u2225V -1/4 t m t \u2225 2 2 .\nLemma 3. Under Assumption 1, 2, and 3, we have\ng t , x t -x * \u2264 g t,\u039e , x t,\u039e -x * \u039e + d\u03b2 1 \u03bb t-1 D \u221e G \u221e 1 -\u03b2 1 .(11)\nProof. If the i-th entry is not updated at step t, i.e., i \u2208 [d] \\ \u039e t , it can be derived that\n\u03b2 1,t m t-1,i + (1 -\u03b2 1,t )g t,i \u2022 g t,i \u2264 0 \u21d2 \u03b2 1,t m t-1,i + (1 -\u03b2 1,t )g t,i \u2022 sgn(g t,i ) \u2264 0 \u21d2 -\u03b2 1,t |m t-1,i | + (1 -\u03b2 1,t )|g t,i | \u2264 0 \u21d2|g t,i | \u2264 \u03b2 1,t 1 -\u03b2 1,t |m t-1,i | \u21d2|g t,i | \u2264 \u03b2 1,t 1 -\u03b2 1,t G \u221e \u2190 Assumption 1 \u21d2|g t,i | \u2264 \u03b2 1 \u03bb t-1 1 -\u03b2 1 G \u221e , i \u2208 [d] \\ \u039e t . \u2190 Assumption 2\nWith Assumption 3, it immediately yields the desired inequality that\ng t , x t -x * = g t,\u039e , x t,\u039e -x * \u039e + g t,[d]\\\u039e , x t,[d]\\\u039e -x * [d]\\\u039e \u2264 g t,\u039e , x t,\u039e -x * \u039e + d i=1 \u03b2 1 \u03bb t-1 D \u221e G \u221e 1 -\u03b2 1 .\nLemma 4. Given Assumption 1, 2, and 3, we have\nt\u2208[T ] \u03b2 1,t 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t -x * )\u2225 2 \u2264 dD 2 \u221e G \u221e 2\u03b1(1 -\u03b2 1 )(1 -\u03bb) 2 . (12\n) Preprint Proof. t\u2208[T ] \u03b2 1,t 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t -x * )\u2225 2 \u2264 1 2\u03b1(1 -\u03b2 1 ) t\u2208[T ] \u221a t\u03bb t-1 \u2225V 1/4 t (x t -x * )\u2225 2 \u2264 G \u221e 2\u03b1(1 -\u03b2 1 ) t\u2208[T ] \u221a t\u03bb t-1 \u2225x t -x * \u2225 2 \u2190 Assumption 1 \u2264 dD 2 \u221e G \u221e 2\u03b1(1 -\u03b2 1 ) t\u2208[T ] \u221a t\u03bb t-1 \u2190 Assumption 3 \u2264 dD 2 \u221e G \u221e 2\u03b1(1 -\u03b2 1 ) t\u2208[T ] \u03bb t-1 t \u2264 dD 2 \u221e G \u221e 2\u03b1(1 -\u03b2 1 ) 1 (1 -\u03bb) 2 . Lemma 5 (Reddi et al. (2018) Lemma2). Under Assumption 2, we have t\u2208[T ] \u03b1 t \u2225V -1/4 t m t \u2225 2 \u2264 \u03b1dG \u221e (1 -\u03b3)(1 -\u03b2 1 ) \u221a 1 -\u03b2 2 \u221a T ,(13)\nwhere\n\u03b3 := \u03b2 1 / \u221a \u03b2 2 .\nWe are ready to prove the final results now. Concretely, Theorem 1 is a straightfoward corollary of the following conclusion. Theorem 2. Under the Assumption 1, 2, and 3, the regret is converged with\nR T \u2264 dD 2 \u221e G \u221e \u221a T 2\u03b1(1 -\u03b2 1 ) + d(2\u03b1 + D \u221e )D \u221e G \u221e 2\u03b1(1 -\u03b2 1 )(1 -\u03bb) 2 + \u03b1dG \u221e \u221a T (1 -\u03b3)(1 -\u03b2 1 ) 2 \u221a 1 -\u03b2 2 . (14\n)\nProof. Based on Lemma 1, Lemma 2, and Lemma 3, the regret can be firstly bounded by\nR T = t\u2208[T ] (f t (x t ) -f t (x * )) \u2264 t\u2208[T ] \u27e8g t , x t -x * \u27e9 \u2264 t\u2208[T ] \u27e8g t,\u039et , x t,\u039et -x * \u039et \u27e9 + t\u2208[T ] d\u03b2 1 \u03bb t-1 D \u221e G \u221e 1 -\u03b2 1 \u2264 t\u2208[T ] 1 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 -\u2225V 1/4 t (x t+1,\u039et -x * \u039et )\u2225 2 1 \u20dd + t\u2208[T ] \u03b2 1,t 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t -x * )\u2225 2 2 \u20dd + t\u2208[T ] \u03b1 t 2(1 -\u03b2 1,t ) \u2225V -1/4 t m t \u2225 2 3 \u20dd + t\u2208[T ] \u03b1 t \u03b2 1,t 2(1 -\u03b2 1,t ) \u2225V -1/4 t m t-1 \u2225 2 4 \u20dd + t\u2208[T ] d\u03b2 1 \u03bb t-1 D \u221e G \u221e 1 -\u03b2 1 5 \u20dd .\nLet us address each term in turn. For the first term, we are to separately bound each entry and the results follows from the summation. For the i-th entry, let T i + = [t : i \u2208 \u039e t ] be a sequence collecting Preprint all steps that x i is succesfully updated, and tk \u2208 T i + be the k-th element of T i + . For simplicity, we will omit the superscript without ambiguity. \n1 \u20dd i = t|T + | t= t1 1 2\u03b1 t (1 -\u03b2 1,t ) (v1\n\u03b1 t|T + | \u2264 D 2 \u221e G \u221e \u221a T 2\u03b1(1 -\u03b2 1 )\n.\nHence,     \n1 \u20dd = i\u2208[d] 1 \u20dd i \u2264 dD 2 \u221e G \u221e \u221a T 2\u03b1(1 -\u03b2 1 ) . (15\n) 2 \u20dd = t\u2208[T ] \u03b2 1,t 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t -x * )\u2225 2 \u2264 dD 2 \u221e G \u221e 2\u03b1(1 -\u03b2 1 )(1 -\u03bb) 2 \u2190 Lemma 4. 3 \u20dd = t\u2208[T ] \u03b1 t 2(1 -\u03b2 1,t ) \u2225V -1/4 t m t \u2225 2 \u2264 1 2(1 -\u03b2 1 ) t\u2208[T ] \u03b1 t \u2225V -1/4 t m t \u2225 2 \u2264 \u03b1dG \u221e \u221a T 2(1 -\u03b3)(1 -\u03b2 1 ) 2 \u221a 1 -\u03b2 2 . \u2190 Lemma 5 4 \u20dd = t\u2208[T ] \u03b1 t \u03b2 1,t 2(1 -\u03b2 1,t ) \u2225V -1/4 t m t-1 \u2225 2 \u2264 1 2(1 -\u03b2 1 ) t\u2208[T ] \u03b1 t \u2225V -1/4 t-1 m t-1 \u2225 2 \u2264 1 2(1 -\u03b2 1 ) t\u2208[T ] \u03b1 t-1 \u2225V -1/4 t-1 m t-1 \u2225 2 = 1 2(1 -\u03b2 1 ) t\u2208[T -1] \u03b1 t \u2225V -1/4 t m t \u2225 2 \u2264 \u03b1dG \u221e \u221a T 2(1 -\u03b3)(1 -\u03b2 1 ) 2 \u221a 1 -\u03b2 2 . \u2190Lemma", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Finally, we have", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B HYPERPARAMETERS B.1 NUMERICAL EXPERIMENT", "text": "Distribution Shift For the distribution shift experiments, we used the following hyperparameters: a cycle length of 40, a learning rate \u03b1 = 0.5, exponential decay rates for the first and second moment estimates \u03b2 1 = 0.9 and \u03b2 2 = 0.999 respectively, and a small constant \u03f5 = 1 \u00d7 10 -8 to prevent division by zero. The number of time steps was set to T = 100.\nNoisy Samples For the noisy samples experiments, the hyperparameters were set as follows: a learning rate of 0.1, \u03b2 1 = 0.9, \u03b2 2 = 0.999, \u03f5 = 1 \u00d7 10 -8 , and a maximum number of iterations T = 1500.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 CNN ON IMAGE CLASSIFICATION", "text": "For the CNN-based image classification experiments on the CIFAR-10 dataset, we used a learning rate of 3 \u00d7 10 -4 , \u03b2 1 = 0.9, \u03b2 2 = 0.999, weight decay of 0.0005, and \u03f5 = 1 \u00d7 10 -8 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3 PUBLIC ADVERTISEMENT DATASET", "text": "Due to resource limitations, we performed a grid search over the learning rates for each optimizer and model using the following range: {lr default/5, lr default/2, lr default, 2 \u00d7 lr default, 5 \u00d7 lr default}, where lr default is the default learning rate specified in the FuxiCTR library. We reported the best performance for each optimizer based on this search. All other hyperparameters were kept the same as those in the FuxiCTR library (Zhu et al., 2021;2022).", "publication_ref": ["b45"], "figure_ref": [], "table_ref": []}, {"heading": "C ADDITIONAL EXPERIMENTS", "text": "C.1 NUMERICAL EXPERIMENTS Figure 5 illustrate how both optimizers perform in a noise-free environment.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 EXPERIMENT ON RESNET AND DENSENET", "text": "We perform experiments on Resnet (He et al., 2016) and Densenet (Huang et al., 2017) to further illustrate the effectiveness of CAdam.", "publication_ref": ["b14", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "C.3 RELATIONSHIP BETWEEN LEARNING RATE, PERFORMANCE, AND ALIGNMENT RATIO", "text": "We tested different learning rates on the Criteo x4 001 dataset using the DeepFM model to understand the relationship between the learning rate, performance, and alignment ratio. The results in 4 show that the performance initially increases with the learning rate but starts to decline as the learning rate continues to rise. Conversely, the consistent ratio R steadily decreases as the learning rate increases.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A new regret analysis for adam-type algorithms", "journal": "", "year": "", "authors": "Ahmet Alacaoglu; Yura Malitsky; Panayotis Mertikopoulos; Volkan Cevher"}, {"ref_id": "b1", "title": "", "journal": "", "year": "2020", "authors": " Pmlr"}, {"ref_id": "b2", "title": "An image is worth 16x16 words: Transformers for image recognition at scale", "journal": "", "year": "2020", "authors": "Dosovitskiy Alexey"}, {"ref_id": "b3", "title": "Learning from time-changing data with adaptive windowing", "journal": "SIAM", "year": "2007", "authors": "Albert Bifet; Ricard Gavalda"}, {"ref_id": "b4", "title": "Wide & deep learning for recommender systems", "journal": "", "year": "2016", "authors": "Heng-Tze Cheng; Levent Koc; Jeremiah Harmsen; Tal Shaked; Tushar Chandra; Hrishi Aradhye; Glen Anderson; Greg Corrado; Wei Chai; Mustafa Ispir"}, {"ref_id": "b5", "title": "Deep neural networks for youtube recommendations", "journal": "", "year": "2016", "authors": "Paul Covington; Jay Adams; Emre Sargin"}, {"ref_id": "b6", "title": "A simple convergence proof of adam and adagrad", "journal": "Transactions on Machine Learning Research", "year": "", "authors": "Alexandre D\u00e9fossez; Leon Bottou; Francis Bach; Nicolas Usunier"}, {"ref_id": "b7", "title": "Adaptive subgradient methods for online learning and stochastic optimization", "journal": "Journal of machine learning research", "year": "2011", "authors": "John Duchi; Elad Hazan; Yoram Singer"}, {"ref_id": "b8", "title": "Model-agnostic meta-learning for fast adaptation of deep networks", "journal": "PMLR", "year": "2017", "authors": "Chelsea Finn; Pieter Abbeel; Sergey Levine"}, {"ref_id": "b9", "title": "Online robust and adaptive learning from data streams", "journal": "", "year": "2020", "authors": "Shintaro Fukushima; Atsushi Nitanda; Kenji Yamanishi"}, {"ref_id": "b10", "title": "Automated data denoising for recommendation", "journal": "", "year": "2023", "authors": "Yingqiang Ge; Mostafa Rahmani; Athirai Irissappane; Jose Sepulveda; James Caverlee; Fei Wang"}, {"ref_id": "b11", "title": "Robust loss functions under label noise for deep neural networks", "journal": "", "year": "2017", "authors": "Aritra Ghosh; Himanshu Kumar; Shanti Sastry"}, {"ref_id": "b12", "title": "Deepfm: a factorizationmachine based neural network for ctr prediction", "journal": "", "year": "2017", "authors": "Huifeng Guo; Ruiming Tang; Yunming Ye; Zhenguo Li; Xiuqiang He"}, {"ref_id": "b13", "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "journal": "", "year": "2010", "authors": "Michael Gutmann; Aapo Hyv\u00e4rinen"}, {"ref_id": "b14", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b15", "title": "Densely connected convolutional networks", "journal": "", "year": "2017", "authors": "Gao Huang; Zhuang Liu; Laurens Van Der Maaten; Kilian Q Weinberger"}, {"ref_id": "b16", "title": "Display advertising challenge", "journal": "", "year": "2014", "authors": "Olivier Chapelle; Jean-Baptiste Tien; Joycenv "}, {"ref_id": "b17", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b18", "title": "A survey of recommendation systems: recommendation models, techniques, and application fields", "journal": "Electronics", "year": "2022", "authors": "Hyeyoung Ko; Suyeon Lee; Yoonseo Park; Anna Choi"}, {"ref_id": "b19", "title": "Learning multiple layers of features from tiny images", "journal": "", "year": "2009", "authors": "Alex Krizhevsky; Geoffrey Hinton"}, {"ref_id": "b20", "title": "On the variance of the adaptive learning rate and beyond", "journal": "", "year": "2019", "authors": "Liyuan Liu; Haoming Jiang; Pengcheng He; Weizhu Chen; Xiaodong Liu; Jianfeng Gao; Jiawei Han"}, {"ref_id": "b21", "title": "Decoupled weight decay regularization", "journal": "", "year": "2017", "authors": " Loshchilov"}, {"ref_id": "b22", "title": "Learning under concept drift: A review", "journal": "IEEE transactions on knowledge and data engineering", "year": "2018", "authors": "Jie Lu; Anjin Liu; Fan Dong; Feng Gu; Joao Gama; Guangquan Zhang"}, {"ref_id": "b23", "title": "Dtec: Dual training error based correction approach for recommender systems", "journal": "Software Impacts", "year": "2021", "authors": "Costas Panagiotakis; Harris Papadakis; Antonis Papagrigoriou; Paraskevi Fragopoulou"}, {"ref_id": "b24", "title": "Pytorch: An imperative style, highperformance deep learning library", "journal": "Advances in neural information processing systems", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga"}, {"ref_id": "b25", "title": "On the momentum term in gradient descent learning algorithms", "journal": "Neural networks", "year": "1999", "authors": " Ning Qian"}, {"ref_id": "b26", "title": "Product-based neural networks for user response prediction", "journal": "IEEE", "year": "2016", "authors": "Yanru Qu; Han Cai; Kan Ren; Weinan Zhang; Yong Yu; Ying Wen; Jun Wang"}, {"ref_id": "b27", "title": "On the convergence of adam and beyond", "journal": "", "year": "2018", "authors": "J Sashank; Satyen Reddi; Sanjiv Kale;  Kumar"}, {"ref_id": "b28", "title": "A semantic approach to remove incoherent items from a user profile and improve the accuracy of a recommender system", "journal": "Journal of Intelligent Information Systems", "year": "2016", "authors": "Roberto Saia; Ludovico Boratto; Salvatore Carta"}, {"ref_id": "b29", "title": "Proximal policy optimization algorithms", "journal": "", "year": "2017", "authors": "John Schulman; Filip Wolski; Prafulla Dhariwal; Alec Radford; Oleg Klimov"}, {"ref_id": "b30", "title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2014", "authors": "Karen Simonyan; Andrew Zisserman"}, {"ref_id": "b31", "title": "Dropout: a simple way to prevent neural networks from overfitting", "journal": "The journal of machine learning research", "year": "2014", "authors": "Nitish Srivastava; Geoffrey Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov"}, {"ref_id": "b32", "title": "A streaming ensemble algorithm (sea) for large-scale classification", "journal": "", "year": "2001", "authors": "Nick Street; Yongseog Kim"}, {"ref_id": "b33", "title": "Elastic online deep learning for dynamic streaming data", "journal": "Information Sciences", "year": "2024", "authors": "Rui Su; Husheng Guo; Wenjian Wang"}, {"ref_id": "b34", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": " Vaswani"}, {"ref_id": "b35", "title": "A case study of batch and incremental recommender systems in supermarket data under concept drifts and cold start", "journal": "Expert Systems with Applications", "year": "2021", "authors": "David Ant\u00f4nio; Jean Viniski; Alceu Paul Barddal;  De Souza; Fabr\u00edcio Britto; Humberto Enembreck;  Vinicius Aparecido De Campos"}, {"ref_id": "b36", "title": "Closing the gap between the upper bound and lower bound of adam's iteration complexity", "journal": "Advances in Neural Information Processing Systems", "year": "2024", "authors": "Bohan Wang; Jingwen Fu; Huishuai Zhang; Nanning Zheng; Wei Chen"}, {"ref_id": "b37", "title": "Deep & cross network for ad click predictions", "journal": "", "year": "2017", "authors": "Ruoxi Wang; Bin Fu; Gang Fu; Mingliang Wang"}, {"ref_id": "b38", "title": "Learning robust recommender from noisy implicit feedback", "journal": "", "year": "2021", "authors": "Wenjie Wang; Fuli Feng; Xiangnan He; Liqiang Nie; Tat-Seng Chua"}, {"ref_id": "b39", "title": "Improving recommendation quality through outlier removal", "journal": "International Journal of Machine Learning and Cybernetics", "year": "2022", "authors": "Yuan-Yuan Xu; Shen-Ming Gu; Fan Min"}, {"ref_id": "b40", "title": "A gradient-based approach for online robust deep neural network training with noisy labels", "journal": "", "year": "2023", "authors": "Yifan Yang; Alec Koppel; Zheng Zhang"}, {"ref_id": "b41", "title": "Adaptive methods for nonconvex optimization", "journal": "Advances in neural information processing systems", "year": "2018", "authors": "Manzil Zaheer; Sashank Reddi; Devendra Sachan; Satyen Kale; Sanjiv Kumar"}, {"ref_id": "b42", "title": "Adadelta: an adaptive learning rate method", "journal": "", "year": "2012", "authors": "D Matthew;  Zeiler"}, {"ref_id": "b43", "title": "Adaptive online incremental learning for evolving data streams", "journal": "Applied Soft Computing", "year": "2021", "authors": "Si-Si Zhang; Jian-Wei Liu; Xin Zuo"}, {"ref_id": "b44", "title": "Adam can converge without any modification on update rules", "journal": "Advances in neural information processing systems", "year": "2022", "authors": "Yushun Zhang; Congliang Chen; Naichen Shi; Ruoyu Sun; Zhi-Quan Luo"}, {"ref_id": "b45", "title": "Open benchmarking for clickthrough rate prediction", "journal": "", "year": "2021", "authors": "Jieming Zhu; Jinyang Liu; Shuai Yang; Qi Zhang; Xiuqiang He"}, {"ref_id": "b46", "title": "Bars: Towards open benchmarking for recommender systems", "journal": "", "year": "2022", "authors": "Jieming Zhu; Quanyu Dai; Liangcai Su; Rong Ma; Jinyang Liu; Guohao Cai; Xi Xiao; Rui Zhang"}, {"ref_id": "b47", "title": "Adabelief optimizer: Adapting stepsizes by the belief in observed gradients", "journal": "Advances in neural information processing systems", "year": "2020", "authors": "Juntang Zhuang; Tommy Tang; Yifan Ding; C Sekhar; Nicha Tatikonda; Xenophon Dvornek; James Papademetris;  Duncan"}, {"ref_id": "b48", "title": "Online convex programming and generalized infinitesimal gradient ascent", "journal": "", "year": "2003", "authors": "Martin Zinkevich"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Trajectory of Adam (top row) and CAdam (bottom row) under different distribution changes: (Left) sudden change, (Middle) linear change, and (Right) sinusoidal change. The first row corresponds to the L1 loss landscapes, while the second row corresponds to the L2 loss landscapes. Adam's X and CAdam's X denote the locations of the optimization trajectories for Adam and CAdam, respectively, while X * represents the location of the optimal solution. CAdam shows superior adaptability to distribution shifts.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Trajectory of Adam (top row) and CAdam (bottom row) under noisy conditions on four different optimization landscapes: (Left to Right) separable L1 loss, inseparable L1 loss, inseparable L2 loss, and Rosenbrock function. Each column shows the optimization trajectory in the presence of noise, where each dimension's gradient is randomly flipped with a 50% probability. CAdam demonstrates superior robustness, maintaining more stable convergence paths than Adam across all tested functions.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "The results of these experiments are presented in Figure1. Across different loss functions and distribution changes, CAdam closely follows the trajectory of the minimum point, being less affected by incorrect momentum, exhibiting lower regret and demonstrating its superior ability to adapt to shifting distributions.Noisy Samples To compare Adam and CAdam in noisy environments, we conducted experiments on four different optimization 2-d landscapes:", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: (Left) Performance of CAdam and Adam under different rotation speeds corresponding to sudden distribution shift. CAdam demonstrates superior performance, with a more pronounced advantage over Adam in the presence of rotation. (Right) A detailed view at a 60-degree rotation between steps 1400 to 2300, showing the Alignment Ratio, Accuracy, and Loss. The red dashed lines indicate the rotation points, where the alignment ratio decreases, resulting in fewer parameter updates. This is followed by a gradual recovery in both the alignment ratio and accuracy, and a decline in loss. CAdam's accuracy drop is slower, and its recovery is faster than Adam's, illustrating its enhanced ability to adapt to distribution shifts.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "PreprintFigure 4 :4Figure 4: (Left) Performance of CAdam and Adam under continuous distribution shifts with different rotation speeds. CAdam demonstrates superior performance, with its advantage becoming more pronounced as the rotation speed increases. (Right) The effect of adding noise to the samples. CAdam exhibits a slower accuracy drop compared to Adam, showcasing its enhanced robustness to noisy data.", "figure_data": ""}, {"figure_label": "55", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "5 PreprintFigure 5 :55Figure 5: Performance of Adam (top row) and CAdam (bottom row) on four different optimization landscapes without noise: (Left to Right) separable L1 loss, inseparable L1 loss, inseparable L2 loss, and Rosenbrock function. This comparison highlights the natural behavior of both optimizers in a noise-free environment.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: Performance of CAdam and Adam under different rotation speeds corresponding to sudden distribution shift. The results for Resnet are shown on the left, while those for Densenet are presented on the right.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure 7: Performance of CAdam and Adam under different rotation speeds corresponding to continuous distribution shift. The results for Resnet are shown on the left, while those for Densenet are presented on the right.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 :8Figure 8: Performance of CAdam and Adam under noisy data. The results for Resnet are shown on the left, while those for Densenet are presented on the right.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "AUC performance of different optimizers on the Criteo dataset across various models. Results are averaged over three seeds with mean and standard deviation (\u00b1) reported. CAmsGrad denotes the AMSGrad variant of CAdam, which achieves the highest average performance. CAmsGrad 80.90 \u00b1.006 80.93 \u00b1.007 80.92 \u00b1.005 80.94 \u00b1.009 81.09 \u00b1.010 80.96", "figure_data": "DeepFMWideDeepDNNPNNDCNAvgSGD71.90 \u00b1.00671.88 \u00b1.01368.12 \u00b1.04367.61 \u00b1.31869.55 \u00b1.02669.81SGDM76.59 \u00b1.04476.32 \u00b1.02178.80 \u00b1.01476.17 \u00b1.05077.90 \u00b1.01877.16AdaGrad71.77 \u00b1.03271.50 \u00b1.01168.65 \u00b1.02267.49 \u00b1.02769.55 \u00b1.02069.79AdaDelta71.91 \u00b1.07171.64 \u00b1.00569.76 \u00b1.00467.59 \u00b1.02569.76 \u00b1.02470.13RMSProp71.82 \u00b1.01071.54 \u00b1.02168.72 \u00b1.00567.51 \u00b1.00469.60 \u00b1.00769.84Adam80.87 \u00b1.01180.90 \u00b1.00480.89 \u00b1.00380.90 \u00b1.00681.05 \u00b1.00580.92AdaBelief80.84 \u00b1.00880.90 \u00b1.00280.88 \u00b1.01180.89 \u00b1.00281.02 \u00b1.04480.91AdamW80.87 \u00b1.00880.90 \u00b1.01080.88 \u00b1.01080.90 \u00b1.00281.00 \u00b1.04780.91AmsGrad80.88 \u00b1.00480.92 \u00b1.00880.91 \u00b1.00180.92 \u00b1.00981.08 \u00b1.00980.94CAdam80.88 \u00b1.00880.93 \u00b1.00480.90 \u00b1.00280.93 \u00b1.00681.06 \u00b1.00980.94"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Results of Adam and CAdam on the Noisy Criteo dataset, averaged over three seeds. \"Drop\" indicates the decrease in performance compared to training on the original Criteo dataset. CAdam shows a smaller performance drop, highlighting its robustness to noise. CAdam Drop -0.08 \u00b1.014 -0.14 \u00b1.009 -0.12 \u00b1.004 +0.04 \u00b1.031 -0.28 \u00b1.015", "figure_data": "DeepFMWideDeepDNNPNNDCNAdam80.51 \u00b1.00880.47 \u00b1.00680.48 \u00b1.01480.66 \u00b1.00680.51 \u00b1.010CAdam80.81 \u00b1.007 80.79 \u00b1.006 80.78 \u00b1.005 80.96 \u00b1.026 80.77 \u00b1.007Adam Drop-0.36 \u00b1.014-0.43 \u00b1.007-0.41 \u00b1.016-0.23 \u00b1.012-0.54 \u00b1.013"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "GAUC results for Adam and CAdam across seven internal experiment settings. \"Pr\" denotes pre-ranking, \"Rec\" represents recall, and \"Rk\" indicates ranking. CAdam consistently outperforms Adam, highlighting its effectiveness in real-world recommendation scenarios.", "figure_data": "MetricPr 1Pr 2Rec 1Rec 2Rec 3Rec 4Rk 1AverageAdam87.41% 82.89% 90.18% 82.41% 84.57% 85.39% 88.52% 85.34%CAdam 87.61% 83.28% 90.43% 82.61% 85.06% 85.49% 88.74% 85.64%Impr.0.20%0.39%0.25%0.20%0.49%0.10%0.22%0.30%"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Performance Metrics and Alignment Ratio for Different Learning Rates.", "figure_data": "Preprint"}], "formulas": [{"formula_id": "formula_0", "formula_text": "m t = \u03b2 1 \u2022 m t-1 + (1 -\u03b2 1 ) \u2022 g t .", "formula_coordinates": [3.0, 383.02, 601.74, 120.98, 9.65]}, {"formula_id": "formula_1", "formula_text": "v t = \u03b2 2 \u2022 v t-1 + (1 -\u03b2 2 ) \u2022 g 2 t .", "formula_coordinates": [3.0, 319.8, 615.61, 123.91, 12.19]}, {"formula_id": "formula_2", "formula_text": "1-\u03b2 t 1 and vt = vt 1-\u03b2 t 2 .", "formula_coordinates": [3.0, 145.06, 632.62, 358.94, 27.33]}, {"formula_id": "formula_3", "formula_text": "t \u2190 t + 1 4: g t \u2190 \u2207 \u03b8 f t (\u03b8 t-1 ) 5: m t \u2190 \u03b2 1 \u2022 m t-1 + (1 -\u03b2 1 ) \u2022 g t 6: v t \u2190 \u03b2 2 \u2022 v t-1 + (1 -\u03b2 2 ) \u2022 g 2 t 7: mt \u2190 m t /(1 -\u03b2 t 1 ) 8: vt \u2190 v t /(1 -\u03b2 t 2 ) 9:", "formula_coordinates": [4.0, 112.98, 120.09, 156.55, 74.49]}, {"formula_id": "formula_4", "formula_text": "vmax,t \u2190 max(v max,t-1 , vt ) 11: else 12: vmax,t \u2190 vt 13: end if 14: mt \u2190 mt \u2299 I(m t \u2299 g t > 0) \u25b7 Element-wise mask out elements where m i t \u2022 g i t \u2264 0 15: \u03b8 t \u2190 \u03b8 t-1 -\u03b1 \u2022 mt /( vmax,t + \u03f5) 16: end while 17: return \u03b8 t", "formula_coordinates": [4.0, 108.5, 196.8, 395.5, 87.75]}, {"formula_id": "formula_5", "formula_text": "R T := T t=1 f t (\u03b8 t ) - T t=1 f t (\u03b8 * ), \u03b8 * = argmin \u03b8 T t=1 f t (\u03b8).(1)", "formula_coordinates": [4.0, 190.51, 665.31, 313.49, 30.2]}, {"formula_id": "formula_6", "formula_text": "L(x, t) = |x -x * (t)|, L1 loss, (x -x * (t)) 2 , L2 loss,", "formula_coordinates": [6.0, 232.77, 430.82, 145.27, 23.68]}, {"formula_id": "formula_7", "formula_text": "x * (t) = \uf8f1 \uf8f2 \uf8f3 \u230a t T \u230b", "formula_coordinates": [6.0, 212.59, 486.92, 60.78, 34.15]}, {"formula_id": "formula_8", "formula_text": "f 1 (x, y) = |x| + |y|. 2. Inseparable L1 Loss: f 2 (x, y) = |x + y| + |x-y| 10 . 3. Inseparable L2 Loss: f 3 (x, y) = (x + y) 2 + (x-y) 2 10 . 4. Rosenbrock Function: f 4 (x, y) = (a -x) 2 + b(y -x 2 ) 2", "formula_coordinates": [6.0, 131.41, 636.87, 240.67, 63.84]}, {"formula_id": "formula_9", "formula_text": "\u2207 noisy (x, y) = \u2207f (x, y) \u2022 U (-1, 1), with probability p = 0.5, \u2207f (x, y), otherwise,", "formula_coordinates": [7.0, 173.87, 483.86, 263.06, 22.11]}, {"formula_id": "formula_10", "formula_text": "R T := T t=1 f t (x t ) - T t=1 f t (x * ), x * = argmin x T t=1 f t (x).(3)", "formula_coordinates": [14.0, 188.89, 134.25, 315.11, 30.2]}, {"formula_id": "formula_11", "formula_text": "m t = \u03b2 1,t m t-1 + (1 -\u03b2 1,t )g t ,(4)", "formula_coordinates": [14.0, 242.47, 185.93, 261.53, 9.65]}, {"formula_id": "formula_12", "formula_text": "v t = \u03b2 2 v t-1 + (1 -\u03b2 2 )g 2 t ,(5)", "formula_coordinates": [14.0, 259.74, 199.74, 244.26, 12.69]}, {"formula_id": "formula_13", "formula_text": "m t,\u039et = m t,i , i \u2208 \u039e t 0, else ,(6)", "formula_coordinates": [14.0, 254.08, 217.75, 249.92, 19.92]}, {"formula_id": "formula_14", "formula_text": "vt = max(v t-1 , v t ),(7)", "formula_coordinates": [14.0, 287.93, 244.65, 216.07, 9.65]}, {"formula_id": "formula_15", "formula_text": "x t+1 = x t -\u03b1 t m t,\u039et /v t . (8", "formula_coordinates": [14.0, 266.43, 258.6, 233.7, 9.65]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [14.0, 500.13, 258.92, 3.87, 8.64]}, {"formula_id": "formula_17", "formula_text": "\u039e t := {i \u2208 [d] : m t,i \u2022 g t,i \u2265 0}", "formula_coordinates": [14.0, 135.54, 273.72, 138.68, 9.65]}, {"formula_id": "formula_18", "formula_text": "\u2225\u2207f t (x)\u2225 \u221e \u2264 G \u221e for all t \u2208 [T ].", "formula_coordinates": [14.0, 166.68, 348.31, 138.71, 9.65]}, {"formula_id": "formula_19", "formula_text": "\u03b1 t = \u03b1/ \u221a t, \u03b2 1,t = \u03b2 1 \u03bb t-1 < 1, \u03b3 = \u03b2 1 / \u221a \u03b2 2 < 1 for all t \u2208 [T ]", "formula_coordinates": [14.0, 108.0, 355.45, 396.0, 29.91]}, {"formula_id": "formula_20", "formula_text": "t -x * \u2225 \u221e \u2264 D \u221e /2.", "formula_coordinates": [14.0, 348.22, 398.91, 81.81, 11.23]}, {"formula_id": "formula_21", "formula_text": "f (x) -f (y) \u2264 \u27e8\u2207f (x), x -y\u27e9.", "formula_coordinates": [14.0, 241.23, 471.13, 129.54, 8.74]}, {"formula_id": "formula_22", "formula_text": "g t,\u039et , x t,\u039et -x * \u039et \u2264 1 2\u03b1t(1-\u03b21,t) \u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 -\u2225V 1/4 t (x t+1,\u039et -x * \u039et )\u2225 2 + \u03b21,t 2\u03b1t(1-\u03b21,t) \u2225V 1/4 t (x t -x * )\u2225 2 + \u03b1t 2(1-\u03b21,t) \u2225V -1/4 t m t \u2225 2 + \u03b1t\u03b21,t 2(1-\u03b21,t) \u2225V -1/4 t m t-1 \u2225 2 , (10", "formula_coordinates": [14.0, 131.16, 501.79, 368.69, 57.97]}, {"formula_id": "formula_23", "formula_text": ") where V t := diag(v t ).", "formula_coordinates": [14.0, 108.0, 551.12, 396.0, 20.29]}, {"formula_id": "formula_24", "formula_text": "x t+1,\u039et = x t,\u039et -\u03b1 t m t,\u039et / vt = x t,\u039et -\u03b1 t V -1/2 t \u03b2 1,t m t-1,\u039et + (1 -\u03b2 1,t )g t,\u039et . Subtracting x * from both sides yields \u2225V 1/4 t (x t+1,\u039et -x * \u039et )\u2225 2 2 =\u2225V 1/4 t (x t,\u039et -x * \u039et ) -\u03b1 t V -1/4 t m t,\u039et \u2225 2 2 =\u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 2 -2\u27e8\u03b1 t V -1/4 t m t,\u039et , V 1/4 t (x t,\u039et -x * \u039et )\u27e9 + \u2225\u03b1 t V -1/4 t m t,\u039et \u2225 2 2 =\u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 2 -2\u03b1 t \u27e8\u03b2 1,t m t-1,\u039et + (1 -\u03b2 1,t )g t,\u039et , x t,\u039et -x * \u039et \u27e9 + \u2225\u03b1 t V -1/4 t m t,\u039et \u2225 2 2 .", "formula_coordinates": [14.0, 108.0, 601.02, 394.83, 103.51]}, {"formula_id": "formula_25", "formula_text": "g t,\u039et , x t,\u039et -x * \u039et = 1 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 2 -\u2225V 1/4 t (x t+1,\u039et -x * \u039et )\u2225 2 2 - \u03b2 1,t 1 -\u03b2 1,t m t-1,\u039et , x t,\u039et -x * \u039et + \u03b1 t 2(1 -\u03b2 1,t ) \u2225V -1/4 t m t,\u039et \u2225 2 2 .", "formula_coordinates": [15.0, 129.71, 101.33, 358.64, 50.55]}, {"formula_id": "formula_26", "formula_text": "\u03b2 1,t 1 -\u03b2 1,t m t-1,\u039et , x t,\u039et -x * \u039et = \u03b2 1,t 1 -\u03b2 1,t m t-1,\u039et , x * \u039et -x t,\u039et = \u03b2 1,t 1 -\u03b2 1,t \u221a \u03b1 t V -1/4 t m t-1,\u039et , 1 \u221a \u03b1 t V 1/4 t (x * \u039et -x t,\u039et ) \u2264 \u03b2 1,t 1 -\u03b2 1,t \u221a \u03b1 t \u2225V -1/4 t m t-1,\u039et \u2225 \u2022 1 \u221a \u03b1 t \u2225V 1/4 t (x * \u039et -x t,\u039et )\u2225 \u2264 \u03b2 1,t 1 -\u03b2 1,t \u03b1 t 2 \u2225V -1/4 t m t-1,\u039et \u2225 2 + 1 2\u03b1 t \u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 \u2264 \u03b2 1,t 1 -\u03b2 1,t \u03b1 t 2 \u2225V -1/4 t m t-1 \u2225 2 + 1 2\u03b1 t \u2225V 1/4 t (x t -x * )\u2225 2 ,", "formula_coordinates": [15.0, 116.94, 182.38, 390.73, 133.11]}, {"formula_id": "formula_27", "formula_text": "-1/4 t m t,\u039et \u2225 2 2 \u2264 \u2225V -1/4 t m t \u2225 2 2 .", "formula_coordinates": [15.0, 185.09, 323.68, 122.84, 13.76]}, {"formula_id": "formula_28", "formula_text": "g t , x t -x * \u2264 g t,\u039e , x t,\u039e -x * \u039e + d\u03b2 1 \u03bb t-1 D \u221e G \u221e 1 -\u03b2 1 .(11)", "formula_coordinates": [15.0, 198.17, 377.98, 305.83, 24.8]}, {"formula_id": "formula_29", "formula_text": "\u03b2 1,t m t-1,i + (1 -\u03b2 1,t )g t,i \u2022 g t,i \u2264 0 \u21d2 \u03b2 1,t m t-1,i + (1 -\u03b2 1,t )g t,i \u2022 sgn(g t,i ) \u2264 0 \u21d2 -\u03b2 1,t |m t-1,i | + (1 -\u03b2 1,t )|g t,i | \u2264 0 \u21d2|g t,i | \u2264 \u03b2 1,t 1 -\u03b2 1,t |m t-1,i | \u21d2|g t,i | \u2264 \u03b2 1,t 1 -\u03b2 1,t G \u221e \u2190 Assumption 1 \u21d2|g t,i | \u2264 \u03b2 1 \u03bb t-1 1 -\u03b2 1 G \u221e , i \u2208 [d] \\ \u039e t . \u2190 Assumption 2", "formula_coordinates": [15.0, 154.31, 435.15, 303.38, 124.35]}, {"formula_id": "formula_30", "formula_text": "g t , x t -x * = g t,\u039e , x t,\u039e -x * \u039e + g t,[d]\\\u039e , x t,[d]\\\u039e -x * [d]\\\u039e \u2264 g t,\u039e , x t,\u039e -x * \u039e + d i=1 \u03b2 1 \u03bb t-1 D \u221e G \u221e 1 -\u03b2 1 .", "formula_coordinates": [15.0, 179.05, 606.38, 253.4, 50.74]}, {"formula_id": "formula_31", "formula_text": "t\u2208[T ] \u03b2 1,t 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t -x * )\u2225 2 \u2264 dD 2 \u221e G \u221e 2\u03b1(1 -\u03b2 1 )(1 -\u03bb) 2 . (12", "formula_coordinates": [15.0, 179.81, 704.61, 320.04, 28.84]}, {"formula_id": "formula_32", "formula_text": ") Preprint Proof. t\u2208[T ] \u03b2 1,t 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t -x * )\u2225 2 \u2264 1 2\u03b1(1 -\u03b2 1 ) t\u2208[T ] \u221a t\u03bb t-1 \u2225V 1/4 t (x t -x * )\u2225 2 \u2264 G \u221e 2\u03b1(1 -\u03b2 1 ) t\u2208[T ] \u221a t\u03bb t-1 \u2225x t -x * \u2225 2 \u2190 Assumption 1 \u2264 dD 2 \u221e G \u221e 2\u03b1(1 -\u03b2 1 ) t\u2208[T ] \u221a t\u03bb t-1 \u2190 Assumption 3 \u2264 dD 2 \u221e G \u221e 2\u03b1(1 -\u03b2 1 ) t\u2208[T ] \u03bb t-1 t \u2264 dD 2 \u221e G \u221e 2\u03b1(1 -\u03b2 1 ) 1 (1 -\u03bb) 2 . Lemma 5 (Reddi et al. (2018) Lemma2). Under Assumption 2, we have t\u2208[T ] \u03b1 t \u2225V -1/4 t m t \u2225 2 \u2264 \u03b1dG \u221e (1 -\u03b3)(1 -\u03b2 1 ) \u221a 1 -\u03b2 2 \u221a T ,(13)", "formula_coordinates": [15.0, 499.85, 713.25, 4.15, 8.64]}, {"formula_id": "formula_33", "formula_text": "\u03b3 := \u03b2 1 / \u221a \u03b2 2 .", "formula_coordinates": [16.0, 134.47, 363.09, 57.75, 17.17]}, {"formula_id": "formula_34", "formula_text": "R T \u2264 dD 2 \u221e G \u221e \u221a T 2\u03b1(1 -\u03b2 1 ) + d(2\u03b1 + D \u221e )D \u221e G \u221e 2\u03b1(1 -\u03b2 1 )(1 -\u03bb) 2 + \u03b1dG \u221e \u221a T (1 -\u03b3)(1 -\u03b2 1 ) 2 \u221a 1 -\u03b2 2 . (14", "formula_coordinates": [16.0, 152.56, 427.45, 347.3, 32.05]}, {"formula_id": "formula_35", "formula_text": ")", "formula_coordinates": [16.0, 499.85, 442.94, 4.15, 8.64]}, {"formula_id": "formula_36", "formula_text": "R T = t\u2208[T ] (f t (x t ) -f t (x * )) \u2264 t\u2208[T ] \u27e8g t , x t -x * \u27e9 \u2264 t\u2208[T ] \u27e8g t,\u039et , x t,\u039et -x * \u039et \u27e9 + t\u2208[T ] d\u03b2 1 \u03bb t-1 D \u221e G \u221e 1 -\u03b2 1 \u2264 t\u2208[T ] 1 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t,\u039et -x * \u039et )\u2225 2 -\u2225V 1/4 t (x t+1,\u039et -x * \u039et )\u2225 2 1 \u20dd + t\u2208[T ] \u03b2 1,t 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t -x * )\u2225 2 2 \u20dd + t\u2208[T ] \u03b1 t 2(1 -\u03b2 1,t ) \u2225V -1/4 t m t \u2225 2 3 \u20dd + t\u2208[T ] \u03b1 t \u03b2 1,t 2(1 -\u03b2 1,t ) \u2225V -1/4 t m t-1 \u2225 2 4 \u20dd + t\u2208[T ] d\u03b2 1 \u03bb t-1 D \u221e G \u221e 1 -\u03b2 1 5 \u20dd .", "formula_coordinates": [16.0, 153.55, 491.12, 304.39, 212.47]}, {"formula_id": "formula_37", "formula_text": "1 \u20dd i = t|T + | t= t1 1 2\u03b1 t (1 -\u03b2 1,t ) (v1", "formula_coordinates": [17.0, 108.0, 125.56, 122.67, 35.86]}, {"formula_id": "formula_38", "formula_text": "\u03b1 t|T + | \u2264 D 2 \u221e G \u221e \u221a T 2\u03b1(1 -\u03b2 1 )", "formula_coordinates": [17.0, 179.96, 425.17, 89.59, 35.26]}, {"formula_id": "formula_39", "formula_text": "1 \u20dd = i\u2208[d] 1 \u20dd i \u2264 dD 2 \u221e G \u221e \u221a T 2\u03b1(1 -\u03b2 1 ) . (15", "formula_coordinates": [17.0, 242.44, 468.26, 257.42, 35.7]}, {"formula_id": "formula_40", "formula_text": ") 2 \u20dd = t\u2208[T ] \u03b2 1,t 2\u03b1 t (1 -\u03b2 1,t ) \u2225V 1/4 t (x t -x * )\u2225 2 \u2264 dD 2 \u221e G \u221e 2\u03b1(1 -\u03b2 1 )(1 -\u03bb) 2 \u2190 Lemma 4. 3 \u20dd = t\u2208[T ] \u03b1 t 2(1 -\u03b2 1,t ) \u2225V -1/4 t m t \u2225 2 \u2264 1 2(1 -\u03b2 1 ) t\u2208[T ] \u03b1 t \u2225V -1/4 t m t \u2225 2 \u2264 \u03b1dG \u221e \u221a T 2(1 -\u03b3)(1 -\u03b2 1 ) 2 \u221a 1 -\u03b2 2 . \u2190 Lemma 5 4 \u20dd = t\u2208[T ] \u03b1 t \u03b2 1,t 2(1 -\u03b2 1,t ) \u2225V -1/4 t m t-1 \u2225 2 \u2264 1 2(1 -\u03b2 1 ) t\u2208[T ] \u03b1 t \u2225V -1/4 t-1 m t-1 \u2225 2 \u2264 1 2(1 -\u03b2 1 ) t\u2208[T ] \u03b1 t-1 \u2225V -1/4 t-1 m t-1 \u2225 2 = 1 2(1 -\u03b2 1 ) t\u2208[T -1] \u03b1 t \u2225V -1/4 t m t \u2225 2 \u2264 \u03b1dG \u221e \u221a T 2(1 -\u03b3)(1 -\u03b2 1 ) 2 \u221a 1 -\u03b2 2 . \u2190Lemma", "formula_coordinates": [17.0, 114.42, 483.76, 389.58, 246.46]}], "doi": ""}
