{"Modeling Orders of User Behaviors via Differentiable Sorting: A Multi-task Framework to Predicting User Post-click Conversion": "Menghan Wang wangmengh@zju.edu.cn eBay Inc. Shanghai, China Jinming Yang yangjm67@sjtu.edu.cn Shanghai Jiaotong University Shanghai, China Yuchen Guo yuchguo@ebay.com eBay Inc. Shanghai, China Yuming Shen yumishen@ebay.com eBay Inc. Shanghai, China Mengying Zhu mengyingzhu@zju.edu.cn Zhejiang University Hangzhou, China", "ABSTRACT": "User post-click conversion prediction is of high interest to researchers and developers. Recent studies employ multi-task learning to tackle the selection bias and data sparsity problem, two severe challenges in post-click behavior prediction, by incorporating click data. However, prior works mainly foucsed on pointwise learning and the orders of labels (i.e., click and post-click) are not well explored, which naturally poses a listwise learning problem. Inspired by recent advances on differentiable sorting, in this paper, we propose a novel multi-task framework that leverages orders of user behaviors to predict user post-click conversion in an end-to-end approach. Specifically, we define an aggregation operator to combine predicted outputs of different tasks to a unified score, then we use the computed scores to model the label relations via differentiable sorting. Extensive experiments on public and industrial datasets show the superiority of our proposed model against competitive baselines.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Learning to rank ; Rank aggregation .", "KEYWORDS": "Recommendation, multi-task learning, differentiable sorting", "ACMReference Format:": "Menghan Wang, Jinming Yang, Yuchen Guo, Yuming Shen, Mengying Zhu, and Yanlin Wang. 2023. Modeling Orders of User Behaviors via Differentiable Sorting: A Multi-task Framework to Predicting User Post-click Conversion. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23), July 23-27, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3539618. 3592023 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '23, July 23-27, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9408-6/23/07...$15.00 https://doi.org/10.1145/3539618.3592023", "Yanlin Wang": "wangylin36@mail.sysu.edu.cn Sun Yat-sen University Zhuhai, China Figure 1: An explainable example of motivation; a) a snapshot of user behaviors on a list of items, and b) two preference list sorted by pure clicks and label orders.", "1 INTRODUCTION": "User post-click behaviors (e.g., purchase, download, and registration) reflect explicit preference of users and are often coherent with business metrics, attracting more research efforts nowadays to predict subsequent actions of users after they click some items. Early work revealed two non-trivial challenges when directly estimating post-click conversion (often abbreviated as CVR): 1) The sample selection bias [14]. Conventional CVR models are trained on dataset composed of clicked impressions, while are utilized to make inference on the entire space with samples of all impressions. This inconsistency will hurt the generalization performance of trained models. 2) The data sparsity problem. In practice, postclick behaviors are much sparser than clicks, which would pose negative impacts on the effectiveness of CVR models. A common and natural solution is to embrace multi-task learning (MTL), i.e., learning click and post-click predictions at the same time. We could feed the whole dataset, without additional sampling for post-click prediction, into MTL models and share knowledge across tasks. Evidences [7, 13] showed MTL can help to circumvent the sample selection bias and alleviate the sparsity issue. Then a subsequent issue arises under the MTL scenario is how to model the label relations of the multiple tasks. Ideally, a good answer should be 1) scalable . It is often the case that Post-click is a subsequence since a user may take several actions after click, so the method should handle multiple labels as well as their predictions. For example in Fig. 1, Click -> AddtoCart -> Purchase is a case with multiple post-click behaviors. To our best knowledge, there is no previous study that intentionally addressed this scalability issue; existing methods [7, 13] that take probability multiplication may lead SIGIR '23, July 23-27, 2023, Taipei, Taiwan Menghan Wang et al. to a high-variance porblem when the number of labels increases. 2) differentiable . The merit of MTL relies on the end-to-end learning paradigm; by sharing the process of back propagation each task can benefit from other tasks. Similarly, the learning process of label relations should also be differentiable to fully leverage the potential of MTL. Moreover, we make a reasonable assumption: the order of user behavior follows the ordering of the label sequence. That is, in the above example user have to click before AddtoCart , and AddtoCart before Purchase ; of course users can stop at any node in real scenarios. In this regard, the longer a user interacts an item along the sequence, the deeper engagement we can infer he/she builds with the item. Then, we can depict a relative preference order, i.e., Purchase > AddtoCart > Click , which is more reasonable than the counterpart sorted by a single label (i.e., click) in Fig. 1. Thus we argue that sorting user preference by label orders may be a plausible answer to the label relation concern of MTL. Building on the aforementioned insights, in this paper, we propose a scalable multi-task framework that leverages orders of user behaviors to predict user post-click conversion in an end-to-end approach. Specifically, we define a general aggregation operator to combine predicted output of different tasks to a unified score, then we use the computed scores to model the label relations. Inspired by recent advances on differentiable sorting, we seamlessly integrated a label sorting component into the MTL structure so they can be simultaneously optimized in training. We call the model MTLDS which reveals the two main techniques: M ultiT ask L earning and D ifferentiable S orting. Extensive experiments on public and industrial datasets show the superiority of our proposed framework against competitive baselines.", "2 RELATED WORK": "Post-click conversion prediction is widely explored in many online applications. Many efforts are devoted on the sample selection bias and sparsity problem. . Ma et al. [7] proposed an entire space multitask model for predicting CVR, which remedies the data sparsity problem. Further, Wen et al. [13] leveraged supervisory signals from users' post-click behaviors other than conversions to further alleviate the data sparsity problem. Bao et al. [1] utilized graph convolutional networks (GCN) to enhance the conventional CVR modeling. However, these methods all belong to pointwise learning. Another branch of related work is differentiable sorting. Prior works [5, 8] have proposed relaxations of permutation matrices to the Birkhoff polytope, which is defined as the convex hull of the set of doubly-stochastic matrices. Recent works [4, 12] mapped permutation matrices to the set of unimodal row-stochastic matrices and then perform softmax operation to model the sorted index of permutation matrices.", "3 FRAMEWORK": "Assuming we have a dataset { \ud835\udc4b, \ud835\udc3f } , where \ud835\udc4b is feature set and \ud835\udc3f is the ordered label sequence of length \ud835\udc47 . Concretely, \ud835\udc3f = ( \ud835\udc59 1 , \ud835\udc59 2 , ..., \ud835\udc59 \ud835\udc47 ) ; \ud835\udc3f \ud835\udc61 \u2208 { 0 , 1 } , \u2200 \ud835\udc61 \u2208 [ 1 , \ud835\udc47 ] . In other words, each label is a binary variable. For post-click behaviors, we denote those labels with index \ud835\udc61 > \ud835\udc3f \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 If our target is click, the problem degenerates to clickthrough rate prediction. For simplicity and clarity, we set \ud835\udc47 = 2 in the remaining part of this chapter, and we denote \ud835\udc3f \ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 = \ud835\udc3f 1 Figure 2: The structure overview of proposed framework. and \ud835\udc3f \ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc50 \ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc58 = \ud835\udc3f 2. Note that our framework applies to arbitrary length of \ud835\udc47 . Regarding the sorting part, we follow the definition of Grover et al. [4]. An \ud835\udc5b -dimensional permutation z = [ \ud835\udc67 1 , \ud835\udc67 2 , . . . , \ud835\udc67 \ud835\udc5b ] \ud835\udc47 is a list of unique indices { 1 , 2 , . . . , \ud835\udc5b } . Every permutation z is associated with a permutation matrix \ud835\udc43 z \u2208 { 0 , 1 } \ud835\udc5b \u00d7 \ud835\udc5b with entries given as:  This is simply the one-hot representation of \ud835\udf0b . Note that with these definitions, the mapping sort : R \ud835\udc5b \u2192 R \ud835\udc5b that sorts \ud835\udc60 in decreasing order is sort ( \ud835\udc60 ) = \ud835\udc43 argsort ( \ud835\udc60 ) \ud835\udc60 . The differentiable sorting in this paper mainly indicates making \ud835\udc43 argsort ( \ud835\udc60 ) differentiable.", "3.1 Architecture overview.": "We choose a multi-task structure as the backbone of our framework MTLDS. As shown form Fig. 2, we adopt hard parameter sharing to common bottom layers, and a task-specific neural network for each task. Hard parameter sharing is the most commonly used approach to multi-task learning in neural networks, which has been empirically proved to reduce the risk of fitting. On top of the backbone, we have an aggregation module and a sorting module . The aggregation module aggregates outputs and labels of each task into a unified score and label, and then feeds them into the sorting module. Next, the sorting module will rank the orders of labels and predictions, which yields a sorting loss. Finally, the total loss function becomes  where \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc61 is task-specific loss.", "3.2 Aggregations.": "Here we introduce and discuss two sources utilized in the aggregation modular: predictions and labels of multi-tasks. 1) Prediction aggregation. Without loss of generality, we define an aggregation operator \ud835\udc54 (\u00b7) to combine predictions of multiple tasks. So for a predicted probability sequence \u02c6 \ud835\udc3f we can compute a score \ud835\udc60 = \ud835\udc54 ( \u02c6 \ud835\udc3f ) . We implemented the following three candidates: \u00b7 Mul -operator. This idea originates from conditional probability decomposition, which assumes post-click behaviors Modeling Orders of User Behaviors via Differentiable Sorting: A Multi-task Framework to Predicting User Post-click Conversion SIGIR '23, July 23-27, 2023, Taipei, Taiwan Table 1: Explainable example of three aggregation operators. are conditioned on clicks,. a widely adopted idea. One drawback is the probability decomposition is difficult to get when \ud835\udc47 is very large. \ud835\udc54 = \u02db \ud835\udc47 \ud835\udc61 \u02c6 \ud835\udc3f \ud835\udc61 . \u00b7 Max -operator. \ud835\udc60 is assigned as the maximum value of \u02c6 \ud835\udc3f , i.e., \ud835\udc54 = \ud835\udc5a\ud835\udc4e\ud835\udc65 ( \u02c6 \ud835\udc3f ) . \u00b7 Sum -operator. This operator assumes \ud835\udc54 = \u02dd \ud835\udc47 \ud835\udc61 \ud835\udc64 \ud835\udc61 \u02c6 \ud835\udc3f \ud835\udc61 , where \ud835\udc64 \ud835\udc61 is a task-specific weight. In experiments we employ two variants: Add and Linear . The former sets all \ud835\udc64 \ud835\udc61 to 1, and the latter sets \ud835\udc64 \ud835\udc61 as learnable parameters. Discussion The above three operators come from different assumptions with respect to data distributions and user behaviors. We provide an explainable example in Table 1 to demonstrate their difference and applicability. Mul -operator has a relaxed mapping to probability decomposition: \ud835\udc43 ( Click , Post-click ) = \ud835\udc43 ( Click ) \u2217 \ud835\udc43 ( Post-click | Click ) , which is widely adopted by previous work. However, multiplication may not well discriminate some cases, e.g., Sample 1-3 in Table 1 achieve the same aggregated score 0 . 09, while the raw \ud835\udc43 ( Click ) and \ud835\udc43 ( Post-click ) are completely different. Meanwhile, Mul -operator implicitly suffers from a high variance problem. \ud835\udc43 ( Click ) and \ud835\udc43 ( Post-click ) need to be calibrated to ensure an accurate multiplication. Max -operator, which equals to the one-dimension global max pooling layer, extracts the most prominent output from multiple tasks. In this regard, we can assume user actions on one item are driven by the highest probability. In contrast, Sum -operator provides a smooth approach to linearly consider the relations between different labels. It has the most moderate assumption thus it can tell the difference between Sample 1-2 with w 3:2 (weights), which Mul -operator and Max -operator fail to distinguish. 2) Label aggregation. We adopt a simple but reasonable approach to merge labels. For a single sample, we sum all the binary labels of multiple tasks to a score \ud835\udc60 . Then for samples under the same impression, we use computed \ud835\udc60 to form a permutation matrix. The permutation matrix is served as the label for the sorting loss. Note that exploring orders of user actions across samples is meaningful.", "3.3 Learning orders of user actions.": "After receiving prediction sequence \ud835\udc60 and permutation matrix \ud835\udc67 from the aggregation modular, we turn to computing permutation probability via mapping prediction matrices to a set of unimodal row-stochastic matrices. Here we apply softsort [12], a simple but efficient implementation of differentiable sorting. Concretely,  \u00ab \u2039 where the softmax operator is applied row-wise, \ud835\udc51 is an elementwise Manhattan Distance function, and \ud835\udf0f is a temperature parameter that controls the degree of the approximation. This sorting operation relaxes permutation matrices to a set of unimodal row-stochastic matrices (the former is row-stochastic and columnstochastic). The definition of Unimodal Row Stochastic Matrices Grover et al. [4] summarizes the properties of our relaxed operator: Definition 3.1. (Unimodal Row Stochastic Matrices). An \ud835\udc5b \u00d7 \ud835\udc5b matrix is Unimodal Row Stochastic (URS) if it satisfies the following conditions: (1) Non-negativity: \ud835\udc48 [ \ud835\udc56, \ud835\udc57 ] \u2265 0 \u2200 \ud835\udc56, \ud835\udc57 \u2208 { 1 , 2 , . . . , \ud835\udc5b } . (2) Row Affinity: \u02dd \ud835\udc5b \ud835\udc57 = 1 \ud835\udc48 [ \ud835\udc56, \ud835\udc57 ] = 1 \u2200 \ud835\udc56 \u2208 { 1 , 2 , . . . , \ud835\udc5b } . (3) Argmax Permutation: Let \ud835\udc62 denote a vector of size \ud835\udc5b such that \ud835\udc62 \ud835\udc56 = arg max \ud835\udc57 \ud835\udc48 [ \ud835\udc56, \ud835\udc57 ] \u2200 \ud835\udc56 \u2208 { 1 , 2 , . . . , \ud835\udc5b } . Then, \ud835\udc62 \u2208 S \ud835\udc5b , i.e., it is a valid permutation. All row stochastic matrices satisfy the first two conditions. The third condition is useful for gradient based optimization involving sorting-based losses. The whole proof and discussion of can be found in Prillo and Eisenschlos [12] and Grover et al. [4]. In simple words: the \ud835\udc5f -th row of the operator is the softmax of the negative distances to the \ud835\udc5f -th largest element . Wethen minimized the cross-entropy loss between the predicted matrix \u02c6 \ud835\udc43 \ud835\udc67 and the ground-truth permutation matrix \ud835\udc43 \ud835\udc67 , concretely,  where \ud835\udc64 \ud835\udc56 is penalty weight at position index \ud835\udc56 . Similar to the setting of NDCG, we apply \ud835\udc64 \ud835\udc56 = 1 log 2 ( \ud835\udc56 + 1 ) and find it useful in practice.", "4 EXPERIMENTS": "Datasets We selected two datasets for evaluating recommendation performance, one public dataset and one industrial dataset. For the public dataset 1 , we choose the Alibaba E-commerce user behavior dataset. For the industrial dataset, we crawled 7 days' data from one placement in an e-commerce website, with click, addtoCart, and purchase labels. The statistics are detailed in Table 2. Table 2: Statistics of datasets (size \u00d7 10 6 ). Baselines We conduct experiments with several competitive methods on CVR modeling. 1) DNN-pointwise [9]. This is a representative single-task model with point-wise binary cross-entropy loss where we only utilize purchase as labels. 2) DNN-pairwise. In this model, we construct a combined label (0, 1, 2) by adding click label to purchase label and implement paiwise logistic loss. 1 http://yongfeng.me/dataset/ SIGIR '23, July 23-27, 2023, Taipei, Taiwan Menghan Wang et al. Table 3: Comparisons of different models on two datasets. 3) DNN-DiffSort . This model implements the proposed listwise sorting loss with the combined label as used in DNN-pairwise . 4) ESMM [7]. This is a multi-task DNN model simultaneously predicting the CTR, CVR and their multiplication CTCVR with shared user/item embeddings. 5) ESMM-Pairwise . This model replaces the pointwise binary cross-entropy loss used in ESMM to ranknet loss [2]. 6) MTL-Linear-ListNet . This model follows the proposed user behavior order modeling multi-task structure with its loss function replaced by listNet ranking loss [3] and linear aggregator. 7) MTL-Linear-NeuralNDCG . This model also follows the proposed multi-task structure using NeuralNDCG loss [11] and linear aggregator. Note that in all MTL models, we apply ranknet loss [2] to domain-specific task losses, which we find competitive. Relation to listwise learning . Differentiable sorting can also be viewed as list-wise learning. So we include two list-wise loss into MTLDS for comparison. 1) ListNet [3] utilizes SoftMax method to project both predicted scores and labels into probability space and then minimizes the cross-entropy loss between them. In other words, ListNet generates soft labels (i.e., softmax output of list labels) for model learning, which is an implicit mehtod for listwise learning. 2) NeuralNDCG [11] is a differentiable approximated Normalised Discounted Cumulative Gain (NDCG) loss based on another differentiable sorting function: Neuralsort [4]. The sorting approximation relies on an identity that expresses the sum of the top \ud835\udc58 elements of a vector \ud835\udc60 \u2208 R \ud835\udc5b as a symmetric function of \ud835\udc60 that only involves max and min operations [10, Lemma 1]. Comparing with the above two counterparts, our proposed MTLDS model is a more direct way to optimize the ranking ability across the whole list, and easier to implement and optimize. Moreover, MTLDS is extended to multi-task learning with a general aggregation operator. Table 4: Model performance with/without AddtoCart label. Experimental results of purchase prediction . A common practice of post-click conversion prediction is purchase prediction. Here we assume label orders follows Click -> Purchase and conduct experiments on the aforementioned two datasets. We report model performance in terms of AUC, NDCG@2. NDCG@6, and NDCG@12 of purchase prediction. Results are collected in Table 3, from which we can reveal following findings: 1) MTLDS variants outperform other competitors consistently on all metrics across both datasets, showing the superiority of our proposed model. 2) Models with listwise loss are better than those with pairwise loss or pointwise loss, e.g., DNN-Diffsort vs DNN-Purchase and DNN-Pairwise, ESMMPairwise vs ESMM. These results meet expectation as it is widely discussed in literature [6]. 3) Comparing among three listwise losses (i.e., ListNet, NeuralNDCG, and MTLDS-Linear), we observe a considerable improvement of the differentiable sorting function against other two losses. This finding implies that directly estimating the permutation matrix is a better way. 4) Regarding the four different aggregators in MTLDS, the linear aggregator performs the best on two datasets consistently. This confirms the previous demonstration that Mul -operator, Max -operator and vanilla Sum -operator could fail to distinguish certain user behavior sequences. The Linear -operator has the best scoring ability. Experimental results of multiple post-click labels . To verify MTLDS's scalability with more than one post-click labels, we introduce another label AddtoCart and assume a Click -> AddtoCart -> Purchase ordering for user behavior. Results in Table 4 show that incorporating AddtoCart labels helps improve the performance of MTLDS with all aggregation operators in purchase prediction. Note that we don't have to design additional loss functions or aggregation operators for additional post-click labels. In this regard, we argue MTLDS is scalable and general.", "5 CONCLUSION": "In this paper we propose a novel multi-task framework MTLDS that leverages orders of user behaviors to predict user post-click conversion in an end-to-end approach. Concretely, we define an aggregation operator to combine predicted output of different tasks to a unified score, then we use the computed scores to model the label relations via differentiable sorting. Extensive experiments on public and industrial datasets show the superiority of our proposed model against competitive baselines. In the future, we plan to explore MTLDS with graph-based user behaviors. Modeling Orders of User Behaviors via Differentiable Sorting: A Multi-task Framework to Predicting User Post-click Conversion SIGIR '23, July 23-27, 2023, Taipei, Taiwan", "REFERENCES": "[1] Wentian Bao, Hong Wen, Sha Li, Xiao-Yang Liu, Quan Lin, and Keping Yang. 2020. Gmcm: Graph-based micro-behavior conversion model for post-click conversion rate estimation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2201-2210. [2] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning . 89-96. [3] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In Proceedings of the 24th international conference on Machine learning . 129-136. [4] Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. 2018. Stochastic Optimization of Sorting Networks via Continuous Relaxations. In International Conference on Learning Representations . [5] Scott Linderman, Gonzalo Mena, Hal Cooper, Liam Paninski, and John Cunningham. 2018. Reparameterizing the birkhoff polytope for variational permutation inference. In International Conference on Artificial Intelligence and Statistics . PMLR, 1618-1627. [6] Tie-Yan Liu. 2011. Learning to rank for information retrieval. (2011). [7] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 1137-1140. [8] Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. 2018. Learning latent permutations with gumbel-sinkhorn networks. arXiv preprint arXiv:1802.08665 (2018). [9] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, CaroleJean Wu, Alisson G Azzolini, et al. 2019. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 (2019). [10] Wlodzimierz Ogryczak and Arie Tamir. 2003. Minimizing the sum of the k largest functions in linear time. Inform. Process. Lett. 85, 3 (2003), 117-122. [11] Przemys\u0142aw Pobrotyn and Rados\u0142aw Bia\u0142obrzeski. 2021. NeuralNDCG: Direct Optimisation of a Ranking Metric via Differentiable Relaxation of Sorting. arXiv preprint arXiv:2102.07831 (2021). [12] Sebastian Prillo and Julian Eisenschlos. 2020. SoftSort: A continuous relaxation for the argsort operator. In International Conference on Machine Learning . PMLR, 7793-7802. [13] Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, and Keping Yang. 2020. Entire space multi-task modeling via post-click behavior decomposition for conversion rate prediction. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 2377-2386. [14] Bianca Zadrozny. 2004. Learning and evaluating classifiers under sample selection bias. In Proceedings of the twenty-first international conference on Machine learning . 114."}
