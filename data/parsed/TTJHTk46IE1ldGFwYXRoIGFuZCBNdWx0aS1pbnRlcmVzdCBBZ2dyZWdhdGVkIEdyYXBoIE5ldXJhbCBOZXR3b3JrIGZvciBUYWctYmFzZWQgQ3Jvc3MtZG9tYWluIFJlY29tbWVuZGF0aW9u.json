{"M2GNN: Metapath and Multi-interest Aggregated Graph Neural Network for Tag-based Cross-domain Recommendation": "", "Zepeng Huai": "School of Artificial Intelligence, University of Chinese Academy of Sciences Beijing, China zepenghuai6@gmail.com Yuji Yang Meituan Beijing, China yangyujiyyj@gmail.com Mengdi Zhang Meituan Beijing, China zhangmengdi02@meituan.com", "Zhongyi Zhang": "Meituan Beijing, China zhangzhongyi02@meituan.com Yichun Li Meituan Beijing, China yichun.li@meituan.com", "ABSTRACT": "Wei Wu Meituan Beijing, China wuwei19850318@gmail.com", "CCS CONCEPTS": "Cross-domain recommendation (CDR) is an effective way to alleviate the data sparsity problem. Content-based CDR is one of the most promising branches since most kinds of products can be described by a piece of text, especially when cold-start users or items have few interactions. However, two vital issues are still under-explored: (1) From the content modeling perspective, sufficient long-text descriptions are usually scarce in a real recommender system, more often the light-weight textual features, such as a few keywords or tags, are more accessible, which is improperly modeled by existing methods. (2) From the CDR perspective, not all inter-domain interests are helpful to infer intra-domain interests. Caused by domain-specific features, there are part of signals benefiting for recommendation in the source domain but harmful for that in the target domain. Therefore, how to distill useful interests is crucial. To tackle the above two problems, we propose a m etapath and m ulti-interest aggregated g raph n eural n etwork ( M2GNN ). Specifically, to model the tag-based contents, we construct a heterogeneous information network to hold the semantic relatedness between users, items, and tags in all domains. The metapath schema is predefined according to domain-specific knowledge, with one metapath for one domain. User representations are learned by GNN with a hierarchical aggregation framework, where the intra-metapath aggregation firstly filters out trivial tags and the inter-metapath aggregation further filters out useless interests. Offline experiments and online A/B tests demonstrate that M2GNN achieves significant improvements over the state-of-the-art methods and current industrial recommender system in Dianping, respectively. Further analysis shows that M2GNN offers an interpretable recommendation. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '23, July 23-27, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9408-6/23/07...$15.00 https://doi.org/10.1145/3539618.3591720 \u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "Cross-domain Recommendation, Graph Neural Network, Tag-based Recommendation", "ACMReference Format:": "Zepeng Huai, Yuji Yang, Mengdi Zhang, Zhongyi Zhang, Yichun Li, and Wei Wu. 2023. M2GNN: Metapath and Multi-interest Aggregated Graph Neural Network for Tag-based Cross-domain Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23), July 23-27, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3539618.3591720", "1 INTRODUCTION": "Figure 1: Dianping review feed recommendaiton and some examples to explain how tags are associated with the review, search, and consumption domain. Best viewed in color. bcijing barbccuc Scarch Bar Consumed POI Beijing Good Atmosphere Rcvicw Fccd Revicw of Barbecuc Red Winc Consumer Taikoo Li Bullet Sanlitun Bar Revicw Fced Collcct Rcvciw 0283 Associated IS Dianping is one of the most popular online platforms in China 1 for discovering local life services, including dining, shopping, entertainment, hotel, etc.. With the purpose of seeking trusted local 1 Monthly, hundreds of million UV and ten million user-generated reviews are accumulated. SIGIR '23, July 23-27, 2023, Taipei, Taiwan Zepeng Huai, Yuji Yang, Mengdi Zhang, Zhongyi Zhang, Yichun Li, and Wei Wu business information, users are eager to search and consume in Dianping and write their reviews to share experiences. To increase user retention rates, Dianping establishes a review feed recommender system on the homepage as shown on the left side of Figure 1, which faces a severe data sparsity problem in the collection log 2 . Take one-month data (Nov. 2022) as an example, Figure 2(a) shows the distribution of collection numbers across the users. Cold-start users account for about 96% of all users, while active users (with more than ten collect histories) less than 0.2%. Furthermore, we analyze whether all associated tags of search and consumption are advantageous for review feed recommendation, that is, the proportion of useful tags. We gather tags associated with the above three domains in Nov. 2022, called training tags, and then collect tags just associated with reviews in the following week (from Dec. 1, 2022 to Dec. 7, 2022), called testing tags. All tags are embedded by SBERT [18], which is pretrained on Dianping corpus specifically. For testing tags, we mine the relevant training tags by setting a threshold of cosine similarity between two embeddings. Figure 3 visualizes tag embeddings of a randomly selected user and shows the percentage of useful tags by averaging all users at the bottom right, which reveals a reasonable phenomenon that only a small part of tags (less than 10% when the threshold is 0.7) can represent user interests towards reviews. Therefore, (Q2) another pivotal problem is to distill a fraction of useful tags representing intra-domain interests from a large quantity, most of which are noises. To solve this dilemma, we introduce the more accessible lightweight textual side information called tags. Tag is a word or phrase, which refers to an object or activity, like hot pot and skiing. Take Figure 1 as an example, we input beijing barbecue in the search bar (blue box) and the associated tags are beijing and barbecue . Obviously, user tag-based interests extracted from other domains are useful for suggesting reviews on the homepage. For example, a user might get interested in a bar review because he searches for wines. We count tags associated with three domains (review, search, and consumption) via the same one-month data as shown in Figure 1. The results are in Figure 2(b) and show that users have rich behaviors in the latter two domains, especially for cold-start and inactive users. Therefore, (Q1) how to leverage tag-based contents to enhance recommendations in the target domain is a key issue. Cross-domain recommendation (CDR) is an intuitive branch to solve the above problems. Early research [22, 35] focus on different mapping strategies for user cross-domain preferences. However, all of them don't introduce content information. To achieve content enhanced recommendation, some single-domain workings realize great success [21, 31, 32, 39]. Based on the previous efforts, content enhanced CDR methods firstly extract content-aware interests and then transfer them across domains. CATN [38] adopts text convolution to learn aspect-level features, which are matched between the source and target domain via a correlation matrix. MTNet [10] attentively distills useful contents via a memory network and selectively feeds them into a transfer network. However, these approaches neglect how to model tag-based contents. In recent years, 2 Here, our goal is to improve the collection rate rather than the click-through rate, because the former is the most important indicator for profit growth in our business data analysis. 2.0 are 6% of cold-start 1.8 users who have no collect behavior: 1 1.6 5 1.2 810 0.8 0.2 0.0 6  7 Collect number There (a) Data sparsity of collection log. (b) Tag distribution across the users in three domains. Figure 2: User cross-domain behavior analysis. 350 Tags per user from the homepage Tags per user from search 300 Tags per user from consumption 250 200 100 active Inactive cold-start groups of users Four Figure 3: The proportion of useful tags and an example. Crab mcat Training tag Stewed snow vericclli clam with casserole Testing tag pawpaw Baked oysters with cheese with rclated semantics fricd prawns \u00c9tJw Strcamed Koran fricd chicken Fabulous chicken Two Michclin #uk starrcd Fried Japancsc cuisinc chickcn Chickcn barbccuc granule English Mcat cakc Piano songs Nails and beauty Nail Stir-Fricd Sliccd salon Beef Saltedcroissant Japancsc coflee Boilcd mncat Pastry Christmas roll cake Popular and Urban camping old shop Dcsign acsthctics Statcroom Mansion similarity Tags Crisp graph neural network (GNN) based CDR has attracted considerable research attention for its superiority in modeling high-order connections, like PPGN [37] and HecRec [36]. However, the aggregation via the GNN layer in the above methods overlooks noisy neighbor nodes, which are indeed harmful when the unuseful nodes account for the most. In this paper, we propose a metapath and multi-interest aggregated graph neural network for tag-based cross-domain recommendation, called M2GNN. To solve Q1 , we leverage the ability of graph to recognize the pattern of non-euclidean data [40] including tagbased contents. Technically, a tag-associated heterogeneous graph is established to model the multi-homed behaviors, where each domain is represented by a metapath. Then a GNN-based method is designed to explore high-order tag-based cross-domain interests for enhancing recommendation in the target domain. To address Q2 , we introduce a novel two-step aggregation mechanism employing the dynamic routing network and self-attention network. The former filters out the trivial tags and learns multiple high-level user interests in each domain. The latter furthermore transfers more significant interests into the target domain. Moreover, we introduce M2GNN: Metapath and Multi-interest Aggregated Graph Neural Network for Tag-based Cross-domain Recommendation SIGIR '23, July 23-27, 2023, Taipei, Taiwan the skip-gram regularization to model the semantic correlation between tags. To summarize, the main contributions of this work are as follows: \u00b7 We propose a novel CDR problem whose side information is tags and design a GNN-based method to solve it. \u00b7 We conduct offline experiments on a newly proposed industrial dataset and a public dataset, which proves the effectiveness and interpretability of M2GNN. Online A/B tests demonstrate the improvement to the current industrial recommender system. \u00b7 We present a hierarchical aggregation schema to learn user preferences when noisy neighbor nodes are in the majority.", "2 PRELIMINARIES": "", "2.1 Problem Setup": "Tag-based cross-domain recommendation (TCDR) . We have a target domain (e.g. review collection) and some source domains (e.g. search and consumption), denoted as \ud835\udc37 \ud835\udc61 and \ud835\udc37 \ud835\udc60 , where \ud835\udc46 = { \ud835\udc60 1 , \ud835\udc60 2 , ... } represents there are | \ud835\udc46 | source domains. Each domain includes its users \ud835\udc62 , items \ud835\udc56 , and interactions ^ \ud835\udc66 \ud835\udc62\ud835\udc56 . Note that users are full overlap, also known as multi-homed users, while items are non-overlap. We identify the item in different domains by its superscript, such as \ud835\udc56 \ud835\udc61 , \ud835\udc56 \ud835\udc60 1 , and \ud835\udc56 \ud835\udc60 2 . According to interaction numbers in the target domain, users are divided into three groups, which are cold-start, inactive and active users. Let \ud835\udc48 \ud835\udc50\ud835\udc60 , \ud835\udc48 \ud835\udc56\ud835\udc4e and \ud835\udc48 \ud835\udc4e represent them, respectively. In this paper, the bound to distinguish the latter two groups is set to be 10 times. Cold-start users have no interactions with items in \ud835\udc37 \ud835\udc61 , but with items in \ud835\udc37 \ud835\udc60 . Each item in all domains has the corresponding content information denoted by \ud835\udc37 \ud835\udc56 . Different from the document with a common structure of the linguistic sequence, \ud835\udc37 \ud835\udc56 is a set of tags. Note that all tags come from the same vocabulary 3 , which is shared within all domains . The goal of TCDR is to improve the recommendation performance in the target domain by exploring latent tag-based interests behind cross-domain behaviors, which is especially needed for \ud835\udc48 \ud835\udc50\ud835\udc60 and \ud835\udc48 \ud835\udc56\ud835\udc4e . In this work, we target at the top-K recommendation task 4 .", "2.2 Graph Construction": "Definition 2.1. Tag-associated Heterogenous Graph. Figure 4 illustrates three kinds of interactions from the review feed, search, and consumption domain in Dianping APP. There are six kinds of entities in this TCDR task, which are user, review feed, search query, POI, review of POI, and tag. We denote them as U = { \ud835\udc62 } , R = { \ud835\udc5f } , Q = { \ud835\udc5e } , P = { \ud835\udc5d } , RP = { \ud835\udc5f \ud835\udc5d } and T = { \ud835\udc61 } , respectively. Obviously, \ud835\udc56 \ud835\udc61 is identical to \ud835\udc5f , \ud835\udc56 \ud835\udc60 = { \ud835\udc56 \ud835\udc60 1 , \ud835\udc56 \ud835\udc60 2 } = { \ud835\udc5e, \ud835\udc5d } and \ud835\udc37 \ud835\udc56 = { \ud835\udc61 } . Eight relations are used to connect the above nodes, which are { \ud835\udc62 -\ud835\udc5f, \ud835\udc62 -\ud835\udc5e,\ud835\udc62 -\ud835\udc5d, \ud835\udc5d -\ud835\udc5f \ud835\udc5d , \ud835\udc5f -\ud835\udc61, \ud835\udc5e -\ud835\udc61, \ud835\udc5f \ud835\udc5d -\ud835\udc61, \ud835\udc61 -\ud835\udc61 } . The first seven edges are easy to understand and we explain the last relation. \ud835\udc61 -\ud835\udc61 represents two tags that have related semantics, like Japanese barbecue and Japanese coffee . We use Elasticsearch (ES) to store all tags and utilize its search engine to construct \ud835\udc61 -\ud835\udc61 by regarding each tag as a query. Then a heterogeneous graph G is defined as G = {V , E} with nodes 3 Dianping constructs a large-scale tag database extracted from user-generated contents via a customized tokenizer and provides a unified API to associate tags with contents. 4 Dianping review feed recommender system is a complex industrial system with a tandem structure. After the top-K rating, also known as the matching or recall process, a customized CTR module will rate all candidate items by taking into account a cluster of additional features, like user age, occupation, and whether paid for advertising. Therefore, this work just aims to improve the recall performance. Figure 4: The tag-associated heterogenous graph constructed by Dianping data. Best viewed in color. Review associated has related feed with semantics search Query {(Word/Phrase): has intra-domain Review of all cross-domain POI consumers Acollect Tag consume V and edges E . In Dianping dataset, V = {U , R , Q , P , RP , T} and E = { \ud835\udc62 -\ud835\udc5f, \ud835\udc62 -\ud835\udc5e,\ud835\udc62 -\ud835\udc5d, \ud835\udc5d -\ud835\udc5f \ud835\udc5d , \ud835\udc5f -\ud835\udc61, \ud835\udc5e -\ud835\udc61, \ud835\udc5f \ud835\udc5d -\ud835\udc61, \ud835\udc61 -\ud835\udc61 } . Note that only \ud835\udc61 -\ud835\udc61 is a symmetric relation and the others are directed. Definition 2.3. Metapath-based neighbor. Given a metapath \ud835\udf0c , the metapath-based neighbors N \ud835\udf0c \ud835\udc63 of a node \ud835\udc63 is defined as the set of nodes that connect with \ud835\udc63 using \ud835\udf0c . Definition 2.2. Domain-specific Metapath. A metapath \ud835\udf0c is a path defined on the graph G , and is denoted in the form of ordered nodes \ud835\udc34 1 \u2192 \ud835\udc34 2 \u2192 ... \u2192 \ud835\udc34 \ud835\udc3f . \ud835\udc51 \ud835\udf0c is the number of metapaths. In Dianping dataset, three are three metapaths ( \ud835\udc51 \ud835\udf0c = 3 ) to represent domain-specific knowledge. \ud835\udc62 -\ud835\udc5f -\ud835\udc61 represents user interested tags from the collected review feed. \ud835\udc62 -\ud835\udc5e -\ud835\udc61 contains user potential preference inferred from his search log. \ud835\udc62 -\ud835\udc5d -\ud835\udc5f \ud835\udc5d -\ud835\udc61 describes what content of a local business can develop users' willingness to consume.", "3 THE M2GNN FRAMEWORK": "", "3.1 Motivation": "As aforementioned in Q1 , the content side information in this TCDR task is tags. Different from contents with the linguistic sequence, tags have no natural sequential distribution, leading to the recent works focusing on sequential feature extraction like CATN, which will be compared in Section 4, is less applicable. What's more, tags have an important feature, that is global shareability, which means any items in all domains are described by the same tag dict. This inspires us to recall the review feed including the same or sim- ilar tags appeared in other domains. For example, \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f \ud835\udc60\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc50\u210e - - - - - - \u2192 \ud835\udc58\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc5b\ud835\udc4f\ud835\udc4e\ud835\udc5f\ud835\udc4f\ud835\udc52\ud835\udc50\ud835\udc62\ud835\udc52 \ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc5c\ud835\udc50\ud835\udc56\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc51 \ud835\udc64\ud835\udc56\ud835\udc61\u210e - - - - - - - - - - - - - - \u2192 \ud835\udc4f\ud835\udc4e\ud835\udc5f\ud835\udc4f\ud835\udc52\ud835\udc50\ud835\udc62\ud835\udc52 \u210e\ud835\udc4e\ud835\udc60 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc51 \ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50\ud835\udc60 \u2190- - - - - - - - - - - - - - - - -\u2192 \ud835\udc57\ud835\udc4e\ud835\udc5b\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc52 \ud835\udc4f\ud835\udc4e\ud835\udc5f\ud835\udc4f\ud835\udc52\ud835\udc50\ud835\udc62\ud835\udc52 \ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc5c\ud835\udc50\ud835\udc56\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc51 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \u2190 - - - - - - - - - - - - - -\ud835\udc5f\ud835\udc52\ud835\udc63\ud835\udc56\ud835\udc52\ud835\udc64 \ud835\udc53 \ud835\udc52\ud835\udc52\ud835\udc51 can help to provide a review about janpanese barbecue for a user who searches korean barbecue. GNN, an intuitive tool to explore such a highorder reasoning chain, motivates us to build a tag-based graph and corresponding neural networks. With determining to employ the GNN framework, we face severe noisy information in aggregation as shown in Q2 . This is an inevitable problem when the number of attention members is large, which also appears in GraphAir [8]. Hard attention [34], also known as nondeterministic attention, is a close solution to focus on exactly one or a few members, which will be compared in Section 4. However, the tag is a fine-grained object, which means a cluster of related tags indicates one entire high-level user preference. Therefore, discarding most of tags directly adopted in hard attention will SIGIR '23, July 23-27, 2023, Taipei, Taiwan Zepeng Huai, Yuji Yang, Mengdi Zhang, Zhongyi Zhang, Yichun Li, and Wei Wu Figure 5: The framework of M2GNN. Best viewed in color. User aggregation Multi-interest Dynamic Routing MI Intra-domain Sum Pooling M2 Dynamic Self- Interest Routing Attention M3 Skip-gram Regularization Tag Dynamic Routing Intra-domain Bring Closer Review aggregation Tag aggregation Cross-domain Metahpathl(MI) Bring Farther Connected Metahpath2 (M2) Mean Mean Pooling Pooling Unconnected Metahpath3(M3) lose content side information and degrade performance. Inspired by the success of multi-interest recommender systems (RSs), we find extracting multiple interests from each domain can distill intra-domain interests in a corpus where noisy contents account for the most with two reasons: (1) learning different aspects of user preference can preserve main but filter out trivial tags; (2) attention mechanism works better in transferring high-level interests into target domain than transferring tags directly because the number of interests is much less than that of tags.", "3.2 Overview": "A general framework that can generalize most GNN-based RSs (e.g. NGCF [28], LightGCN [6], KGAT [26], KGIN [29], GC-SAN [33] and SURGE [2]) adopts the following scheme: aggregate feature (neighbor) nodes to user or item (center) nodes and update \u2192 pool node representations after multiple layers \u2192 train the model according to downstream tasks like top-K recommendation or CTR prediction. Inspired by the above straightforward but effective idea, M2GNN aggregates tag nodes into user nodes by domain-specific metapaths without self-connection, learns item embeddings by their corresponding tags, and enhances representations of tag nodes by their homogeneous neighbors with related semantics. Since the number of tags associated with an item is small, adopting complex aggregation methods is unnecessary and here we use average pooling. The inherence of the tag-to-tag update is to bring the vectors of two tags with a similar meaning closer in latent semantic space, we also utilize average pooling and leave other aggregation functions like attention as a future work. Then sum operation is conducted to pool multi-layer embeddings, which is widely used in GNN for the recommendation task to fuse multi-hop collaborative filtering signals. Finally, the inner product is adopted to predict top-K items. Formally, the above message passing and recommendation mechanism are defined as follows,      where \ud835\udc63 can be either a user or an item node in target domain \ud835\udc5f , e \ud835\udc59 \ud835\udc63 \u2208 R \ud835\udc51 \u00d7 1 denotes the representation of node \ud835\udc63 in the \ud835\udc59 -th layer, superscript \u2217 indicates the final representation for predicting, \ud835\udc66 ( \ud835\udc62, \ud835\udc5f ) is the matching score of a pair of user and item. Obviously, the key component is equation 3. As mentioned in Section 3.1, we first learn multiple interests in each domain (cf. Section 3.3) and transfer them into the target domain (cf. Section 3.4).", "3.3 Intra-domain Aggregation": "We utilize the dynamic routing network from [20] as multi-interest extractor. For each domain (metapath), tag embeddings and user preferences are regarded as low and high-level capsules, respectively. Given a set of metapath-based neighbor tag nodes \ud835\udc61 \u2208 N \ud835\udf0c \ud835\udc62 , we learn interest capsules as  where c \ud835\udf0c,\ud835\udc59 \ud835\udc62,\ud835\udc57 denotes \ud835\udc57 -th interest capsule of user \ud835\udc62 using metapath \ud835\udf0c in \ud835\udc59 -th layer, \ud835\udc64 \ud835\udc61 \ud835\udc57 represents the contribution degree of low-level capsule \ud835\udc61 to high-level capsule \ud835\udc57 , S \u2208 R \ud835\udc51 \u00d7 \ud835\udc51 is transformation matrix and is shared across all interest capsules and metapaths. The superiority of dynamic routing is to update \ud835\udc64 \ud835\udc61 \ud835\udc57 via a new parameter called routing logit \ud835\udc4f \ud835\udc61 \ud835\udc57 in an iterative way. We initialize \ud835\udc4f \ud835\udc61 \ud835\udc57 randomly in the first iteration. \ud835\udc64 \ud835\udc61 \ud835\udc57 is calculated by performing softmax on \ud835\udc4f \ud835\udc61 \ud835\udc57 as  where \ud835\udc3e is the number of interest capsules and is adaptively calculated by the number of interacted tags as  M2GNN: Metapath and Multi-interest Aggregated Graph Neural Network for Tag-based Cross-domain Recommendation SIGIR '23, July 23-27, 2023, Taipei, Taiwan where \ud835\udc3e max is the maximum number of interest capsules and \u2308 \ud835\udc65 \u2309 refers to the smallest integer that is more than or equal to \ud835\udc65 . To normalize the length of the output vector of a capsule between 0 and 1, which benefits updating \ud835\udc4f \ud835\udc61 \ud835\udc57 in next iteration, a non-linear squashing function is utilized as  Then \ud835\udc4f \ud835\udc61 \ud835\udc57 is updated by  Finally, the above routing process is repeated multiple times, which is set to be three, to obtain the converged output of interest capsules.", "3.4 Inter-domain Aggregation": "We collect all interest embeddings as a matrix Z \ud835\udc59 \ud835\udc62 = [ z \ud835\udf0c 1 ,\ud835\udc59 \ud835\udc62, 1 , ..., z \ud835\udf0c 1 ,\ud835\udc59 \ud835\udc62,\ud835\udc3e max , z \ud835\udf0c 2 ,\ud835\udc59 \ud835\udc62, 1 , ... z \ud835\udf0c \ud835\udc51\ud835\udf0c ,\ud835\udc59 \ud835\udc62,\ud835\udc3e max ] \u2208 R \ud835\udc51 \u00d7( \ud835\udc51 \ud835\udf0c \u00b7 \ud835\udc3e max ) . For faster and unified tensor calculation, we pad zero vectors when \ud835\udc3e < \ud835\udc3e max . Now each user has an interest matrix ( Z \ud835\udc59 \ud835\udc62 ) consisting of many interest embeddings ( z \ud835\udf0c,\ud835\udc59 \ud835\udc62,\ud835\udc57 ). To transfer useful preferences to the target domain, we assign a weight \ud835\udefc \ud835\udf0c,\ud835\udc59 \ud835\udc62,\ud835\udc57 to each interest embedding and output an intra-domain interest embedding by pooling Z \ud835\udc59 \ud835\udc62 attentively. Since sparse data in the target domain is undesirable to train accurate weights, a reasonable alternative solution is to utilize user similarity. For example, if two users have the same search and consumption log, it is reasonable to recommend a review feed collected by one user to the other. Technologically, we aim to assign the same weight to a pair of close interest embeddings from two users with similar portraits (a.k.a. Z \ud835\udc59 \ud835\udc62 ). Therefore, a global shared mechanism is required. To achieve this, a self-attention module is designed as  where H \ud835\udc59 \ud835\udc62 \u2208 R 1 \u00d7( \ud835\udc51 \ud835\udf0c \u00b7 \ud835\udc3e max ) is the attention vector, S 1 \u2208 R \ud835\udc51 \u00d7 \ud835\udc51 and S 2 \u2208 R \ud835\udc51 \u00d7 1 are trainable matrixes, which are shared across all users. Q2 reveals that only a fraction of interests contributes to the target domain, therefore we introduce the element-wise exponentiation operator before a softmax function to calculate weights as:  where \u210e \ud835\udc59 \ud835\udc62,\ud835\udc57 is the \ud835\udc57 -th element in H \ud835\udc59 \ud835\udc62 and \ud835\udefe is a hyperparameter to control the attention distribution. As \ud835\udefe increases, more significant interest embeddings will receive more attention and bigger weights, and vice versa. Consider the limit case, \ud835\udefe = 0 is identical to mean pooling and \ud835\udefe = \u221e means the hard attention which chooses only one candidate as output. Therefore, \ud835\udefe is set to be bigger than 1, which can alleviate the problem in Q2 .  Finally, the user representation is inferred to represent his intradomain interests as where A \ud835\udc59 \ud835\udc62 = [ \ud835\udefc \ud835\udf0c,\ud835\udc59 \ud835\udc62, 1 , ..., \ud835\udefc \ud835\udf0c,\ud835\udc59 \ud835\udc62,\ud835\udc51 \ud835\udf0c \u00b7 \ud835\udc3e max ] \u2208 R 1 \u00d7( \ud835\udc51 \ud835\udf0c \u00b7 \ud835\udc3e max ) is the weight matrix of user \ud835\udc62 in \ud835\udc59 layer.", "3.5 Skip-gram Regularization": "It is clear that the quality of tag embeddings deeply affects the clustering performance in Section 3.3 and weight computing in Section 3.4. To further draw related tags closer and unconcerned tags removed in latent semantic space, we adopt skip-gram idea to regularize the training process. Specifically, a tag node is regarded as the center word, its homogeneous neighbor nodes are set to be nearby words. The principle is to maximize the co-occurrence probability of related tags, and vice versa. We randomly sample a negative node for each positive pair of tags and define skip-gram objective loss as  where |E \ud835\udc61 -\ud835\udc61 | denotes the number of all edges belonging to type \ud835\udc61 -\ud835\udc61 in graph G . The basic formulation to model probability \ud835\udc5d is the global softmax performed on all tags, which is impractical for the huge cost of time and calculation. Inspired by hierarchical softmax, an auxiliary set of trainable embeddings V \u2208 R \ud835\udc51 \u00d7|T| is introduced to define \ud835\udc5d as  where v \ud835\udc61 \u2032 is the embedding of tag \ud835\udc61 \u2032 from V , e 0 \ud835\udc61 is the initial embedding of tag \ud835\udc61 .", "3.6 Model Optimization": "With the user representation e \u2217 \ud835\udc62 and the review representation e \u2217 \ud835\udc5f ready, equation 5 is adopted to predict top-K items. We employ the BPR loss [19] as  where O = {( \ud835\udc62, \ud835\udc5f + , \ud835\udc5f - )|( \ud835\udc62, \ud835\udc5f + ) \u2208 ^ \ud835\udc66 \ud835\udc62\ud835\udc56 \ud835\udc61 , ( \ud835\udc62, \ud835\udc5f - ) \u2209 ^ \ud835\udc66 \ud835\udc62\ud835\udc56 \ud835\udc61 } denotes the training set, where \ud835\udc5f + is the observed interaction and \ud835\udc5f -is a negative counterpart which is sampled from the unobserved interactions randomly, \ud835\udf0e (\u00b7) is the sigmoid function. \u0398 = { e 0 \ud835\udc65 , S , S 1 , S 2 , v \ud835\udc61 | \ud835\udc65 \u2208 {U , R , T} , \ud835\udc61 \u2208 T} is the model parameter set. \ud835\udc3f 2 regularization parameterized by \ud835\udf06 on \u0398 is conducted to prevent overfitting. Finally, the joint loss is", "4 EXPERIMENTS": "We evaluate our proposed M2GNN method on two benchmark datasets to answer the following research questions: \u00b7 RQ1 : Does our proposed M2GNN outperform the state-of-the-art recommendation methods? \u00b7 RQ3 : Does M2GNN achieve great improvement towards coldstart and inactive users? \u00b7 RQ2 : How do different components (i.e., two-step aggregation module and skip-gram regularization) and hyperparameters (i.e. capsule numbers, GNN layer numbers, and exponent before softmax) affect M2GNN? SIGIR '23, July 23-27, 2023, Taipei, Taiwan Table 1: Statistics of two datasets. \ud835\udc5a and \ud835\udc4f denote movie and book, respectively. 'int.' indicates the interaction data of each domain. \u00b7 RQ4 : Is there an online improvement of review feed recommendation in Dianping homepage by M2GNN? \u00b7 RQ5 : Can M2GNN provide potential explanations about tagbased preferences of users?", "4.1 Experimental Settings": "Dataset Description . We propose a novel industrial dataset, called DPBJ (Dianping Beijing). The dataset is encrypted and desensitized, and does not contain any personally identifiable information. To avoid data leakage, we collect the multi-site data in one month (from Nov. 1, 2022, to Nov. 30, 2022) as the training and validation set. Then the latter one-week data (from Dec. 1, 2022, to Dec. 7, 2022) is the testing set. We also use a common dataset, Amazon review dataset 5 , where the book and movie are chosen as the source and target domain. To ensure data quality, we remove the interaction records without review text and some low-quality items following earlier studies [7, 11]. We split the review text into individual words and filter out stopwords by word tokenization tool like nltk 6 . The above all words comprise the tag dict. The detailed statistics of these two datasets are reported in Table 1. Baselines . To demonstrate the effectiveness, we compare M2GNN with single-domain (MF, lightGCN), content-free crossdomain (EMCDR, PTUPCDR), content-based cross-domain (CATN, MTNet), and GNN-based cross-domain (HeroGRAPH, HCDIR, GA) methods. \u00b7 MF [19] is a matrix factorization method optimized by the Bayesian personalized ranking (BPR) loss. \u00b7 lightGCN [6] simplifies NGCF [28] and provides simple but effective model for user-item bipartite graph-based RS. 5 http://jmcauley.ucsd.edu/data/amazon/ 6 https://github.com/nltk/nltk Zepeng Huai, Yuji Yang, Mengdi Zhang, Zhongyi Zhang, Yichun Li, and Wei Wu \u00b7 EMCDR [17] first pretrains embeddings in each domain and then learns a mapping function to transfer interests across domains. \u00b7 CATN [26] extracts multiple aspects from the documents and learns aspect correlations across domains attentively. \u00b7 PTUPCDR [42] designs a meta network fed with users' characteristic embeddings to generate personalized bridge functions to achieve personalized transfer of user preferences. \u00b7 MTNet [9] designs a memory component to find the words highly relevant to the user preferences, which are then fused with source domain-specific knowledge to improve final prediction. \u00b7 HCDIR [1] separately learns user embeddings in each domain via an insurance information network and then employs MLP to perform latent space matching across domains. \u00b7 HeroGRAPH [3] constructs a heterogeneous graph to fuse all interactions as a whole and learn additional global embeddings to enhance recommendation in all domains. \u00b7 GA [41] proposes a unified framework, which constructs separate heterogeneous graphs to generate more representative embeddings and uses an element-wise attention mechanism to effectively combine them of common entities. Parameter Settings . We implement our M2GNN model in Pytorch and Deep Graph Library (DGL) 7 , which is a Python package for deep learning on graphs. Since DPBJ has million-scale nodes and billion-scale edges, the API of distributed data parallel (DDP) in Pytorch is used to train M2GNN and the data loading package in DGL is adopted to generate subgraph samples for each batch. We released all implementations (code, datasets, parameter settings, and training logs) to facilitate reproducibility 8 . The embedding size of all models is set to be 64 for a fair comparison. We adopt Adam [12] as the optimizer and the batch size is fixed at 1024 for all methods. Evaluation Metrics . As mentioned in Section 2.1, Recall@ \ud835\udc3e and Hit@ \ud835\udc3e 9 are used in the task of top-K recommendation.", "4.2 Performance Comparison (RQ1)": "We report the empirical results in Table 2. The observations are as follows: \u00b7 M2GNN consistently achieves the best performance on two datasets in terms of all measures. Specifically, it achieves significant improvements over the strongest baselines w.r.t. Recall@50 by 18.46%, 6.80% in DPBJ and Amazon, respectively. The results prove the effectiveness of M2GNN. \u00b7 MTNet outperforms CATN in DPBJ while underperforms CATN in Amazon. The reason is the difference between content extractors in these two models. MTNet adopts an attention network \u00b7 Despite some particular cases, the average performances of content-aware (content-based&GNN-based) cross-domain, content-free cross-domain, and single-domain models are in decreasing order. A clear reason is whether to introduce the auxiliary information, like contents or cross-domain interactions. 7 https://github.com/dmlc/dgl 9 K is set to be 50 rather than a commonly used value like 20 or 100. The reason is that the subsequent CTR module only ranks the top 50 items from the recall phase. 8 https://github.com/huaizepeng2020/M2GNN_git M2GNN: Metapath and Multi-interest Aggregated Graph Neural Network for Tag-based Cross-domain Recommendation SIGIR '23, July 23-27, 2023, Taipei, Taiwan Table 2: Performance comparison. \u25b2 % denotes the relative improvement of M2GNN over the best SOTA algorithm. Table 3: Impact of two-step aggregation and skip-gram regularization. The results are shown in the upper part of Table 3 and the major findings are as follows: to model unstructured text without regarding the sequential feature, thus performing better in DPBJ dataset. CATN utilizes text convolution and has superiority in modeling sequential signals, which is more suitable for the Amazon dataset. \u00b7 HeroGRAPH outperforms GA-MTCDR-P (HCDIR) in DPBJ while underperforms GA-MTCDR-P (HCDIR) in Amazon. DPBJ has a large number of noisy tags, leading to a big gap between intra and cross-domain interests. Therefore, using multiple embeddings to represent domain-specific interests separately (HeroGRAPH) performs better than using one representation (GA-MTCDRP/HCDIR). However, Amazon dataset has higher consistency in user preferences toward books and movies, thus aggregating cross-domain interests is more applicable.", "4.3 Study of M2GNN (RQ2)": "Impact of two-step aggregation module . To demonstrate the superiority of the two-step aggregation module, we compare the performance of M2GNN with the following four variants which adopt different methods of aggregation. \u00b7 M2GNN-mean uses average pooling in both two steps. \u00b7 M2GNN-HAN regards intra-domain and inter-domain aggregation as node-level and semantic-level attention in HAN [30]. \u00b7 M2GNN-softmax adopts the standard schema of attention, proposed in Transformer [24], in both two steps. \ud835\udc3e is user embedding, while \ud835\udc44 and \ud835\udc49 are tag embeddings. \u00b7 M2GNN-hard utilizes local alignment attention in [16], which extends original hard attention [34] to select a subset of input members via implementing a softmax distribution. \u00b7 M2GNN-mean and M2GNN-softmax achieve the worst performances on the two datasets. The reason is both of them don't track the noisy content problem. The former regards all tags as equally important, while the latter receives a large number of attention members, leading to the weights of significant tags are not trained as expected. \u00b7 M2GNN-hard performs the best among the four variants. A clear reason is that only a subset of significant tags are aggregated into user representations, which avoids using all tags to infer final embeddings. However, as aforementioned in Section 3.1, it is inevitable to discard some useful tags in hard attention, which causes that M2GNN-hard underperforms M2GNN. \u00b7 M2GNN-HAN slightly outperforms the first two variants. A possible reason is that the different mechanism to calculate attention scores using the type-specific transformation matrix and the weight coefficient helps to increase the weights of useful tags. Impact of skip-gram regularization . To demonstrate the superiority of the skip-gram regularization, we discard L 1 and only leverage collaborative filtering signals to train the model, termed M2GNN-w/o L 1 . The lower part of Table 3 shows that M2GNNw/o L 1 sharply underperforms M2GNN without regularizing tag embeddings via L 1 , which proves the necessity of the skip-gram regularization. Impact of hyperparameters . We investigate the influence of three hyperparameters: layer numbers, capsule numbers, and exponent before softmax, all of which greatly affect multi-interest extraction and transfer. \ud835\udc3f , \ud835\udc3e max and \ud835\udefe are searched in { 1 , 2 , 3 } , { 2 , 4 , 6 , 8 } and { 2 , 4 , 6 , 8 } , respectively. Figure 6 shows the results in the DPBJ dataset and it has a similar conclusion in the Amazon dataset, which is omitted. From the observations above, we make several conclusions. Firstly, \ud835\udc3f = 2 results in the best performance, which means 2-order connections integrate more similar tags into users and stacking more layers continually brings in noisy tags, which is harmful to the recommendation. Secondly, the increase of interest capsules indeed improves the recall list since local businesses contain multiple aspects, while there is a saturation limit which has a similar conclusion in [13]. Thirdly, the optimal number of \ud835\udefe is 6~8. The reason is that a proper exponent helps the softmax SIGIR '23, July 23-27, 2023, Taipei, Taiwan Zepeng Huai, Yuji Yang, Mengdi Zhang, Zhongyi Zhang, Yichun Li, and Wei Wu Figure 6: Effect of hyperparameters. Best viewed in color. 0.0610 0.0610 0.0630 0.0595 0.0595 0.0590 0.0580 0.0580 0.0550 Kmax 4,Y = 4 0.0565 L=2,Y= 2 0.0565 L=2, Kmax Kmax L=2,Y =4 0.0510 Kmax 6,Y = L=2,Y= 6 L = 2, Kmax Kmax 6,Y = L=2,Y=8 L =2, Kmax Kmax 6,Y = 0.0550 0.0550 0.0470 Kmax 0.0960 0.0960 0.0960 0.0940 0.0940 0.0920 #0.0920 #0.0920 50.0880 Kmax 4,Y =4 L=2,Y =2 L =2, Kmax = 2 Kmax 4,Y = 0.0900 L=2,Y= 4 0.0900 L = 2, Kmax 0.0840 Kmax 6,Y = L=2,Y = 6 L=2, Kmax Kmax Y = 6 L=2,Y=8 L=2 6,Y = 0.0880 0.0880 0.0800 Kmar Kmax Kmax Figure 7: Performance comparison over the sparsity distribution of user groups. The background histograms indicate the density of each user group and \u2031 means permyriad. 0.0750 0.1000 0.74%00 0.0900 0.0550 0.08008 1 @ 0.0450 0.0700 PTUPCDR 0.0600 MTNet HeroGRAPH GA-MTCDR-P 0.0250 active inactive users users users users %ooo function to identify more important interests and assign bigger scores.", "4.4 Cold-start and Inactive Users Analysis (RQ3)": "One motivation for exploiting CDR is to alleviate the severe sparsity problem in Dianping, where an overwhelming majority of users are cold-start in Figure 2. Therefore, we investigate whether introducing CDR helps alleviate this issue. Towards this end, we divide the testing set into four groups based on the interaction number per user, which has the same criterion in Section 1. Metrics are calculated for each group individually. EMCDR and HCDIR have similiar performances with PTUPCDR and GA-MTCDR-P respectively, therefore they are omitted. Figure 7 illustrates the results and we have two conclusions: (1) All models perform better for active Table 4: Online metrics in a week. users since they have richer interactions in the target domain. (2) There is a marked decrease caused by the data sparsity problem for inactive and cold-start users in baselines, while M2GNN has the least impact. This proves M2GNN has stronger robustness to serve the inactive and cold-start users.", "4.5 Online A/B Test (RQ4)": "We report a 2-week online A/B test experiment of M2GNN in Beijing, from Dec. 1, 2022, to Dec. 15, 2022. Specifically, the current recommender system on the Dianping homepage 10 is the base model. M2GNN is integrated into it as an additional recall method and directly offers 200 reviews for the later rank stage, which is regarded as the tested model. 3.4 million users participated in this A/B test. Three kinds of metrics are chosen. 'Collection number' (CN) indicates the number of all collected reviews, which is an absolute amount. 'Collection number per user' (UV-CN) means the number of collected reviews per user, which eliminates users with no collection behaviors. 'Collection rate' (CR) is the ratio of collected reviews to all exposed reviews. We just give the increase rate since the true value of each metric is sensitive. As in Table 4, M2GNN achieves great improvements relative to the base model. What's more, the improvement of inactive and cold-start users (1.1986% CR) is more significant than that of active users (0.4663% CR), which demonstrates the superiority of M2GNN in alleviating data sparsity problems by transferring cross-domain knowledge. 10 This is not disclosed for confidentiality reasons. M2GNN: Metapath and Multi-interest Aggregated Graph Neural Network for Tag-based Cross-domain Recommendation SIGIR '23, July 23-27, 2023, Taipei, Taiwan Figure 8: Case study. Each tag is represented by a color block. The original Chinese text is in brackets and we translate it into English. Best viewed in color. Intra-domain interest Reviews in testing set A famous birthday cake in Beijing We use intra-domain interest to 001 0,.049 0,042 recall items and top-50 results (2) A bar with a strong ambience of Multi-interest extraction contain all reviews appeared in literature and art Self-Attention in each domain testing set. A private space for meeting Dynamic Routing Dynamic Routing in Collection None training set Ciyun Temple(zz ) Damei centre(is) Scarch log Nanchang rice Consumed POI Mashed Tags log Beijing lar( 4", "4.6 Case Study (RQ5)": "To prove the interpretability of M2GNN, we visualize the two-step aggregation process to show how to distill intra-domain interests from tags, most of which are noises. Figure 8 takes a specific user as an example. The bottom lists all tags from three domains, and obviously he is a cold-start user because of no interactions in the target domain. In the pink block, there are four interest capsules in each domain according to equation 8. The number x \u25cb near to a capsule z represents that \ud835\udc65 -th tag has the largest routing logit towards z among four capsules, which is also reflected by highlighting tags with the same color. The results indicate that tags are clustered via their semantics and the interest capsule represents the high-level preference of each domain. For example, in the consumption domain, the first capsule consists of tags about cakes (e.g. Dessert , Cream and Beijing cake ). What's more, the influence of trivial tags (e.g. Mashed tar ) is weakened since a capsule can only represent one aspect of semantics. Then eight weights are calculated by the self-attention module as shown in the blue block. Three capsules, which are concerned with barbecue (0.213), cake (0.413), and popular shop (0.203) respectively, account for the major portion of final intra-domain interest. Finally, the top-100 candidate reviews contain all ground-truth items at the right of the figure, which are highly relevant to the above three interest capsules and confirm the reasonability and interpretability of M2GNN. [25] framework to recommend items. The second category of solutions fulfills high-level semantics extraction, also known as aspects [23, 38]. CATN [38] leverages the aspect gate as a soft on-off switch to control which latent feature derived from the text convolution layer is relevant to the aspect, and then performs pair-wise aspect matchings as the final rating prediction. Despite their success in processing text content, solutions in both categories don't specially take into consideration the corpus where the majority of tokens are noise. To our best knowledge, M2GNN is the first attempt to filter out noisy content and distill intra-domain interests simultaneously. GNN-basedCDR. This category of algorithms aims to leverage the high-order connectivity of GNN by transforming domain-specific knowledge into graphs. There are two common graph data topologies: (1) user-item bipartite graph [3, 14, 15, 27]. BiTGCF [15] incorporates graph neural network to a bi-direction dual transfer learning approach, where each domain has a unique user-item bipartite graph and the output of each GNN layer is integrated across domains in the propagation process. (2) heterogeneous graph [1, 36]. HCDIR [1] transforms four types of insurance products with their properties and relations into an insurance graph and utilizes relational neighbor aggregation to learn embeddings in each domain, which is fed into an MLP to transfer knowledge. However, all of the above models fail to incorporate review information into the graph explicitly and just use documents to generate initial representations. To our best knowledge, M2GNN is the first attempt to regard the words (tags) as one type of node in GNN-based CDR.", "5 RELATED WORK": "Our work is highly related to two subareas of recommendation, which are content-based and GNN-based CDR. Content-based CDR. The underlying assumption of this method is that the interests extracted from descriptive information in the content can be shared in different domains because of the consistency of user preferences. According to the method for modeling text features, there are two kinds of content-based CDR. The first way is directly projecting documents into the common lowdimensional space [4, 5, 9]. RC-DFM [5] learns latent vectors of reviews by applying a max pooling operation over pretrained word embeddings and utilizes the output of hidden layer under SDAE", "6 CONCLUSION": "In this paper, we aim to improve the review feed recommender system in the Dianping app. The proposed M2GNN adopts the GNN-based cross-domain framework to extract content-based interests via tags from other sites like search and consumption logs to enhance target recommendation. A key contribution is a novel two-step aggregation mechanism, which can distill intra-domain interests from a large number of tags, most of which are noises. Both offline and online experiments demonstrate the effectiveness SIGIR '23, July 23-27, 2023, Taipei, Taiwan and explainability of M2GNN. For future work, we plan to investigate how to filter out unuseful tags via node sampling technology in GNN.", "REFERENCES": "[1] Ye Bi, Liqiang Song, Mengqiu Yao, Zhenyu Wu, Jianming Wang, and Jing Xiao. 2020. A heterogeneous information network based cross domain insurance recommendation system for cold start users. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 2211-2220. [3] Qiang Cui, Tao Wei, Yafeng Zhang, and Qing Zhang. 2020. HeroGRAPH: A Heterogeneous Graph Framework for Multi-Target Cross-Domain Recommendation.. In ORSUM@ RecSys . [2] Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng Jin, and Yong Li. 2021. Sequential Recommendation with Graph Neural Networks. In SIGIR . 378-387. [4] Ali Mamdouh Elkahky, Yang Song, and Xiaodong He. 2015. A multi-view deep learning approach for cross domain user modeling in recommendation systems. In Proceedings of the 24th international conference on world wide web . 278-288. [6] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 639-648. [5] Wenjing Fu, Zhaohui Peng, Senzhang Wang, Yang Xu, and Jin Li. 2019. Deeply fusing reviews and contents for cold start users in cross-domain recommendation systems. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 94-101. [7] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173-182. [9] Guangneng Hu, Yu Zhang, and Qiang Yang. 2018. MTNet: a neural approach for cross-domain recommendation with unstructured text. KDD Deep Learning Day (2018), 1-10. [8] Fenyu Hu, Yanqiao Zhu, Shu Wu, Weiran Huang, Liang Wang, and Tieniu Tan. 2021. Graphair: Graph representation learning with neighborhood aggregation and interaction. Pattern Recognition 112 (2021), 107745. [10] Wenyi Huang and Jack W Stokes. 2016. MtNet: a multi-task neural network for dynamic malware classification. In International conference on detection of intrusions and malware, and vulnerability assessment . Springer, 399-418. [12] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [11] SeongKu Kang, Junyoung Hwang, Dongha Lee, and Hwanjo Yu. 2019. Semisupervised learning for cross-domain recommendation to cold-start users. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 1563-1572. [13] Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang, Guoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-interest network with dynamic routing for recommendation at Tmall. In KDD . 26152623. [15] Meng Liu, Jianjun Li, Guohui Li, and Peng Pan. 2020. Cross domain recommendation via bi-directional transfer graph collaborative filtering networks. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 885-894. [14] Jin Li, Zhaohui Peng, Senzhang Wang, Xiaokang Xu, Philip S Yu, and Zhenyun Hao. 2020. Heterogeneous Graph Embedding for Cross-Domain Recommendation Through Adversarial Learning. In International Conference on Database Systems for Advanced Applications . Springer, 507-522. [16] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attention-based neural machine translation. EMNLP (2015). [18] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019). [17] Tong Man, Huawei Shen, Xiaolong Jin, and Xueqi Cheng. 2017. Cross-domain recommendation: An embedding and mapping approach.. In IJCAI , Vol. 17. 24642470. [19] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [21] Sungyong Seo, Jing Huang, Hao Yang, and Yan Liu. 2017. Interpretable convolutional neural networks with dual local and global attention for review rating prediction. In Proceedings of the eleventh ACM conference on recommender systems . 297-305. [20] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017. Dynamic routing between capsules. Advances in neural information processing systems 30 (2017). [22] Ajit P Singh and Geoffrey J Gordon. 2008. Relational learning via collective matrix factorization. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining . 650-658. Zepeng Huai, Yuji Yang, Mengdi Zhang, Zhongyi Zhang, Yichun Li, and Wei Wu [23] Tianhang Song, Zhaohui Peng, Senzhang Wang, Wenjing Fu, Xiaoguang Hong, and Philip S Yu. 2017. Based cross-domain recommendation through joint tensor factorization. In International conference on database systems for advanced applications . Springer, 525-540. [25] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and L\u00e9on Bottou. 2010. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of machine learning research 11, 12 (2010). [24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [26] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat: Knowledge graph attention network for recommendation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 950-958. [28] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval . 165-174. [27] Xiang Wang, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2017. Item silk road: Recommending items from information domains to social users. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval . 185-194. [29] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, and Tat-Seng Chua. 2021. Learning Intents behind Interactions with Knowledge Graph for Recommendation. In Proceedings of the Web Conference 2021 . 878-887. [31] Libing Wu, Cong Quan, Chenliang Li, Qian Wang, Bolong Zheng, and Xiangyang Luo. 2019. A context-aware user-item representation learning for item recommendation. ACM Transactions on Information Systems (TOIS) 37, 2 (2019), 1-29. [30] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. 2019. Heterogeneous graph attention network. In WWW . 2022-2032. [32] Haifeng Xia, Zengmao Wang, Bo Du, Lefei Zhang, Shuai Chen, and Gang Chun. 2019. Leveraging ratings and reviews with gating mechanism for recommendation. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 1573-1582. [34] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning . PMLR, 2048-2057. [33] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Fuzhen Zhuang, Junhua Fang, and Xiaofang Zhou. 2019. Graph Contextualized SelfAttention Network for Session-based Recommendation.. In IJCAI , Vol. 19. 39403946. [35] Huan Yan, Xiangning Chen, Chen Gao, Yong Li, and Depeng Jin. 2019. Deepapf: Deep attentive probabilistic factorization for multi-site video recommendation. IJCAI 2019 (2019). [37] Cheng Zhao, Chenliang Li, and Cong Fu. 2019. Cross-domain recommendation via preference propagation graphnet. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 2165-2168. [36] Jiangyi Yin, Yuchun Guo, and Yishuai Chen. 2019. Heterogenous Information Network Embedding Based Cross-Domain Recommendation System. In 2019 International Conference on Data Mining Workshops (ICDMW) . IEEE, 362-369. [38] Cheng Zhao, Chenliang Li, Rong Xiao, Hongbo Deng, and Aixin Sun. 2020. CATN: Cross-domain recommendation for cold-start users via aspect transfer network. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 229-238. [40] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks: A review of methods and applications. AI Open 1 (2020), 57-81. [39] Lei Zheng, Vahid Noroozi, and Philip S Yu. 2017. Joint deep modeling of users and items using reviews for recommendation. In Proceedings of the tenth ACM international conference on web search and data mining . 425-434. [41] Feng Zhu, Yan Wang, Jun Zhou, Chaochao Chen, Longfei Li, and Guanfeng Liu. 2021. A unified framework for cross-domain and cross-system recommendations. IEEE Transactions on Knowledge and Data Engineering (2021). [42] Yongchun Zhu, Zhenwei Tang, Yudan Liu, Fuzhen Zhuang, Ruobing Xie, Xu Zhang, Leyu Lin, and Qing He. 2022. Personalized transfer of user preferences for cross-domain recommendation. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 1507-1515."}
