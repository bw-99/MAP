{
  "Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model": "Wentao Ouyang Xiuwu Zhang Alibaba Group maiwei.oywt@alibaba-inc.com xiuwu.zxw@alibaba-inc.com Chaofeng Guo Shukui Ren Alibaba Group chaofeng.gcf@alibaba-inc.com shukui.rsk@alibaba-inc.com Yupei Sui Kun Zhang Alibaba Group yupei.syp@alibaba-inc.com jerry.zk@alibaba-inc.com Jinmei Luo Yunfeng Chen Alibaba Group cathy.jm@alibaba-inc.com chenyunfeng@alibaba-inc.com Dongbo Xu Xiangzheng Liu Alibaba Group dongbo.xdb@alibaba-inc.com xiangzheng.lxz@alibaba-inc.com",
  "ABSTRACT": "",
  "CCS CONCEPTS": "In real-world advertising systems, conversions have different types in nature and ads can be shown in different display scenarios, both of which highly impact the actual conversion rate (CVR). This results in the multi-type and multi-scenario CVR prediction problem. A desired model for this problem should satisfy the following requirements: 1) Accuracy: the model should achieve fine-grained accuracy with respect to any conversion type in any display scenario. 2) Scalability: the model parameter size should be affordable. 3) Convenience: the model should not require a large amount of effort in data partitioning, subset processing and separate storage. Existing approaches cannot simultaneously satisfy these requirements. For example, building a separate model for each (conversion type, display scenario) pair is neither scalable nor convenient. Building a unified model trained on all the data with conversion type and display scenario included as two features is not accurate enough. In this paper, we propose the Masked Multi-domain Network (MMN) to solve this problem. To achieve the accuracy requirement, we model domain-specific parameters and propose a dynamically weighted loss to account for the loss scale imbalance issue within each mini-batch. To achieve the scalability requirement, we propose a parameter sharing and composition strategy to reduce model parameters from a product space to a sum space. To achieve the convenience requirement, we propose an auto-masking strategy which can take mixed data from all the domains as input. It avoids the overhead caused by data partitioning, individual processing and separate storage. Both offline and online experimental results validate the superiority of MMN for multi-type and multi-scenario CVR prediction. MMN is now the serving model for real-time CVR prediction in UC Toutiao. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Â© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0124-5/23/10...$15.00 https://doi.org/10.1145/3583780.3614697 Â· Information systems â†’ Online advertising ;",
  "KEYWORDS": "Online advertising; CVR prediction; Multiple conversion types; Multiple display scenarios",
  "ACMReference Format:": "Wentao Ouyang, Xiuwu Zhang, Chaofeng Guo, Shukui Ren, Yupei Sui, Kun Zhang, Jinmei Luo, Yunfeng Chen, Dongbo Xu, Xiangzheng Liu, and Yanlong Du. 2023. Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23), October 21-25, 2023, Birmingham, United Kingdom. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3583780.3614697",
  "1 INTRODUCTION": "Conversion rate (CVR) prediction [6, 13, 15] is an essential task in online advertising systems. In optimized Cost-Per-Click (oCPC) and Cost-Per-Action (CPA) advertising, advertisers seek to maximize conversions for a given budget [18, 22, 32]. The predicted CVR impacts both the ad ranking strategy and the ad charging model. Conversions have different types in nature (e.g., purchase a product vs. sign up an account) and ads can be shown in different display scenarios (e.g, shown as a banner on a page vs. interleaved with news in the news channel ), both of which highly impact the actual CVR. As an evidence, the ground-truth CVRs vary significantly over different conversion types (e.g., from 6.3% to 18.9%) and over different display scenarios (e.g., from 0.9% to 14.9%) in the Criteo dataset (Table 1). This results in the multi-type and multi-scenario CVR prediction problem. A desired model for this problem should satisfy the following requirements: 1) Accuracy: the model should achieve fine-grained accuracy with respect to any conversion type in any display scenario. 2) Scalability: the model parameter size should be affordable. 3) Convenience: the model should not require a large amount of effort in data partitioning, subset processing and separate storage. One approach to solve this problem is to build a separate model for each (conversion type, display scenario) pair. However, it is neither scalable nor convenient. Another approach is to build a Yanlong Du Alibaba Group yanlong.dyl@alibaba-inc.com CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Wentao Ouyang et al. Figure 1: ğ´ and ğµ are two input instances. (a) In multi-task learning, domains are the same and tasks are different. One input impacts the model parameters of all tasks. (b) In multidomain learning, domains are different and tasks are the same. One input impacts the parameters for that domain. A B Task 1 (Dom. 1) Task 2 (Dom. 1) Task 3 (Dom. 1) (a) Multi-task learning A B Task 1 (Dom. 1) Task 1 (Dom. 2) Task 1 (Dom. 3) (b) Multi-domain learning unified model trained on all the data, with conversion type and display scenario included as two feature fields. However, it fails to capture different natures of various conversion types and display scenarios, and is thus not accurate enough. In this paper, we formulate the multi-type and multi-scenario CVR prediction problem as a multi-domain learning problem. We would like to illustrate the differences between multi-task learning [5] and multi-domain learning [9] in Figure 1. In multi-task learning, domains are the same and tasks are different. For example, given a video, simultaneously estimating what rating a user will give and how long the user will spend on the video is a multi-task learning problem [31]. In multi-domain learning, domains are different and tasks are the same. For example, estimating the CVRs of two instances of conversion type 'purchase' and 'sign up' by the same model is a multi-domain learning problem. Moreover, in multi-task learning, one input impacts the model parameters of all tasks; while in multi-domain learning, one input only impacts the model parameters corresponding to that domain. Because of such differences, existing multi-task learning models such as SharedBottom, OMoE, MMoE, CGC and PLE [16, 17, 27, 31] cannot well address our problem. MT-FwFM [22], STAR [25] and ADIN [11] are more relevant state-of-the-art multi-domain models. However, as they are not designed for our problem, they also cannot satisfy the aforementioned three requirements. Given these limitations, we propose the Masked Multi-domain Network (MMN) in this paper. We will show how we design various strategies to make MMN simultaneously satisfy the accuracy, scalability and convenience requirements. The main contributions of this work are summarized as follows: (1) We propose the MMN model to address the multi-type and multi-scenario CVR prediction problem in a real-world advertising system. (2) In order to achieve the accuracy requirement, we model domain-specific parameters and we propose a dynamically weighted loss to account for the loss scale imbalance issue within each mini-batch. (3) In order to achieve the scalability requirement, we propose a parameter sharing and composition strategy to reduce model parameters from a product space to a sum space. (4) In order to achieve the convenience requirement, we propose an auto-masking strategy which can take mixed data from all the domains as input. It avoids the overhead caused by data partitioning, individual processing and separate storage. (5) We conduct both offline and online experiments to test the performance of MMN and several state-of-the-art methods for multi-type and multi-scenario CVR prediction.",
  "2 MASKED MULTI-DOMAIN NETWORK": "",
  "2.1 Multi-Type and Multi-Scenario CVR Prediction": "In omnimedia advertising platforms, different conversion types (e.g., purchase a product, sign up an account or fill out an online form) and different display scenarios (e.g., shown as a banner, interleaved with news or interleaved with micro-videos) highly impact the actual CVR. In the Criteo dataset (Table 1), the ground-truth CVRs vary significantly over different conversion types (e.g., from 6.3% to 18.9%) and over different display scenarios (e.g., from 0.9% to 14.9%). In our news feed advertising dataset (Table 1), the corresponding values vary from 0.05% to 27.6% and from 1.6% to 4.1% respectively. Denote the input instance as ğ‘¥ , which contains multiple features, such as user features, ad features, context features and cross features. Denote the conversion type of the ad as ğ‘“ ğ‘¡ ( ğ‘¥ ) and the display scenario of the ad as ğ‘“ ğ‘  ( ğ‘¥ ) . Further denote the click label as ğ‘¦ âˆˆ { 0 , 1 } and the conversion label as ğ‘§ âˆˆ { 0 , 1 } . Mathematically, the multi-type and multi-scenario CVR prediction problem is to estimate ğ‘ ( ğ‘§ = 1 | ğ‘¦ = 1 , ğ‘¥, ğ‘“ ğ‘¡ ( ğ‘¥ ) , ğ‘“ ğ‘  ( ğ‘¥ )) . It is a feature-rich problem.",
  "2.2 Modeling Adaptive Parameters": "Figure 2 illustrates the structure of the proposed MMN model. It consists a CTR tower and multiple CVR towers. The inclusion of a CTR tower is to utilize the available data in the entire sample space (i.e., not only click data but also impression data) [18]. For simplicity, all the CTR and CVR towers use the multi-layer perceptron (MLP) consisting of several fully connected layers. We treat each (conversion type, display scenario) pair as a domain and create a CVR tower for each domain. But we use a parameter sharing and composition strategy to reduce the number of domain-specific parameters from a product space to a sum space. At the same time, these parameters can well capture each domain's specific properties. Given an input instance ğ‘¥ , we denote the concatenated feature embedding vector after embedding lookup as x . We then generate the CVR prediction logit with ReLU activations [20] as  where ğœ½ is the set of model parameters, ğ‘“ ğ‘¡ ( ğ‘¥ ) is the conversion type and ğ‘“ ğ‘  ( ğ‘¥ ) is the display scenario of the input instance ğ‘¥ . One possible way to model ğœ½ GLYPH<0> ğ‘“ ğ‘¡ ( ğ‘¥ ) , ğ‘“ ğ‘  ( ğ‘¥ ) GLYPH<1> is to create a separate set of parameters for each (conversion type, display scenario) domain to make the model adaptive, as illustrated in Figure 3(a). Denote the number of conversion types as ğ‘ ğ‘¡ and the number of display scenarios as ğ‘ ğ‘  . It will result in ğ‘ ğ‘¡ Ã— ğ‘ ğ‘  sets of parameters, which makes the model too large to be trained. In order to 1) capture commonalities across different domains, 2) reflect specific natures of different conversion types and display scenarios and 3) reduce the number of parameters, we design a parameter sharing and composition strategy as follows (illustrated in Figure 3(b)). In particular, we design  Masked Multi-Domain Network CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Figure 2: Masked multi-domain network. For simplicity, all the CTR and CVR prediction towers use the multi-layer perceptron consisting of several fully connected layers. CTR tower CVR tower t 1 s 1 Shared Embedding Lookup Table Input features pCTR pCVRt1s1 mask t 1 s 1 1 0 1 0 0 0 0 0 0 1 0 0 x 1 : t 1 , s 1 , â€¦ x 2 : t 2 , s 1 , â€¦ x 3 : t 1 , s 1 , â€¦ x 4 : t 2 , s 2 , â€¦ CVR tower t 1 s 2 CVR tower t 2 s 1 CVR tower t 2 s 2 Auto-masking acc. to conversion type & display scenario Wb, W t1 , W t2 , W s1 , W s2 , ... 0 0 0 1 mask t 2 s 2 mask t 2 s 1 mask t 1 s 2 mask t 1 s 1 mask t 1 s 2 mask t 2 s 1 mask t 2 s 2 pCVR CVR tower parameters pCVRt1s2 pCVRt2s1 pCVRt2s2 masked pCVRt1s1 masked pCVRt2s2 pCTCVR CTR loss CTCVR loss emb. vector Parameter sharing & composition { W ğ‘™ ğ‘ } ğ‘™ is a set of common parameters that is shared by all the domains, in order to capture domain commonalities. ğ¿ is the number of fully connected layers. { W ğ‘™ ğ‘“ ğ‘¡ ( ğ‘¥ ) } ğ‘™ is a set of conversion type-specific parameters for ğ‘“ ğ‘¡ ( ğ‘¥ ) and { W ğ‘™ ğ‘“ ğ‘  ( ğ‘¥ ) } ğ‘™ is a set of display scenariospecific parameters for ğ‘“ ğ‘  ( ğ‘¥ ) . These parameters are then combined to form ğœ½ ( ğ‘“ ğ‘¡ ( ğ‘¥ ) , ğ‘“ ğ‘  ( ğ‘¥ )) . In this way, we avoid using samples that have both type ğ‘“ ğ‘¡ ( ğ‘¥ ) and scenario ğ‘“ ğ‘  ( ğ‘¥ ) to train a specific set of parameters as shown in Figure 3(a), and thus mitigate the data sparsity problem. Moreover, we only create ğ‘ ğ‘¡ + ğ‘ ğ‘  + 1 sets of domain-specific parameters, but can generate ğ‘ ğ‘¡ Ã— ğ‘ ğ‘  combinations to serve all different (conversion type, display scenario) domains. After we obtain the logit â„ , the predicted CVR is given by ğ‘ cvr ( ğ‘¥ ) = ğœ ( â„ ) , where ğœ is the sigmoid function.",
  "2.3 Auto-Masking": "The above design seems fine. However, the model needs to choose specific parameters for each instance according to its conversion type and display scenario. In modern deep learning systems such as Tensorflow [1], instances are processed in mini-batches with the same operators. As a result, how to process a mini-batch of instances simultaneously with the same operators but allow each instance to use different domain-specific parameters is still a problem. Previous works [4, 11, 21, 25] avoid this problem by splitting data into small per-domain datasets (Figure 3(a)). Accordingly, multiple input interfaces are created, one for each dataset. However, applying (b) Parameter sharing and composition; one dataset with auto-masking CVR tower t 1 s 1 CVR tower t 1 s 2 CVR tower t 2 s 1 CVR tower t 2 s 2 Wt1s1 CVR tower t Ms 1 CVR tower t Ms 2 Wt1s2 Wt2s1 Wt2s2 WtMs1 WtMs2 ... ... Parameters : per-domain parameters Data t1s1 ... Data : partitioned datasets Data t1s2 Data t2s1 Data t2s2 Data tMs1 Data tMs2 (a) Per-domain CVR tower parameter; per-domain dataset CVR tower t 1 s 1 CVR tower t 1 s 2 CVR tower t 2 s 1 CVR tower t 2 s 2 CVR tower t Ms 1 CVR tower t Ms 2 ... ... base parameters per-type parameters per-scenario parameters Wb Wt1 Wt2 WtM Ws1 Ws2 Parameters : One dataset Auto-masking Data : one dataset Figure 3: (a) Per-domain CVR tower parameter; per-domain dataset. (b) Parameter sharing and composition; one dataset with auto-masking. this approach to our problem will result in ğ‘ ğ‘¡ Ã— ğ‘ ğ‘  datasets, which incurs tremendous data processing, storage and maintenance cost. We propose the following auto-masking strategy to solve this problem (Figure 4). Denote a mini-batch of ğ‘ instances as ğ‘‹ = CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Wentao Ouyang et al. Figure 4: Illustration of how auto-masking can allow a mini-batch of instances from different (conversion type, display scenario) domains to be computed simultaneously with the same operators but different domain-specific parameters. Back propagation is fast because no gradient needs to be computed for masked entries (i.e., setting to 0). masked pCVRt1s1 pt1s1 (x 1 , t 1 , s 1 ) 0 pt1s1 (x 3 , t 1 , s 1 ) 0 pCVR pt1s1 (x 1 , t 1 , s 1 ) pt2s1 (x 2 , t 2 , s 1 ) pt1s1 (x 3 , t 1 , s 1 ) pt2s2 (x 4 , t 2 , s 2 ) masked pCVRt1s2 masked pCVRt2s1 masked pCVRt2s2 pCVRt1s1 Original pCVR per tower Auto- masking per tower Overall pCVR 0 0 0 0 0 pt2s1 (x 2 , t 2 , s 1 ) 0 0 0 0 0 pt2s2 (x 4 , t 2 , s s ) pt1s1 (x 1 , t 1 , s 1 ) pt1s1 (x 2 , t 2 , s 1 ) pt1s1 (x 3 , t 1 , s 1 ) pt1s1 (x 4 , t 2 , s 2 ) pCVRt1s2 pCVRt2s1 pCVRt2s2 pt1s2 (x 1 , t 1 , s 1 ) pt1s2 (x 2 , t 2 , s 1 ) pt1s2 (x 3 , t 1 , s 1 ) pt1s2 (x 4 , t 2 , s 2 ) pt2s1 (x 1 , t 1 , s 1 ) pt2s1 (x 2 , t 2 , s 1 ) pt2s1 (x 3 , t 1 , s 1 ) pt2s1 (x 4 , t 2 , s s ) pt2s2 (x 1 , t 1 , s 1 ) pt2s2 (x 2 , t 2 , s 1 ) pt2s2 (x 3 , t 1 , s 1 ) pt2s2 (x 4 , t 2 , s s ) âŠ— 1 0 1 0 âŠ— 0 0 0 0 âŠ— 0 1 0 0 âŠ— 0 0 0 1 maskt1s1 maskt1s2 maskt2s1 maskt2s2 x 1 , t 1 , s 1 x 2 , t 2 , s 1 x 3 , t 1 , s 1 x 4 , t 2 , s s A mini-batch of 4 insts. [ ğ‘¥ 1 , ğ‘¥ 2 , Â· Â· Â· , ğ‘¥ ğ‘ ] â€² , where â€² is the transpose operator. We create a mask vector m ğ‘¡ ğ‘– ğ‘  ğ‘— for each ( ğ‘¡ ğ‘– , ğ‘  ğ‘— ) domain, where ğ‘¡ ğ‘– is the ğ‘– th conversion type and ğ‘  ğ‘— is the ğ‘— th display scenario. The size of m ğ‘¡ ğ‘– ğ‘  ğ‘— equals the batch size ğ‘ . In particular, we compute m ğ‘¡ ğ‘– ğ‘  ğ‘— as By using the auto-masking strategy, we allow a mini-batch of instances from different (conversion type, display scenario) domains to be computed simultaneously with the same operators but adaptively with different domain-specific parameters.  where ğ¼ (Â·) is an indication function. We show an example in Figure 4, where we have 4 instances, ğ‘“ ğ‘¡ ( ğ‘‹ ) = [ ğ‘¡ 1 , ğ‘¡ 2 , ğ‘¡ 1 , ğ‘¡ 2 ] â€² and ğ‘“ ğ‘  ( ğ‘‹ ) = [ ğ‘  1 , ğ‘  1 , ğ‘  1 , ğ‘  2 ] â€² . We then have    The mini-batch of instances go through all the CVR towers. Denote the output (i.e., mini-batch prediction) of the CVR tower for ( ğ‘¡ ğ‘– , ğ‘  ğ‘— ) domain as p ğ‘¡ ğ‘– ğ‘  ğ‘— ( ğ‘‹ ) . Clearly, ğ‘‹ may contain instances that do not match ( ğ‘¡ ğ‘– , ğ‘  ğ‘— ) and these results are incorrect. For example, the mini-batch CVR prediction of tower ( ğ‘¡ 1 , ğ‘  1 ) in Figure 4 is  where gray color indicates incorrect prediction because of parameter mismatch. We obtain the correct mini-batch CVR prediction p cvr ( ğ‘‹ ) by combining the masked outputs of all different towers  = [ ğ‘ ğ‘¡ 1 ğ‘  1 ( ğ‘¥ 1 , ğ‘¡ 1 , ğ‘  1 ) , 0 , ğ‘ ğ‘¡ 1 ğ‘  1 ( ğ‘¥ 3 , ğ‘¡ 1 , ğ‘  1 ) , 0 ] â€² + [ 0 , 0 , 0 , 0 ] â€² + [ 0 , ğ‘ ğ‘¡ 2 ğ‘  1 ( ğ‘¥ 2 , ğ‘¡ 2 , ğ‘  1 ) , 0 , 0 ] â€² + [ 0 , 0 , 0 , ğ‘ ğ‘¡ 2 ğ‘  2 ( ğ‘¥ 4 , ğ‘¡ 2 , ğ‘  2 )] â€² = [ ğ‘ ğ‘¡ 1 ğ‘  1 ( ğ‘¥ 1 , ğ‘¡ 1 , ğ‘  1 ) , ğ‘ ğ‘¡ 2 ğ‘  1 ( ğ‘¥ 2 , ğ‘¡ 2 , ğ‘  1 ) , ğ‘ ğ‘¡ 1 ğ‘  1 ( ğ‘¥ 3 , ğ‘¡ 1 , ğ‘  1 ) , ğ‘ ğ‘¡ 2 ğ‘  2 ( ğ‘¥ 4 , ğ‘¡ 2 , ğ‘  2 )] â€² , where âŠ™ is element-wise product.",
  "2.4 Training with Dynamically Weighted Loss": "The training of MMN is much faster than that of multi-task learning models such as MMoE [17], because each entry of p cvr ( ğ‘‹ ) only impacts the parameters of one tower rather than all the towers. No gradient needs to be computed during gradient back propagation for masked entries (i.e., setting to 0). The training of the model may use the classical loss of ESMM [18], which combines a CTR loss and a CTCVR loss as ğ‘™ğ‘œğ‘ ğ‘  = ğ‘™ğ‘œğ‘ ğ‘  ctr + ğ›¼ğ‘™ğ‘œğ‘ ğ‘  ctcvr , where ğ›¼ is a balancing parameter and  ctr ( ğ‘¥ ğ‘› ) = -{ ğ‘¦ ğ‘› log [ ğ‘ ctr ( ğ‘¥ ğ‘› )] + ( 1 -ğ‘¦ ğ‘› ) log [ 1 -ğ‘ ctr ( ğ‘¥ ğ‘› )]} ,  ğ‘ is the batch size, ğ‘¥ ğ‘› is the ğ‘› th instance, ğ‘¦ ğ‘› is the click label, ğ‘§ ğ‘› is the conversion label, and ğ‘™ ctr , ğ‘™ ctcvr are cross-entropy losses. Further examination on the loss reveals that it is not quite suitable for MMN. Denote the number of CVR domains as ğ¶ , the ğ‘ th domain as D ğ‘ and the number of instances that fall in the ğ‘ th domain in a mini-batch as ğ‘ ğ‘ . We further denote the expectation of Masked Multi-Domain Network CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Table 1: Statistics of experimental datasets. the loss on each instance as Â¯ ğ‘ , i.e., ğ¸ [ ğ‘™ ctcvr ( ğ‘¥ ğ‘› )] = Â¯ ğ‘ . We then have  That is, the average loss applied on each domain is actually ğ‘ ğ‘ ğ‘ Â¯ ğ‘ â‰¤ Â¯ ğ‘ and varies from one domain to another. For each domain, we actually do not average over the instances corresponding to it but use an incorrect normalization value ğ‘ . Therefore, we propose the dynamically weighted loss as   where ğ‘¤ğ‘”ğ‘¡ ( ğ‘¥ ğ‘› ) = ğ‘ ğ‘ ğ‘ if ğ‘¥ ğ‘› âˆˆ D ğ‘ . Note that the weight ğ‘¤ğ‘”ğ‘¡ ( ğ‘¥ ğ‘› ) is not pre-computed before training, but dynamically computed within each mini-batch . This is why wecall the loss as dynamically weighted loss. In other words, even if two instances belong to the same domain, their weights could be different if they appear in different mini-batches because the number ğ‘ ğ‘ of samples in that domain could vary in different mini-batches.",
  "2.5 Fast Online Prediction": "As the online prediction is on demand, there is no need to let an online request go through all the CVR towers and use auto-masking to get the correct prediction. Given an online request ğ‘¥ with the corresponding conversion type ğ‘“ ğ‘¡ ( ğ‘¥ ) and display scenario ğ‘“ ğ‘  ( ğ‘¥ ) , we simply let ğ‘¥ go through the tower for ( ğ‘“ ğ‘¡ ( ğ‘¥ ) , ğ‘“ ğ‘  ( ğ‘¥ )) to calculate a single predicted CVR, i.e., until a colored node in Figure 2(a). This makes the online prediction of MMN fast.",
  "3 EXPERIMENTS": "",
  "3.1 Datasets": "The statistics of the datasets are listed in Table 1. (1) News feed conversion log dataset. This dataset contains a random sample of 14 days of ad impression, click and conversion logs from an industrial news feed advertising system for UC Toutiao. It contains ğ‘ ğ‘¡ = 19 conversion types and ğ‘ ğ‘  = 8 display scenarios. (2) Criteo conversion log dataset 1 . This dataset contains a sample of 90 days of Criteo live traffic data [26]. There are ğ‘ ğ‘¡ = 21 conversion types and ğ‘ ğ‘  = 17 display scenarios. All the features are hashed. Each training set is further split into an initial training set (70%) and a validation set (30%) to find the optimal hyperparameters.",
  "3.2 Methods in Comparison": "We compare three categories of methods. A. Single-task methods (1) DNN . Deep Neural Network [8]. It contains an embedding layer, several fully connected layers and an output layer. (2) ESMM . Entire Space Multi-task Model [18]. It models both CVR and CTR prediction tasks. B. Multi-task methods (3) MMoE . Multi-gate Mixture-of-Experts model [17]. It substitutes the shared bottom network with an MoE layer. Each task has a separate gating network. (4) PLE . Progressive Layered Extraction model [27]. It separates shared components and task-specific components explicitly and adopts a progressive routing mechanism. C. Multi-domain methods (5) MT-FwFM . Multi-Task Field-weighted Factorization Machine [22]. As it models type-specific parameters for multitype CVR prediction, we classify it as a multi-domain method. (6) STAR . Star Topology Adaptive Recommender [25]. It is proposed for CTR prediction over multiple business domains such as 'Banner' and 'Guess What You Like'. (7) MMN . Masked Multi-domain Network in this paper. Parameter Settings: We set the dimension of the embedding vectors for features as 10. For neural network-based models, we set the number of fully connected layers in a CTR / CVR tower as ğ¿ = 4 with units {512, 256, 128, 128}. All the methods are implemented in Tensorflow [1] and optimized by the Adagrad algorithm [10]. We run each method 3 times and report the average results.",
  "3.3 Evaluation Metrics": "Accuracy: The Area Under the ROC Curve (AUC) is a widely used metric for CVR prediction [22]. The larger the better. We calculate fine-grained metrics as per conversion type AUC and per display scenario AUC. In addition, we also calculate the average AUC over all the conversion types and display scenarios. Scalability: We use the number of sets of domain-specific parameters to evaluate the scalability. Convenience: We use the number of separate datasets needed to train the model to evaluate the convenience. 1 https://ailab.criteo.com/criteo-sponsored-search-conversion-log-dataset/ CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Wentao Ouyang et al. Table 2: Test AUCs on the News Feed ad dataset. The best result is in bold font. Randomly selected conversion types / display scenarios that span the training CVR spectrum are shown. (type / scen.): a method optimizes only the type / scenario dimension because of the out-of-memory issue. A small improvement in AUC (e.g., 0.0020) can lead to a significant increase in online CVR. Table 3: Test AUCs on the Criteo ad dataset. The best result is in bold font.",
  "3.4 Offline Performance": "3.4.1 Accuracy. Table 2 and Table 3 list the test AUCs. As multitask methods are not quite suitable for our problem setting, MMoE and PLE perform even worse than single-task methods such as DNN and ESMM on the Criteo dataset. On the News Feed dataset, their performance is comparable. Multi-domain method STAR outperforms single-task methods such as DNN and ESMM. MMN achieves the highest AUC in most cases. It is because MMN has the ability to tackle both conversion type and display scenario simultaneously by design. In contrast, other multi-task / multi-domain methods can either optimize on the flattened type Ã— scenario space with significant data sparsity issue, or optimize only one dimension because of the out-of-memory issue. 3.4.3 Convenience. We use the number of separate datasets needed to train the model to evaluate the convenience. Single-task / multi-task methods use only one dataset and they are convenient. With the help of auto-masking, MMN also uses only one dataset. However, other multi-domain methods need to create a separate dataset for each domain and results in ğ‘ ğ‘¡ Ã— ğ‘ ğ‘  sub datasets. Compared with these methods, MMN reduces the number of separate datasets from 357 to 1 (i.e., 99.7% reduction) in the Criteo dataset and from 152 to 1 (i.e., 99.3% reduction) in the News Feed dataset. 3.4.2 Scalability. We use the number of sets of domain-specific parameters to evaluate the scalability. Single-task methods do not model domain-specific parameters and do not have the scalability issue. MMN models ğ‘ ğ‘¡ + ğ‘ ğ‘  + 1 sets while other multi-task / multidomain methods model ğ‘ ğ‘¡ Ã— ğ‘ ğ‘  sets of domain-specific parameters. Compared with these methods, MMN reduces the number of sets of domain-specific parameters from 357 to 39 (i.e., 89.1% reduction) in the Criteo dataset and from 152 to 28 (i.e., 81.6% reduction) in the News Feed dataset. 3.4.4 Ablation Study: effect of type-specific and scenariospecific parameters. Figure 5 plots the AUC difference between MMNand MMN (common params.). In MMN (common params.), we only use shared common parameters { W ğ‘™ ğ‘ } ğ‘™ for all the domains. In other words, no type-specific and scenario-specific parameters are learned. It is observed that MMN outperforms MMN (common params.) in most cases. These results show that different conversion types and display scenarios do have their own properties and using the same parameter set cannot well capture individual properties. 3.4.5 Ablation Study: effect of dynamically weighted loss. Figure 6 plots the AUC difference between MMN and MMN (no dynamic weight). In the latter, we use the loss in Eq.(2) for model Masked Multi-Domain Network CIKM '23, October 21-25, 2023, Birmingham, United Kingdom (a) News Feed: AUC(MMN) - AUC GLYPH<0> MMN(common params.) GLYPH<1> 0.070 0.060 1 0.040 0.020 g.88g S1 52 53 54 0.015 1 0.010 0.005 0.000 ~0.005 S1 52 53 S5 (b) Criteo: AUC(MMN) - AUC GLYPH<0> MMN(common params.) GLYPH<1> Figure 5: Effect of domain-specific parameters. training. It is observed that AUCs of MMN improve a lot compared with MMN (no dynamic weight) in most cases. These results validate the effectiveness of the dynamically weighted loss.",
  "3.5 Online Deployment": "We deployed MMN in an industrial news feed advertising system for UC Toutiao which serves hundreds of millions of traffic every day. MMN is trained using a distributed CPU cluster which contains 8 servers each with 96 CPU core units and 512 GB RAM. By design, MMN can satisfy the scalability and convenience requirements. In practice, we do not encounter the out-of-memory issue when training MMN but when training STAR on the type Ã— scenario space. Moreover, MMN needs only one dataset to train the model, and it significantly reduces the data processing overhead. We conducted online experiments in an A/B test framework over 14 days in August 2022, where the base serving model is ESMM. By design, the online prediction time of MMN is short. In practice, the prediction time of MMNis 2.3 ms, which is only slightly longer than that of ESMM, which is 2.2 ms. MMN leads to clear online CVR increases in all the 19 conversion types and the 8 display scenarios, where the increases range from 3.2% to 12.6%. Moreover, the CPM (revenue) is increased by 5.1%. After the A/B test, MMN provides real-time CVR prediction for all ad traffic in our system.",
  "4 RELATED WORK": "CVR prediction. The task of CVR prediction [6, 13, 15, 23] in online advertising is to estimate the probability of a user makes a conversion event on a specific ad after a click event. [13] estimates CVR based on past performance observations along data hierarchies. [7] proposes an LR model for CVR prediction and [2] proposes a log-linear model. [18] proposes the ESMM model to S6 Figure 6: Effect of dynamically weighted loss. (a) News Feed: AUC(MMN) - AUC GLYPH<0> MMN(no dynamic weight) GLYPH<1> (b) Criteo: AUC(MMN) - AUC GLYPH<0> MMN(no dynamic weight) GLYPH<1> 0.070 0.060 1 0.040 0.020 52 54 0.015 0.010 1 0.005 0.000 ~0.005 t1 52 54 S5 S6 jointly consider CVR and CTR prediction tasks and exploit data in the entire sample space. ESM 2 [29], GMCM [3] and HM 3 [28] exploit additional purchase-related behaviors after click (e.g., favorite, add to cart and read reviews) for CVR prediction. But such behaviors are not always available in different advertising systems. Multi-task/multi-domain learning. In multi-task learning [14, 19, 24], the domains are the same and the tasks are different. In multi-domain learning [9, 12, 30], the domains are different and the tasks are the same. Because of such differences, existing multi-task learning models such as Shared-Bottom, OMoE, MMoE, CGC and PLE[16, 17, 27, 31] cannot well address our problem. MT-FwFM [22], STAR [25] and ADIN [11] are multi-domain models that are more closely related to our work. However, they cannot address both different conversion types and different display scenarios. Moreover, they are not designed to satisfy the scalability and convenience requirements for our problem.",
  "5 CONCLUSION": "In this paper, we present the design and evaluation of masked multi-domain network (MMN) to address the multi-type and multiscenario CVR prediction problem in a real-world advertising system. We design various strategies such as modeling domain-specific parameters, parameter sharing and composition, auto-masking and dynamically weighted loss in order that MMN can simultaneously satisfy the accuracy, scalability and convenience requirements as an industrial model. Experimental results show that MMN does achieve these requirements and it outperforms several state-of-theart models for multi-type and multi-scenario CVR prediction.",
  "REFERENCES": "[1] MartÃ­n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Wentao Ouyang et al. 2016. Tensorflow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI) . USENIX, 265-283. [2] Deepak Agarwal, Rahul Agrawal, Rajiv Khanna, and Nagaraj Kota. 2010. Estimating rates of rare events with multiple hierarchies through scalable log-linear models. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) . 213-222. [3] Wentian Bao, Hong Wen, Sha Li, Xiao-Yang Liu, Quan Lin, and Keping Yang. 2020. GMCM:Graph-based micro-behavior conversion model for post-click conversion rate estimation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . 2201-2210. [4] Jiangxia Cao, Shaoshuai Li, Bowen Yu, Xiaobo Guo, Tingwen Liu, and Bin Wang. 2023. Towards Universal Cross-Domain Recommendation. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining (WSDM) . 78-86. [5] Rich Caruana. 1997. Multitask learning. Machine learning 28, 1 (1997), 41-75. [6] Olivier Chapelle. 2014. Modeling delayed feedback in display advertising. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) . 1097-1105. [7] Olivier Chapelle, Eren Manavoglu, and Romer Rosales. 2014. Simple and scalable response prediction for display advertising. ACM Transactions on Intelligent Systems and Technology (TIST) 5, 4 (2014), 1-34. [8] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS) . ACM, 7-10. [9] Mark Dredze, Alex Kulesza, and Koby Crammer. 2010. Multi-domain learning by confidence-weighted parameter combination. Machine Learning 79, 1 (2010), 123-149. [10] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research 12, Jul (2011), 2121-2159. [11] Yuchen Jiang, Qi Li, Han Zhu, Jinbei Yu, Jin Li, Ziru Xu, Huihui Dong, and Bo Zheng. 2022. Adaptive Domain Interest Network for Multi-domain Recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (CIKM) . 3212-3221. [12] Mahesh Joshi, Mark Dredze, William Cohen, and Carolyn Rose. 2012. Multidomain learning: when do domains matter?. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning . 1302-1312. [13] Kuang-chih Lee, Burkay Orten, Ali Dasdan, and Wentong Li. 2012. Estimating conversion rate in display advertising from past erformance data. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) . 768-776. [14] Ziru Liu, Jiejie Tian, Qingpeng Cai, Xiangyu Zhao, Jingtong Gao, Shuchang Liu, Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, et al. 2023. Multi-Task Recommendations with Reinforcement Learning. In Proceedings of the ACM Web Conference 2023 (WWW) . 1273-1282. [15] Quan Lu, Shengjun Pan, Liang Wang, Junwei Pan, Fengdan Wan, and Hongxia Yang. 2017. A practical framework of conversion rate prediction for online display advertising. In Proceedings of the International Workshop on Data Mining for Online Advertising (ADKDD) . 1-9. [16] Jiaqi Ma, Zhe Zhao, Jilin Chen, Ang Li, Lichan Hong, and Ed H Chi. 2019. SNR: Sub-network routing for flexible parameter sharing in multi-task learning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 216-223. [17] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD International Conference on [32] Han Zhu, Junqi Jin, Chang Tan, Fei Pan, Yifan Zeng, Han Li, and Kun Gai. 2017. Optimized cost per click in taobao display advertising. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) . 2191-2200.",
  "keywords_parsed": [
    "Online advertising",
    "CVR prediction",
    "Multiple conversion types",
    "Multiple display scenarios"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Tensorflow: A system for large-scale machine learning"
    },
    {
      "ref_id": "b2",
      "title": "Estimating rates of rare events with multiple hierarchies through scalable log-linear models"
    },
    {
      "ref_id": "b3",
      "title": "GMCM:Graph-based micro-behavior conversion model for post-click conversion rate estimation"
    },
    {
      "ref_id": "b4",
      "title": "Towards Universal Cross-Domain Recommendation"
    },
    {
      "ref_id": "b5",
      "title": "Multitask learning"
    },
    {
      "ref_id": "b6",
      "title": "Modeling delayed feedback in display advertising"
    },
    {
      "ref_id": "b7",
      "title": "Simple and scalable response prediction for display advertising"
    },
    {
      "ref_id": "b8",
      "title": "Wide & deep learning for recommender systems"
    },
    {
      "ref_id": "b9",
      "title": "Multi-domain learning by confidence-weighted parameter combination"
    },
    {
      "ref_id": "b10",
      "title": "Adaptive subgradient methods for online learning and stochastic optimization"
    },
    {
      "ref_id": "b11",
      "title": "Adaptive Domain Interest Network for Multi-domain Recommendation"
    },
    {
      "ref_id": "b12",
      "title": "Multidomain learning: when do domains matter?"
    },
    {
      "ref_id": "b13",
      "title": "Estimating conversion rate in display advertising from past performance data"
    },
    {
      "ref_id": "b14",
      "title": "Multi-Task Recommendations with Reinforcement Learning"
    },
    {
      "ref_id": "b15",
      "title": "A practical framework of conversion rate prediction for online display advertising"
    },
    {
      "ref_id": "b16",
      "title": "SNR: Sub-network routing for flexible parameter sharing in multi-task learning"
    },
    {
      "ref_id": "b17",
      "title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts"
    },
    {
      "ref_id": "b32",
      "title": "Optimized cost per click in taobao display advertising"
    }
  ]
}