{
  "Assessing enactment of content regulation policies: A post hoc crowd-sourced audit of election misinformation on YouTube": "Prerna Juneja University of Washington Seattle, Washington, USA prerna79@uw.edu Md Momen Bhuiyan Virginia Tech Blacksburg, Virginia, USA momen@vt.edu Tanushree Mitra University of Washington Seattle, Washington, USA tmitra@uw.edu",
  "ABSTRACT": "With the 2022 US midterm elections approaching, conspiratorial claims about the 2020 presidential elections continue to threaten users' trust in the electoral process. To regulate election misinformation, YouTube introduced policies to remove such content from its searches and recommendations. In this paper, we conduct a 9-day crowd-sourced audit on YouTube to assess the extent of enactment of such policies. We recruited 99 users who installed a browser extension that enabled us to collect up-next recommendation trails and search results for 45 videos and 88 search queries about the 2020 elections. We find that YouTube's search results, irrespective of search query bias, contain more videos that oppose rather than support election misinformation. However, watching misinformative election videos still lead users to a small number of misinformative videos in the up-next trails. Our results imply that while YouTube largely seems successful in regulating election misinformation, there is still room for improvement.",
  "CCS CONCEPTS": "· Information systems → Websearch engines ; Webcrawling ; Personalization ; Content ranking ; · Human-centered computing → Human computer interaction (HCI) .",
  "KEYWORDS": "misinformation, elections, voter fraud, algorithm audit, fairness, recommendations",
  "ACMReference Format:": "Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra. 2023. Assessing enactment of content regulation policies: A post hoc crowd-sourced audit of election misinformation on YouTube . In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23), April 2328, 2023, Hamburg, Germany. ACM, New York, NY, USA, 22 pages. https: //doi.org/10.1145/3544548.3580846",
  "1 INTRODUCTION": "'Oregon GOP frontrunner for governor embraces claims of election fraud... said he doubted Oregon's vote-by-mail system'-The Texas Tribune, Feb 11, 2022 [62] 'Election Deniers Go Door-to-Door to Confront Voters After Losses (in US primaries)'-Bloomberg, Aug 23 2022 [7] Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). CHI '23, April 23-28, 2023, Hamburg, Germany © 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9421-5/23/04. https://doi.org/10.1145/3544548.3580846 'With 10 weeks until midterms, election deniers are hampering some election preparations Some election deniers have 'weaponized' against us, one election official says.'-ABC News, Aug 30, 2022 [64] Skepticism around the legitimacy of the US electoral process, which primarily gained momentum during the 2020 US presidential election, had serious ramifications. For example, endorsement of election conspiracy theories was found to be positively associated with lower turnout in the 2021 US Senate election in Georgia [31]. In 2022, the false narratives around the 2020 elections still persist [46, 48] and continue to threaten democratic participation in the upcoming US midterm elections [46, 48]. In the last two years, 19 US states altered voting procedures and enacted laws to make voting more restrictive, creating information gaps and fresh opportunities for election misinformation to emerge and proliferate in the real and online world [48]. Thus, battling election misinformation has never been more important. Studies show that social media platforms have become important mediums for political discourse [2, 74]. In particular, YouTube-the most popular platform among US adults [54]-has emerged as a political battleground as demonstrated by the fact that both political parties extensively used the platform for election campaigning [69]. However, the platform came under fire from technology critics for being a hub of electoral conspiracy theories [40, 75]. Given the concern that search engines can play a significant role in shifting voting decisions [24, 25] and can confine users into a filter bubble of misinformation [34], there has been a push for online platforms to enact policies that minimize election misinformation [60]. In response to this push, YouTube introduced content policies to remove videos spreading election-related falsehoods and claimed that misinformative videos would not prominently surface or get recommended on the platform [45, 65, 68, 83]. However, the formulation of policies does not equate to effective enactment. It's evident from the results of two misinformation audits conducted on the platform for the same conspiratorial topics (such as vaccine controversies, and 9/11 conspiracies), first in 2019 [34] and second in 2021 [70], both of which found echo chambers of misinformation on the platform. Despite changes to YouTube's misinformation policies in 2020 [67], the authors of the second audit study did not find improvements when compared to the results of the first audit, rather they found recommendations worsening for topics like vaccination. These findings iterate the need to continuously audit platforms to investigate how a platform's algorithms fare with respect to problematic content and how effectively a platform's content policies are implemented [63]. While multiple studies have audited YouTube for misinformation [34, 53, 70], these were mostly conducted using sock-puppets (bot accounts emulating real users) CHI '23, April 23-28, 2023, Hamburg, Germany Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra in conservative settings 1 which often do not reflect true user behavior. There is a dearth of crowd-sourced misinformation audits that test the algorithms' behavior with real-world users ([9] is one of the few exceptions). In this paper, we fill this gap by conducting a large-scale crowd-sourced audit on YouTube to determine how effectively YouTube has regulated its algorithms-search and recommendation-for election misinformation. To conduct the audit, we recruited 99 participants who filled out a survey and installed TubeCapture , a browser extension built to collect users' YouTube search results, and recommendations. The extension conducted searches for 88 search queries related to the 2020 US presidential elections. We also seeded TubeCapture with 45 seed videos with three differing stances on election misinformationsupporting, neutral, and opposing. The extension collected up-next recommendation trails-five consecutive up-next recommendation videos-for each seed video. TubeCapture simultaneously collected YouTube components from both personalized standard and unpersonalized incognito windows allowing us to measure the extent of personalization. This leads us to our first research question: RQ1 Extent of personalization: What is the extent of personalization in various YouTube components? RQ1a: How much are search results personalized for search queries about the 2020 US presidential elections and the surrounding voter fraud claims? RQ1b: How much are YouTube's up-next recommendation trails personalized for seed videos with different stances on election misinformation-supporting, neutral and opposing? We find that while search results have very little personalization, up-next trails are highly personalized. We next venture into determining the amount of election misinformation real users could be exposed to under different conditions, such as following up-next trails for videos supporting or opposing election misinformation. RQ2: Amount of election misinformation: What is the impact of watching a sequence of YouTube up-next recommendation videos starting with seed videos with different stances on election misinformation (supporting, neutral, and opposing) on various YouTube components? RQ2a: How much do search results get contaminated with election misinformation? RQ2b: What is the amount of misinformation returned in users' up-next recommendation trails? RQ2c: What is the amount of misinformation that appears in users' homepage video recommendations? Wefind that YouTube presents debunking videos in search results for most of the queries. We also observe an echo chamber effect in recommendations where trails with supporting seeds contain more misinformation than trails with neutral and opposing seeds. Since election misinformation is closely entangled with political beliefs with several right-leaning news sources amplifying the claims of voter fraud [12, 49], we also study the diversity and composition of the content presented by YouTube in its various components. We ask, 1 For example, sock-puppet building account history by watching videos that only promote misinformation. RQ3: Impact on composition and diversity: What is the impact on content diversity when watching a sequence of YouTube up-next recommendation videos starting with seed videos with different stances on election misinformation (supporting, neutral, and opposing)? RQ3a: How diverse are the search results ? RQ3b: How diverse are the up-next recommendation trails? We find that YouTube ensures source diversity in its search results. We also find a large number of impressions for left-leaning late-night shows (e.g. Last Week Tonight with John Oliver) and right-leaning Fox news in users' up-next trails. Overall, our work makes the following contributions: · We conduct a post hoc audit on YouTube to determine how its algorithms fare with respect to election misinformation; post hoc auditing comprises investigating a platform for a past topic or event which could have a significant impact on citizenry in the present and future. In turn, we are able to test the effectiveness of YouTube's content policies enforced to curb election misinformation. · We extend prior work on misinformation audits by conducting an ethical crowd-sourced audit to see the impact of performing certain actions on the searches and recommendations of real-world people with complex platform histories instead of conservative settings of sock puppet audits. · Our audit reveals that YouTube search results contain more videos that oppose election misinformation as compared to videos supporting election misinformation, especially for search queries about election fraud in presidential elections. However, a filter bubble effect still persists in the up-next recommendation trails, where a small number of misinformative videos are presented to users watching videos supporting election misinformation.",
  "2 RELATED WORK": "",
  "2.1 Algorithmic audits": "Search engines and social media platforms act as information gatekeepers, with their algorithmically generated feed, timeline, and recommendations affecting the information exposure of people. Given the ubiquitousness of the algorithms and the influence they hold over the citizenry, scholars have emphasized the need for auditing online platforms, i.e., conducting a systematic investigation to determine whether the algorithmic output is aligned with 'laws and regulations, societal values, ethical desiderata, or industry standards' [61]. As a result, several research studies have audited algorithmic systems for distortion (e.g. hyper-personalization [4], ideological skew [6, 42, 71] ), discrimination (e.g. racial and gender discrimination [3, 13, 43]), exploitation (e.g. exploiting users' private and sensitive information [15, 20]) and misjudgment (e.g. incorrect algorithmic predictions [23, 39]) [4]. These scholarly studies have used a myriad of audit research methods, including code audits, scraping audits, sock puppet audits, and crowd-sourced audits (see [58] for a review). Among them, sock puppet auditing, where researchers create bots or fake user accounts to impersonate real-life users is the most popular since this audit method gives researchers the greatest control over experimental variables [79] and doesn't require high participant recruitment cost like in the A post hoc crowd-sourced audit of election misinformation on YouTube CHI '23, April 23-28, 2023, Hamburg, Germany case of crowd-sourced auditing [58]. Thus, several past studies have employed this audit method [3, 5, 6, 34, 38, 71]. However, in sockpuppet auditing, the bot histories are built in very conservative settings that do not emulate real-world users' complex account histories [38]. Thus, as an alternative, scholars have collected and audited algorithmic outputs from real-world users to study and identify problematic algorithmic behaviors in users' naturalistic settings [5, 9, 56, 57, 72]. We add to the existing crowd-sourced audit studies by conducting a crowd-sourced audit of YouTube to measure the amount of election misinformation in the searches, and recommendations of real-world users. In our study, we use a list of pre-selected videos and search queries to collect data from users' YouTube accounts to test whether users' existing account histories could lead them to misinformative content on the platform. In the next section, we present the audits conducted specifically on YouTube and discuss how our work adds to the growing literature on platform audits.",
  "2.2 Auditing YouTube for problematic content": "Given the popularity of YouTube and the criticism the platform has faced for not regulating problematic content, several scholarly studies have audited YouTube's search and recommendation algorithms for the prevalence of misinformation, extremism, and echo chambers of problematic content. Sock puppet audits on YouTube revealed that while the platform's channel recommendations radicalize users by recommending extreme channels [55], video recommendations drive users away from radical content by recommending videos from mainstream news channels [35]. A crowdsourced audit further revealed that real users with high prior levels of racial resentment get more exposure to extremist content since they typically subscribe to extremist channels [17]. In another line of inquiry, several studies audited YouTube for conspiracy theories [26, 34, 53, 59]. Notably, first such audit on YouTube was conducted by Hussein et al [34]. This audit revealed the prevalence of echo chambers of misinformation in YouTube's top-5 video recommendations for topics such as the moon landing, 9/11 conspiracies, etc. [34]. Recently, Tomlein et. al re-conducted the audit performed by Hussein and Juneja et al [34] and found that video recommendations for topics like 9/11 conspiracies have worsened on the platform [70]. Another study (conducted in the fall of 2020), closest to this work collected real-world YouTube recommendations for election fraud videos by asking users to manually click on recommendations following certain traversal rules [9]. The study aimed at proving that users skeptical about the legitimacy of elections receive more voter fraud videos in their recommendations. On the other hand, we audit YouTube's searches, homepages, and default algorithmic pathway (up-next videos that are auto-played by the platform) of users with different political leanings and investigate how its algorithm fares under different conditions (watching videos of different stances) for the same individual. Additionally, we conduct the audit two years after the presidential election event. Post hoc auditing of the platform allows us to determine how well the platform has enacted its content policies and regulated harmful content.",
  "3 METHODOLOGY": "",
  "3.1 Developing search queries to measure election fraud based misinformation": "The first methodological step in any algorithmic audit is to determine a viable set of relevant search queries that would be used to probe the algorithmic system. For our study, we identified search queries that satisfy two properties. First, we select high-impact search queries that were used by people to search about Presidential Election as well as the voter fraud claims about the 2020 elections. Second, we curate search queries that have a high probability of returning misinformative results which would result in meaningful measurements of algorithmically curated misinformation about the audit topic. To compile such queries, we used Google Trends and YouTube video tags (refer Figure 1). 3.1.1 Curating high-impact queries via Google Trends. First, we leveraged Google Trends which contain Google's daily and realtime search trends data. As the most popular search service, its trends are a good indicator for understanding the real-world search behavior of a large number of people. Using Election Fraud 2020 and Presidential Election as search topics, United States as location, April 2020 to Present as date range, and search service as YouTube search, we extracted the top 15 most and least popular search queries that people used on YouTube. We choose April 7 as the start date since this was the day when Donald Trump made one of his first fraudulent claims about the security of mail-in ballots [36]. We included the most popular queries since they represent the ones that people mostly use to get information on elections. To explore the data-voids [30] associated with our audit topic, we also included the least popular search queries to determine if those terms have been hijacked by conspiracists to surface misinformation. 3.1.2 Curating misinfo-queries queries using YouTube video tags. Second, we used YouTube video tags that content creators associated with misinformative videos while uploading them on the YouTube platform (see Figure 2 for an example). These tags could be thought of as search words representing how content creators would like their videos to be discovered. To extract video tags associated with election misinformation videos, we leveraged a largescale Voter Fraud 2020 dataset released by Abilov et al [1]. The dataset contains over 12,002 YouTube video URLs that were shared on Twitter by accounts that tend to refute and promote voter fraud claims. We extracted YouTube video tags associated with videos shared by accounts promoting voter fraud claims to probe YouTube (n=200K). To curate a viable number of search queries from the extracted video tags, we employed several steps. First, we manually curated a list of 10 keywords related to elections and fraudulent claims surrounding the elections 2 from the list of keywords provided by Abilov et al [1] as well election 2020 misinformation report produced by the Election Integrity Partnership [27]. Then for each of the keywords, we extracted 15 top and 15 least occurring video tags containing that term. For example, one of the most occurring tags containing keyword whistleblower was 'usps whistleblower' while the least occurring tag was 'whistleblower jesse morgan'. 2 steal, fraud, ballot, elect, seal, dominion, sharpiegate, whistleblower, harvest, and sunrise zoom CHI '23, April 23-28, 2023, Hamburg, Germany Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra Figure 1: Figure illustrating our method to curate search queries for our audit experiment Extract high impact queries from Google Trends Extract tags from YouTube videos shared by users promoting voter fraud claims on Twitter Extract relevant tags using keyword matching Combine search queries Filter search queries Final set of search queries Figure 2: List of video tags associated with YouTube video titled Is Voter Fraud Real? (video id: RkLuXvIxFew ) that promotes voter fraud misinformation. Video tags are added by content creators while uploading YouTube videos on the platform. The tags can be extracted from videos via YouTube APIs or third-party tools. We use tags associated with videos shared by users promoting voter fraud claims on Twitter as search queries in our audit experiments. threat to democracy election meddling election tampering ballot harvesting non-citizen voters voter fraud 3.1.3 Filtering search queries to obtain the final set. We combined search queries obtained from both Google Trends and YouTube video tags in our final query set and employed several filtering steps to obtain a reasonable number of relevant search queries. First, we only kept queries related to the election 2020, for example, we kept 'election fraud 2020' and removed 'election fraud 2016'. We removed duplicate and redundant search queries and replaced them with a single randomly selected query. For example, we replaced queries 'voter fraud 2020', 'voter fraud', and 'vote fraud' with 'voter fraud 2020'. We removed queries with lengths greater than five since they were overly specific (e.g. 'we've got pictures of the check stubs paid to people to ballot harvest'). We also removed queries containing names of news channels, news anchors, and presidential candidates because they were too generic and not directly related to the audit topic. However, we kept the search queries where the names of the presidential candidates were together with the election or election fraud-related terms (e.g. 'Joe Biden voter fraud'). We also removed search queries that were in languages other than English. Finally, we had 88 search queries in total. Table 1 presents a sample.",
  "3.2 Determining popular seed videos to collect up-next video trails": "The second step of our audit experiment is to curate YouTube videos that would act as seed videos to collect the up-next video recommendation trails. We again leveraged Abilov et al's YouTube video dataset [1]. Recall, the authors identified clusters of Twitter users who either shared tweets promoting or detracting from voter fraud presidential election 2020 us elections 2020 latest news election fraud 2020 rigged election dominion voting exposed mail in ballots 2020 stop the steal joe biden voter fraud usps whistleblower voter fraud evidence trump biden general election dominion voter fraud Table 1: Sample search queries for our YouTube audit claims and released the YouTube videos related to election fraud 2020 shared by those users. At the the time of analysis, out of the ∼ 12K videos present in the dataset, 8.9K were present on YouTube. The remaining videos were either removed or made private. Out of the videos that were still present, 1K videos were shared by users in the detractor cluster, 6.5K videos were shared by users in the promoting cluster, and the rest were shared by users who were suspended from Twitter. We sampled 445 videos that had accumulated the maximum number of views from both the promoting and detracting clusters (890 in total). Since the videos were not annotated by the authors for misinformation, we could not assume that videos shared by users in the promoting cluster would contain misinformation. Therefore, we conducted an intensive and iterative process to determine the labels and heuristics for annotating the YouTube videos for misinformation. We describe the process in detail in Section 3.6. Through the annotation process, we labeled the videos as supporting, neutral, or opposing election misinformation. Out of the 890 videos, 74 were opposing, 16 were neutral, 101 supported election misinformation while remaining were irrelevant. We selected the top 15 videos that had accumulated maximum engagement, determined by the number of views, for each stance (except the irrelevant) as seeds. Figure 3 illustrates the seed video curation method. Table 2 presents a sample of seed videos.",
  "3.3 Experimental design": "To conduct the crowd-sourced audit, we designed a chrome browser extension named TubeCapture that enabled us to watch videos, conduct searches, and collect various YouTube components from users' A post hoc crowd-sourced audit of election misinformation on YouTube CHI '23, April 23-28, 2023, Hamburg, Germany Election fraud dataset by Abilov et al Select top 15 videos with most views as seeds from each stance Extract videos with max views Annotate videos Supporting Opposing Neutral Figure 3: Figure illustrating our method to curate seed videos for our audit experiment fraud misinformation Table 2: Sample seed videos curated for the audit experiment. browsers. Figure 4 presents an overview of our experimental design. To select the study participants, we conducted a screening survey of a large sample of people (details in Section 3.4). Next, participants were instructed on how to use TubeCapture and provided with a unique code to activate the extension. Once activated, they used TubeCapture for a period of 9 days. We seeded our extension with 45 seed videos and 88 search queries. For each participant, each day the extension opened YouTube in two browser windows, one standard window and one incognito window. While the personalized results act as treatment for our experiments, results obtained from incognito act as control since YouTube does not personalize content in the incognito browsing window [84]. By comparing the results from standard and incognito windows, we determine the role of YouTube's personalization algorithms in exposing users to misinformative content. TubeCapture first collected and stored the user's YouTube homepage from standard and incognito windows. The extension ensured that the user had signed in to their YouTube account in the standard window and remained logged in using the same YouTube account throughout the study period. We also ensured that the homepage from the standard window is stored without the user's email address to ensure the participant's anonymity. Next, the extension opened a seed video (previously selected) that supports election misinformation, watched it for 2 minutes, saved the video page, clicked on the up-next video, and again saved the video page of the up-next video. This process was repeated until we collected 5 levels of up-next recommendations' video pages. We refer to the collection of 5 up-next video recommendations as up-next trails. Each day we collected up-next trails for five seed videos. Then, the extension again collected the user's homepage followed by personalized (via standard window) and unpersonalized (via incognito window) search results for the curated search queries. The extension collected the search results for queries in the same order for every participant to control for carry-over effects of the search queries [32]. For days 1-3, the extension collected up-next trails for seed videos supporting election misinformation. At the beginning of the fourth day, the extension deleted the search and watch history created by the browser extension. According to YouTube, removing an item from search or watch history removes the impact of consuming that content on future searches and recommendations. This essential step helped us in two ways- 1) it ensured that the history created by our extension in the first three days does not impact the rest of the experiment, and 2) it also ensured that the user histories built by our extension did not pollute users' future recommendations and search results after the study period is over. For days 4-6, the extension collected up-next trails for seed videos that were neutral in stance. At beginning of the seventh day, again search and watch history developed by the extension was deleted. For days 7-9, the extension collected up-next trails for opposing seed videos. Towards the end of the 9th day, we again deleted the YouTube history developed by the extension. All the data collected by the extension was sent to a back-end server. The participants were instructed on how to remove the extension after the study period was over. Our current mixed design allows us to test how YouTube's algorithm fares under different conditions-watching videos of different stances-for individuals with different political beliefs. Note that we did not opt for a randomized assignment in a between-subject design since it would require a large number of participants to test all the conditions (3 political affiliations X 3 misinformation stances). Webuilt the YouTube capture extension using JavaScript libraries. The back-end server was set up using Flask and Nginx. We loadtested the server using Jmeter and ensured that the server could simultaneously handle 500 GET and 200 POST requests and added mechanisms to handle errors and server timeouts. We used a MySQL database for storing the data collected using the extension. The communication between the extension and our back-end server was encrypted using SSL. Note that to collect data, TubeCapture opened windows in the background of the currently active browser window, thereby allowing participants to continue working on their device while the extension is running. In case, the participant accidentally closed any of the windows opened by our extension, we informed users via a pop-up window and instructed them on how to resume running the extension. After building the TubeCapture extension, we tested it with our research group and conducted three pilot studies. The aim of the pilot studies was to fix technical issues, examine the impact of running the extension on devices with different configurations, CHI '23, April 23-28, 2023, Hamburg, Germany Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra Figure 4: Figure (a) presents an overview of our crowd-sourced audit of YouTube for election misinformation, Figures (b) and (c) show how our extension TubeCapture collected YouTube components from both standard and incognito windows simultaneously. Install chrome extension Fill screening survey Curate search queries Collect up-next trails for supporting seed videos Collect search results Fill study survey User registers extension using a unique code Standard (signed-in) window Incognito window Seed video (trailhead) Level #1 Level #2 Standard (signed-in) window Incognito window Delete YouTube watch search history for last 3 days Delete YouTube watch search history for last 3 days click on up- next video click on up- next video click on up- next video click on up- next video repeat for three days collect up-next trails for 3 seed videos collect search results for 88 queries collect search results for 88 queries Collect up-next trails Experimental design Collect search results If selected Collect up-next trails for neutral  seed videos Collect search results repeat for three days collect up-next trails for 3 seed videos Collect up-next trails for opposing  seed videos Collect search results repeat for three days collect up-next trails for 3 seed videos upto level #5 upto level #5 collect search results for 88 queries (a) (b) (c) Curate seed videos Section 3.1 Section 3.2 Section 3.4 Section 3.4 Section 3.3 Delete YouTube watch search history for last 3 days Inccanto Incognio A post hoc crowd-sourced audit of election misinformation on YouTube CHI '23, April 23-28, 2023, Hamburg, Germany RAM, and operating systems as well as improve the usability of the extension.",
  "3.4 Screening and study survey": "In order to select participants for our study, we screened users according to several criteria. To be eligible for the study, users should be 1) 18 years of age or older, 2) reside in the United States, 3) have a YouTube account, 4) consume content on YouTube primarily in the English language, 5) have a chrome browser installed, 6) willing to run a chrome browser extension for 9 days and 7) have at least 8GB RAM on their device to ensure smooth running of the extension 3 . The users who qualified for the screening survey were sent another study survey. The study survey contained questions about users' demographics, political affiliation, YouTube usage, trust in online information, their opinion on personalization and bias in various components of YouTube, and their view on the results of the presidential elections 2020 as well as conspiracies surrounding the elections. We also included two attention-check questions. The study survey was also used for screening participants. We disqualified users who 1) answered both attention check questions incorrectly, 2) did not frequently use YouTube, and 3) did not use YouTube to access news or information about the 2020 presidential elections. We also used the survey responses to obtain a balanced number of participants across three political affiliations (Democrats, Republicans, and Independents). Later in the recruitment phase, we had enough democrats and independents as participants and thus, added being a republican as a qualifying criterion in the study survey.",
  "3.5 Recruitment and study deployment": "For our pilot studies, we recruited users from a combination of platforms such as Reddit 4 , Facebook ads, Twitter, and Amazon Mechanical Turk (AMT). The retention rate was highest for participants recruited from Twitter and AMT. Thus, we used these two platforms to recruit participants for the main study. The pilots and the main study were approved by our university's Institutional Review Board. Out of the 575 users who submitted the screening survey, 400 qualified, and 99 participated in the study. Out of the 99 participants, 94 ran the extension for the entire study duration. Overall, our study sample of 99 users constituted of 60.6% males and 39.39% females, was predominantly White/Caucasian (60.6%) and the majority (53.53%) of the participants had a bachelor's degree. Politically, 39.39% of our participants were Democrats, 34.34% independents, and 26.26% Republicans. Based on the results of 2020 presidential elections 5 , 66.67% of our participants lived in the blue states, 32.32% in red while one individual resided in Puerto Rico 6 . We report additional participants' characteristics in Appendix A.1. 3 We warned users against participating in the study if their device's RAM is less than 8GB and informed them that their device or browser might hang in such a situation 4 https://www.reddit.com/r/SampleSize/ 5 https://www.politico.com/2020-election/results/president/ 6 Puerto Rico is not considered a state but is considered an unincorporated territory of the United States",
  "3.6 Developing data annotation scheme": "Developing the qualitative coding scheme to label YouTube videos for election misinformation was hard and time-consuming, requiring four rounds of discussions and consultation with an expert to reach a consensus on the annotation heuristics. In the first round, the first author and an undergraduate research assistant sampled 196 YouTube videos from Abilov et al's YouTube dataset [1] and separately annotated the videos. They considered prior work on election misinformation narratives [27] and YouTube content policy [83] as references to identify election misinformation, and came up with an initial annotation scale and heuristics to classify videos. Then they came together to reach a consensus on the annotation values. However, even after multiple rounds of discussions, annotations diverged for 33.6% of the videos. We then conducted additional rounds of annotation exercises with seven researchers, out of which five had extensive work experience on online misinformation. In every round, researchers independently annotated 15 videos and later discussed every video's annotation value and the researchers' annotation process. We also reached out to a postdoctoral researcher who has extensive research experience on online multi-modal election misinformation for feedback. Based on the insights provided by the external researchers and postdoc, we refined the annotation criteria and heuristics 7 . Below we describe the annotation guidelines and heuristics in detail. 3.6.1 Annotation guidelines. In order to annotate a YouTube video, the annotators were required to go through several fields present on the video page in the following order: title and description, the overall premise of the video which could be determined by going through the video transcript or watching the video content, and considering channel bias. We encouraged participants to perform an online search to gain more contextual information about events or individuals discussed in the video that they were unaware of. This strategy is grounded in the lateral reading technique that is often used by fact-checkers for credibility assessments [80]. Note that we did not ask participants to consider video comments for the annotations because we found during our annotation exercises that comments could be misleading. For example, video Dominion Voting Systems representative demonstrates voting machines (Q7kPSzYsR6Y) contains a demonstration of dominion voting machines, however, the comments indicate the video to be supporting misinformation. 3.6.2 Annotation heuristics. In this section, we describe our annotation scale and heuristics. Supportingelection misinformation (1) : This category includes YouTube videos that support or provide evidence for misleading narratives around the presidential elections. We did not include videos showing incidents of mail dumping, destroyed ballots, etc. in isolation. However, if the videos use these incidents to push a specific narrative/agenda like undermining confidence in mail-in voting, then we considered them as supporting misinformation. We also considered live YouTube videos (live press conferences, court hearings, etc.) that highlighted voter fraud claims without giving any additional context in the title, description, or beginning of the video 7 It is important to note that all annotators and the post-doctoral researcher are left and center-left leaning individuals which may have affected how the content of YouTube videos was perceived and how the annotation heuristics were developed. CHI '23, April 23-28, 2023, Hamburg, Germany Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra as supporting misinformation. A few examples of videos in this category include NO RETREAT! America Is About To #StopTheSteal | Good Morning #MugClub (Xqcwzi8Onsk) where video's title, description, and content hint towards massive voter fraud incidents in the US 2020 presidential elections and LIVE: Trump Legal Team Presents CLEAR Evidence of Fraud Before Georgia Senate Committee 12/3/20 (e35f4pUIYOg) which contains live footage capturing the testimony of individuals claiming occurrence of voter fraud in 2020 presidential elections. The video's description, title, and beginning do not contain any statements questioning or contradicting the claims of widespread voter fraud. Neutral (0) : We consider videos as neutral when they are related to the 2020 elections but do not support or oppose false narratives surrounding the elections. For example, video WATCH: The first 2020 presidential debate (w3KxBME7DpM) is considered neutral since it covers the first presidential debate of the elections. Opposing (-1) : We annotate videos as opposing when they oppose or debunk the misinformation narratives behind the 2020 US presidential elections. We also include satire videos making fun of the misinformative claims in this category. For example, video Trump Has Yet To Show Real Evidence Of Fraud, But Getting Him Out Of Office May Be A Bumpy Ride (7mJwuKhfvqY) whose title and description indicate that Donald Trump made false claims of massive voter fraud. Other annotations : We mark a video as Irrelevant (2) if its content is not related to the presidential elections, as URL not accessible (3) if the YouTube video was not accessible at the time of annotation and as Other languages (4) when the content, title, or description of the YouTube video was in a language other than English.",
  "3.7 Classifying YouTube videos for election misinformation": "Our crowd-sourced audit experiments resulted in ∼ 47K unique YouTube videos and 35 unique YouTube shorts 8 . Given a large number of videos, we scaled the annotation process using a machine learning classifier. In this section, we present our method of creating the ground truth dataset, a description of features used in our classification model, model architecture, and the results of our classification. 3.7.1 Creating a ground truth dataset. Two researchers manually annotated 1196 videos using the guidelines and heuristics mentioned in Section 3.6. We obtained annotations for 545 additional videos using AMT. We describe the process of obtaining video annotations from AMT workers in Figure 16 and Appendix A.2. Overall, in our ground truth dataset, we had 1741 videos out of which 124 are supporting 9 , 257 opposing, 228 neutral, and 1132 irrelevant videos. 3.7.2 Feature description. We considered the following features for our classifier. Snippet (title+description) : We concatenated the title of the YouTube video with its description together, as done by [53], and used the concatenated string as a feature. Transcript : Transcript contains the textual content of the video. 8 YouTube shorts are short YouTube videos with lengths equal to or less than 60 seconds 9 Out of these 67 videos were removed from the platform at the time of analysis. We use transcripts auto-generated by YouTube. Tags : Video tags are words that a content creator associates with their video while uploading it on the platform. Video Statistics : Video statistics include the number of views, likes, comments, and date of publication. Channel Bias : Since the election misinformation is closely entangled with the political beliefs [12, 49], we used partisan bias of YouTube channels as a feature. Using existing data sets on media bias and manual annotations (described in Appendix A.3), we annotated YouTube channels' partisan bias on a 5-point scale of far-left to far-right. Apart from the features listed above, we also tried several other features like LIWC dictionary [66], Credibility Cues [50], and hashtag matching from the Voter Fraud dataset on the text features [1] that didn't improve performance. Therefore, we do not discuss them in detail. Recall, while manually annotating the videos, we discovered that comments are not a good indicator of the veracity of the video. Therefore, we chose not to include those in our feature set. 3.7.3 Classifier Selection. To find a classifier that performs well on our dataset, we applied a series of machine learning classifiers on several combinations of feature sets. To create feature vectors, we tested two types of word vectors (count and tf-idf vectors) and two types of sentence vectors (FastText 10 and BERT [21]). For word vector generation, we cleaned the dataset by removing stop words and lemmatization, followed by up to 3-gram generation. To deal with data imbalance in our dataset, we used Synthetic Minority Over-sampling Technique [16] We applied several classifier models on our feature set including support vector machine, stochastic gradient descent, decision trees, nearest neighbor, and ensemble models. To find the best model, we performed a grid search on a five-fold cross-validation dataset by looking into standard parameter space for each classifier. For the sake of brevity, we only show a sample of combinations tested in Table 3. Out of all the combinations, both SVM and XGBoost performed the best (ACC=91%) when trained with snippet, tags, and channel bias features and tf-idf text vectorizer 11 . Based on Occam's Razor principle [78], we selected SVM as the final classifier, i.e., the simplest model with maximum accuracy. Using our final classifier, we determined the annotation labels for the remaining videos. In total, our dataset consisted of 431 supporting, 1868 opposing, 1658 neutral, and 43041 irrelevant videos.",
  "4 ETHICAL CONSIDERATIONS": "Our browser extension TubeCapture uses crowd workers' YouTube account to watch videos (including videos containing election misinformation) and conduct searches on the platform. It was possible that participants would have seen more misinformation than they would have otherwise during and also after the research study due to the watch and search history built during the audit. In order to eliminate the potential harm of our experiments, we included two essential steps in our experimental design. First, our extension 10 https://fasttext.cc/ 11 If we merge irrelevant and neutral videos into one class resulting in a three-class classification problem, SVM classifier performs with a 93% accuracy. A post hoc crowd-sourced audit of election misinformation on YouTube CHI '23, April 23-28, 2023, Hamburg, Germany Table 3: A sample of classifiers and feature set with the performance progression. always opened the browser window in the background so that participants don't actively see the videos being played. Second, the extension deleted users' search and watch history built during the study period. Note that YouTube allows the deletion of items from the search and watch history for a specific date range. YouTube's website [73, 82] clearly states that ' search entries you delete will no longer influence your recommendations. At any time you can (also) remove videos (from watch history) to influence what YouTube recommends to you '. We explicitly informed users that their YouTube history during the study period would be deleted. We ensured that the extension expires after the study period so that it does not perform any action. In addition, we ensured that the YouTube pages saved by our extension do not contain users' personally identifiable information such as email addresses. penalized. Both jaccard and RBO scores range between 0 and 1, with 1 indicating that the two lists have similar elements while 0 indicating that the lists are completely different.",
  "5 RQ1 RESULTS: EXTENT OF PERSONALIZATION": "To measure the extent of personalization in YouTube components, we compare the personalized list of video URLs present in the standard window with the baseline unpersonalized videos obtained from the incognito window. Below we discuss the metrics that we used to quantify personalization. Measuring personalization in web search: In our study, to determine personalization in search results, we employ two metrics: jaccard index and rank bias overlap (RBO). Jaccard index measures the similarity between two lists and has been used in several previous audit studies to measure personalization in web search [32, 38, 41]. However, Jaccard index does not take into account the rank of the lists being compared. Thus, we used the RBO metric introduced by Webber et al [76] which takes into account the order of elements in the list. The RBO function includes a parameter p which indicates the top-weightedness of the metric, i.e. how much will the metric penalize the difference in the top rankings. A previous audit study used the click-through rate (CTR) of Google search results to estimate the value of p [57]. Because of the lack of CTR statistics available for YouTube, we consider the default value of p which is 1 (prior audit studies such as [44] opted for a similar approach), indicating that differences in all rankings are equally Measuring personalization in up-next trails: To measure personalization in up-next trails, we employ jaccard index and Damerau-Levenshtein (DL) distance [19]. DL distance is the enhanced version of edit distance that computes the number of transpositions in addition to insertions, deletions, and substitutions required to make the treatment list identical to the control list. DL distance has been used by prior audit work as a metric to estimate the ranking differences between two lists [14]. It returns a score from 0 to 1 (identical lists) indicating how similar the two lists are. We refrain from using the RBO metric to determine personalization in up-next trails because RBO is suitable for indefinite lists while the trails collected through our experiments have a known maximum length of five. We also refrain from using the Kendall tau metric since it requires the two ranked lists being compared to be conjoint 12 . Given, jaccard, RBO, and DL distance return similarity values, we define personalization as:-",
  "5.1 RQ1a: Personalization in search results": "When asked in our study survey how much YouTube personalizes search results (Figure 5a), 34.34% believed YouTube personalizes search results to a great extent while 19.19% believed the extent of personalization to be very little. On quantitatively measuring the extent of personalization in YouTube search results, we found little to no personalization indicating that search results present in standard and incognito windows are highly similar. Figures 5b and 5c show the extent of personalization in SERPs calculated using jaccard index and RBO metric respectively for democrats, republicans, and independents for each day of the experiment run. We did not find any significant difference in the personalization values of SERPs for participants with respect to their political leaning. 12 There are alternative versions of Kendall Tau that assume the dissimilar elements to be present at the end of the list. However, conceptually, the metric does not fit our collected trail data. CHI '23, April 23-28, 2023, Hamburg, Germany Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra (a) Participant's belief in extent of personalization in YouTube search results Very little Somewhat To a great extent 0 10 20 30 40 Count 19.2% 46.5% 34.3% Day0 Day1 Day2 Day3 Day4 Day5 Day6 Day7 Day8 0.0 0.2 0.4 0.6 0.8 1.0 Personalization (Jaccard index) Democrat Republican Independent Day0 Day1 Day2 Day3 Day4 Day5 Day6 Day7 Day8 0.0 0.2 0.4 0.6 0.8 1.0 Personalization (RBO) Democrat Republican Independent (b) Measuring extent of personalization in SERPs using jaccard index (c) Measuring extent of personalization in SERPs using RBO Figure 5: RQ1a results: Figure (a) shows participants' response to the survey question: 'How much, if at all, do you think YouTube personalizes search results'. Figures (b) and (c) show personalization calculated via jaccard index values and RBO metric values respectively in YouTube's standard-incognito SERP pairs. We observe that search results are slightly personalized meaning search results obtained from standard windows are very similar to the search results obtained from incognito windows. Very little Somewhat To a great extent 0 10 20 30 40 50 60 Count 9.1% 39.4% 51.5% 0 10 20 30 40 50 60 70 80 90 Percentage of trail videos coming from users' subscribed channels 0 10 20 30 40 50 No. of users (a) Participant's belief in extent of personalization in YouTube up-next recommendations (b) Distribution of percentage of up-next video recommendations coming from users' subscribed channels. Trails with supporting seed (Day1-3) Trails with neutral seed (Day4-6) Trails with opposing seed (Day7-9) 0.0 0.2 0.4 0.6 0.8 1.0 Personalization (jaccard index) Democrat Republican Independent (c) Measuring extent of personalization using jaccard index Trails with supporting seed (Day1-3) Trails with neutral seed (Day4-6) Trails with opposing seed (Day7-9) 0.0 0.2 0.4 0.6 0.8 1.0 Personalization (DL dist.) Democrat Republican Independent (d) Measuring extent of personalization using DL index Figure 6: RQ1b results: Figure (a) shows participants' response to the survey question: 'How much, if at all, do you think YouTube personalizes up-next recommendations'. Figure (b) shows the distribution of the percentage of YouTube videos recommended to our study participants from their subscribed channels. Figures (c) and (d) show personalization calculated via jaccard index values and DL distance metric values respectively in YouTube's standard-incognito up-next trails pairs. We observe that up-next recommendation trails are highly personalized. A post hoc crowd-sourced audit of election misinformation on YouTube CHI '23, April 23-28, 2023, Hamburg, Germany",
  "5.2 RQ1b: Personalization in up-next trails": "When asked how much YouTube personalizes up-next recommendations, 51.5% of participants believed that YouTube personalizes up-next recommendations to a great extent (refer Figure 6a). The quantitative measurements are in line with this belief showing that up-next trails are highly personalized. Figures 6c and 6d show the extent of personalization in up-next trails using jaccard index and DL distance. The graphs indicate that the up-next trails obtained from the users' standard and incognito windows are highly dissimilar and thus, highly personalized. Statistical test revealed that the amount of personalization in trails with supporting, neutral, and opposing seeds is significantly different [F(2)=15.2, p<0.0001]. Post hoc test revealed that up-next trails with seed videos opposing misinformation have lesser personalization (higher jaccard index 13 ) when compared with up-next trails with supporting and neutral seed videos. Next, we checked the influence of users' subscriptions on personalized trails. 81 (out of 99) participants had subscribed to at least one YouTube channel (mean=109.4, median=31, SD=207.8). The maximum number of subscriptions for a participant was 1073 and the minimum was 1. The participants had subscribed to 7670 unique channels out of which 79 either did not exist or were suspended due to violation of YouTube's moderation policy and thus, we did not consider these channels for analysis. To determine how many video recommendations in users' up-next trails were coming from their subscriptions, first, for each user we extracted the unique videos recommended in all the up-next trails collected for the user. Then we filtered and calculated the number of videos coming from the users' subscribed channels. Figure 6b shows the distribution of the percentage of videos recommended to our participants in up-next trails that are coming from their subscribed channels. This percentage value is moderately correlated with the number of channels subscribed (r=0.61) and highly correlated with the number of news-related channels subscribed 14 (r=0.71).",
  "6 RQ2 RESULTS: AMOUNT OF MISINFORMATION": "Whenasked how much do participants trust the credibility of videos in search results and recommendations, less than 20% reported that they trust the credibility of content shown to them by YouTube to a great extent (Figure 7). To determine how much credible information is presented by YouTube to users in reality, we quantify the misinformation present in the YouTube components by adopting the misinformation bias score developed by Hussein and Juneja et al [34]. The score determines the misinformation in ranked lists and is calculated as ˝ 𝑛 𝑟 = 1 ( 𝑥 𝑟 ∗( 𝑛 -𝑟 + 1 )) 𝑛 ∗( 𝑛 + 1 ) ; where x is the video annotation, 𝑟 is 2 13 The jaccard index values obtained were highly correlated with DL distance scores (pearson correlation coefficient = 0.96). Thus, we used jaccard index values to perform the statistical test. 14 To get a rough estimate of YouTube channels that broadcast news, we considered the news sources from mediabiasfactcheck.com and allsides.com . Additionally, we extracted the description of each channel and categorized it as a news channel if the description contained terms such as 'breaking news', 'politic*', 'current affairs', 'government', 'national tv', 'national news', 'international news', 'world news', 'global news', 'current affairs', 'wall street' etc. These terms were curated by the first author after manually going through the description of 50 national and regional news channels on YouTube. We found that 44 users had subscribed to news and politics-related channels. rank of the video, and 𝑛 is the total number of videos present in the SERP/up-next trail. To conform to the video annotation scale in [34], we map our annotation values to a normalized scale of -1, 0, and 1. We assign scores of -1 and 1 to videos opposing and supporting election misinformation respectively. Videos marked as irrelevant, neutral, belonging to a non-English language, or removed from the platform are assigned a 0 score. Thus, the misinformation bias score of a SERP/trail is a continuous value ranging between -1 (all videos are opposing election misinformation) to +1 (all videos are supporting election misinformation). Note that a positive score indicates a lean towards misinformation, while a negative score indicates a lean towards content opposing misinformation. For analysis, we consider the top ten search results and five consecutive videos in the up-next trails.",
  "6.1 RQ2a: Misinformation in search results": "The results of RQ1 showed that YouTube's SERPs are very slightly personalized suggesting that search results present in the standard and incognito windows are mostly similar. Therefore, to quantify the misinformation bias in SERPs we only consider the SERPs obtained from the standard YouTube windows of all the participants. We first calculated the average misinformation bias score for each of the 88 search queries for 9 days of the experiment run across all 99 participants. Figure 8 shows the distribution of misinformation bias scores for all the search queries. We observe that the average misinformation bias scores of 84 (out of 88) search queries are negative indicating that the search results contain more videos that oppose election misinformation as compared to videos supporting election misinformation 15 . Furthermore, we observe in Figure 8 that the misinformation bias scores of the SERPs form a bimodal distribution constituting two clusters of search queries (Table 4). The cluster1 search queries have the most negative bias, i.e. they contain more opposing videos. This cluster mostly consists of search queries containing the keyword fraud in conjunction with keywords voter , election , and dominion . Cluster2 on the other hand consists of search queries with keywords election and 2020 . Overall, cluster1 consists of more search queries biased towards finding misinformation compared to search queries in cluster2. This indicates that YouTube pays more attention to search queries about election fraud and ensures that users are exposed to opposing videos when searching about fraudulent claims surrounding the elections. Figure 9a shows five search queries with the highest and 5 search queries with the lowest misinformation bias. The search query 'voter fraud claims' has the least amount of misinformation bias, indicating that most of the search results for this query oppose election misinformation. On the other hand, the search query 'stop the seal' has the most amount of videos supporting election fraud claims. Next, we determine how do misinformation bias scores in SERPs vary for democrats, independents, and republicans. Figure 9b shows that the bias values for democrats, independents, and republicans for all days coincide indicating that the amount of misinformation bias is almost constant for all days for all participants irrespective of their partisanship. Overall, our RQ2 results indicate 15 Only four search queries in our query set ('stop the seal', 'voting machine fraud', 'ballots in garbage' and 'ballots thrown out') have a positive misinformation bias. CHI '23, April 23-28, 2023, Hamburg, Germany Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra (a) Participant's trust in the credibility of information presented in search results Not at all Very little Somewhat To a great extent 0 10 20 30 40 50 60 Count 4.0% 16.2% 60.6% 19.2% Not at all Very little Somewhat To a great extent 0 10 20 30 40 50 60 Count 7.1% 18.2% 60.6% 14.1% (b) Participant's trust in the credibility of information presented in up-next recommendations Figure 7: RQ2: Figure showing participants' response to survey question: 'How much do you trust the credibility of information present in the ' a) search results and b) up-next videos recommended by YouTube. Cluster1 Cluster2 Cluster1: Search queries containing keyword fraud in conjunction with keywords voter, election, and dominion voter fraud evidence, dominion voter machine scandal, sharpie voter fraud, election fraud 2020, election fraud whistleblower Cluster2: Search queries containing keywords elec- tion, and 2020 trump biden general election, presidential election 2020, presidential election results 2020, mail in ballots 2020 Figure 8: RQ2a results: Mean misinformation bias scores for 88 search queries for all participants. A negative score indicates that SERPs contain more videos opposing election misinformation. Table 4: The misinformation bias scores form a bimodal distribution, each constituting a cluster of similar queries. This table describes the clusters and presents sample queries for each cluster. 1.0 0.5 0.0 0.5 1.0 Misinformation bias score stop the steal voting machine fraud ballots in garbage ballots thrown out us elections 2020 pennsylvania voter fraud claims voter fraud evidence ballot fraud electoral fraud ballot box fraud (a) Search queries with highest and lowest mean misinformation bias scores Day0 Day1 Day2 Day3 Day4 Day5 Day6 Day7 Day8 1.0 0.5 0.0 0.5 1.0 Misinformation bias scores Democrat Republican Independent (b) Misinformation bias scores of search queries for each day of experiment run Figure 9: RQ2a results: a) Search queries with highest (labeled in red) and lowest (labeled in blue) mean misinformation bias scores. Positive misinformation bias scores indicate a lean toward misinformation where as negative bias scores indicate a lean toward information that opposes misinformation. b) Figure showing the distribution of misinformation bias scores of search queries for democrats, republicans, and independents. Note that the bias scores for the participants belonging to the different political leanings coincide indicating that misinformation bias in SERPs remains constant throughout for each participant. A post hoc crowd-sourced audit of election misinformation on YouTube CHI '23, April 23-28, 2023, Hamburg, Germany that YouTube pushes debunking information in search results, more for search queries about voter fraud claims as compared to generic queries about the presidential elections.",
  "6.2 RQ2b: Misinformation in up-next trails": "The results of RQ1 showed that participants' up-next trails are highly personalized. In other words, videos in up-next trails obtained from the standard window are different from videos in trails obtained from the incognito window. Recall, that trails extracted from the incognito window act as baseline unpersonalized trails while trails extracted from the standard window, where users had signed into their accounts, act as personalized treatment trails. Therefore, to determine the impact of personalization on the amount of misinformation in up-next trails, we compare the misinformation bias scores of trails collected in standard windows with the trails collected in incognito windows. We find that the difference in misinformation bias scores of standard and incognito up-next trails is not significant (t=-0.62, p=0.53). This means that although the standard up-next trails are very different from the incognito up-next trails, there is no difference in the amount of misinformation present in them. To avoid inflating our sample size, for further downstream analysis, we only consider up-next trails obtained from participants' standard windows. This similar strategy was adopted by Robertson et al for analyzing bias in Google search results when they did not see any significant difference in the amount of partisan bias in incognito-standard SERP pairs [57]. 6.2.1 Misinformation in standard up-next trails for different scenarios. In this section, we determine the amount of misinformation encountered by our study participants in the standard up-next trails for seed videos with different stances on election misinformationsupporting, neutral and opposing. Figure 10 shows the mean misinformation scores of different up-next trails collected from the standard windows of democrats, republicans, and independents. Recall that a positive misinformation score (>0) indicates a lean toward misinformation, while a negative misinformation score indicates a lean toward information that opposes election misinformation. We conduct within-group statistical tests to determine the difference in misinformation for the three scenarios (following trails for supporting, neutral, and opposing seed videos). The tests indicate a filter bubble effect. If users watch supporting videos, they are led to supporting videos in the trails. But if they watch neutral videos, they are led to less misinformation compared to when they watched supporting videos. However, if users watch opposing videos, they are led to more opposing videos in the up-next trails. The same trend is observed for democrats, republicans, and independents. Is the amount of misinformation in trails with different seeds different for democrats, republicans, and independents? Betweengroup statistical tests reveal that the amount of misinformation in supporting trails (KW H(2)=11.9,p=0.002) and neutral trails (KW H(2)=8.69,p=0.01) for democrats, independents, and republicans is significantly different. We find that independents in our sample receive more misinformation in their supporting trails as compared to democrats. Additionally, republicans receive more misinformation in their neutral trails compared to democrats. Overall, by observing Figure 10, we realize misinformation scores of supporting trails are positive and opposing trails are negative. However, the magnitude of misinformation scores of opposing trails is much more than the supporting trails indicating that the strength of the filter bubble effect was more when our study participants watched videos opposing election misinformation. 6.2.2 Transitions in standard up-next trails. In this section, we gain more insights into the anatomy of YouTube's up-next trails by studying the various transitions present in them. This allows us to determine how users get pushed towards misinformative or debunking videos in the trails. Since our annotation scale consists of three values, supporting (S), neutral (N), and opposing (O), there are 9 transitions possible in the trails (S->S, S->N, S->O, N->S, N->N, N>O, O->S, O->N, N->O). For each participant, we first individually determine the percentage of each of these transitions present in the three types of standard up-next trails collected (ones starting with a supporting seed video, neutral seed videos, and opposing seed video). Then we calculated the mean percentage of all of these transitions for democrats, independents, and republicans. From Figure 11, we see that the maximum number of transitions across all participants and all types of up-next trails is N->N. Problematic transitions like S->S and O->S are less than 2% in trails of all users. However, comparatively S->S transitions are still more in the supporting upnext trails of independents (1.78%) compared to democrats (0.38%) and republicans (0.86%). In the neutral up-next trails of republicans and independents, N->S transitions dominate (after N->N transitions) indicating that independents and republicans are sometimes led to supporting videos in their up-next recommendations even when they are viewing neutral YouTube videos. We also observe that the opposing up-next trails majorly consist of transitions O>N and N->O (after N->N transitions) indicating that once a user watches a video that opposes election misinformation, YouTube pushes more videos that are either neutral or opposing in stance in the up-next trails of all the participants. We also observe that S->O transitions are less than S->N transitions in the supporting trails of democrats, republicans, and independents. Previous work has shown that watching YouTube videos that debunk misinformation helps in bursting filter bubbles of misinformation [70]. Our work also shows that opposing videos could lead to more opposing videos (O->O transitions in opposing trails). Thus, increasing the number of S->O transitions can lead users to trustworthy information on the platform.",
  "6.3 RQ2c: Misinformation in homepages": "We collected participants' YouTube homepages to determine how the bias in the homepage changes ( 𝛿 ) after watching a trail of videos starting with a seed video that is either supporting ( 𝛿 𝑆 ), opposing ( 𝛿 𝑂 ) or neutral ( 𝛿 𝑁 ) in stance with respect to election misinformation. We calculated the impact of trails by using the following formula:- 𝛿 𝑠𝑡𝑎𝑛𝑐𝑒 = Misinformation 𝑠𝑐𝑜𝑟𝑒 𝐻𝑜𝑚𝑒𝑝𝑎𝑔𝑒 _ 𝑏𝑒𝑓 𝑜𝑟𝑒 _ 𝑡ℎ𝑒 _ 𝑡𝑟𝑎𝑖𝑙 -Misinformation 𝑠𝑐𝑜𝑟𝑒 𝐻𝑜𝑚𝑒𝑝𝑎𝑔𝑒 _ 𝑎𝑓 𝑡𝑒𝑟 _ 𝑡ℎ𝑒 _ 𝑡𝑟𝑎𝑖𝑙 𝛿 𝑆 , 𝛿 𝑁 and 𝛿 𝑂 represent the change in the amount of bias present in homepages because of watching a trail of up-next videos starting with supporting, opposing and neutral seeds. A negative 𝛿 would indicate that the YouTube homepage collected after the trail contained more opposing videos compared to the YouTube homepage before the trail. A positive 𝛿 , on the other hand, indicates either CHI '23, April 23-28, 2023, Hamburg, Germany Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra 1.0 0.5 0.0 0.5 1.0 Figure 10: RQ2b results: Mean misinformation scores of standard up-next trails with seed videos that are supporting (S), neutral (N), or opposing election misinformation (O) for Democrats, Independents, and Republicans. A positive misinformation score indicates a lean toward misinformative content while a negative score indicates a lean toward content that opposes election misinformation. Statistical tests reveal a significant difference in the amount of misinformation contained in up-next trails. We find that democrats, republicans, and independents find more misinformation in supporting trails compared to neutral trails, and more misinformation in neutral trails as compared to opposing trails. Dem Repub Indep S->S S->N S->O N->S N->N N->O O->S O->N O->O 0.38 0.86 1.78 18.57 19.18 19.20 2.52 2.27 2.55 1.41 1.61 2.34 66.73 68.73 66.76 3.21 2.45 1.89 0.00 0.11 0.09 4.37 4.05 3.66 2.81 0.74 1.73 Supporting trails Dem Repub Indep S->S S->N S->O N->S N->N N->O O->S O->N O->O 0.60 0.63 0.77 2.03 3.17 3.49 0.03 0.00 0.08 2.67 3.78 4.26 85.85 86.66 82.75 3.99 2.88 4.01 0.05 0.15 0.08 2.98 1.99 2.97 1.79 0.73 1.59 Neutral trails Dem Repub Indep S->S S->N S->O N->S N->N N->O O->S O->N O->O 0.11 0.11 0.08 0.60 0.66 0.52 0.00 0.00 0.04 0.46 0.72 0.71 52.37 54.20 51.85 4.93 4.29 3.70 0.32 0.16 0.16 21.04 21.39 20.79 20.18 18.48 22.15 Opposing trails 0 20 40 60 80 100 (a) Mean % of transitions in trails with seed videos supporting elec. misinfo. (b) Mean % of transitions in trails with neutral seed videos (c) Mean % of transitions in trails with seed videos opposing elec. misinfo. Figure 11: RQ2b results: Mean percentage of various transitions present in the standard up-next trails of democrats, independents, and republicans. S represents a video supporting election misinformation, N represents a neutral video and O represents a video opposing election misinformation. Transition S->S denotes that a YouTube video supporting election misinformation leads to an up-next video recommendation supporting election misinformation. Figure 12: RQ2c results: Figure showing the average change in the amount of bias present in homepages because of watching a trail of up-next videos starting with either supporting, opposing, or neutral seed videos for democrats, republicans, and independents. Avg. S Avg. N Avg. O Dem. Repub. Indep. -0.01 0.002 -0.005 -0.009 -0.014 0.006 0.004 0.01 -0.06 1.0 0.5 0.0 0.5 1.0 presence of more videos supporting election misinformation or a lesser number of opposing videos on the homepage collected after the trail as compared to the homepage collected before the trail. We consider the top ten recommendations present on the homepage for analysis. Figure 12 shows 𝛿 values for all three kinds of trails for democrats, republicans, and independents. We discuss a few results. We observe that after following the up-next video trails starting from a neutral seed, the homepages of democrats and independents contain more supporting videos. However, recall that the average misinformation score of the up-next trails with neutral seeds for both democrats and independents was negative (Figure 10). This indicates that although the up-next trails with neutral seeds lead users to more opposing videos, the homepages, however, contain more misinformation or a lesser number of opposing videos after the trail. We also observe that after watching up-next trail videos with A post hoc crowd-sourced audit of election misinformation on YouTube CHI '23, April 23-28, 2023, Hamburg, Germany Figure 13: RQ3 results: a) Figure showing Top-10 YouTube channels with impressions in the most number of search queries for all study participants. For example, on average CNN appears in 61.86% of search queries for all our study participants. b) Figure showing the average number of impressions for Top-10 YouTube channels that appear in the most number of standard up-trails collected for users. For example, on average, videos from the Fox News channel appear 3.27 times in those up-next trails where videos from the channel are observed. is a left-leaning channel, is right-leaning and is center-leaning. 0 10 20 30 40 50 60 Mean percentage of total impressions in search queries CNN NBC News CBS News CNBC Television 60 Minutes MSNBC Fox News PBS NewsHour 11Alive TODAY 61.86 39.85 33.64 31.36 29.51 27.79 27.08 26.27 24.72 23.88 (a) 0 1 2 3 4 Average no. of impressions per up-next trail where the channel was observed LastWeekTonight Saturday Night Live Fox News Late Night with Seth Meyers The Late Show with Stephen Colbert Jimmy Kimmel Live NBC News Sky News Australia Fox Business PBS NewsHour 4.00 3.60 3.27 2.98 2.68 2.65 2.02 1.93 1.92 1.71 (b) supporting seed, republicans' homepage contain more opposing videos (Figure 12) while the trail itself contained more misinformation (Figure 10). However, note that the magnitude of the 𝛿 is low in all the conditions indicating that fewer videos supporting or opposing election misinformation appear on the participants' homepages.",
  "7.1 RQ3a: Diversity in search results": "",
  "7 RQ3: COMPOSITION AND DIVERSITY": "In this research question, we want to characterize source diversity in YouTube when users search for election misinformation on the platform. Source diversity in searches and recommendations is an important characterization of fairness [28]. Furthermore, given that the narratives about the election misinformation were closely intertwined with news sources and their leanings, it is important to determine what kinds of YouTube channels are users exposed to. News and media diversity can be characterized in multiple ways [37]. One typology characterizes media diversity with respect to source (content providers), content (perspectives) and exposure (actual consumption of diverse content) [51, 71]. Our work analyzed the content diversity in RQ2 by analyzing the video's stance on election misinformation. We cannot study exposure diversity since it requires determining the actual content consumed (clicked, watched, etc) by our study participants in their naturalistic settings. For this study, we focus on source diversity in terms of the identity of top content providers (YouTube channels) and distribution and concentration of channels in the standard SERPs and up-next trails. We acknowledge that future studies should also examine the ideological position of news sources and study the filter bubbles of partisan content on the platform. For analysis, we consider the top ten search results in standard SERPs. Figure 13a shows the top 10 YouTube channels with impressions in the most number of search queries. 16 Here, we define impression as the occurrence of a channel's video in SERP. We observe that the left-leaning channel CNN on average appears in SERPs of more than half (61.86%) search queries. Additionally, except Fox news and 11Alive, all other top channels are left-leaning. We further analyzed which channels were responsible for the most relevant YouTube videos in our collected data. In our standard SERPs, we obtained a total of 4901 unique videos out of which 1940 (39.51%) videos were relevant, i.e. related to elections (959 opposing, 865 neutral, and 103 supporting). Overall, in these relevant videos, most videos come from CNN and MSNBC. The most opposing videos come from channels MSNBC followed by CNN, most supporting videos come from Fox News followed by Daily Mail while most neutral videos come from NBC news followed by CNN. Given, CNN is one of the channels with the most opposing videos, it is encouraging to see that it has the most search query impressions. Next, we determine the source diversity in the SERPs using gini coefficient metric [28, 71, 81]. Gini coefficient determines inequality in a frequency distribution. For our case, we use this metric to determine the inequality in the distribution of YouTube channel impressions. For a given SERP consisting of videos from n unique 16 The top 10 YouTube channels and their mean percentage of total impressions were almost similar when calculated separately for democrats, republicans, and independents. Thus, we show the overall distribution for all users combined together.",
  "CHI '23, April 23-28, 2023, Hamburg, Germany": "0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 Avg. gini index 0 10 20 30 40 50 Count 48(54.5%) 29(33.0%) 8(9.1%) 2(2.3%) 1(1.1%) 0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 Avg. gini index 0 10 20 30 40 50 Count 48(54.5%) 29(33.0%) 8(9.1%) 2(2.3%) 1(1.1%) Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra 0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 Avg. gini index 0 10 20 30 40 50 Count 50(56.8%) 27(30.7%) 8(9.1%) 2(2.3%) 1(1.1%) (a) Democrats (b) Republicans (c) Independents Figure 14: RQ3a results: Distribution of Gini coefficients for all search queries (n=88) for a) Democrats, b) Republicans, and c) Independents, calculated based on the distribution of impressions of YouTube channels appearing in the search results. 0 25 50 75 100 Perc. of users in whose trails the channel appears Fox News NBC News PBS NewsHour LastWeek- Tonight CBS News Channels appearing in most supporting trails for democrats 100.00% 51.28% 51.28% 33.33% 58.97% 0 25 50 75 100 Perc. of users in whose trails the channel appears Fox News Fox Business Sky News Australia CBS News PBS NewsHour Channels appearing in most supporting trails for republicans 100.00% 80.77% 76.92% 76.92% 69.23% 0 25 50 75 100 Perc. of users in whose trails the channel appears Fox News NBC News PBS NewsHour CBS News Sky News Australia Channels appearing in most supporting trails for independents 100.00% 58.82% 67.65% 52.94% 41.18% (a) Democrats (supp. trails) 0 25 50 75 100 Perc. of users in whose trails the channel appears Fox News PowerfulJRE Fox Business The Late Show with Stephen Colbert Late Night with Seth Meyers Channels appearing in most neutral trails for democrats 100.00% 61.54% 51.28% 33.33% 28.21% (d) Democrats (neutral trails) 0 25 50 75 100 Perc. of users in whose trails the channel appears Saturday Night Live LastWeek- Tonight Late Night with Seth Meyers The Late Show with Stephen Colbert The Daily Show with Trevor Noah Channels appearing in most opposing trails for democrats 97.44% 97.44% 97.44% 97.44% 74.36%",
  "(b) Republicans (supp. trails)": "0 25 50 75 100 Perc. of users in whose trails the channel appears Fox News Sky News Australia PowerfulJRE Fox Business CBS News Channels appearing in most neutral trails for republicans 100.00% 69.23% 76.92% 57.69% 50.00%",
  "(e) Republicans (neutral trails)": "0 25 50 75 100 Perc. of users in whose trails the channel appears Saturday Night Live LastWeek- Tonight Late Night with Seth Meyers Jimmy Kimmel Live The Daily Show with Trevor Noah Channels appearing in most opposing trails for republicans 100.00% 100.00% 96.15% 96.15% 61.54% (c) Independents (supp. trails) 0 25 50 75 100 Perc. of users in whose trails the channel appears Fox News PowerfulJRE Fox Business NBC News Sky News Australia Channels appearing in most neutral trails for independents 100.00% 64.71% 58.82% 50.00% 44.12%",
  "(f) Independents (neutral trails)": "(g) Democrats (oppos. trails) 0 25 50 75 100 Perc. of users in whose trails the channel appears Saturday Night Live LastWeek- Tonight Late Night with Seth Meyers Jimmy Kimmel Live The Daily Show with Trevor Noah Channels appearing in most opposing trails for independents 100.00% 100.00% 97.06% 97.06% 67.65% (h) Republicans (oppos. trails) (i) Independents (oppos. trails) Figure 15: RQ3b results: Figure showing the top YouTube channels appearing in supporting, neutral, and opposing trails of democrats, republicans, and independents and the percentage of users in whose trails these channels appear. is a left-leaning channel, is right-leaning and is center-leaning. A post hoc crowd-sourced audit of election misinformation on YouTube CHI '23, April 23-28, 2023, Hamburg, Germany channels, given a list of impressions for all YouTube channels [ 𝑔 1, 𝑔 2,... 𝑔 𝑛 ], then gini coefficient would be calculated as, Gini coefficient (G) = 1 2¯ 𝑔𝑛 2 Σ | 𝑛 | 𝑖 = 1 Σ | 𝑛 | 𝑗 = 1 | 𝑔 𝑖 -𝑔 𝑗 | where ¯ 𝑔 is the mean of all impressions. Afairer search engine would have lower values of gini coefficient indicating uniform distributions of YouTube channel impressions. Figure 14 shows the distribution of gini coefficients for all SERPs for democrats, republicans, and independents. The distributions are similar for users with different political leanings. Furthermore, for approximately 96% of search queries, the gini coefficient of SERPs is less than 0.3 indicating that YouTube has mostly evenly distributed videos from different channels in its search results.",
  "7.2 RQ3b: Diversity in up-next trails": "Overall, we collected 6943 videos in standard trails out of which 1082 are relevant, i.e. related to elections. The most number of opposing videos in trails come from channels MSNBC and Late Night with Seth Meyers*, most supporting videos in trails come from Fox News* and Fox Business, and most neutral videos come from Fox News* and NBC News 17 . Next, we determine the top ten YouTube channels occurring in the standard trails. Note, we do not consider the seed videos while analyzing the trails. Figure 13b shows the average number of impressions of the top 10 channels appearing the most number of times in the trails. Here, impression indicates the number of occurrences of a channel's videos in a trail, while considering trails containing videos from that channel. Note that the top channels are also channels of some of the seed videos in our dataset. The figure reveals that on average, videos from LastWeekTonight, Saturday Night Live, and Fox News appear more than 3 times in a trail, when taking into account all the trails where the channel was observed. This finding indicates that videos from these channels lead to more videos from these channels in the up-next recommendations. Next, to determine the diversity in trails, we determine the proportion of channels that are different than the channel of the seed video in the trails. We find that on average, in an up-next trail of length five, we find 2.07 YouTube channels other than the channel of the seed video. The number of non-seed channels in up-next trails is the least for trails with seed videos from Saturday Night Live (0.85), LastWeekTonight (0.86), and Late Night with Seth Meyers (1.07). Note, we did not calculate this metric for supporting, neutral, and opposing seeds separately since the channels of our supporting, opposing, and neutral videos are not unique. For example, we have a supporting as well as a neutral seed from Fox news. Given this scenario, there is no way to determine whether the videos appearing in the trails are due to the channel lean of the seed video or because of other factors. We also refrain from determining the diversity in up-next trails using gini coefficient since several trails had just one or two unique channels (M=3.1, SD=1.46) in which case gini coefficient would not give a good representation of diversity. To get a sense of what kinds of channels are presented to users in the up-next trails, we determine the channels appearing in the most number of trails of democrats, republicans, and independents for trails with supporting, neutral, and opposing seeds (Figure 15). Weobserve that Fox news appears in up-next trails with supporting 17 * indicates that seed videos of our experiments also belonged to these channels. and neutral seeds of all users. Fox Business and Sky News Australia appear in both the supporting and neutral up-next trails of more than half of the republicans (Figure 15b, and 15e). None of the seed videos belonged to these channels and they still appear in the up-next trails. Similarly, Sky News Australia also appears in the neutral up-next trails of 44.12% independents (Figure 15f) despite no neutral seed belonging to the channel. Furthermore, PowerfulJRE (Joe Rogan's YouTube channel) did not appear in the neutral up-next trails of all the users even though two neutral seed videos belonged to the channel (Figure 15d, 15e and 15f). On the other hand, the top channels appearing in the up-next trails with opposing seeds of all users (Figure 15g, 15h and 15i) are the channels of the opposing seed videos used in our experiment. Furthermore, three channels out of the top four appear in the trails of more than 96% of the users. This indicates that watching a video belonging to these left-leaning channels will probably lead to one or more videos belonging to this channel in the up-next recommendation trail.",
  "8 DISCUSSION": "In this paper, we conduct a crowd-sourced audit of the YouTube platform to determine how effectively the platform removed election misinformation from its various components. We discuss the implications of our findings below.",
  "8.1 Standardization of search results": "Wefind little to no personalization in the search results. We also did not find any effect of personalization on the amount of misinformation returned in search results. Throughout the study period, the amount of personalization and misinformation remained constant in the searches. On analyzing the standard SERPs, we find that YouTube returns more videos opposing election misinformation in 95% of the search queries that we tested. Interestingly, we see that misinformation scores of search queries having a misinformation lean (e.g. dominion voter fraud) are more negative compared to misinformation scores of queries that are neutral in stance (e.g. presidential election 2020). This finding implies that YouTube has paid more attention to the queries with misinformation lean and ensured that users are exposed to more debunking information when they search about the fraudulent claims surrounding the elections. This selective attention is also in-line with results of past audits that showed YouTube improving the recommendations of topics like vaccination over 9/11 conspiracies [34]. Our analysis also indicates that gini index of 96% of search queries is less than 0.3, with ∼ 54% queries having a gini index of less than 0.1. Such low values of gini index imply that YouTube is ensuring source diversity in searches by evenly distributing videos from different channels in its SERPs. Furthermore, the distribution of gini coefficients was similar for all users irrespective of their partisanship. This finding indicates YouTube's attempt to expose users to videos from different channels rather than a select few based on participants' partisanship. Interestingly, in line with a previous audit on Google search [71], we find that CNN is one of the top channels whose videos appear in 61.8% of search queries. Future studies can test whether the dominance is due to emergent bias or the strategies adopted by the channel to enhance algorithmic visibility [71]. Overall, our analysis reveals that YouTube's search CHI '23, April 23-28, 2023, Hamburg, Germany Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra results are largely unpersonalized and the platform has had varying levels of success in removing misinformation and presenting videos that debunk election-related falsehoods in different clusters of search queries.",
  "8.2 Scope for improvement in up-next trail recommendations": "Wefind that up-next trails are highly personalized. However, for 50% of the users, only up to 10% videos in the up-next recommendations come from users' subscribed channels. Future audit studies should further investigate the impact of users' channel subscriptions (both news and non-news channels) on the platform's recommendations. We also find that there is no significant difference in the amount of misinformation that users are exposed to in up-next recommendation trails in the signed-in standard window and unpersonalized incognito window. On examining the standard up-next trails, we do find an echo-chamber effect. Users, irrespective of their partisanship, receive more misinformation in the up-next trails with supporting seeds as compared to the trails with neutral and opposing seeds (Figure 10). We also observe that the magnitude of misinformation scores of trails with opposing seeds is more than the magnitude of misinformation scores of trails with supporting seeds. This implies that users are exposed to a small number of misinformative videos when they follow the up-next recommendations of a video supporting election misinformation. On the other hand, users are exposed to a larger number of opposing videos in the opposing up-next trails. This is a key finding also supported by prior work that showed that echo chambers of misinformation can be burst by watching debunking videos [70]. The platform can leverage this phenomenon by making its recommendation engine present more debunking videos to users which would then expose them to more credible videos in the recommendation trails. Wealso examine various transitions in the up-next trails to study how users get pushed towards misinformation. Overall, we observe that problematic transitions where a supporting video is recommended in the up-next video recommendation of a supporting (S->S) or opposing video (O->S) are less than 2%. However, S->S transitions are more in trails with supporting seeds for independents compared to democrats and republicans. Furthermore, N->S transitions are also high in up-next trails with neutral seeds for independents. These findings are problematic. Showing misinformative videos to independents who might not have developed a strong opinion on the election fraud conspiracies could increase their chances of forming a pro-conspiracy belief. We also observe that N->S transitions are more for republicans in the up-next trails with neutral seeds (3.78%) compared to trails with supporting seeds (1.61%). This finding is again troublesome. Past studies have indicated that republicans are more susceptible to electoral fake news [52]. Thus, recommending videos supporting election misinformation to republicans watching neutral videos would expose them to more misinformation which might reinforce or lead to forming conspiratorial beliefs. On analyzing the up-next trails for channel diversity, we observe several interesting phenomena. First, the number of impressions for left-leaning late-night show channels on YouTube such as LastWeekTonight is very high. On average, approximately 3-4 videos from these channels appear in the up-next trails (of length five) when starting with opposing seed videos. Furthermore, these channels appear in the video recommendations of almost all of our study participants. Similar to the late-night shows, we find that fox news also appears on average 3.27 times in the up-next trails of all participants. Future studies can look into the reasons behind the strong 'algorithmic recognizability' [29] and high amplification of these channels in YouTube recommendations. Overall, we conclude that while YouTube has reduced misinformative videos in its up-next recommendations, there is still scope for improving the recommendation algorithm.",
  "8.3 Participants' beliefs vs algorithmic reality": "The study survey conducted before our audit experiment provided us with an opportunity to map participants' beliefs about personalization and trust in YouTube's algorithms with the reality of the situation as determined by our audits. The majority of participants believe that YouTube somewhat personalizes search results. However, in reality, they are hardly personalized. On the other hand, only half of the participants believe up-next recommendations to be highly personalized which is in line with our findings. This mismatch in beliefs and reality indicates users' lack of algorithmic awareness. It also acts as a call to action for the platform to make users aware of the functioning of the algorithms. Users could be made aware of personalization or lack of it by adding design features that promote algorithmic reflection, for example, seeing search results or recommendations of other users [8]. Our survey also showed that, respectively, 19.2% and 14.1% users trust the credibility of information presented to them by YouTube in the search results and up-next recommendations to a great extent. This belief is problematic and indicates reliance on the platform's algorithms to show credible information. In reality, while we find the majority of YouTube's search results to be credible, up-next recommendations still contained misinformative videos. One way to make people spot misinformation on the platform and not blindly trust YouTube's recommendations could be by providing additional context about the content that the participant is searching for or viewing. While YouTube has started displaying Wikipedia links on the platforms [22], additional cues in the form of credibility citations, existing fact-checks or knowledge panel 18 could also be helpful [33].",
  "9 LIMITATIONS AND FUTURE WORK": "Our work is not without limitations. Our audit study is observational in nature, i.e our experiment does not isolate user attributes that produce the differences in misinformation measurements. We only make observations on the differences in misinformation received in searches and recommendations of users with different political affiliations. We recruited participants who used YouTube extensively to get information about the 2020 elections. However, for ethical reasons, we did not analyze participants' account histories to verify their self-reported data. Our participant sample was also not balanced with respect to demographic attributes and political affiliation. We selected YouTube videos that had accumulated the most number of views as the seed videos for our audit experiments. One potential pitfall of such a sampling strategy is that it 18 https://support.google.com/knowledgepanel/answer/9163198?hl=en A post hoc crowd-sourced audit of election misinformation on YouTube CHI '23, April 23-28, 2023, Hamburg, Germany reduces the ecological validity of the experiment since the participants in our study might not have engaged with those videos in the past. Another limitation is that YouTube might have specifically tailored the recommendations of popular misinformative videos. Future studies could consider alternative strategies for sampling videos, such as selecting videos that were more recently published on YouTube or sampling a combination of videos that have accumulated the least and most amount of engagement. The search queries used in our audit also might not be representative of how our study participants formulate queries about the elections. Future studies can survey the study participants to determine how they used YouTube searches in the context of political elections as well as their information needs about the elections. Our classifier developed to annotate the YouTube videos for election misinformation has an error rate of 9% which could have affected the downstream analysis that we performed to quantify the amount of misinformation in various YouTube components. Additionally, we assign an annotation value of 0 to all videos that were removed from YouTube after our audit data collection. While the number of such videos is very small (<1%), it would result in a conservative estimate of misinformation bias present in the search results and recommendations. We use the misinformation bias score adopted from Hussein and Juneja et al's study that captures the amount of misinformation along with the rank of the video [34]. However, this metric does not take into account the relevance of the videos. Future studies can use metrics that measure simultaneously the relevance and credibility in ranked lists such as Normalised Weighted Cumulative Score and Convex Aggregating Measure [47]. In our audit experiment, after testing every condition (watching supporting, neutral, and opposing videos), we performed a step to delete users' YouTube history created by our extension so that it does not impact the other experimental condition. The first author tested out the effect of deletion on users' search and watch history for a few sample queries and videos and found that the effect of such deletion is almost immediate. However, we did not test out this scenario for all search queries and videos used in our audit. Future studies can determine how soon the deletion of history impacts users' recommendations and search results across various topics. Our study focuses on users' beliefs about the personalization and credibility of content on YouTube as well as the role of YouTube's algorithms in driving users to the filter bubbles of problematic content. Future studies can focus on the impact of algorithmic recommendations on the radicalization of users. There are several scholars who argue that algorithms are not centrally culpable for the polarization or the filter bubbles that users experience on online platforms [10, 11, 77]. Many times the users of social media have a more diverse media diet than the non-users [10, 11]. Scholars posit that while algorithms can observe what a user consumes on social media, they cannot determine what the user actually prefers [18]. In other words, a digital choice is not always a true reflection of an individual's preference [18]. Furthermore, users might use different online platforms for different types of content [18]. Thus, to gain a holistic idea of the extent algorithms play a role in user polarization, future audit studies can conduct multiplatform crowd-sourced audits for individuals. These audit studies can determine the impact of algorithmic recommendations on users' social/political viewpoints via surveys and monitor users' patterns of content consumption simultaneously on multiple search engines and social media platforms used by the users.",
  "10 CONCLUSION": "In this study, we conducted a crowd-sourced audit on YouTube to determine the effectiveness of its content regulation policies with respect to election misinformation. We find that YouTube returns videos that debunk election misinformation in its searches. We also find that YouTube leads users to a small number of misinformative videos in up-next trails with seed videos that support election misinformation. Overall, our study shows that while YouTube has been largely successful in removing election misinformation from its searches, there is still scope to fix up-next recommendations.",
  "REFERENCES": "[1] Anton Abilov, Yiqing Hua, Hana Matatov, Ofra Amir, and Mor Naaman. 2021. VoterFraud2020: a Multi-modal Dataset of Election Fraud Claims on Twitter. Proceedings of the International AAAI Conference on Web and Social Media 15, 1 (May 2021), 901-912. https://ojs.aaai.org/index.php/ICWSM/article/view/18113 [2] Hunt Allcott and Matthew Gentzkow. 2017. Social media and fake news in the 2016 election. Journal of economic perspectives 31, 2 (2017), 211-36. [3] Joshua Asplund, Motahhare Eslami, Hari Sundaram, Christian Sandvig, and Karrie Karahalios. 2020. Auditing race and gender discrimination in online housing markets. In Proceedings of the International AAAI Conference on Web and Social Media , Vol. 14. 24-35. [4] Jack Bandy. 2021. Problematic Machine Behavior: A Systematic Literature Review of Algorithm Audits. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 74 (April 2021), 34 pages. https://doi.org/10.1145/3449148 [5] Jack Bandy and Nicholas Diakopoulos. 2020. Auditing news curation systems: A case study examining algorithmic and editorial logic in Apple News. In Proceedings of the International AAAI Conference on Web and Social Media , Vol. 14. 36-47. [6] Jack Bandy and Nicholas Diakopoulos. 2021. More accounts, fewer links: How algorithmic curation impacts media exposure in Twitter timelines. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1-28. [7] Ryan Teague Beckwith. 2022. US Primaries: Election Deniers Go Door-to-Door to Confront Voters After Losses -Bloomberg. https: //www.bloomberg.com/news/articles/2022-08-23/election-deniers-go-door-todoor-to-confront-voters-after-losses?leadSource=uverify%20wall. (Accessed on 09/07/2022). [8] Md Momen Bhuiyan, Carlos Augusto Bautista Isaza, Tanushree Mitra, and Sang Won Lee. 2022. OtherTube: Facilitating Content Discovery and Reflection by Exchanging YouTube Recommendations with Strangers. In CHI Conference on Human Factors in Computing Systems . 1-17. [9] James Bisbee, Megan Brown, Angela Lai, Richard Bonneau, Jonathan Nagler, and Joshua A Tucker. 2022. Election Fraud, YouTube, and Public Perception of the Legitimacy of President Biden. Journal of Online Trust and Safety 1, 3 (2022). [10] Axel Bruns. 2019. Are filter bubbles real? (2019). [11] Axel Bruns. 2019. Filter bubble. Internet Policy Review 8, 4 (2019). [12] Philip Bump. 2021. The unique role of Fox News in the misinformation universe - The Washington Post. https://www.washingtonpost.com/politics/2021/11/08/ unique-role-fox-news-misinformation-universe/. (Accessed on 09/10/2022). [13] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency . PMLR, 77-91. [14] Jonatas C. dos Santos, Sean WM Siqueira, Bernardo Pereira Nunes, Pedro P. Balestrassi, and Fabrício RS Pereira. 2020. Is there personalization in twitter search? a study on polarized opinions about the brazilian welfare reform. In 12th ACM Conference on Web Science . 267-276. [15] José González Cabañas, Ángel Cuevas, and Rubén Cuevas. 2018. Unveiling and quantifying facebook exploitation of sensitive personal data for advertising purposes. In 27th { USENIX } Security Symposium ( { USENIX } Security 18) . 479495. [16] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. 2002. SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research 16 (2002), 321-357. [17] Annie Y Chen, Brendan Nyhan, Jason Reifler, Ronald E Robertson, and Christo Wilson. 2022. Subscriptions and external links help drive resentful users to alternative and extremist YouTube videos. arXiv preprint arXiv:2204.10921 (2022). [18] Peter M Dahlgren. 2021. A critical review of filter bubbles and a comparison with selective exposure. Nordicom Review 42, 1 (2021), 15-33. CHI '23, April 23-28, 2023, Hamburg, Germany Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra [42] Juhi Kulshrestha, Motahhare Eslami, Johnnatan Messias, Muhammad Bilal Zafar, Saptarshi Ghosh, Krishna P. Gummadi, and Karrie Karahalios. 2017. Quantifying A post hoc crowd-sourced audit of election misinformation on YouTube CHI '23, April 23-28, 2023, Hamburg, Germany //oregoncapitalchronicle.com/2022/02/01/oregon-gop-frontrunner-forgovernor-embraces-claims-of-election-fraud/. (Accessed on 09/08/2022). [63] Jakub Simko, Matus Tomlein, Branislav Pecher, Robert Moro, Ivan Srba, Elena Stefancova, Andrea Hrckova, Michal Kompan, Juraj Podrouzek, and Maria Bielikova. 2021. Towards Continuous Automatic Audits of Social Media Adaptive Behavior and its Role in Misinformation Spreading. In Adjunct Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization . 411-414. [64] Patrick Linehan Soo Rin Kim, Laura Romero and Kate Holland. 2022. With 10 weeks until midterms, election deniers are hampering some election preparations - ABC News. https://abcnews.go.com/US/10-weeks-midterms-election-deniershampering-election-preparations/story?id=89007798. (Accessed on 09/07/2022). [65] Hana Stepnick. 2022. How will social media platforms respond to election misinformation? It isn't clear -Poynter. https://www.poynter.org/factchecking/2022/how-will-social-media-platforms-respond-to-electionmisinformation-it-isnt-clear/. (Accessed on 09/08/2022). [66] Yla R Tausczik and James W Pennebaker. 2010. The psychological meaning of words: LIWC and computerized text analysis methods. Journal of language and social psychology 29, 1 (2010), 24-54. [67] The YouTube Team. 2020. Managing harmful conspiracy theories on YouTube. https://blog.youtube/news-and-events/harmful-conspiracy-theoriesyoutube/. (Accessed on 09/08/2022). [68] The YouTube Team. 2020. Supporting the 2020 U.S. election. https://blog.youtube/ news-and-events/supporting-the-2020-us-election/. [69] Alex Thompson. 2020. Trump deploys YouTube as his secret weapon in 2020 POLITICO. https://www.politico.com/news/2020/09/06/trumpyoutube-electioncomeback-408576. (Accessed on 09/08/2022). [70] Matus Tomlein, Branislav Pecher, Jakub Simko, Ivan Srba, Robert Moro, Elena Stefancova, Michal Kompan, Andrea Hrckova, Juraj Podrouzek, and Maria Bielikova. 2021. An Audit of Misinformation Filter Bubbles on YouTube: Bubble Bursting and Recent Behavior Changes. In Fifteenth ACM Conference on Recommender Systems . 1-11. [71] Daniel Trielli and Nicholas Diakopoulos. 2019. Search as news curator: The role of Google in shaping attention to news information. In Proceedings of the 2019 CHI Conference on human factors in computing systems . 1-15. [72] Giridhari Venkatadri, Piotr Sapiezynski, Elissa M Redmiles, Alan Mislove, Oana Goga, Michelle Mazurek, and Krishna P Gummadi. 2019. Auditing Offline Data Brokers via Facebook's Advertising Platform. In The World Wide Web Conference . 1920-1930. [73] YouTube Viewers. 2022. Learn about watch history on YouTube -YouTube. https://www.youtube.com/watch?v=YbWZcgOYHAc&ab_channel= YouTubeViewers. (Accessed on 02/07/2022). [74] Jessica Vitak, Paul Zube, Andrew Smock, Caleb T Carr, Nicole Ellison, and Cliff Lampe. 2011. It's complicated: Facebook users' political participation in the 2008 election. CyberPsychology, behavior, and social networking 14, 3 (2011), 107-114. [75] Daisuke Wakabayashi. Accessed on 09/08/2022. Election misinformation continues staying up on YouTube. -The New York Times. https://www.nytimes.com/2020/11/10/technology/election-misinformationcontinues-staying-up-on-youtube.html. [76] William Webber, Alistair Moffat, and Justin Zobel. 2010. A similarity measure for indefinite rankings. ACM Transactions on Information Systems (TOIS) 28, 4 (2010), 1-38. [77] Joe Whittaker, Seán Looney, Alastair Reed, and Fabio Votta. 2021. Recommender systems and the amplification of extremist content. Internet Policy Review 10, 2 (2021), 1-29. [78] Wikipedia contributors. 2022. Occam's razor - Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/w/index.php?title=Occam%27s_razor&oldid= 1119577554 [Online; accessed 13-September-2022]. [79] Christo Wilson. 2019. The Promise and Peril of Algorithm Audits for Increasing Transparency and Accountability of Donated Datasets. (2019). [80] Sam Wineburg and Sarah McGrew. 2017. Lateral reading: Reading less and learning more when evaluating digital information. (2017). [81] Wenyi Xiao, Huan Zhao, Haojie Pan, Yangqiu Song, Vincent W Zheng, and Qiang Yang. 2019. Beyond personalization: Social content recommendation for creator equality and consumer satisfaction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 235-245. [82] YouTube. 2022. View or delete search history -Computer -YouTube Help. https://support.google.com/youtube/answer/57711?co=GENIE.Platform% 3DDesktop&hl=en. (Accessed on 02/07/2022). [83] YouTube. Accessed on 09/08/2022. Elections misinformation policies - YouTube Help. https://support.google.com/youtube/answer/10835034?hl=en. [84] YouTube. Accessed on 09/14/2022. Browse YouTube while incognito on mobile devices - YouTube Help. https://support.google.com/youtube/answer/9040743? hl=en.",
  "A APPENDIX": "",
  "A.1 Participants' characteristics": "In our study survey, we asked participants how often they used the YouTube platform for getting news about the 2020 US Elections. Out of the 99 users who participated in the study, 40.4% reported using YouTube to access election-related news several times a day. When asked whether the 2020 election was stolen from Donald Trump, 59.59% strongly disagreed, 14.14% somewhat disagreed, 11.11% somewhat agreed, and 11.11% strongly agreed. 19.19% participants believed that the US presidential elections were a result of illegal voting or election rigging while 70.70% believed it to be legitimate and accurate. 31.31% participants believed fraud in the United States with respect to the presidential election 2020 - that is, votes being cast in the name of people who are not eligible to vote was a major problem. 41.41% believed voter disenfranchisement in the United States with respect to the presidential election 2020, i.e, eligible voters being prevented from casting their ballots or not having their ballots counted was a major problem. 30.30% participants believed fraud in voting by mail in the U.S. with respect to the presidential election 2020 to be a major problem. 57.57% participants rated Donald Trump's conduct during the presidential elections as poor while 13.13% rated it as excellent. On the other hand, 17.17% of participants rated Joe Biden's conduct during presidential elections as poor while 16.16% rated it as excellent.",
  "A.2 Amazon Mechanical Turk Job": "We used AMT to get annotations for 545 videos for the ground truth dataset. During our manual annotations, we realized that the majority of the videos were irrelevant. In order to get annotations for relevant videos-classes supporting, opposing, and neutral, we curated a list of keywords (such as 'fraud', 'ballot', 'election', 'steal', etc.) and news channels (such as CNN, NTD, Fox news, etc.) on YouTube. Then, we filtered out videos that were published by the curated channels and had the keywords in the title or description. To get high-quality annotations for these videos, we trained and screened the AMT workers. Below we describe the screening process and our annotation task briefly. Worker training and screening: To train workers to do the annotation task and screen them on the basis of their understanding of the annotations, we created a qualification test. The test first described in detail the annotation labels, heuristics, and the annotation task. We provided several examples of YouTube videos for each annotation label and described the process and reason behind assigning a particular label to the video. To ensure that our description of the annotation labels and the task was clear and comprehensive, we posted on r/mturk- a subreddit community of AMT workers as well as AMT workers' unofficial slack channel. After receiving positive feedback from the AMT community, we released the qualification test. We also included three questions in the qualifying test asking AMT workers to annotate YouTube videos whose annotation labels were known in advance. These videos were already annotated previously by the authors. AMT workers who correctly labeled all three videos (100% score) qualified for the YouTube annotation task. In addition to getting a perfect score on the qualification test, we also required AMT workers to have at least a 90% approval rating CHI '23, April 23-28, 2023, Hamburg, Germany Prerna Juneja, Md Momen Bhuiyan, and Tanushree Mitra AMT workers 1) Approval rating>90 2) Reside in US Train workers Screen workers Work on annota- tion task Annotate three YouTube videos whose labels were known in advance 1) Annotate YouTube video 2) Provide rationale Description of annotation task, and labels with sev- eral examples Qualification test Full score in screening AMT workers' unofficial slack community Receive positive feedback r/mturk: subreddit comm- unity of AMT workers labels, heuristics and the Figure 16: Figure illustrating the process of obtaining YouTube video annotations from AMT workers. The workers were screened via a qualification test where they were first trained by providing detailed descriptions of the annotation labels. To test their understanding, they were asked to annotate three YouTube videos whose labels were known in advance. Workers who correctly labeled the three videos proceeded to work on the annotation task. To ensure that our description of the annotation labels and the task was clear and comprehensive, we posted on r/mturk-a subreddit community of AMT workers and AMT workers' unofficial slack channel. We released our qualification test and annotation task after receiving positive feedback from the AMT community. on the AMT platform. YouTube annotation task: The YouTube annotation task required AMT workers to assign a label to the video and also provide the rationale behind selecting the label. We enforced a minimum word limit of 10 characters for the rationale. We released YouTube videos in a batch sizes of 10 and 15 and obtained three annotations for each video. The majority response was selected as the annotation label for the video. In total, we obtained annotations for 545 videos, out of which consensus was reached on 516 videos (Supporting: 26, Opposing: 74, Neutral: 77, Irrelevant: 318, YouTube video in a language other than English: 6, URL not accessible: 15). KPRC 20 are affiliated with national channels. If we did not find the bias ratings for such local channels, we assigned them the label of their affiliations. For example, KHOU is associated with center-left CBS and thus, was also assigned a center-left rating. We assigned channels that didn't fall under the news category the neutral label. We manually checked a random sample (n=50) of non-news channels and found only one channel that had content about the news. Therefore, this process produced channel bias annotations (to be used as a feature in our classifier) with reasonable accuracy for our study, given that channel bias detection is not the main focus of our work.",
  "A.3 Annotating YouTube channels for partisan bias": "Our dataset of unique videos came from a large number of YouTube channels ( ∼ 17.5K) devoted to both news and non-news content. We coded the leaning of the channel on a 5-point Likert scale (far-left, center-left, neutral, center-right, and far-right) using computational methods and several heuristics. First, to identify news-related channels, we used several pattern-matching techniques (e.g., finding keyword news in the channel's name, etc.) and discovered a total of 802 news channels. Then we used existing datasets on media bias from mediabiasfactcheck.com and allsides.com for annotating the channels. For channels whose annotations were not available in the datasets, we manually went through their title, description, sample videos, and related information from their website, Wikipedia, and/or google search to identify their leaning or the leaning of their affiliations. Many local news channels such as KHOU 19 or 19 https://www.youtube.com/c/KHOU 20 https://www.youtube.com/c/KPRC2Click2Houston",
  "keywords_parsed": [
    "misinformation",
    "elections",
    "voter fraud",
    "algorithm audit",
    "fairness",
    "recommendations"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "VoterFraud2020: a Multi-modal Dataset of Election Fraud Claims on Twitter"
    },
    {
      "ref_id": "b2",
      "title": "Social media and fake news in the 2016 election"
    },
    {
      "ref_id": "b3",
      "title": "Auditing race and gender discrimination in online housing markets"
    },
    {
      "ref_id": "b4",
      "title": "Problematic Machine Behavior: A Systematic Literature Review of Algorithm Audits"
    },
    {
      "ref_id": "b5",
      "title": "Auditing news curation systems: A case study examining algorithmic and editorial logic in Apple News"
    },
    {
      "ref_id": "b6",
      "title": "More accounts, fewer links: How algorithmic curation impacts media exposure in Twitter timelines"
    },
    {
      "ref_id": "b7",
      "title": "US Primaries: Election Deniers Go Door-to-Door to Confront Voters After Losses -Bloomberg"
    },
    {
      "ref_id": "b8",
      "title": "OtherTube: Facilitating Content Discovery and Reflection by Exchanging YouTube Recommendations with Strangers"
    },
    {
      "ref_id": "b9",
      "title": "Election Fraud, YouTube, and Public Perception of the Legitimacy of President Biden"
    },
    {
      "ref_id": "b10",
      "title": "Are filter bubbles real?"
    },
    {
      "ref_id": "b11",
      "title": "Filter bubble"
    },
    {
      "ref_id": "b12",
      "title": "The unique role of Fox News in the misinformation universe - The Washington Post"
    },
    {
      "ref_id": "b13",
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification"
    },
    {
      "ref_id": "b14",
      "title": "Is there personalization in twitter search? a study on polarized opinions about the brazilian welfare reform"
    },
    {
      "ref_id": "b15",
      "title": "Unveiling and quantifying facebook exploitation of sensitive personal data for advertising purposes"
    },
    {
      "ref_id": "b16",
      "title": "SMOTE: synthetic minority over-sampling technique"
    },
    {
      "ref_id": "b17",
      "title": "Subscriptions and external links help drive resentful users to alternative and extremist YouTube videos"
    },
    {
      "ref_id": "b18",
      "title": "A critical review of filter bubbles and a comparison with selective exposure"
    },
    {
      "ref_id": "b42",
      "title": "Quantifying A post hoc crowd-sourced audit of election misinformation on YouTube"
    },
    {
      "ref_id": "b63",
      "title": "Towards Continuous Automatic Audits of Social Media Adaptive Behavior and its Role in Misinformation Spreading"
    },
    {
      "ref_id": "b64",
      "title": "With 10 weeks until midterms, election deniers are hampering some election preparations - ABC News"
    },
    {
      "ref_id": "b65",
      "title": "How will social media platforms respond to election misinformation? It isn't clear -Poynter"
    },
    {
      "ref_id": "b66",
      "title": "The psychological meaning of words: LIWC and computerized text analysis methods"
    },
    {
      "ref_id": "b67",
      "title": "Managing harmful conspiracy theories on YouTube"
    },
    {
      "ref_id": "b68",
      "title": "Supporting the 2020 U.S. election"
    },
    {
      "ref_id": "b69",
      "title": "Trump deploys YouTube as his secret weapon in 2020 POLITICO"
    },
    {
      "ref_id": "b70",
      "title": "An Audit of Misinformation Filter Bubbles on YouTube: Bubble Bursting and Recent Behavior Changes"
    },
    {
      "ref_id": "b71",
      "title": "Search as news curator: The role of Google in shaping attention to news information"
    },
    {
      "ref_id": "b72",
      "title": "Auditing Offline Data Brokers via Facebook's Advertising Platform"
    },
    {
      "ref_id": "b73",
      "title": "Learn about watch history on YouTube -YouTube"
    },
    {
      "ref_id": "b74",
      "title": "It's complicated: Facebook users' political participation in the 2008 election"
    },
    {
      "ref_id": "b75",
      "title": "Election misinformation continues staying up on YouTube -The New York Times"
    },
    {
      "ref_id": "b76",
      "title": "A similarity measure for indefinite rankings"
    },
    {
      "ref_id": "b77",
      "title": "Recommender systems and the amplification of extremist content"
    },
    {
      "ref_id": "b78",
      "title": "Occam's razor - Wikipedia, The Free Encyclopedia"
    },
    {
      "ref_id": "b79",
      "title": "The Promise and Peril of Algorithm Audits for Increasing Transparency and Accountability of Donated Datasets"
    },
    {
      "ref_id": "b80",
      "title": "Lateral reading: Reading less and learning more when evaluating digital information"
    },
    {
      "ref_id": "b81",
      "title": "Beyond personalization: Social content recommendation for creator equality and consumer satisfaction"
    },
    {
      "ref_id": "b82",
      "title": "View or delete search history -Computer -YouTube Help"
    },
    {
      "ref_id": "b83",
      "title": "Elections misinformation policies - YouTube Help"
    },
    {
      "ref_id": "b84",
      "title": "Browse YouTube while incognito on mobile devices - YouTube Help"
    }
  ]
}