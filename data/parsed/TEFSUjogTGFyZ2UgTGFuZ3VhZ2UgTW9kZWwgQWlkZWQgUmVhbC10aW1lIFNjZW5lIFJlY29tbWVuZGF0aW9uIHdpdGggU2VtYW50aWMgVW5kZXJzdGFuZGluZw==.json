{
  "LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding": "Zhizhong Wan wanzhizhong@meituan.com Meituan Beijing, China Bin Yin yinbin05@meituan.com Meituan Beijing, China Junjie Xie xiejunjie02@meituan.com Meituan Beijing, China",
  "Fei Jiang": "jiangfei05@meituan.com Meituan Beijing, China",
  "Xiang Li": "lixiang245@meituan.com Meituan Beijing, China",
  "Wei Lin": "linwei31@meituan.com Meituan Beijing, China",
  "ABSTRACT": "",
  "ACMReference Format:": "Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS), aiming to provide personalized recommendation services for users in many aspects such as food delivery, e-commerce and so on. However, traditional RS relies on collaborative signals, which lacks semantic understanding to real-time scenes. We also noticed that a major challenge in utilizing Large Language Models (LLMs) for practical recommendation purposes is their efficiency in dealing with long text input. To break through the problems above, we propose Large Language Model Aided Real-time Scene Recommendation(LARR), adopt LLMs for semantic understanding, utilizing real-time scene information in RS without requiring LLM to process the entire real-time scene text directly, thereby enhancing the efficiency of LLM-based CTR modeling. Specifically, recommendation domain-specific knowledge is injected into LLM and then RS employs an aggregation encoder to build real-time scene information from separate LLM's outputs. Firstly, a LLM is continual pretrained on corpus built from recommendation data with the aid of special tokens. Subsequently, the LLM is fine-tuned via contrastive learning on three kinds of sample construction strategies. Through this step, LLM is transformed into a text embedding model. Finally, LLM's separate outputs for different scene features are aggregated by an encoder, aligning to collaborative signals in RS, enhancing the performance of recommendation model.",
  "CCS CONCEPTS": "· Computing methodologies → Artificial intelligence .",
  "KEYWORDS": "Recommendation System, Large Language Model, Contrastive Learning Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys '24, October 14-18, 2024, Bari, Italy © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0505-2/24/10...$15.00 https://doi.org/10.1145/3640457.3688135 Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin. 2024. LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding. In 18th ACM Conference on Recommender Systems (RecSys '24), October 14-18, 2024, Bari, Italy. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3640457.3688135",
  "1 INTRODUCTION": "As an important task in field of recommendation systems (RS), ClickThrough Rate (CTR) prediction aims to forecast whether a user will click on a certain product, content, or advertisement. In the domain of RS, models need to analyze data such as user characteristics and item features to predict the probability of a user clicking on a specific objective, thereby helping to optimize the effectiveness of recommendation strategies. In the context of food delivery service, the volume of data that needs to be processed each day reaches hundreds of millions. Unlike typical e-commerce recommendations or content recommendations, food delivery recommendations place a stronger emphasis on analyzing real-time scenes such as geographical location, mealtime, weather, etc. Among them, there exist strong correlations and rich semantics. Consider a specific scene: a user traveling to another city arrives late at night while many restaurants are already closed, and it is raining heavily with low temperatures. Due to the user's unfamiliarity with the surrounding environment and the poor weather conditions, there is a high probability that he will order a takeout from a nearby restaurant. What's more, he's more likely to order some hot food or local specialties. Points of Interest (POI) is a concept we define based on food delivery recommendations, aimed at abstracting the focus of user interest. It can be understood as a collection of POIs, environment, and geographic locations. Traditional recommendation models use feature crossing to handle POIs in real-time scene data. For correlated and important features, common feature crossing methods include manual explicit crossing followed by logistic regression [5], explicit low-order crossing [5], or directly using deep learning for automatic high-order crossing [12, 15, 29]. However, regardless of which method is used, the essence of feature crossing is based on the co-occurrence probability of strongly correlated features to determine whether a user will click, lacking an understanding of the POI in the scene's semantic information. RecSys '24, October 14-18, 2024, Bari, Italy Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin Large Language Models (LLMs) possess extensive semantic knowledge and excellent reasoning capabilities [13, 14], which can effectively understand the semantic information in real-time scenes. However, in the field of RS, LLMs still face some difficulties. In particular, LLMs in their original form may not possess in-depth knowledge specific to the recommendation domain, which could affect their comprehensive understanding of POI and user information. At the same time, LLMs typically do not directly handle the cross-feature signals present in traditional recommendation models, which may limit their ability to accurately capture and understand user needs. The unique recommendation scene of food delivery presents us with two core challenges: · Typical RS treats each food delivery POI as mapped unique id token as inputs of the recommendation model to predict user behavior. However, this approach treats all POI equally without considering semantic information of POIs, such as the menu similarity of food delivery, relevance of restaurant's main business direction to the scene and so on. For instance, pizza shops with different names would be treated as two unrelated POIs in a normal RS, neglecting the fact that both of them primarily sell pizza. During summer, people tend to order something cold rather than taste spicy and hot food. · Industrial RS needs to serve for millions of users, unacceptable time consuming of LLM's inference stays a unsolved problem in the exploring of combination with RS and LLM. Real-time scenes are composed of many individual scene features, with combinations of different scene features giving rise to an exponential number of different real-time scenes. To understand real-time scenes, LLMs need to process combinations of scene feature texts during serving, which seems that LLMs have to be involved in inference, leading to service latency. How to efficiently utilize LLMs to handle real-time scenes is an urgent problem to be solved. To address the two problems mentioned above, inspired by recent work in the LLM field [19, 24] and recommendation system domain [7, 11], we propose a three-stage model LARR (Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding). Figure 1 uses an intuitive example to show how LARR works. The LLM, infused with recommendation domain knowledge, Figure 1: How LLM understands the real-time scenes and help Recommendation System work. This User may like ice-cream/spicy hot pot_. Hot. Maybe Hot Pot is not suitable_. dessert shop nearby: A, B, C. A mainly sells bubble tea_ Recommendation B mainly sells ice-cream C mainly sells yogurt_ LLM User: 20 years old. Loving Dessert/Hot Pot/ . Loc: Seashore, XX, XXX Env: Its Saturday afternoon, 32 degrees. Hist: Yesterday evening, bubble tea(half sugar) / . Very processes various scene information based on its knowledge, such as user profiles, geographic locations, weather, etc. The LLM contemplates the input information according to its existing knowledge, assisting the recommendation system in providing recommendation services. In the example of Figure 1, LLM receives input information including user profiles, geographic locations, environment, user history and so on. Because LLM is injected with recommendation domain knowledge in advance, LLM knows what style of food the user may like, which kind of food is more popular in current environment, where to find tasty shops nearby. From the user profile (likes desserts/hotpot), LLM knows ice-cream or spicy hot pot is ok; furthermore, since it is very hot, hot pot is obviously not quite suitable, ice cream or a cold drink would be a better choice. There are three stores A, B, and C selling desserts around the input geographic location, and their takeout information is... Like this, step by step, the LLM finally summarizes useful information and contributes to RS in recommending takeout for the user. Our core contributions are as follows: · Based on food delivery domain-specific recommendation data, We employ a LLM to deal with the semantic problem of how to correctly understand the POI in food delivery real-time scene. We continue pretraining and fine-tuning the LLM, infusing them with knowledge of recommendation field, promoting the LLMs' understanding of food delivery real-time scenes. · We propose a novel industrial-friendly recommendation framework, LARR, which can effectively integrate the semantic information of food delivery real-time scenes without LLM's participation in inference. We use a bidirectional encoder to aggregate the separate real-time scene embedding produced by LLM to extract the semantic information of real-time scenes, since the semantic associations between various scene features have already been implicitly encoded into the LLM's output vectors. · We conducted extensive offline and online experiments on the food delivery dataset from Meituan Waimai, validating that our method can fully understand the semantic information of realtime scenes and effectively integrate multimodal features, enhancing the recommendation results to align the semantic information understood by large language models about the food delivery real-time scene with the recommendation models based on collaborative signals.",
  "2 RELATED WORK": "",
  "2.1 Large Language Model": "Substantial work [8, 16, 37] has shown that Pre-trained models (PTMs) on a large corpus can learn universal language representations, which are beneficial for downstream NLP tasks and can avoid training a new model from scratch [26]. Large language models (LLMs) mainly refer to transformer-based [31] neural language models that contain tens to hundreds of billions of parameters [38]. Those language models are pre-trained on massive text data, such as PaLM [6], LLaMA [30], and GPT-4 [1]. Compared to PLMs, LLMs are not only significantly larger in model size but also demonstrate superior language understanding and generation capabilities, more importantly, they exhibit emergent abilities [32] that are absent in smaller-scale language models. LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding RecSys '24, October 14-18, 2024, Bari, Italy LLMs have already drawn a lot of attention due to the strong performance on a wide range of natural language tasks, many researchers in other fileds try to combine LLM with domain-specific work to promote their progress and has achieved success [17, 18, 34].",
  "2.2 LLM in Recommend System": "Traditional recommendation models are designed to leverage a huge amount of ID tokens to train, which is an ID paradigm, with the majority of parameters concentrated in the embedding layer [21, 29]. In contrast, LLMs would use a tokenizer to segment the text into vocab tokens to reduce the size of the vocabulary at the very first of input, which is a tokenization paradigm, with the bulk of parameters being concentrated in the network itself. Recommendation models excel in memorization, while LLMs demonstrate superior capabilities in logical reasoning and generalization. A 6billion parameter LLM such as ChatGLM-3 [9] has a vocabulary less than 65,000 tokens, a scale that is significantly smaller than the number of ID tokens in a recommendation model of the same scale [23]. It's apparent that simple integration of recommendation systems with LLMs is infeasible. If ID tokens are directly used as input, tokenization may establish connections between unrelated IDs (e.g., 'id_499' tokenized into 'id', '_ ', '4', '9', '9', creating overlapping embeddings with IDs containing the digits 4 and 9, which does not meet our expectations). Alternatively, regard the ID tokens as special tokens of LLM's vocabulary, that is, without tokenization on ID input, would face gap problem between ID tokens representing user/item collaborative information and pre-trained vocab tokens holding content semantics information [4]. Moreover, the typically vast number of ID tokens could dilute the LLM's own vocabulary, inject noise into the vocab tokens, and result in poor learning of the ID tokens. Despite the challenges mentioned above associated with applying language models to the field of recommendation, researchers continue to dedicate efforts to applying language models to Recommender Systems (RS), due to the astonishing capabilities demonstrated by LLMs [20, 33]. The researchers of P5 [11] proposed a unified text-to-text paradigm recommendation model based on T5 [27], hoping to handle rating prediction task, sequential recommendation task and more downstream tasks by zero-shot or few-shots. Anembedding method called whole-word embedding whose design inspiration is very similar to position embedding is introduced to address ID-related gap problem mentioned above. M6-rec [7] converts all recommend downstream tasks into language understanding or generation tasks by representing user behavior data and candidates data if necessary as natural language plain texts for fine-tuning based on M6 [22]. CLLM4Rec [39] propose a novel soft/hard prompting strategy, mutually-regularized pre-training two LLMs and two set of id tokens on two corpora to facilitate language modeling on RS-specific corpora with heterogeneous user/item collaborative tokens and content tokens. As an unsupervised method, Contrastive Learning (CL) assumes some observed pairs of text that are more semantically similar than randomly sampled text. By maximazing their mutual information, neural network could learn useful embeddings for downstream tasks [2, 36]. OpenAI has attempted to convert pre-trained language models into vector models for text and code using contrastive learning, achieving notable results [24]. RLMRec [28] has proven that contrastive learning between the semantic embeddings from LLMs and the collaborative embeddings from recommender systems could capture their shared information and alleviate the noise information. ControlRec [25] uses contrastive learning for heterogeneous feature matching to align the ID representations with the natural language in the semantic space. Li proposed a two-stage model, CTRL [19]. In the first stage, an LLM is used to encode textual data and a lightweight collaborative model is used to encode tabular data, cross-modal contrastive learning is employed to fine-grained align knowledge of the two modalities. In the second stage, the lightweight collaborative model is fine-tuned on downstream tasks. During inference, CTRL only deploys the collaborative model, with the LLM not being involved in computation, making it an industrial-friendly model architecture.",
  "3 METHODOLOGY": "In this section, we will then introduce the framework of LARR, which comprises three stages, with the overall structure illustrated in Fig 2. The first stage is the continual pretraining stage, where we construct natural language texts corpus from the Meituan Waimai dataset. Then we continue pretraining the LLM that has been pretrained on general corpora based on corpus to inject the domainspecific knowledge. In the second stage, we additionally add billions of user profile and user history behavior data into the corpus, which is proven to be useful [10], constructing 3 kinds of contrastive learning positive and negative samples, transforming the LLM into a text embedding model, enabling the LLM to fully understand the semantics of real-time scenes. The final third stage is the multi-modal alignment stage, where contrastive learning is used to maximize the mutual information between semantic embeddings and collaborative embeddings, aligning the takeout scene semantic information understood by the LLM with the collaborative signals extracted by the fine-tuning model from ID tokens, cross features, and statistical features. The aligned semantic information will enhance performance of recommendation system.",
  "3.1 Continual Pretraining Task Design": "To facilitate LLM's understanding in food delivery domain-specific knowledge from the natural language perspective, we first construct a corpus using datasets relevant to all POIs. Following this, we perform continual pretraining task on the LLM which had already been pre-trained on generic corpora, using this corpus to enhance its understanding of domain-specific knowledge. How to use id tokens in LLM efficiently remains an unsolved problem for a long-time. In industrial scene, encoding the ids of millions of different POIs as whole words is impractical due to the high time and memory costs involved. Moreover, simply treating POI id tokens as indivisible inputs to the LLM brings a semantic and collaborative signal gap, as mentioned in section 2.2; segmenting and encoding ids could also introduce unintended connections between id tokens. Consequently, we abandoned the use of id token as LLM inputs and instead used unique natural language description texts to represent id features. We noted that a POI can be uniquely identified by its its name and geographical location, which is equivalent RecSys '24, October 14-18, 2024, Bari, Italy Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin Figure 2: Model overview including 3 stages. In stage 1, The LLM undergoes a continual pretraining task on shop-related corpus; In stage 2, LLM is fine-tuned and transformed into a text embedding model via contrastive learning on 3 types of positive samples; In stage 3, alignment is applying on LLM's semantic embedding and RS's collaborative embedding for enhancing the performance of the recommendation results. Stage 3 User LLM User Emb Large Language Model RS Shop LLM Shop Emb Textualize Embedding Layer User Shop ID 1 ID 2 ID 3 Stage 2 User Profile <sp>lt mainly sells fast food<lspz<spzlt sells User Action Large Language Model Stage 1 Delivery Info Shop Delivery Info Shop XX<Isp> User to an id feature. The advantage of using name and geographical location to represent a POI is apparently: natural language input makes it easier for LLM to understand. It retains the uniqueness of id inputs while avoiding the semantic meaninglessness of id inputs. Specifically, assuming that there are 𝑁 𝑝 POIs, we merge name, geographical location, introduction, and statistical information to form a description 𝐷 𝑖 for each POI 𝑃𝑂𝐼 𝑖 .  The description 𝐷 𝑖 is cut into 𝑛 𝑘 different slices and we named each slice a keyword, such as name, location, tag name... The 𝑖 𝑡ℎ restaurant's description 𝐷 𝑖 could be formulated as a text set 𝑡 𝑖 :   where [ ... ] is concatenation between different text and 𝑡 𝑖 𝑘 is a discrete text feature, 𝑡 𝑖 is the aggregation of a bunch of discrete text features. Through the descriptive text, the LLM could capture the similarity among different POIs. Since the language model was pre-trained on general corpora, thus there are some gap between the pretraining and continual pretraining corpora. To address this problem, we introduce some special tokens to assist the LLM in better understanding the content during the continual pretraining stage. To be specific, we use a set of special token 𝑠𝑝 for 𝑛 𝑘 keywords in description to wrap key information.  For example, <POI_name> and </POI_name> would be added at the very first and the very last of the 0 𝑡ℎ keyword POI name respectively, the former special token is 𝑠𝑝 𝑏𝑜𝑠 0 indicating that the following information pertains to the restaurant's name while the trailing one is is 𝑠𝑝 𝑒𝑜𝑠 0 , signaling the end of the name input. With the help of special token set 𝑠𝑝 , We split the original description of each POI into 𝐷 𝑖 = [ 𝑥 𝑖 , 𝑦 𝑖 ] for LLM. The text input 𝑥 𝑖 and expected text output 𝑦 𝑖 could be formulated as follows:   where [ ... ] means concatenation between different text. We denote the LLM parameters as 𝜃 and employ a generative loss 𝐿 𝑐𝑝 for continual pretraining.  The input of LLM consists of restaurant's name and location 𝑥 𝑖 , aiming to predict other detail information 𝑦 𝑖 such as POI's introduction, main dishes offered... During the training process, the model learned the associations between similar dishes and built connections among geographical locations, POI names, menu and so on.",
  "3.2 Text Embedding via Contrastive Learning": "There is a huge gap between the normal decoder-only architecture of LLM and text embedding models, because language models train and infer in a way of autoregressive approach. To efficiently leverage the semantic information from the LLM, a critical problem we faced is how to transform the LM into a text embedding model. Common methods for converting a language model into one that could encode a sentence into a embedding include using the output embedding corresponding to some special tokens as the sentence embedding, or directly use some pooling methods on embeddings of all words in sentence to obtain the sentence embedding. Inspired by OpenAI [24], we decide to adopt contrastive learning to convert the LLM into a text embedding model. In NLP field, contrastive learning methods usually construct positive sample pairs by applying corruptions such as dropout on text to generate positive sample pairs, random sampling for negative sample pairs. However, in RS, the importance of user, POI pairs in the food delivery context is very high, so we do not simply use corruption. Instead, we constructed positive and negative sample pairs from three perspectives: user-user, POI-POI, and user-POI. As is shown in figure 3, the LLM tends to in classify three kinds of input is positive or not in contrastive learning procedure. First of all, because of user-side text is added, we utilize a set special tokens 𝑠𝑝 𝑢 for user-side text similar to 𝑠𝑝 𝑝 in section 3.1.  LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding RecSys '24, October 14-18, 2024, Bari, Italy Figure 3: Positive pairs construction. Red curves represent alignment procedure. CL <sp>Nick Name <Isp> <sp>Env History<lsp> <spvage<lsp> User <sp>Purchasing History<lsp> <sp>Preference tag<lsp> Profile Action Up Ua User CL U POI P Deliver Shop Info Info <sp>Description<lsp> <sp>Location<lsp> Pa Pb CL User The 𝑛 𝑞 represents the number of keywords in user-related text. We denote the union of two sets as 𝑠𝑝 .   Here, set 𝑠𝑝 has 𝑛 𝑘 + 𝑛 𝑞 elements. Wedefine a LLM-based score functiona 𝑠 𝜃 for evaluating the similarity of inputs. The score functiona 𝑠 𝜃 would calculate the similarity of two input text base on 𝑃 𝜃 . Similar to continual pretraining stage, the description 𝑧 of POI or user is divided into several segments, interspersed with special tokens in 𝑆𝑃 . Assuming that there's a series of text input 𝑧 = [ 𝑠𝑝 𝑏𝑜𝑠 0 , 𝑧 0 , 𝑠𝑝 𝑒𝑜𝑠 0 , 𝑠𝑝 𝑏𝑜𝑠 1 , 𝑧 1 , 𝑠𝑝 𝑒𝑜𝑠 1 ...𝑠𝑝 𝑏𝑜𝑠 𝑙 , 𝑧 𝑙 , 𝑠𝑝 𝑒𝑜𝑠 𝑙 ] , it would be first tokenized into a series of tokens and mapping to a series of token embedding Z = [ z0 , z1 ... zm ] , then sending to LLM after padding.  where 𝑀𝐿𝑃 is a linear layer to project the embedding of last token in last hidden layer to a continuous vector space for scoring.  Here, 𝑠 could be a similarity function such as cosine similarity; or negative form of the output of a distance function such as cosine distance. It could be symmetric or asymmetric. If 𝑠 is symmetric, then 𝑠 𝜃 is symmetric; otherwise 𝑠 𝜃 asymmetric. USER-USER In user-user contrastive learning, we focus on user's profile text descriptions 𝑈 𝑝 (such as nicknames, gender, etc.) and the user's action text descriptions 𝑈 𝑎 (such as historical actions, environments, click and order price statistics...). These two types of text descriptions are positive samples for the same user and negative samples for different users. During training, we adopt a contrastive learning approach, with the loss being the Info NCE Loss. For user-user contrastive loss 𝐿 𝑈𝑈 , considering the general situation, we assume that 𝑠 𝜃 is asymmetric. As a result, the form of contrastive loss is symmetrical and the sub loss 𝐿 𝑈𝑈 𝑝𝑎 could be fomulated as follows:  here 𝑠 𝜃 is a score function for evaluating the similarity of inputs.symbol + means corresponding positive sample and -is negative samples that using in-batch negative sampling strategy. The final loss of user-user contrastive loss is average of 𝑈 𝑝 to 𝑈 𝑎 and 𝑈 𝑎 to 𝑈 𝑝 .  POI-POI POI-POI contrastive learning considers the POI's food delivery business text description 𝑃 𝑑 (such as name, location, menu) and the POI's basic information text description 𝑃 𝑏 (such as shop normal introduction and some tags). Descriptions of the same POI are positive sample pairs, while those of different POIs form negative sample pairs. Similarly, the sub loss 𝐿 𝑃𝑃 𝑑𝑏 POI-POI contrastive loss is as follows:  The POI-POI contrastive loss 𝐿 𝑃𝑃 is just like what 𝐿 𝑢𝑢 organizes:  USER-POI The user-POI pairs are based on user's comprehensive text description 𝑈 (including user profile and user history texts):  The POI's comprehensive text description 𝑃 (including food delivery business text and basic information text):  If a user paid for delivery in a POI, the comprehensive text descriptions of that user and POI constitute a positive sample pair; RecSys '24, October 14-18, 2024, Bari, Italy Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin otherwise, they are a negative sample pair.  (19) To make it symmetric, 𝐿 𝑈𝑃 is arithmetic mean of 𝐿 𝑈𝑃 𝑢𝑝 and 𝐿 𝑈𝑃 𝑝𝑢 .  Through the design of these three types of sample pairs, model can not only deeply understand the personalized characteristics of users' but also the delivery information of POIs' on user-user and POI-POI contrastive learning respectively. As 𝑈 𝑎 records each delivery order and corresponding environment, LLM could precisely align user demands with POI offerings in complex and everchanging environments.  𝜆 here is for scaling the separate loss. In inference, features that have appeared for users and POIs are sequentially fed into the LLM to infer and produce the corresponding description vectors from the feature level. The reason for not producing LLM vectors at the text level (aggregation of features) is due to the consideration that some features may change during the serving stage, such as weather, time slots, statistical features, etc. The cost of exhaustively generating all possible feature combination texts is exponential. Additionally, the user's long interaction sequence is also used to produce interaction sequence vectors.",
  "3.3 Information Alignment in RS": "In this section, contrastive learning is applied to alignment, maximizing the mutual information between the semantic information of the food delivery scene understood by LLM and the collaborative signals extracted from ID tokens, cross features, and statistical features by the RS model. The alignment not only extracts the shared information between semantic and collaborative signals but also reduces the noise from both, significantly enhancing the performance of the recommendation. Specifically, our RS utilizes real-time scene embedding produced by the LLM after continual pretraining in stage 1 and fine-tuning in stage 2. Throughout this process, all parameters of the LLM are frozen, meaning the LLM acts like an encoder, converting a scene description into continuous vector representations to aid the training of the recommendation model. Since we want the model to be industry-friendly, the LLM should avoid participating in realtime inference as much as possible. We select 𝑟 real-time scene text features and decide to deal with scene text 𝑠 𝑖 one by one, storing them in advance for the recommendation model to utilize, where 0 ≤ 𝑖 ≤ 𝑟 .  Here 𝑠𝑝 𝑒𝑜𝑠 𝑖 represents the position of corresponding key real-time scene feature's end special token. We use the embedding of end special token in last hidden layer to represent real-time scene text. In reality, we selected 10 real-time scene text, that is, 𝑟 is set to 10. We employ a bidirectional transformer encoder 𝜉 to tackle the problems that discrete real-time embeddings lack interaction. Since the LLM's outputs have already implicitly encoded the semantic associations between different scene features, so interactions provided by 𝜉 is necessary. These semantic embeddings produced by LLM are stacked into a sequence and fed into 𝜉 for further aggregation and processing. To more effectively aggregate scene information, we introduced a trick at the beginning of the input sequence, a trainable aggregation token < 𝑎𝑔𝑔 > , corresponding to a vector agg that has the same dimension with real-time scene embedding. This special token plays a crucial role in aggregating all scene keyword and is trained within the transformer encoder to capture and integrate key information from different scene embedding. After processing of transformer encoder 𝜉 , pooling method and a projection head (a normal MLP) is applied to the output embedding sequence, the result is denoted as e 𝑠 , the real-time scene embedding with scene semantic information.  Here 𝑀𝐿𝑃 projects the semantic embedding into the vector space of recommendation model and aligns the dimensions of pooling result and alignment objective. 𝑟 represents the number of real-time scene texts, as mentioned above. Then we align e 𝑠 with target embedding e 𝑡 produced by recommendation model. e 𝑡 is the concatenation of user embedding and POI embedding in recommendation system. The procedure of alignment could be formulated as:  Here 𝑠 is the similarity scoring function. Considering the different symmetry properties of 𝑠 , 𝑁 is batch size. total loss is the sum of two symmetric sub losses.  This design not only improves the quality of both representations but also enables the model to more accurately grasp the connection between user needs and real-time scenes, leading to a significant improvement in recommendation results. With this approach, our recommendation system can provide more personalized and precise recommendations in real-time scenes. During training, the loss is a weighted sum of the recommendation CTR loss 𝐿 𝑐𝑡𝑟 and the contrastive learning loss 𝐿 𝑐𝑙 . 𝐿 𝑐𝑡𝑟 is BCE which is widely-used.",
  "4 EXPERIMENT": "",
  "4.1 Experiment Setup": "Dataset. Our evaluation is conducted on a real-world dataset from Meituan Waimai. For the Click-Through Rate (CTR) prediction task, as shown in Table 1, we perform negative sampling on the log data collected from April 1st to April 7th, 2024, yielding approximately 3.5 billion training samples. For the test set, we uniformly sample the data from the subsequent day, April 8th, 2024, resulting in 84 million samples. Specifically, the input traditional features include user statistical features, real-time contextual features, userPOI cross-statistical features, POI statistical features, and the label, LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding RecSys '24, October 14-18, 2024, Bari, Italy among others. Semantic features comprise descriptions of user foundational information, POI foundational information, as well as statistical descriptions of both user and POI. Table 1: Statistics of the dataset. During the continual pretraining phase, we select information from 4 million POIs to construct approximately 1B training corpora, which is then mixed with 9B Chinese corpora from Wudao [35], forming a total of 10 billion pre-training textual corpora. In the fine-tuning phase, we extract 1 million samples from the historical behaviors of 2 million users for model fine-tuning. Additionally, all user and POI information data has been anonymized during the training of the LLM. Baseline. To evaluate the performance of our experiments, we have selected a variety of classic experimental baselines, encompassing both traditional and semantic models. Specifically: · Wide&Deep [5] is a machine learning model that integrates linear components (Wide) for handling sparse input features with deep neural networks (Deep) to learn feature interactions. It aims to leverage both memorization and generalization and is widely used in recommendation systems. · DeepFM [12] is a recommendation model that combines Factorization Machines (FM) with deep neural networks, designed to retain the advantages of FM while automatically learning high-order feature interactions. · P5 [11] is a model based on the language model T5 that transforms the recommendation task into a text generation task. Additionally, given that P5 is a generative model, we fine-tuned it using the food delivery dataset to fulfill tasks such as CTR prediction. · PLE [29] is a multi-task model based on the Mixed Model of Experts (MMOE) structure, which also serves as the baseline model in our model. · LARR is our proposed model, opts for the PLE model as the foundation for the traditional model component, and Baichuan2-7B [3] for the semantic model part, with the aim of fully understanding the semantic information of real-time scenes and effectively integrating multimodal features to enhance recommendation results. Parameter Setting To evaluate performance, we utilize ClickThrough Rate (CTR AUC) and Conversion Rate (CTCVR AUC) as evaluation metrics, along with Group AUC (GAUC) for assessment.Our backbone LLM utilizes the open-sourced Baichuan2-7B [3]. As detailed in Section 3, the backbone LLM first constructs a descriptive language corpus for all POIs on Meituan, upon which it performs continual pretraining. The epoch is set to 1, with a learning rate of 1e-4. The continual pretraining runs for approximately 96 hours on 32 A100-80G GPUs. Upon completion of continual pretraining, contrastive learning is conducted from three dimensions: \"user-user,\" \"POI-POI,\" and \"user-POI\" to construct the vector model, encoding scene features into 128-dimensional vectors and stacking them into sequences. A 1-layer transformer encoder is employed to aggregate the encoded scene vectors, producing realtime scene information. During the alignment phase, an MLP is used as the projection head to project real-time scene information into the vector space of the alignment target. The projected vectors and alignment targets undergo contrastive learning with Info-NCE loss, treating vectors from the same sample as positive and those from different samples as negative. The temperature is set to 0.1, and the loss balancing coefficient is set to 0.05. The projected vectors and alignment targets, after concatenation, are used for CTR and CTCVR prediction. For fair comparison across all methods, we chose the following parameter configurations: a batch size of 2400, training for one epoch, with a learning rate of 8e-4. The tests run on 8 A100 GPUs, and for other parameters, we follow the best results from the original papers or source codes.",
  "4.2 Performance Comparison": "In this section, we provide a detailed comparison of the performance of our model, LARR, against various baselines. As shown in Table 2, an analysis of the experimental results reveals the following findings: Firstly, we observe that traditional recommendation system models, such as Wide&Deep and DeepFM, generally underperform more advanced deep learning models like PLE. This phenomenon suggests that complex deep network structures have significant advantages in learning feature interactions and capturing complex user interest patterns. By introducing deeper and more intricate interactions, these networks can more finely mine the underlying motivations behind user behavior, thereby achieving better accuracy in recommendations. Secondly, our experimental results indicate that the semantic model P5 does not perform well in the food delivery recommendation task. This may suggest that in recommendation systems, traditional statistical features can sometimes more directly reflect the actual interests of users compared to semantic features. Although semantic models can provide rich contextual information, statistical analysis of user historical behavior may more directly and effectively reveal user preferences in practical recommendation scenes. Finally, our model, LARR, outperforms all baseline models. Compared to the PLE model, LARR achieves a significant 0.58% improvement in CTR AUC and also gains an 0.34% increase in CTCVR AUC. These results not only confirm the superiority of our model but also emphasize the effectiveness of LARR in integrating the semantic information of real-time scenes with recommendation modality information. Through this integration, LARR can capture users' immediate needs and potential interests more accurately, significantly enhancing the overall performance and user satisfaction of the recommendation.",
  "4.3 Ablation Study": "We introduce a novel multimodal information fusion framework, LARR. Initially, based on Food Delivery recommendation data, we perform continual pretraining and fine-tuning on LLMs to infuse RecSys '24, October 14-18, 2024, Bari, Italy Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin Table 2: Performance comparison between our LARR and baselines Table 3: Performance comparison of LARR and different variants. domain knowledge and enhance the LLMs' understanding of realtime scenes. Subsequently, in the CTR task, we effectively integrate the real-time semantic information with recommendation modality information, thereby improving recommendation results. To verify the effectiveness of these designs, we conducted ablation studies. · LARR-PLE Variant: The model variant that completely removes the LLM module, essentially the original PLE model. · LARR-noCPT Variant: The model variant that omits the continual pretraining phase of the LLM. · LARR-noFT Variant: The model variant that excludes the vector fine-tuning phase of the LLM. · LARR-noAL Variant: The model variant that removes the multimodal feature alignment stage, relying solely on a simple feature concatenation strategy. · LARR Complete: The complete LARR framework. The results are illustrated in the figure 3. The improvement of LARR Complete over LARR-noCPT Variant indicates that domain-specific continual pretraining of LLMs can significantly enhance the model's understanding of domain knowledge, thus capturing the semantic information of user and POI more accurately. The comparison between LARR Complete and LARR-noFT Variant demonstrates that the base model of LLM, without vector fine-tuning, does not yield ideal results; however, an LLM that has undergone vector fine-tuning can more effectively understand the personalized features of users and POIs, generating more expressive feature vectors. The comparison between LARR Complete and LARR-noAL Variant underscores the importance of multimodal feature alignment. Simple feature concatenation cannot compensate for the spatial differences between semantic features and traditional collaborative features, whereas a carefully designed multimodal alignment mechanism can achieve effective integration of features from different modalities. Ultimately, LARR Complete surpasses all variants on all metrics, validating the effectiveness of our adopted strategies in enhancing recommendation quality. Overall, the success of LARR is attributed to its profound understanding of complex user behaviors and diverse POI features in real-time scenes, as well as its efficient capability to fuse multimodal information. Our research offers fresh perspectives on how to leverage large language models and contrastive learning to enhance recommendation results.",
  "4.4 Hyperparameter Analysis": "In Section 4.4, our aim is to align the deep semantic information of food delivery scenes as understood by LLMs with the collaborative signals extracted from ID tokens, cross-features, and statistical features by the conventional fine-tuning models. We seek to extract the shared information between semantic and collaborative data to enhance model performance. During the alignment phase of the model, we focus on two key hyperparameters: the temperature coefficient and the negative sample sampling ratio. The temperature coefficient plays a crucial role in the loss function of contrastive learning, adjusting the sensitivity of the loss and influencing the model's ability to distinguish between positive and negative samples. A lower temperature coefficient increases the difficulty for the model to differentiate samples, causing it to focus more on challenging sample pairs. Conversely, the negative sample sampling ratio controls the ratio of negative to positive samples during the contrastive learning process. In this section, we conduct experimental analysis on these two hyperparameters with respect to CTR AUC, with results depicted in Figure 4 (a) Temperature Coefficient vs CTR AUC (b) Negative Sampling Ratio vs CTR AUC 0.8035 0.8025 8020 8015 8010 8005 10 Temperature Coefficient 804 803 802 801 800 799 0.2 0.4 0.6 0.8 1.0 Negative Sampling Ratio",
  "Figure 4: Hyperparameter Analysis": "Figure 4a illustrates the relationship between the temperature coefficient and experimental performance. We find that when the temperature coefficient is set to 0.1, the model achieves optimal performance, indicating that at this specific setting, the model can effectively learn from the data and improve its discrimination ability. However, if the temperature coefficient is set too low or too high, the learning efficiency of the model is impacted, leading to decreased performance. On the other hand, the experimental results regarding the negative sample sampling ratio, as shown in Figure 4b, indicate that introducing negative sample sampling in LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding RecSys '24, October 14-18, 2024, Bari, Italy our setup does not lead to the anticipated performance improvement. Instead, we observe a decline in model performance after employing negative sample sampling. This may be due to the fact that, in certain data and tasks, increasing the number of negative samples does not provide additional information but may instead introduce noise or cause the model to overly focus on negative samples, thereby affecting the model's learning efficiency with positive samples.",
  "4.5 Case Study": "We present a case study to further demonstrate the model's proficiency. Given that out-of-area ordering (where users place orders from locations other than their usual residence for various reasons) has always been an important scenario in food delivery services, we are particularly interested in whether the model can generate good recommendations for out-of-area users based on relevant semantics. We selected some random users who have never been to Guangdong, with the scene of \"user visiting Guangdong for vacation\", designed a simple recall experiment to observe the details of the top 10 POIs in terms of recall scores. A typical result is as follows, with each response presented as location; main dish. Sensitive information such as POI names and details have been omitted. Top1: Guangzhou; Rice Noodle Roll Top2:. Chaoshan; Stir-fried Beef Rice Noodles Top3:. Guangzhou; Stir-fried Beef Rice Noodles Top4:. Shenzhen; Seafood Claypot Congee Top5: Guangzhou; Cantonese Barbecue Top6: Guangzhou; Claypot Rice Top7: Guangzhou; Rice Noodle Roll Top8: Guangzhou; Sweet Soup Top9: Dongguan; Sweet Soup Top10: Foshan; Claypot Rice In this case, Guangdong appeared the most frequently (5 times), with recalled POI primarily offering dishes such as Rice Noodle Roll, Stir-fried Beef Rice Noodles, Cantonese Barbecue, Claypot Rice, and Sweet Soup. ChaoShan, Shenzhen, Dongguan, and Foshan each appeared once or twice. This geographical distribution reflects the culinary diversity and popularity of Guangzhou as the capital city of Guangdong Province, which aligns with our survey findings that these dishes are representative of Guangdong's regional specialties. The recall results cover a variety of classic Cantonese dishes, indicating that the model performs well in capturing local characteristics. The recall results show that the model can identify multiple different types of dishes and accurately match them to the provided scene text, this diversity is positive for enhancing user satisfaction and meeting a wider range of user needs.",
  "4.6 Online A/B Test": "LARR demonstrated its practical value in an online A/B test within the Meituan recommendation system. During the testing period from April 15th to April 21st, 2024, LARR achieved a 2.5% increase in CTR and a 1.2% growth in GMV (Gross Merchandise Volume) compared to the current baseline model. These results confirm the effectiveness of LARR in understanding user needs and enhancing user experience. The increase in CTR indicates a higher acceptance of recommended content by users, which may be attributed to LARR's more accurate capture of user interests, thus providing recommendations that better align with user expectations. The growth in GMV directly reflects the positive impact of LARR on conversion rates and business revenue. This online test verified the practicality and effectiveness of LARR within the Meituan recommendation system.",
  "5 CONCLUSION": "To tackle the lack of understanding of semantic information in RS and the efficiency challenges in LLM-based CTR models, this paper proposes a novel LARR method. LARR continues to pretrain and fine-tune the LLM on some corpora with an enlarged vocabulary, using contrastive learning to align semantic signals and collaborative signals, leverage shared information, and reduce noise. Extensive online and offline experimental results demonstrate that LARR achieves low latency and enhances CTR performance, offering fresh insights for the practical deployment of LLM-based CTR models.",
  "REFERENCES": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. 2019. A theoretical analysis of contrastive unsupervised representation learning. In 36th International Conference on Machine Learning, ICML 2019 . International Machine Learning Society (IMLS), 9904-9923. [3] Baichuan. 2023. Baichuan 2: Open Large-scale Language Models. arXiv preprint arXiv:2309.10305 (2023). https://arxiv.org/abs/2309.10305 [4] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems . 1007-1014. [5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research 24, 240 (2023), 1-113. [7] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6-rec: Generative pretrained language models are open-ended recommender systems. arXiv preprint arXiv:2205.08084 (2022). [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [9] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 320-335. [10] Zhichao Feng, Junjiie Xie, Kaiyuan Li, Yu Qin, Pengfei Wang, Qianzhong Li, Bin Yin, Xiang Li, Wei Lin, and Shangguang Wang. 2024. Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai. arXiv preprint arXiv:2403.12566 (2024). [11] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). In 16th ACM Conference on Recommender Systems, RecSys 2022 . Association for Computing Machinery, Inc, 299-315. [12] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. [n. d.]. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. ([n. d.]). [13] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large language models as zero-shot conversational recommenders. In Proceedings of the 32nd ACM international conference on information and knowledge management . 720-730. [14] Wenyue Hua, Lei Li, Shuyuan Xu, Li Chen, and Yongfeng Zhang. 2023. Tutorial on large language models for recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems . 1281-1283. RecSys '24, October 14-18, 2024, Bari, Italy [15] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM conference on recommender systems . 169-177. [16] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 (2019). [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning . PMLR, 19730-19742. [18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning . PMLR, 12888-12900. [19] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023. Ctrl: Connect tabular and language model for ctr prediction. arXiv preprint arXiv:2306.02841 (2023). [20] Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, and Chunxiao Xing. 2023. E4SRec: An elegant effective efficient extensible solution of large language models for sequential recommendation. arXiv preprint arXiv:2312.02443 (2023). [21] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1754-1763. [22] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, et al. 2021. M6: A chinese multimodal pretrainer. arXiv preprint arXiv:2103.00823 (2021). [23] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930-1939. [24] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2022. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005 (2022). [25] Junyan Qiu, Haitao Wang, Zhaolin Hong, Yiping Yang, Qiang Liu, and Xingxing Wang. 2023. ControlRec: Bridging the semantic gap between language model and personalized recommendation. arXiv preprint arXiv:2311.16441 (2023). [26] XiPeng QIU, TianXiang SUN, YiGe XU, YunFan SHAO, Ning DAI, and XuanJing HUANG. 2017. Pre-trained models for natural language processing: A survey. SCIENCE CHINA Information Sciences 60 (2017), 110100. [27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 1-67. [28] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2023. Representation learning with large language models for recommendation. arXiv preprint arXiv:2310.15950 (2023). [29] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In Proceedings of the 14th ACM Conference on Recommender Systems . 269-278. [30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [32] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022). [33] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models with graph augmentation for recommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining . 806-815. [34] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864 (2023). [35] Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. AI Open 2 (2021), 65-68. [36] Yichi Zhang, Guisheng Yin, and Yuxin Dong. 2023. Contrastive learning with frequency-domain interest trends for sequential recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems . 141-150. [37] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced language representation with informative entities. arXiv preprint arXiv:1905.07129 (2019). Zhizhong Wan, Bin Yin, Junjie Xie, Fei Jiang, Xiang Li, and Wei Lin [38] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [39] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. 2023. Collaborative large language model for recommender systems. arXiv preprint arXiv:2311.01343 (2023). Received 29 April 2024; revised 22 July 2024; accepted 19 August 2024",
  "keywords_parsed": [
    "Recommendation System",
    "Large Language Model",
    "Contrastive Learning"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Gpt-4 technical report"
    },
    {
      "ref_id": "b2",
      "title": "A theoretical analysis of contrastive unsupervised representation learning"
    },
    {
      "ref_id": "b3",
      "title": "Baichuan 2: Open Large-scale Language Models"
    },
    {
      "ref_id": "b4",
      "title": "Tallrec: An effective and efficient tuning framework to align large language model with recommendation"
    },
    {
      "ref_id": "b5",
      "title": "Wide & deep learning for recommender systems"
    },
    {
      "ref_id": "b6",
      "title": "Palm: Scaling language modeling with pathways"
    },
    {
      "ref_id": "b7",
      "title": "M6-rec: Generative pretrained language models are open-ended recommender systems"
    },
    {
      "ref_id": "b8",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "ref_id": "b9",
      "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
    },
    {
      "ref_id": "b10",
      "title": "Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai"
    },
    {
      "ref_id": "b11",
      "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)"
    },
    {
      "ref_id": "b12",
      "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction"
    },
    {
      "ref_id": "b13",
      "title": "Large language models as zero-shot conversational recommenders"
    },
    {
      "ref_id": "b14",
      "title": "Tutorial on large language models for recommendation"
    },
    {
      "ref_id": "b15",
      "title": "FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction"
    },
    {
      "ref_id": "b16",
      "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension"
    },
    {
      "ref_id": "b17",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
    },
    {
      "ref_id": "b18",
      "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
    },
    {
      "ref_id": "b19",
      "title": "Ctrl: Connect tabular and language model for ctr prediction"
    },
    {
      "ref_id": "b20",
      "title": "E4SRec: An elegant effective efficient extensible solution of large language models for sequential recommendation"
    },
    {
      "ref_id": "b21",
      "title": "xdeepfm: Combining explicit and implicit feature interactions for recommender systems"
    },
    {
      "ref_id": "b22",
      "title": "M6: A chinese multimodal pretrainer"
    },
    {
      "ref_id": "b23",
      "title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts"
    },
    {
      "ref_id": "b24",
      "title": "Text and code embeddings by contrastive pre-training"
    },
    {
      "ref_id": "b25",
      "title": "ControlRec: Bridging the semantic gap between language model and personalized recommendation"
    },
    {
      "ref_id": "b26",
      "title": "Pre-trained models for natural language processing: A survey"
    },
    {
      "ref_id": "b27",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer"
    },
    {
      "ref_id": "b28",
      "title": "Representation learning with large language models for recommendation"
    },
    {
      "ref_id": "b29",
      "title": "Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations"
    },
    {
      "ref_id": "b30",
      "title": "Llama: Open and efficient foundation language models"
    },
    {
      "ref_id": "b31",
      "title": "Attention is all you need"
    },
    {
      "ref_id": "b32",
      "title": "Emergent abilities of large language models"
    },
    {
      "ref_id": "b33",
      "title": "Llmrec: Large language models with graph augmentation for recommendation"
    },
    {
      "ref_id": "b34",
      "title": "The rise and potential of large language model based agents: A survey"
    },
    {
      "ref_id": "b35",
      "title": "Wudaocorpora: A super large-scale chinese corpora for pre-training language models"
    },
    {
      "ref_id": "b36",
      "title": "Contrastive learning with frequency-domain interest trends for sequential recommendation"
    },
    {
      "ref_id": "b37",
      "title": "ERNIE: Enhanced language representation with informative entities"
    },
    {
      "ref_id": "b38",
      "title": "A survey of large language models"
    },
    {
      "ref_id": "b39",
      "title": "Collaborative large language model for recommender systems"
    }
  ]
}