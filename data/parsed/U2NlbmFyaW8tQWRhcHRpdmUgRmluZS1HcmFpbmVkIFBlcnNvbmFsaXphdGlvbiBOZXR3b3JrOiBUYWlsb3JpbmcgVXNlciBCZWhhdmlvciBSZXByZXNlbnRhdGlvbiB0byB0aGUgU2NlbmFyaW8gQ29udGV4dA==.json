{"Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User Behavior Representation to the Scenario Context": "Moyu Zhang Lazada Group Beijing, Beijing, China zhangmoyu@bupt.cn Yongxiang Tang Unaffiliated Beijing, Beijing, China tangyongxiang94@gmail.com Jinxin Hu \u2217 Lazada Group Beijing, Beijing, China jinxin.hjx@lazada.com", "ABSTRACT": "As e-commerce has evolved, modern large-scale commercial platforms now accommodate various scenarios to cater to the diverse shopping preferences of users. To conserve resources, current multiscenario methods often utilize a unified framework to deliver personalized recommendations across various scenarios. Given the overlap of users and items in multiple scenarios on commercial platforms, current methods typically employ shared bottom representations, capturing similarities and differences between scenarios through adaptive adjustments. However, existing methods often adjust representations adaptively only after aggregating user behavior sequences. This coarse-grained approach to re-weighting the entire user sequence hampers the model's ability to accurately model the user interest migration across different scenarios. To enhance the model's capacity to capture user interests from historical behavior sequences in each scenario, we develop a ranking framework named the S cenario-Adaptive F ine-Grained P ersonalization Net work (SFPNet), which designs a kind of fine-grained method for multi-scenario personalized recommendations. Specifically, SFPNet comprises a series of blocks named as Scenario-Tailoring Block , stacked sequentially. Each block initially deploys a parameter personalization unit to integrate scenario information at a coarsegrained level by redefining fundamental features. Subsequently, we consolidate scenario-adaptively adjusted feature representations to serve as context information. By employing residual connection, we incorporate this context into the representation of each historical behavior, allowing for context-aware fine-grained customization of the behavior representations at the scenario-level, which in turn supports scenario-aware user interest modeling. Ultimately, the effectiveness of our proposed method is strongly substantiated by extensive experiments and online A/B testing. \u2217 Corresponding Author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Woodstock '18, June 03-05, 2018, Woodstock, NY \u00a9 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/10.1145/1122445.1122456", "Yu Zhang": "Lazada Group Beijing, Beijing, China daoji@lazada.com", "KEYWORDS": "Multi-Scenario Recommendation, Scenario-Specific Behavior Representation, Recommender System", "ACMReference Format:": "Moyu Zhang, Yongxiang Tang, Jinxin Hu, and Yu Zhang. 2018. ScenarioAdaptive Fine-Grained Personalization Network: Tailoring User Behavior Representation to the Scenario Context. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), July 14-18, 2024, Washington D.C., USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/1122445.1122456", "1 INTRODUCTION": "In recent years, with the rise of electronic shopping as a dominant trend, a multitude of e-commerce platforms have emerged, including Amazon, Alibaba's Taobao, Lazada, TikTok, and others [3, 12, 18, 26, 27, 30]. To cater to the growing diversity of user shopping preferences, contemporary commercial platforms must offer personalized services tailored to a variety of shopping contexts. For instance, the online e-commerce platform depicted in Figure 1 offers multiple item recommendation scenarios. The \"Homepage\" scenario primarily suggests a broad range of products potentially appealing to the user, based on their past behavior, whereas the \"My Cart\" and \"My Orders\" scenarios depend more on the user's historical purchase records to generate recommendations. Compared to the \"Homepage\" scenario, user interests in the \"My Cart\" and \"My Orders\" scenarios become more specific. Consequently, these multi-scenario platforms present a challenge to existing recommendation models: how to devise personalized recommendations that consider the distinct characteristics of each scenario? Traditional methods typically employ scenario-specific data to train distinct models for each scenario, enhancing scenario differentiation for online services. However, this isolated model training approach is resource-intensive, often inefficient, and fails to capture inter-scenario correlations, leading to suboptimal performance in data-sparse scenarios. In practice, most current e-commerce platforms experience significant user and item overlap across various scenarios. Consequently, despite the differences in user and item performance across scenarios, substantial correlations persist. Leveraging the correlations across scenarios, we can incorporate data from other scenarios into the training process, alleviating the challenge of data sparsity in individual scenarios. To model Woodstock '18, June 03-05, 2018, Woodstock, NY Moyu Zhang, Yongxiang Tang, Jinxin Hu, and Yu Zhang SACELADY co Homepage Scenario Justfor You Ship 21764 coocoo My Cart Scenario My Orders Scenario Figure 1: An illustrative example of multi-scenarios for the online e-commerce platform Lazada. these inter-scenario correlations, mainstream multi-scenario approaches often employ a unified recommendation modeling framework, broadly categorized into two types: 1) Scenario-specific network structures, influenced by multi-task learning (MTL) [10, 14, 15]. Each scenario is treated as a distinct task, with a common network capturing inter-scenario correlations and separate networks modeling the unique aspects of each scenario. 2) Scenario-adaptive parameter network structures [4, 29], which differ from MTL by acknowledging feature space variations. These methods apply scenario data directly to the core embedding and prediction layers, enabling dynamic adjustments of the feature space and prediction strategy in response to scenario shifts. Compared to MTL methods that require learning distinct network structures for new scenario, the adaptive parameter approach simply incorporates new scenariorelated features, enhancing its flexibility to recognize new scenarios. However, current scenario-adaptive parameter structures face a common issue: they generally transfer information between scenarios coarsely, treating the user's entire historical behavior sequence as a single feature for adjustments, which fails to consider the migration of user interests with the scenario changes. As depicted in Figure 2(a), the weight adjustment method uniformly applies the same weight to each behavior in the user sequence, rather than tailoring scenario-specific information to individual behaviors, thereby impairing the model's capacity to accurately track user interest shifts across scenarios. In fact, modeling user interests is critical in recommendation systems, particularly as users do not always reveal their intentions in non-search scenarios, making it essential to accurately track the migration of user interests for effective multi-scenario recommendations. For instance, on the \"Homepage\" scenario, where user interests tend to be broad, considering each click behavior within their historical behaviors is necessary to provide diverse recommendations. Conversely, in scenarios guided by specific interests, users may exhibit more defined preferences, necessitating a focus on corresponding behaviors within their sequence. Therefore, to model user interests across varied scenarios more effectively, a fine-grained analysis of the relationships between the user's historical behaviors and scenarios is essential. Although the SAR-Net model [20] ever attempted to incorporate scenario information into sequence modeling using a target-attention mechanism [35], as shown in Figure 2(b), this approach of weighted summation does Figure 2: Examples of previous methods and ours. In (a), \ud835\udc7d 1 = \ud835\udc64 1 \ud835\udc7d and \ud835\udc7d 2 = \ud835\udc64 2 \ud835\udc7d , where \ud835\udc7d = \ud835\udc53 ( \ud835\udc83 1 , ..., \ud835\udc83 \ud835\udc3f ) . While in (b), \ud835\udc7d 1 = \ud835\udc53 ( \ud835\udc64 \ud835\udc60 1 1 \ud835\udc83 1 , ..., \ud835\udc64 \ud835\udc60 1 \ud835\udc3f \ud835\udc83 \ud835\udc3f ) and \ud835\udc7d 2 = \ud835\udc53 ( \ud835\udc64 \ud835\udc60 2 1 \ud835\udc83 1 , ..., \ud835\udc64 \ud835\udc60 2 \ud835\udc3f \ud835\udc83 \ud835\udc3f ) . In our method, \ud835\udc7d 1 = \ud835\udc53 ( \ud835\udc83 \ud835\udc60 1 1 , ..., \ud835\udc83 \ud835\udc60 1 \ud835\udc3f ) , where \ud835\udc83 \ud835\udc60 1 1 is obtained by ( \ud835\udc7d , \ud835\udc6a , \ud835\udc7a 1 , \ud835\udc83 1 ) . \ud835\udc4f ! \ud835\udc4f \" \ud835\udc4f # \ud835\udc4f $ \u22ee \ud835\udc7d \ud835\udc7d ! \ud835\udc7d \" \ud835\udc4f ! \ud835\udc4f \" \ud835\udc4f # \ud835\udc4f $ \u22ee \ud835\udc7d ! \ud835\udc7d \" (a) Corse-Grained (b) SAR-Net \ud835\udc4f ! \ud835\udc4f \" \ud835\udc4f # \ud835\udc4f $ \u22ee \ud835\udc7a ! \ud835\udc7d ! \ud835\udc7a \" \ud835\udc7d \" (c) Our Method \ud835\udc7d \ud835\udc6a \ud835\udc4f ! \" ! \ud835\udc4f # \" ! \ud835\udc4f $ \" ! \u22ee \ud835\udc4f ! \" \" \ud835\udc4f # \" \" \ud835\udc4f $ \" \" \u22ee \ud835\udc7d \ud835\udc6a \ud835\udc64 ! \ud835\udc64 \" \ud835\udc64 ! % ! \ud835\udc64 $ % ! \ud835\udc64 ! % \" \ud835\udc64 $ % \" \ud835\udc4f \" User behavior sequence \ud835\udc7d Sequence representation vector / Calculations in Scenario 1 / Calculations in Scenario 2 \ud835\udc6a Context features Scenario information of scenario 1/2 \ud835\udc7a \" \ud835\udc7a # / \ud835\udc4f $ little to alter the actual representation of the user sequence. Its ability to represent the complex interplay between real-life scenarios and user behaviors is markedly limited. Consequently, adaptively and reasonably adjusting the representations of user historical behavior to match various scenarios remains an urgent challenge in contemporary multi-scenario modeling. To tackle the above challenge of multi-scenario modeling, we propose a new framework, the S cenario-Adaptive F ine-Grained P ersonalization Net work (SFPNet), designed to finely capture user interest changes across different scenarios and enhance recommendation accuracy in a multi-scenario context. SFPNet features a deep network architecture built from a series of stacked blocks named as Scenario-Tailoring Block . Each block consists of two critical modules: the Scenario-Adaptive Module and the Residual-Tailoring Module. The former module adjusts foundational features to serve as instance-specific contextual information, facilitating scenario awareness, while the latter module uses this contextual information to tailor scenario representations for each feature. Specifically, as identified in numerous studies [4, 24], the foundational feature representation vector is frequently deemed the most critical bottleneck in recommendation models. Therefore, SFPNet utilizes a Scenario-Adaptive Module (SAM) that dynamically scales foundational features. This is achieved by inputting scenario prior features into a scenario-specific gate, followed by an elementwise multiplication with the original feature representations. These scaled representations embody scenario-aware contextual information, signifying the input instance's presence in the current scenario. Notably, in the SAM, calculations are performed after compressing the sequence into a multi-dimensional vector, which aims to enhance the perception of the sequence's contextual information for subsequent fine-grained behavioral modeling. Upon acquiring the instance context information, we devised a ResidualTailoring Module (RTM) in response to concerns that the vector-level re-weighting summation to model the shifts in user interests as reflected by user sequences across different scenarios, as utilized by SAR-Net [20], may inadequately capture the intricate relationship between the scenario and user interests. To facilitate scenario-level customization for each behavior in the sequence, we employ residual connections [8] that depart from traditional feature-scenario interactions. Instead, RTM integrates the contextual information of the input instance and leverage a neural network to dynamically Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User Behavior Representation to the Scenario Context Woodstock '18, June 03-05, 2018, Woodstock, NY generate high-order interactions, subsequently tailoring the behavior representation of the sequence via residual computations. The integration of the above two modules allows SFPNet to incorporate scenario-related information into the feature representation vectors with high granularity, markedly enhancing the model's proficiency in discerning commonalities and distinctions across scenarios. The contributions of our paper can be summarized as follows: \u00b7 To our knowledge, SFPNet is the first multi-scenario work to offer fine-grained scenario-specific customization for individual user behaviors, thereby advancing the model's capacity to track shifts in user interests across various scenarios. \u00b7 SFPNet features a novel block comprising two key modules, which capitalizes on the context and scenario-specific information of each input instance to tailor the representation vectors of user behaviors in the sequence and associated context features of the input instance. By stacking the block, SFPNet constructs a deep network that progressively bolsters its ability to model the intricate interplay between scenarios and input instances. \u00b7 Evaluations using offline datasets and online A/B testing have demonstrated the superiority of the proposed SFPNet method over existing state-of-the-art multi-scenario approaches.", "2 RELATED WORK": "With the evolution of e-commerce platforms and growing user engagement, a single scenario is insufficient to satisfy the expanding needs of users [9, 17, 32]. Consequently, modern online e-commerce platforms are required to cater to multiple scenarios simultaneously. Initially, recommendation systems typically trained a distinct model for each scenario to cater to its specific demands. However, with the proliferation of platform scenarios, the drawbacks of this independent training approach have become increasingly evident. This method is not only resource-intensive but also fails to leverage the correlations between scenarios, leading to suboptimal model performance. Recent advances in deep learning and the industry's emphasis on replicability-avoiding the integration of new elements with each additional scenario-have led to a preference for a unified model structure, employing a singular ranking framework for scenario-specific recommendations. Single model structures are primarily categorized into two approaches: 1) Scenario-specific network structures, and 2) Scenario-adaptive parameter network structures. Though both strive to train a unified model capable of serving multiple scenarios, their underlying philosophies differ. Specifically, scenario-specific network structures draw inspiration from multi-task learning (MTL) principles. These approaches seek to harness the similarities between multi-task and multi-scenario applications by considering each scenario as an independent task. Mixture-of-Experts (MoE) [19] is proposed to significantly increase model capacity and capability and is widely used in the multiscenario area. MMoE [15] characterizes the task correlation and learns the function of specific tasks based on shared bottom feature representation. HMoE [13] takes advantage of MMoE to implicitly identify distinctions and commonalities between tasks, and improves the performance with a stacked model learning task relationships in the label space explicitly. Later, PLE [23] shares experts in the share layer and refines tasks uniquely to effectively alleviate the noise caused by other scenarios and improves the effectiveness of feature extraction. M2M [31] utilizes expert networks to solve multi-scenario and multi-task problems based on a meta network to express the scenario information explicitly. AESM 2 [37] proposes a novel expert network structure with automatic selection of fine granularity by calculating the KL divergence to select the most suitable sharing and exclusive experts. Beyond the MoE-based method, there are alternative approaches like STAR [22], which introduces an extra tower for each scenario, combining the parameters of scenario-specific tower with those of the shared tower's parameter. However, scenario-specific approaches learn scenario information transfer in quite implicit ways and often overlook differences in underlying representations, which frequently constitute a bottleneck in recommendation models. Therefore, SASS [34] designs a scenario adaptive transfer module to select and fuse effective transfer information from whole scenario to individual scenario. Adasparse [29] learns adaptively sparse structure for each scenario, achieving better generalization across domains with lower computational cost. HiNet [36] achieves hierarchical extraction based on coarse-to-fine knowledge transfer scheme. DFFM [6] incorporates scenario-related information into the parameters of feature interaction and user behavior modules, allowing scenario-specific learning of these two aspects, which is essentially a scenario-adaptive parameterization method. 3MN [33] proposes a novel three meta networks-based solution to model the complicated task-task, scenario-scenario, and task-scenario interrelations. PEPNet [4] takes features with strong biases as input and dynamically scales the bottom-layer embeddings and the top-layer DNN hidden units in the model through a gate mechanism. MARIA [24] proposes to project the scenario semantic information into the bottom feature representation to derive more discriminative feature representations.", "3 PROBLEM FORMULATION": "Given a set of scenarios S = { \ud835\udc60 \ud835\udc5a } \ud835\udc41 \ud835\udc60 \ud835\udc5a = 1 with a shared feature space X and label space Y , the multi-scenario recommendation task aims to devise a unified ranking formula F : X \u2192 Y , to concurrently provide accurate, personalized recommendations across \ud835\udc40 scenarios. The common feature space typically encompasses a variety of features that can be categorized into distinct fields, such as usercentric and item-centric features, which collectively represent an instance's comprehensive contextual information. Within this paper, the feature space X is constructed as [ \ud835\udc65 1 , \ud835\udc65 2 , ..., \ud835\udc65 \ud835\udc41 \ud835\udc53 , { \ud835\udc4f \ud835\udc56 } \ud835\udc41 \ud835\udc4f \ud835\udc56 = 0 ] , where for clarity in subsequent discussions, { \ud835\udc4f \ud835\udc56 } \ud835\udc41 \ud835\udc4f \ud835\udc56 = 0 denote the user's historical behavior sequence to distinguish it from other \ud835\udc41 \ud835\udc53 features, and \ud835\udc41 \ud835\udc4f represents the number of the user's historical behaviors. Mathematically, the multi-scenario recommendation task involves estimating the probability that the target user will interact with the target item in a given scenario \ud835\udc60 \ud835\udc5a , utilizing the contextual features, as illustrated below:  where \ud835\udc99 \ud835\udc56 \u2208 R \ud835\udc51 denotes the embedding of the \ud835\udc56 -th feature, and \ud835\udc51 denotes the embedding dimension of features. \ud835\udc83 \ud835\udc56 \u2208 R \ud835\udc51 denotes the representation of \ud835\udc56 -th behavior in the behavior sequence, typically derived from a pooled embedding that combines the clicked item embedding and the embeddings of its associated attribute features. \ud835\udc94 \ud835\udc5a represents the representation vector of \ud835\udc5a -th scenario features. Woodstock '18, June 03-05, 2018, Woodstock, NY Moyu Zhang, Yongxiang Tang, Jinxin Hu, and Yu Zhang Figure 3: The network architecture of our proposed Scenario-Adaptive Fine-Grained Personalization Network (SFPNet). Embedding Layer \u22ef Scenario Feature Feature 1 Feature 2 Feature \ud835\udc5b \u22ef Behavior 1 Behavior 2 Behavior \ud835\udc41 ! Historical Behavior Sequence \u22ef \u22ef Scenario-Adaptive Module (SAM) Residual-Tailoring Module (RTM) Scenario- Tailoring Block (L*) \u22ef \u22ef Scenario-Aware DNN \ud835\udc66 \" \u22ef \u22ef Distribution- Aware Pooling Gate \ud835\udc8d Aggregation Interaction \ud835\udc8d Tailor \u22ef \u22ef \ud835\udc8d - th Scenario-Tailoring Block SAM RTM Residual Connection Tailor Tailor Tailor \ud835\udc99 ' ()' \ud835\udc99 * ! ()' \ud835\udc97 ' ()' \ud835\udc97 * \" ()' \ud835\udc94 + \ud835\udc68 ( \ud835\udc6a ( \ud835\udc99 ' ( \ud835\udc99 * ! ( \ud835\udc97 ' ( \ud835\udc97 * \" ( DIN \ud835\udc6f '", "4 METHOD": "", "4.1 Scenario Adaptive Module (SAM)": "As previously discussed, contemporary multi-scenario models typically employ a unified framework to deliver personalized recommendations across various scenarios. While previous sceneadaptive parameter networks are lauded for their flexibility in adjusting the model's foundational representation to excel in multiscenario recommendations, they typically consider the entire user historical behavior sequence as a singular feature field and conduct scenario-based transformations within this sequence. This approach uniformly adjusts all behaviors with the same weight, thus overlooking the distinct semantic information that each behavior may signify in different scenarios. In reality, users' interests frequently shift with the scenario transition, causing each behavior in the sequence to represent varying information across different scenarios. Inadequate cross-scenario modeling precludes precise extraction of user preferences, reflected in individual behaviors, for each scenario, thereby compromising the accuracy of cross-scenario recommendations. To more effectively model the intricate interplay between user historical behaviors and scenarios, this paper introduces a novel framework named Scenario-Adaptive Fine-Grained Personalization Network (SFPNet). It is primarily built from stacked Scenario-Tailoring Block and comprises two main modules: \u00b7 Scenario-Adaptive Module (SAM) , which employs a gating mechanism to integrate scenario-related information into the base representations of features, ensuring that the instance's contextual information is strongly guided by the scenario context. \u00b7 Residual-Tailoring Module (RTM) , which aggregates scenarioaware contextual information and, via residual connections, disaggregates it into distinct representations for each behavior, allowing each behavior to capture contextual-cross information. Drawing inspiration from the LHUC algorithm [21], which focuses on learning the distinct contributions of each speaker's hidden units, a variety of scenario-adaptive parameterization methods have been developed. These methods involve integrating different scenario prior bias features to create personalized network parameters for each instance, thereby enabling distinction of features at the foundational level across different scenarios. Consequently, within each Scenario-Tailoring Block, we also incorporate a Scenario-Adaptive Module (SAM) that coarsely applies scenario prior information to diverse feature representations. This ensures that while serving as contextual information for instances, the distinctions between scenarios are consistently maintained. Specifically, the input to the SAM in the first Scenario-Tailoring Block is the original embedding vector of the features, and subsequent SAM inputs are typically the outputs from the Residual-Tailoring Module of the previous block. As the purpose of SAM is to produce the contextual information for the instance, we compress the user behavior sequence into a distinct feature field, enabling complete perception of the sequence's information during subsequent behavior customization. SAM operates through a two-step process: sequence pooling, and gate personalization. To elucidate, let's examine the computation within SAM of the \ud835\udc59 -th block, delineated as follows: 4.1.1 Sequence Pooling. Since that SAM's output serves solely as contextual information for the instance, our initial step is to pool the sequence. To avoid substantial increases in computational complexity from stacking blocks, we opt for computationally efficient aggregation methods-like summation, max pooling, or average pooling-for user sequences. This contrasts with the more complex and computationally intensive neural network structures used in DIN [35] to capture sequence information related to target items. Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User Behavior Representation to the Scenario Context Woodstock '18, June 03-05, 2018, Woodstock, NY Nonetheless, the above simple pooling methods result in a measure of sequence information loss, meaning we are unable to accurately model the true distribution of sequences from the pooled sequence representations alone. For instance, considering the user sequence distribution adhering to a Gaussian distribution, we need to know both the mean and the variance to reconstruct the true sequence distribution, expressed as \ud835\udc4f \ud835\udc56 ~ \ud835\udc41 ( \ud835\udf07, \ud835\udf0e 2 ) . Consequently, we refined the sequence pooling approach in this paper by developing a Distribution-Aware Pooling method, which compresses the sequence while maximally preserving sequence information through the computation of both mean and variance, with minimal additional computational load. The precise computational procedure for pooling is delineated as follows:  \u00ab \u2039 where \ud835\udc69 \ud835\udc59 \u2208 R 2 \ud835\udc51 \ud835\udc59 -1 denotes the pooled representation of user behavior sequence from the ( \ud835\udc59 -1 ) -th block. \ud835\udc83 \ud835\udc59 -1 \ud835\udc56 \u2208 R \ud835\udc51 \ud835\udc59 -1 is the updated representation of \ud835\udc56 -th behavior output by the ( \ud835\udc59 -1 ) -th block, and \ud835\udc51 \ud835\udc59 -1 is the dimension of features in the ( \ud835\udc59 -1 ) -th block. 4.1.2 Gate Personalization. Given the scenario information for the \ud835\udc5a -th scenario, we have devised a gating function that applies scenario-specific adjustments at the instance level, tailored to each instance's features, to aid in the encoding of scenario semantics within instance feature representations. Specifically, we concatenate all features-encompassing instance-specific, compressed sequence, and scenario features as \ud835\udc7f \ud835\udc59 = ( \ud835\udc99 \ud835\udc59 -1 1 \u2295 \ud835\udc99 \ud835\udc59 -1 2 \u2295 \u00b7 \u00b7 \u00b7 \u2295 \ud835\udc99 \ud835\udc59 -1 \ud835\udc5b \u2295 \ud835\udc69 \ud835\udc59 \u2295 \ud835\udc94 \ud835\udc5a ) \u2208 R ( \ud835\udc5b + 2 ) \ud835\udc51 \ud835\udc59 -1 + \ud835\udc51 , where \ud835\udc99 \ud835\udc59 -1 \ud835\udc56 denotes the representation of \ud835\udc56 -th feature from the ( \ud835\udc59 -1 ) -th block-and employ two neural network layers to output a vector matching the dimensionality of the input instance features. The weight calculation for each element can be detailed as follows:  where \ud835\udc68 \ud835\udc59 \u2208 R ( \ud835\udc5b + 2 ) \ud835\udc51 \ud835\udc59 -1 represents the weight of the element corresponding to the feature, ranging from 0 to 1. \ud835\udf0e (\u00b7) denotes the sigmoid function. \ud835\udc3f\ud835\udc41 (\u00b7) denotes the layer-norm calculation, which aims to help the model learn a more differentiated weight distribution. \ud835\udc7e \ud835\udc59 0 , \ud835\udc7e \ud835\udc59 1 and \ud835\udc83 \ud835\udc59 0 , \ud835\udc83 \ud835\udc59 1 are the weight and bias for the gate network in the \ud835\udc59 -th block, respectively. Subsequently, we apply a re-weighting operation to the input representation of the \ud835\udc59 -th module within SAM, involving elementwise multiplication between the original representation and the calculated weight values, resulting in a new representation vector. Given that the gate's input encompasses scenario information, we can ensure that the re-weighted embeddings at each block layer consistently reflect scenario-specific distinctions. The specific computational formula is defined as follows:  In this way, \u02c6 \ud835\udc7f \ud835\udc59 \u2208 R ( \ud835\udc5b + 2 ) \ud835\udc51 \ud835\udc59 -1 as an effective scenario signal will be further amplified in the following Residual-Tailoring Module, allowing the model to better capture scenario differences.", "4.2 Residual-Tailoring Module (RTM)": "As previously noted, previous multi-scenario modeling methods often differentiate features coarsely, thereby overlooking the intricate interplay of individual user behaviors with different scenarios. This limitation stymies the model's capacity to precisely track the evolution of user interests across scenarios, ultimately impairing the tailored recommendation of items that align with users' genuine interests in a cross-scenario context. To address the above issue, this paper introduces a Residual-Tailoring Module (RTM) that accepts the output of the Scenario-Adaptive Module (SAM). It leverages a neural network to facilitate interaction with context-aware features, producing a distinctive context representation vector for each feature. Additionally, residual connections [8] are employed to integrate the contextual interaction vector into the representation vector of each user behavior, enabling fine-tuned scenario and context-aware customization for every behavior representation. RTM encodes the global semantics of user behavior within specific scenarios, thereby boosting the model's proficiency in detecting nuanced variations of user behavior across scenarios. It should be noted that while our module focuses on customizing user behavior, the same customization processes are applied to other features to augment their capacity to depict scene divergences. Specifically, the RTM comprises two principal steps: the aggregation interaction step and the context-aware tailoring step. The computation within \ud835\udc59 -th block of the RTM is delineated as follows: 4.2.1 Aggregation Interaction Step. Prior research [2] on feature interactions has demonstrated that deep neural networks adeptly capture implicit high-order feature interactions, frequently resulting in enhanced model prediction performance. Consequently, we integrate all scenario-aware contextual information from the input instance and utilize a neural network to dynamically produce highorder global feature interaction signals for each feature within the instance. This process unfolds as follows:  where \ud835\udc6a \ud835\udc59 \u2208 R ( \ud835\udc5b + 2 ) \ud835\udc51 \ud835\udc59 denotes the contextual interaction information of the input features and include scenario information due to the input representations that have been modified to be adaptive to the scenario. \ud835\udc51 \ud835\udc59 is the dimension of the feature in \ud835\udc59 -th block. \ud835\udc7e \ud835\udc59 2 and \ud835\udc7e \ud835\udc59 3 are the weights of contextual information aggregation function, and \ud835\udc83 \ud835\udc59 2 and \ud835\udc83 \ud835\udc59 3 are the bias of function. 4.2.2 Context-Aware Tailor Step. Drawing inspiration from residual architectures [8], we propose that once contextual interaction information is acquired via the aggregation module, residual connections are employed to merge the original feature representation with its corresponding global interaction representation, thus enabling recoding into a globally-aware representation. We begin by detailing the calculation of residual connections for all features, excluding sequences. The calculation process is as follows:  where \ud835\udc99 \ud835\udc59 \ud835\udc56 \u2208 R \ud835\udc51 \ud835\udc59 denotes the context-aware scenario-adaptive representation of the \ud835\udc56 -th feature generated by the \ud835\udc59 -th block. \ud835\udc7e \ud835\udc59 \ud835\udc65 \ud835\udc56 , 1 and \ud835\udc7e \ud835\udc59 \ud835\udc65 \ud835\udc56 , 2 denote the weight matrix for the \ud835\udc56 -th feature. \ud835\udc99 \ud835\udc59 -1 \ud835\udc56 = Woodstock '18, June 03-05, 2018, Woodstock, NY Moyu Zhang, Yongxiang Tang, Jinxin Hu, and Yu Zhang \ud835\udc7f \ud835\udc59 -1 [ \ud835\udc56 \u2217 \ud835\udc51 \ud835\udc59 -1 : ( \ud835\udc56 + 1 ) \u2217 \ud835\udc51 \ud835\udc59 -1 ] denotes the scenario-adaptive representation of the \ud835\udc56 -th feature output by SAM. \ud835\udc84 \ud835\udc59 \ud835\udc56 = \ud835\udc6a \ud835\udc59 [ \ud835\udc56 \u2217 \ud835\udc51 \ud835\udc59 : ( \ud835\udc56 + 1 ) \u2217 \ud835\udc51 \ud835\udc59 ] denotes the context-aware cross information for the \ud835\udc56 -th feature. Fine-Grained Cross-Scenario Sequence Modeling . As emphasized throughout this paper, our goal with sequence features is to recode specific behavioral representations with fine granularity. The residual architecture is equally adaptable for fine-grained coding of sequence features. The specific computation involved in this fine-grained sequence coding is described as follows:  where \ud835\udc97 \ud835\udc59 \ud835\udc56 \u2208 R \ud835\udc51 \ud835\udc59 denotes the encoded representation of the \ud835\udc56 -th behavior of user sequence with awareness of context information and scenario information. \ud835\udc84 \ud835\udc59 \ud835\udc4f = \ud835\udc6a \ud835\udc59 [ \ud835\udc5b \u2217 \ud835\udc51 \ud835\udc59 : ( \ud835\udc5b + 2 ) \u2217 \ud835\udc51 \ud835\udc59 ] denotes the final context-aware scenario-adaptive representation of the sequence feature generated by the \ud835\udc59 -th block. After re-encoding the sequence, each behavioral representation is finely tuned in accordance with the scenario. Aggregating the sequence at this juncture can substantially enhance the model's proficiency in extracting users' interests across various scenarios, thereby improving the model's adeptness at recognizing different situations. This, in turn, increases the accuracy of scenario-specific recommendations.", "4.3 Scenario-Specific Prediction": "In the prediction layer, the sequence must first be compressed to facilitate its integration with other feature vectors for subsequent processing by the deep network. Given the article's focus on sequence representation recoding, we employ commonly used structures for the compression vector, such as the target-attention mechanism of Deep Interest Network (DIN) [35]:  where \ud835\udc3f is the total number of block, and \ud835\udc97 \ud835\udc3f \ud835\udc56 is the representation of \ud835\udc56 -th behavior outputted by the last block. Subsequently, we leverage the benefits of a multi-tower architecture by incorporating scenarioaware DNN towers at the prediction stage. These towers process sequence features and other features in parallel by considering the scenario bias each layer, to forecast the likelihood of users clicking on the target item within the specific scenario \ud835\udc60 \ud835\udc5a :  where \ud835\udc6f 1 = ( \ud835\udc99 \ud835\udc3f 1 \u2295 \ud835\udc99 \ud835\udc3f 2 \u2295 \u00b7 \u00b7 \u00b7 \u2295 \ud835\udc99 \ud835\udc3f \ud835\udc5b \u2295 \ud835\udc7d ) denotes the input of the first layer of the Scenario-Aware DNN (S-DNN). The specific calculation of the \ud835\udc57 -th layer in the S-DNN structure is similar to the calculation in the SAM, and can be depicted as follows:   where \ud835\udc6f \ud835\udc57 denotes the output of the \ud835\udc57 -th layer of S-DNN. \ud835\udc7e \ud835\udc57 6 , \ud835\udc7e \ud835\udc57 7 and \ud835\udc7e \ud835\udc57 8 are the weight for the S-DNN in the \ud835\udc57 -th layer. \ud835\udc83 \ud835\udc57 6 , \ud835\udc83 \ud835\udc57 7 and \ud835\udc83 \ud835\udc57 8 are the bias for S-DNN in the \ud835\udc57 -th layer.", "4.4 Model Learning": "The objective function applied in our model is the cross entropy loss function, defined as:  where \ud835\udc66 \ud835\udc56 \u2208 { 0 , 1 } is the ground truth of instance.", "5 EXPERIMENTS": "In this section, we conduct extensive experiments on public benchmark datasets to validate the effectiveness of our proposed framework and answer the following questions: \u00b7 RQ1: How does SFPNet perform compared with the state-of-theart baseline methods? \u00b7 RQ2: How does the performance of the SFPNet model, trained on specific scenario data for each scene, compare to that of the SFPNet model trained on a composite scenario dataset? \u00b7 RQ3: How about the impact of each part on the overall model? \u00b7 RQ4: How about the impact of hyper-parameters of our model?", "5.1 Datasets": "To validate the efficacy of our proposed method, two real-world large-scale datasets covering diverse scenarios are used for performance evaluation. We conduct experiments on both public available dataset and industrial dataset. The descriptions and statistics of two dataset are detailed in Table 1 and 2, respectively. \u00b7 Industrial Dataset . To assess our multi-scenario recommendation approach in a real-world setting, we gathered a dataset from the Alibaba international advertising platform, Lazada, that spans multiple scenarios from December 10, 2023, to January 10, 2024. This dataset encompasses 12 scenarios, denoted as #A1 through #A12. We allocated the user instances from the final day for testing and the preceding instances for training. \u00b7 Ali-CCP 1 . It is widely used in the relevant literature for multiscenario recommendations [24, 28, 34], which was collected Taobao's recommender system under three scenarios. It's released by Taobao with prepared training and testing set, and we can split the dataset into 3 scenarios according to the scenario id , denoted as #B1, #B2, and #B3 for simplicity.", "5.2 Competitors": "We conduct experiments with several compared methods for the multi-scenario recommendation task. These methods can be divided into three groups as follows: 5.2.1 General Recommenders. Samples from all scenarios are utilized to co-train a universal recommender system capable of multidomain recommendations. \u00b7 BaseDNN . It is a multi-scenario model that shares the parameters of the bottom layer. On top of the shared bottom layer, a single DNN is used for prediction across scenarios. \u00b7 DeepFM [5]. The model resembles BaseDNN; however, it substitutes the DNN architecture with DeepFM, which is a neural network framework grounded in factorization machines. \u00b7 SharedBottom . It replaces the single DNN with multiple DNNs. 1 https://tianchi.aliyun.com/dataset/408 Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User Behavior Representation to the Scenario Context Woodstock '18, June 03-05, 2018, Woodstock, NY Table 1: Statistics of the industrial dataset. Table 2: Statistics of the Ali-CCP dataset. That is, an individual DNN is utilized for each scenario. And we add an auxiliary tower to enhance the ability to characterize the scenario indicator. and a decay rate of 0.9. The batch size is set as 512 and the embedding size is fixed to 40 for all models. Xavier initialization [7] is used here to initialize the parameters. All methods use a three-layer feedforward neural network with hidden sizes of [256, 128, 64] for instance prediction. We apply careful grid-search to find the best hyper-parameters. The number of experts in MMoE, PLE, SharedBottom and its variants is searched in [2, 4, 6, 8]. All regularization coefficients are searched in [1e-7, 1e-5, 1e-3]. 5.2.2 Scenario-specific network structures. Each scenario is treated as a distinct task and inter-scenario correlations are examined through dedicated networks for each scenario. \u00b7 MMoE [15]. It implicitly models task relationships for multi-task learn- ing, where different tasks may have different label spaces. Here we adapt MMoE for multi-scenario learning. The number of experts is equal to the number of experts of Maria. The sum of weighted outputs from the experts are fed into the individual tower for each scenario respectively. \u00b7 PLE [23]. It is a state-of-the-art multi-scenario/multi-task model that organizes the experts into scenario-specific groups and scenarioshared groups for the purpose of avoiding negative transfer or seesaw phenomenon. \u00b7 STAR [22]. It proposes a star topology to accommodate with the scenario-specific characteristics. Specifically, a shared network works as the center node for knowledge sharing and each scenario network connects only with the center node. \u00b7 AESM 2 [37]. It proposes a novel MMoE-based model with automatic search towards the optimal network structure. In contrast to PLE and STAR, an expert can be either scenario-shared or scenariospecific dynamically in an instance-aware manner. 5.2.3 Scenario-adaptive parameter network structures. They disregard feature space differences and applying scenario information directly to the core embedding layer and prediction module, thus allowing the feature space and prediction strategies to dynamically adjust to scenario variations. \u00b7 AdaSparse [29]. It learns adaptively sparse structures for multiscenario prediction and prunes redundant neurons via learned neuron-level weighting factors to improve generalization. \u00b7 PEPNet [4]. It takes features with strong biases as input and dynamically scales the bottom-layer embeddings and the top-layer DNN hidden units in the model through a gate mechanism. \u00b7 MARIA [24]. It designs three components to enable discriminative feature learning in a scenario-aware manner: feature scaling, feature refinement, and feature correlation modeling. Implementation Details . In offline experiments, we implement all the models based on the TensorFlow framework [1]. We use Adam [11] for optimization with an initial learning rate of 0.001 Evaluation Metric . In line with prior studies, this paper employs the area under the ROC curve (AUC) as the performance metric for the public Ali-CCP dataset. However, for our industrial dataset, we adopt a variant of session-weighted AUC. This metric assesses the quality of item rankings within sessions by averaging the AUCs of a user's individual session behaviors. This variant, already implemented on our platform, has proven to align more closely with the system's online performance. For brevity, we refer to this metric as S-GAUC, and its computation is as follows:  where \ud835\udc5b is the number of sessions in the dataset. # \ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b and \ud835\udc34\ud835\udc48\ud835\udc36 \ud835\udc56 are the number of impressions and AUC corresponding to the \ud835\udc56 -th session. A higher S-GAUC score usually denotes a better online recommendation performance. We conduct a Mann-Whitney U test [16] under AUC and S-GAUC metrics.", "5.3 Comparison with Baselines (RQ1)": "Table 3 and Table 4 displays the overall prediction performance of all methods on the industrial and public dataset, respectively, along with the statistical significance of our model against the best baseline model, with the highest results highlighted in bold. From the results in two tables, we can see that SFPNet outperforms all baselines for both datasets, indicating that the Scenario-Tailoring Block can lead to multi-scenario prediction performance improvements. As for General Recommenders , we can find that they often struggle to attain satisfactory performance compared to other baseline models on the both datasets. SharedBottom incorporates dedicated DNNs for each scenario to embed scene-specific biases into the model. However, empirical findings indicate that this straightforward approach of directly incorporating biases can frequently yield detrimental outcomes. This outcome corroborates our hypothesis that bottom-level sharing with a single top structure is inadequate for capturing the complex interplay between scenarios. Consequently, with datasets that exhibit significant scene variability, the performance of General Recommenders tends to be subpar. To address the shortcomings of General Recommenders , ScenarioSpecific Network Structure Methods were proposed. According to experimental outcomes, these methods have resulted in notable performance enhancements. By enhancing the model's capacity to Woodstock '18, June 03-05, 2018, Woodstock, NY Moyu Zhang, Yongxiang Tang, Jinxin Hu, and Yu Zhang Table 3: Prediction performance on the industrial dataset. * indicates p-value < 0.05 in the significance test. Table 4: Prediction performance on the Ali-CCP dataset. * indicates p-value < 0.05 in the significance test. distill shared knowledge across scenarios, both the MMoE and STAR models outperformed the previously mentioned three baseline models. However, similar to the seesaw effect observed in multi-task learning, MMoE and PLE also exhibit a seesaw effect across multiple scenarios. This indicates that these structures alone are insufficient for efficiently handling scenarios with uneven data distribution, and achieving simultaneous performance improvements across all scenarios. To mitigate the seesaw effect, PLE, a variant of MMoE, introduces greater stability across scenarios by segmenting the expert network into two distinct groups. Notably, PLE demonstrates significant improvement over MMoE on the Ali-CCP dataset. Additionally, AESM 2 performs well on the Ali-CCP dataset, yet it underperforms on the industrial dataset. We attribute this to the extensive variety of industrial scenarios. AESM 2 mechanism for automatically selecting expert networks faces challenges in training with such a diversity of scenes. This suggests that in more complex scenarios, the comparatively simpler strategy employed by PLE may more readily achieve optimal performance. Despite the focus on optimization at the top level by the aforementioned methods, some studies have indicated that the bottom level often represents a bottleneck in recommendation systems [4, 24]. As evidenced by the results in Tables 3 and 4, the ScenarioAdaptive Parameter Methods delivered commendable predictive performance on both datasets, lending support to this hypothesis. Notably, Adasparse's performance is marginally inferior to that of PEPNet. We surmise that this is because the mechanism of sparse neurons is relatively challenging to learn. Sparse data scenarios in recommendations frequently result in suboptimal performance. The MARIA model outshines others on both datasets by optimizing both the lower and upper structures of the model. However, it is apparent that these methods do not fully address the pivotal task of modeling user interests across multi-scenarios. This oversight is why our approach, SFPNet, secures the best prediction performance across both datasets, emphatically underscoring the significance of finely-grained modeling of user behaviors across scenarios.", "5.4 Comparison with Training Alone (RQ2)": "While our model SFPNet surpasses the baselines in scenario-based prediction accuracy through multi-scenario joint modeling, we must ascertain whether multi-scenario joint training confers benefits on each individual scenario. Specifically, we question whether training a separate model for each scenario using our custom module might yield better results than joint training. Should some scenarios experience diminished performance following joint training, it may be advantageous to model these scenarios independently to achieve better online outcomes. This would also indicate that our model has room for improvement in accurately modeling inter-scenario relationships. Consequently, to investigate the potential issues outlined above, we trained SFPNet individually on both public and industrial datasets. The detailed experimental results, illustrated in the Figure 4, show a white box representing improved performance when employing joint modeling as compared to separate training. The results reveal that independent training not only requires more manpower and resources but also underperforms compared to joint training. These findings underscore the importance of leveraging data from various scenarios for recommendation tasks, particularly those with sparse data, and also validate our multi-scenario structural design's ability to effectively transfer knowledge between scenarios. Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User Behavior Representation to the Scenario Context Woodstock '18, June 03-05, 2018, Woodstock, NY Table 5: The performance of five variants on the industrial dataset. We record the mean results over 5 runs. Figure 4: The impact of multi-scenario joint training. S-GAUC of Different Training Strategy AUC of Different Training Strategy (a) Industrial Dataset (b) Ali-CCP Dataset Improvements of Joint Training Improvements of Joint Training #A1 #A2 #A3 #A4 #A5 #A6 #A7 #A8 #A9 #A11 #A10 #A12 #B2 #B1 #B3 0.60 0.55 0.52 0,48 S-GAUC of Different \ud835\udc3f AUC of Different \ud835\udc3f (a) Industrial Dataset (b) Ali-CPP Dataset 0,61 L =1 L =1 L =2 L = L =3 L = L=4 L=5 0.55 0.58 #AI #A2 #A3 #A4 #A5 #A6 #A7", "5.5 Ablation Study (RQ3)": "To verify the effectiveness of each module in the proposed model, i.e., the SFPNet method, we conduct a series of ablation studies over the industrial dataset. We have five variants as follows: \u00b7 w/oDAP removes the distribution-aware sequence pooling method and directly adopts the average pooling of sequences. \u00b7 w/o SAM removes the Scenario-Adaptive Module, which means that the block no longer emphasizes scenario information. \u00b7 w/o RTM removes the Residual-Tailoring Module in each block, which implies that the block no longer provides finely-tuned, scenariospecific representations for individual user behaviors. \u00b7 w/o S-DNN removes the scenario-aware DNN and replace it with a normal deep neural network. \u00b7 w/o STB removes the Scenario-Tailoring Block and directly conducts predictions based on S-DNN . Table 5 presents the performance metrics of five SFPNet variant models. The results indicate that SFPNet's performance diminishes with the removal of any module, underscoring the contribution of each component to predictive accuracy. Specifically, the absence of distribution-aware sequence modeling (w/o DAP) impacts prediction performance across all scenarios, aligning with our hypothesis that mean pooling in sequence modeling may result in a loss of critical information. Additionally, we observed that despite the presence of an S-DNN atop SFPNet to assist with scene perception, the removal of SAM (w/o SAM) still results in a decline in model performance, reinforcing our understanding of the critical nature of the bottom representation proposed in previous work-akin to the proverb \"a breach in a dike may cause a collapse\". Furthermore, the Residual-Tailoring Module's absence (w/o RTM) notably affects the performance of each scenario. Indeed, without the RTM, our model reverts to the scene adaptive parameter structure of prior Figure 5: The performance of SFPNet under different predefined hyper-parameters on the both datasets. multi-scenario modeling methods, consequently forfeiting the capability for scenario-specific customization of user behavior. The experimental data suggests that the performance drop from removing either the RTM or the Scenario-Tailoring Block (w/o STB) is quite comparable. This underscores the significance of the RTM and robustly validates our rationale for concentrating on fine-grained scenario modeling exploration. Lastly, the removal of S-DNN (w/o S-DNN) also leads to a measurable reduction in the model's prediction performance. This reveals that, although we prioritize the bottom layer's architecture, the top structure's design must not be overlooked. This reflects the initial design motivation for the method, which was to clearly structure the scenario.", "5.6 Hyper-Parameters Sensitivity Analysis (Q4)": "This section examines the sensitivity of the hyper-parameter of Scenario-Tailoring Block stacking times within the SFPNet model, i.e., \ud835\udc3f . We assessed SFPNet's performance on two datasets with five different values of \ud835\udc3f (1, 2, 3, 4, and 5), with results presented in Figure 5(a) for the industrial dataset and Figure 5(b) for the other. Analysis of the industrial dataset reveals that \ud835\udc3f values of 3 and 4 yield similar and optimal prediction performance; however, an additional block layer increases computational demands. Consequently, we recommend setting \ud835\udc3f to 3. Theoretically, increasing the number of stacks should deepen the model, potentially enhancing its capability to model multiple scenarios. Nevertheless, given the dataset size, elevating the stack count beyond a certain point complicates training and can deteriorate prediction performance. Similarly, for the public dataset, setting \ud835\udc3f = 2 results in peak performance for SFPNet. This optimal setting is attributable to the smaller size of the Ali-CCP dataset compared to the industrial dataset. Excessive block stacking increases the likelihood of overfitting in the model. Woodstock '18, June 03-05, 2018, Woodstock, NY Moyu Zhang, Yongxiang Tang, Jinxin Hu, and Yu Zhang", "5.7 Online A/B Testing Results": "To more robustly validate our model's performance, we conducted an online A/B test on an online e-commerce platform. The control in our test was a base model-a model structure corresponds to the BaseDNN from our baseline comparisons. We averaged the results of the 12 scenarios in our platform from October 21 to 26, 2023, to determine the final outcome of the online test. Specifically, following the implementation of the SFPNet model, we observed a 6.4% increase in cumulative Revenue and a 9.2% rise in user Click-Through Rate (CTR) compared to the base model. The online results further corroborate the effectiveness of the proposed SFPNet approach for multi-scenario recommendation.", "6 CONCLUSIONS": "In this paper, we point out the optimizable space for modeling user sequences within multi-scenario contexts in the multi-scenario recommendation field and propose a novel model named ScenarioAdaptive Fine-Grained Personalization Network (SFPNet) to explore the optimization space for modeling user sequences with awareness of scenarios in a fine-grained way. SFPNet is is composed of the Scenario-Tailoring Block stacked block by block, and each block comprises two key components: the Scenario-Adaptive Module (SAM) and the Residual-Tailoring Module (RTM). Specifically, SAM employs a gating mechanism to infuse scenario-specific information into the instance's contextual features, whereas RTM harnesses the generated scenario-aware contextual information by SAM to craft a distinctive representation vector for each behavior in the sequence, utilizing residual connections for this purpose. This process ultimately bolsters the model's capacity to capture the evolving patterns of user interests across different scenarios. Finally, extensive experiments validate the superiority of SFPNet in the multi-scenario recommendation task.", "REFERENCES": "[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. Tensorflow: A system for large-scale machine learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16) . [2] Alex Beutel, Paul Covington, Sagar Jain, Can Xu, Jia Li, Vince Gatto, and Ed H. Chi. 2018. Latent Cross: Making Use of Context in Recurrent Recommender Systems. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (WSDM) . (Feb. 2018), 46-54. [3] Yichao Wang, Huifeng Guo , Bo Chen, Weiwen Liu, Zhirong Liu, Qi Zhang, Zhicheng He, Hongkun Zhen, Weiwei Yao, Muyu Zhang, Zhenhua Dong, and Ruiming Tang. 2022. CausalInt: Causal Inspired Intervention for Multi-Scenario Recommendation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) . (Aug. 2022), 4090-4099. [4] Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. PEPNet: Parameter and Embedding Personalized Network for Infusing with Personalized Prior Information. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) . (Aug. 2023), 3795-3804. [5] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. Deepfm: a factorization-machine based neural network for ctr prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI) . Melbourne, Australia., 2782-2788. [6] Wei Guo, Chenxu Zhu, Fan Yan, Bo Chen, Weiwen Liu, Huifeng Guo, Hongkun Zheng, Yong Liu, and Ruiming Tang. 2023. DFFM: Domain Facilitated Feature Modeling for CTR Prediction. In Proceedings of the Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM) . (Oct. 2023), 4602-4608. [7] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics . 249-256. [8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . (Jun. 2016), 770-778. [9] Zhaoxin Huan, Ang Li, Xiaolu Zhang, Xu Min, Jieyu Yang, Yong He, and Jun Zhou. 2023. SAMD: An Industrial Framework for Heterogeneous Multi- Scenario Recommendation. In Proceedings of the 29th ACM SIGKDD Con- ference on Knowledge Discovery and Data Mining (KDD) . (Aug. 2023), 4175-4184. [10] Alex Kendall, Yarin Gal, and Roberto Cipolla. 2018. Multi-task learning using uncertainty to weight losses for scene geometry and semantics. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . UT, USA, 7482-7491. [11] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR. [12] Jinyun Li, Huiwen Zheng, Yuanlin Liu, Minfang Lu, Lixia Wu, and Haoyuan Hu. 2023. ADL: Adaptive Distribution Learning Framework for Multi-Scenario CTR Prediction. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . (Jul. 2023), 1786-1790. [13] Pengcheng Li, Runze Li, Qing Da, An-Xiang Zeng, and Lijun Zhang. 2020. Improving Multi-Scenario Learning to Rank in E-commerce by Exploiting Task Relationships in the Label Space. In Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM) . (Oct. 2020), 26052612. [14] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017. Adversarial Multi-task Learning for Text Classification. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics . Vancouver, Canada, 1-10. [15] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) . London, UK, 1930-1939. [16] Simon J Mason and Nicholas E Graham. 2002. Areas beneath the relative operating characteristics (ROC) and relative operating levels (ROL) curves: Statistical significance and interpretation. Quarterly Journal of the Royal Meteorological Society: A journal of the atmospheric sciences, applied meteorology and physical oceanography . 128, 584 (2002), 2145-2166. [17] Shanlei Mu, Penghui Wei, Wayne Xin Zhao, Shaoguo Liu, Liang Wang, Bo Zheng. 2023. Hybrid Contrastive Constraints for Multi-Scenario Ad Ranking. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM) . (Oct. 2023), 1857-1866. [18] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web . 285-295. [19] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. In Proceedings of the 5th International Conference on Learning Representations (ICLR) . (Apr. 2017). [20] Qijie Shen, Wanjie Tao, Jing Zhang, Hong Wen, Zulong Chen, and Quan Lu. 2021. SAR-Net: A Scenario-Aware Ranking Network for Personalized Fair Recommendation in Hundreds of Travel Scenarios. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM) . (Nov. 2021), 4094-4103. [21] Pawel Swietojanski, Jinyu Li, and Steve Renals. 2016. Learning hidden unit contributions for unsupervised acoustic model adaptation. IEEE/ACM Transactions on Audio, Speech, and Language Processing . 24, 8 (2016), 1450-1463. [22] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al. 2021. One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (CIKM) . 4104-4113. [23] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In Fourteenth ACM Conference on Recommender Systems . 269278. [24] Yu Tian, Bofang Li, Si Chen, Xubin Li, Hongbo Deng, Jian Xu, Bo Zheng, Qian Wang, and Chenliang Li. 2023. Multi-Scenario Ranking with Adaptive Feature Learning. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . (Jul. 2023), 517-526. [25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser and Illia Polosukhin. 2017. Attention is All you Need. In Proceedings of 30th Conference on Advances in Neural Information Processing Systems (NIPS) . (Dec. 2017), 5998-6008. [26] Hong Wen, Jing Zhang, Fuyu Lv, Wentian Bao, Tianyi Wang, and Zulong Chen. 2021. Hierarchically Modeling Micro and Macro Behaviors via Multi-Task Learning for Conversion Rate Prediction. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . [27] Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, and Keping Yang. 2020. Entire space multi-task modeling via post-click behavior decomposition for conversion rate prediction. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User Behavior Representation to the Scenario Context Woodstock '18, June 03-05, 2018, Woodstock, NY (SIGIR) . 2377-2386. [28] Yuhao Wang, Xiangyu Zhao, Bo Chen, Qidong Liu, Huifeng Guo, Huan-shuo Liu, Yichao Wang, Rui Zhang, and Ruiming Tang. 2023. PLATE: A Prompt-Enhanced Paradigm for Multi-Scenario Recommendations. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . (Jul. 2023), 1498-1507. [29] Xuanhua Yang, Xiaoyu Peng, Penghui Wei, Shaoguo Liu, Liang Wang and Bo Zheng. 2022. AdaSparse: Learning Adaptively Sparse Structures for Multi-Domain Click-Through Rate Prediction. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management (CIKM) . (Oct. 2022), 46354639. [30] Jing Zhang and Dacheng Tao. 2021. Empowering Things With Intelligence: A Survey of the Progress, Challenges, and Opportunities in Artificial Intelligence of Things. IEEE Internet of Things Journal . 8(10), 7789-7817. [31] Qianqian Zhang, Xinru Liao, Quan Liu, Jian Xu, Bo Zheng. 2022. Leaving No One Behind: A Multi-Scenario Multi-Task Meta Learning Approach for Advertiser Modeling. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM) . (Feb. 2022), 1368-1376. [32] Pengyu Zhao, Xin Gao, Chunxu Xu, and Liang Chen. 2023. M5: Multi-Modal Multi-Interest Multi-Scenario Matching for Over-the-Top Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) . (Aug. 2023), 5650-5659. [33] Yifei Zhang, Hua Hua, Hui Guo, Shuangyang Wang, Chongyu Zhong, and Shijie Zhang. 2023. 3MN: Three Meta Networks for Multi-Scenario and Multi-Task Learning in Online Advertising Recommender Systems. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM) . (Oct. 2023), 4945-4951. [34] Yuanliang Zhang, Xiaofeng Wang, Jinxin Hu, Ke Gao, Chenyi Lei, and Fei Fang. 2022. Scenario-Adaptive and Self-Supervised Model for Multi-Scenario Personalized Recommendation. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management (CIKM) . (Oct. 2022), 3674-3683. [35] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) . 1059-1068. [36] Jie Zhou, Xianshuai Cao, Wenhao Li, Lin Bo, Kun Zhang, Chuan Luo, and Qian Yu. 2023. HiNet: Novel Multi-Scenario & Multi-Task Learning with Hierarchical Information Extraction. In Proceedings of the 39th IEEE International Conference on Data Engineering (ICDE) . (Apr. 2023), 2969-2975. [37] Xinyu Zou, Zhi Hu, Yiming Zhao, Xuchu Ding, Zhongyi Liu, Chenliang Li, Aixin Sun. 2022. Automatic Expert Selection for Multi-Scenario and Multi- Task Search. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . (Jul. 2022), 1535-1544."}
