{"Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai": "Zhichao Feng \u2217 Beijing University of Posts and Telecommunications Beijing, China fengzc@bupt.edu.cn JunJie Xie \u2217 Meituan Beijing, China xiejunjie02@meituan.com Kaiyuan Li Beijing University of Posts and Telecommunications Beijing, China tsotfsk@bupt.edu.cn Yu Qin Meituan Beijing, China qinyu12@meituan.com Xiang Li \u2020 Meituan Beijing, China lixiang245@meituan.com", "Pengfei Wang \u2020": "Beijing University of Posts and Telecommunications Beijing, China wangpengfei@bupt.edu.cn Wei Lin Meituan Beijing, China Qianzhong Li Bin Yin Meituan Beijing, China {liqianzhong,yinbin05}@meituan.com", "Shangguang Wang": "linwei31@meituan.com", "ABSTRACT": "In the recommender system of Meituan Waimai, we are dealing with ever-lengthening user behavior sequences, which pose an increasing challenge to modeling user preference effectively. A number of existing sequential recommendation models struggle to capture long-term dependencies, or they exhibit high complexity, both of which make it difficult to satisfy the unique business requirements of Meituan Waimai's recommender system. To better model user interests, we consider selecting relevant sub-sequences from users' extensive historical behaviors based on their preferences. In this specific scenario, we've noticed that the contexts in which users interact have a significant impact on their preferences. For this purpose, we introduce a novel method called Co ntext-based Fa st R ecommendation S trategy (referred to as CoFARS) to tackle the issue of long sequences. We first identify contexts that share similar user preferences with the target context and then locate the corresponding Points of Interest (PoIs) based on these identified contexts. This approach eliminates the necessity to select a sub-sequence for every candidate PoI, thereby avoiding high time complexity. Specifically, we implement Beijing University of Posts and Telecommunications Beijing, China sgwang@bupt.edu.cn a prototype-based approach to pinpoint contexts that mirror similar user preferences. To amplify accuracy and interpretability, we employ Jensen-Shannon(JS) divergence of PoI attributes such as categories and prices as a measure of similarity between contexts. Subsequently, we construct a temporal graph that encompasses both prototype and context nodes to integrate temporal information. We then identify appropriate prototypes considering both target contexts and short-term user preferences. Following this, we utilize contexts aligned with these prototypes to generate a sub-sequence, aimed at predicting CTR and CTCVR scores with target attention. Since its inception in 2023, this strategy has been adopted in Meituan Waimai's display recommender system, leading to a 4.6% surge in CTR and a 4.2% boost in GMV.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems .", "KEYWORDS": "Click-Through Rate Prediction, User Preference Modeling, Long Sequential User Behavior Data", "ACMReference Format:": "Zhichao Feng, JunJie Xie, Kaiyuan Li, Yu Qin, Pengfei Wang, Qianzhong Li, Bin Yin, Xiang Li, Wei Lin, and Shangguang Wang. 2024. Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai. In Companion Proceedings of the ACM Web Conference 2024 (WWW '24 Companion), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3589335.3648334", "1 INTRODUCTION": "In the intricate ecosystem of Meituan Waimai, China's premier local retail and instant delivery platform, the deployment of an advanced, Company Home Breakfast Launch Midnight 1 Fast food Light Rice BBQ Korean Hot pot Bun Soymilk Fast Pasta Fried BBQ salad noodle cuisine food pizza skewers location-based recommender system is crucial for connecting millions of users with an array of services, from food delivery to pharmaceuticals. The task is made more challenging by the fact that 27% of users have engaged with the app over 1,000 times in the last year, necessitating a highly efficient process for managing extensive user behavior sequences. Traditional models such as RNNs [7, 13] falter in maintaining long-term memory across these sequences, while attention-based methods [12, 28] are hindered by the significant computational demands of processing Meituan Waimai's long user behavior sequences. Recently, two-stage recommendation models [3, 4, 21] have emerged. Based on the target item, the first stage filters out a subsequence, and the second stage performs fine-grained modeling on this sub-sequence. However, these models primarily rely on static features such as item attributes for sequence filtering, without adequately considering the dynamic factors that influence user preferences, such as the context during user interaction. Furthermore, the target attention-based approach integral to these models necessitates traversing the entire sequence for each candidate item, incurring substantial computational overhead. Complicating matters further, these methods rely on the semantic similarity between item embeddings to infer user preferences-a process that can be compromised by the quality of the embeddings, thus affecting the reliability and efficiency of recommendations. In fact, the recommendation process in Meituan Waimai is uniquely influenced by contextual factors like location, time, and weather, diverging from the attribute-centric focus common in e-commerce [15, 27]. This context-driven user preference landscape underscores the inadequacy of traditional methods. Analysis of user interaction data shown in Fig. 1 reveals that preferences vary considerably across different contexts: for instance, the preference for lighter meals like salads or rice noodles is markedly higher at company, while heartier options such as BBQ and hot pot are favored at home. Mealtime further influences choices, with buns and soymilk popular for breakfast, and fast food or pasta preferred for lunch, transitioning to fried skewers and BBQ for dinner. To overcome these challenges, in this work, we adhere to the two-stage paradigm, concentrating on the first stage which involves selecting a sub-sequence from the user's behavior sequence. Considering the context-dependency characteristic of Meituan Waimai, we propose to select PoIs that are interacted with under the target context from the user's historical behaviors, which is a method independent of candidate PoIs. However, if we only retain behaviors that completely align with the target context, the retrieved sub-sequence will be excessively short, thereby limiting the effectiveness of the recommendation system. For this purpose, we propose a Co ntextbased Fa st R ecommendation S trategy (denoted as CoFARS) to address long sequence issues in the Meituan Waimai recommender system. Our goal is to identify contexts similar to the target context based on user preferences. We employ Jensen-Shannon (JS) divergence to measure the similarity of PoI attribute distributions across different contexts, enhancing interpretability and accuracy. To reduce complexity, we use a prototype-based approach, where prototypes act as centroids of semantically similar neighbors in the preference representation space. Since prior knowledge of prototypes is not available, inspired by transfer learning [40], we convert latent representations into probability distributions using an encoder and align them with the actual JS divergence between different contexts obtained from log data using MSE loss. We also build a temporal graph of prototype and context nodes to introduce temporal information. By taking into account the user's short-term preference, we select the prototypes that align with the target context and retrieve the relevant sub-sequence from the user behavior sequence. To verify the effectiveness of the proposed model, we conduct both offline evaluation experiments on a large user-logged dataset and online A/B testing experiments on the Meituan Waimai app. Experimental results show that our proposed framework outperforms the comparison baselines in both metrics. Our contributions can be summarized as follows: \u00b7 We propose a candidate-agnostic method to model long sequences based on contexts. To the best of our knowledge, this is the first initiative to employ contexts in the modeling of long sequences in this domain. \u00b7 We propose a prototype-based approach to address the challenges of long sequences. Leveraging our proposed probability encoder and graph-based temporal aggregator, we can effectively pinpoint contexts that display preferences akin to the target context. \u00b7 We conduct both offline evaluations on large dataset and online A/B tests on the online platform of Meituan Waimai to demonstrate the effectiveness of our approach.", "2 RELATED WORK": "", "2.1 Context-aware Recommendation": "Context-aware Recommendation aims to leverage rich context information to improve the recommendation performance [32]. Firstly, Rendle [26] introduce the factorization machine(FM) to mine the interaction between context features to enhance the recommendation. Inspired by FM, some works [9, 11, 19] have been proposed to model the second-order interactions between context features while others [17, 33] utilize deep neural networks to model high-order interactions between features. Although these methods leverage feature interactions to achieve great performance, their interaction components are empirically predefined. To solve this problem, Cheng et al. [5] propose to use logarithmic neural networks (LNN) [8] to adaptively model the arbitrary-order interactions, while Tian et al. propose EulerNet [29] to model the feature interactions in a complex vector space through Euler formula, where the feature interactions in the model are adaptively learned from data. Recently, several studies also propose to improve the performance of sequential recommendation by integrating contextual information [16, 18, 23, 25, 36]. However, despite their effectiveness, these methods predominantly rely on the self-attention mechanism, which falls short in addressing the challenges posed by modeling long-term sequences.", "2.2 Long User Behavior Sequence Modeling": "As the sequence lengthens, the model's performance correspondingly enhances. [21]. Existing methods modeling long user behavior sequences can be mainly divided into two categories: RNN-based methods and two-stage methods. In RNN-based methods, some models utilize memory networks to memorize users' historical behavior sequences [20, 24], such as HPMN [24] and MIMN [20]. Recently, Wu et al. [35] propose a linear transformer mechanism based on kernel functions to model long sequences. Recently, two-stage methods have attracted increasing attention. Pi et al. [21] first propose a two-stage method for long sequence problems, which utilizes a General Search Unit (GSU) module to retrieve items related to the target item from long sequences in hard or soft manners, and then use Exact Search Unit (ESU) module to finely model the retrieved sub-sequence. To speed up the GSU stage, ETA [4] proposes to use locality-sensitive hashing to transform item embeddings into binary vectors, with Hamming distance measuring item similarity. Similarly, with the hashing method, SDIM consolidates and normalizes item embeddings with the same hash as the target, deducing user preference for the target item. However, the relevance between the items retrieved by GSU and ESU may be relatively low. Therefore, Chang et al.[3] proposed consistency-preserving GSU (CP-GSU) to enhance the consistency of the two stages. Different from the above retrieval-based GSU method, UBCS [37] proposes a clustering-based method to extract representative sub-sequences. In contrast to their studies,our approach involves utilizing context rather than the target item for retrieval during the GSU stage.", "3 METHODOLOGY": "In this section, we first introduce the problem formulation of recommendations in Meituan Waimai. We then describe the proposed CoFARS model in detail.", "3.1 Problem Formulation": "Let U denote a set of users and E denote a set of PoIs, where |U| and |E| are the numbers of users and PoIs. For each user \ud835\udc62 \u2208 U , we use \ud835\udc52 \ud835\udc62 1: \ud835\udc5b = GLYPH<8> \ud835\udc52 \ud835\udc62 1 , \ud835\udc52 \ud835\udc62 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc52 \ud835\udc62 \ud835\udc56 , \u00b7 \u00b7 \u00b7 , \ud835\udc52 \ud835\udc62 \ud835\udc5b GLYPH<9> to represent the interaction sequence of PoIs, where \ud835\udc52 \ud835\udc62 \ud835\udc56 \u2208 E . Each user interaction in the sequence is accompanied by various contextual features, such as dining time, location, weather, etc. We refer to the combination of these contextual features as a context, like \u27e8 breakfast, office, sunny, \u00b7 \u00b7 \u00b7 \u27e9 . We denote by C \ud835\udc62 the set of distinct contexts in which the user has interacted, where C \ud835\udc62 = {C \ud835\udc62 1 , C \ud835\udc62 2 , \u00b7 \u00b7 \u00b7 , C \ud835\udc62 | C \ud835\udc62 | } . Additionally, we use A = { a 1 , a 2 , ..., a | A| } to represent the set of PoI attributes (e.g., categories, prices, etc.), where |A| is the number of attribute fields, and each element within it represents a set of possible values that attribute can take. We discretize the continuous attributes into several buckets. To denoise PoIs unrelated to user preferences based on target contexts, while also avoiding issues of inaccurate modeling due to overly short remaining sequences, our goal is to identify contexts with similar user preferences to the target one, which will then be used for filtering in the user behavior sequence. For simplicity, we describe the technical details of CoFARS for a single user \ud835\udc62 , and it is straightforward to extend the formulas to a set of users. Hence, we drop subscript \ud835\udc62 in the notations for concise presentation.", "3.2 Probability Encoder": "Within the Meituan Waimai recommender system, myriad context features such as geographic location, dining time, weather, and holiday presence significantly influence user preferences. Traditional approaches, including cosine similarity of context embeddings, aimed at identifying contexts with analogous user inclinations and selecting sub-sequences pertinent to the target context, encounter challenges including a lack of interpretability and a reliance on the quality of embeddings. Indeed, the task of accurately capturing user preferences in a specific context using solely latent representations without supervision poses a significant challenge. Nonetheless, we observe that user preferences within particular contexts are more accurately represented through probability distributions across PoI attributes, such as category and price. This insight leads us to employ probability distribution consistency as a measure for comparing preferences across different contexts, thereby enhancing both interpretability and accuracy. A prevalent method involves calculating the Kullback-Leibler (KL) divergence. However, to circumvent its inherent asymmetry, we opted for the symmetric alternative, the Jensen-Shannon (JS) divergence. We use D(C \ud835\udc56 ) = \ud835\udc5d ( \ud835\udc4e 1 , \ud835\udc4e 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc4e | A| |C \ud835\udc56 ) to denote the distribution of attribute values { \ud835\udc4e 1 , \ud835\udc4e 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc4e | A| } under context C \ud835\udc56 , which is a |A| -dimension vector recording the proportions of corresponding discrete attribute values that could be obtained from the log data.The equation of divergence between given context C \ud835\udc56 and context C \ud835\udc57 comes as follows: For clarity, the matching process is shown in Fig. 2 by visualizing the distribution of order probability over the price and category dimensions. The order probability is calculated by log data, and it can be viewed as user's interest in PoI in the context. The described method involves calculating the similarity in preferences between the target context and historical contexts, followed by the selection of a subset of the top similar contexts. However, this strategy is not without its limitations, particularly in its inability to address the challenge of context cold-start. Moreover, the Context \ud835\udc36 ! Context \ud835\udc36 \" Match ? Context \ud835\udc36 # Match ! JS-divergence matching effectiveness of the model is substantially affected by the quantity of contexts selected for each target context, a figure that fluctuates unpredictably across various target contexts. In fact, it is important to recognize that preferences derived from prolonged behavior within a particular context tend to remain relatively stable. Therefore, we introduce the concept of preference prototypes, drawing inspiration from prototype learning methodologies referenced in prototype learning literature [14]. We define the collection of these prototypes as O = { \ud835\udc5c 1 , \ud835\udc5c 2 , \u00b7 \u00b7 \u00b7 , \ud835\udc5c | O | } , where |O| represents the total number of prototypes. These prototypes serve as the centroids within the preference representation space, grouping semantically akin neighbors. Contexts whose preferences closely match the same prototype are considered analogous. However, as our understanding of the prototypes is limited to their latent representations and we lack explicit prior knowledge about them, directly deriving their probability distributions across PoI attributes is not feasible. To calculate the JS divergence between prototypes and context preferences, inspired by transfer learning, we design a probability encoder that maps latent representations into probability distributions over PoI attributes. We align the distributions with the ground truth JS divergence between contexts calculated based on log data. Specifically, we use \u02c6 c \ud835\udc56 \u2208 R \ud835\udc51 to represent the latent representation of the global preference under context C \ud835\udc56 , where \ud835\udc51 represents the number of representation dimensions, and it is shared by all users to reduce the number of parameters. The user's personalized preference is denoted as c \ud835\udc56 = u + \u02c6 c \ud835\udc56 . To approximate the probability distribution, we employ P ( c \ud835\udc56 ) = MLP ( c \ud835\udc56 ) , utilizing a Multilayer Perceptron (MLP) model that incorporates a sigmoid activation function. The MLP's output dimensionality is meticulously set to match the aggregate of value counts across all attributes. Based on the estimated probability distributions, we define the estimated JS divergence between representations c \ud835\udc56 and c \ud835\udc57 as follows: where \ud835\udc50 \ud835\udc56,\ud835\udc58 represent the \ud835\udc58 -th element of c \ud835\udc56 . We aim to encourage the estimated JS divergence to approximate the ground truth value define in Eq. 1 obtained from log data. To achieve this, we add the following constraint: By minimizing L \ud835\udc40\ud835\udc46\ud835\udc38 , we can align the estimated JS divergence and the ground truth. In consequence, we could directly calculate the similarity between latent representations: It's noteworthy that contexts are dynamically clustered, and surplus prototypes contain virtually no contexts. Consequently, when the number of prototypes surpasses a specific threshold, the model's performance remains relatively stable. In other words, the model's efficacy is not solely contingent on this hyperparameter, a fact we have substantiated in the experiments detailed below. Besides, we can calculate the JS divergence among prototypes using the probability encoder. To avoid excessive concentration of the prototypes, we introduce an independence loss to constrain prototype representations. Similarly, we use \u02c6 o \ud835\udc56 \u2208 R \ud835\udc51 to represent the representations shared by users, which follow the same distribution as the representations of context preference, and o \ud835\udc56 = \u02c6 o \ud835\udc56 + u to represent personalized representation for prototype \ud835\udc5c \ud835\udc56 .To promote the distinctiveness of different prototypes, we encourage their separation through JS divergence: By minimizing L \ud835\udc3c \ud835\udc41 \ud835\udc37 , we can make the prototypes more evenly distributed in the latent space.", "3.3 Graph-based Temporal Aggregator": "While the probability encoder effectively clusters contexts, it overlooks the temporal dynamics embedded within sequences. This oversight becomes particularly critical in handling long sequences where traditional RNN-based models may grapple with memory loss over time, and the computational demand of attention-based models becomes prohibitively intricate. Recently, graph neural networks have been applied to sequential recommendation, constructing connected graphs of items to handle long user behavior sequence [2], modeling long-term item dependencies. A common method for graph construction utilizes the co-occurrence relationships between items to establish connections. However, due to the sparsity of items in user behavior sequence, the graphs often degenerate into linear structures, limiting the effectiveness of GNNs. Consequently, GNN applications are mainly found in areas with frequent item repetition, like session-based recommendations [34]. Nevertheless, within context sequences, the restricted diversity of contexts and the extensive sequence length lead to repeated contexts, forming an optimal scenario for GNN modeling. Accordingly, a temporal graph is constructed based on the sequential order of contexts, wherein nodes denote contexts, and edges signify the cooccurrences among adjacent contexts. This framework proficiently encapsulates the inherent sequential dynamics within user behavior, offering a nuanced approach to understanding and modeling user interactions. \u2026 \u2026 Embedding Layer GRU Layer \u2026 \u2026 Build Graph Context Item Aggregated Prototypes \ud835\udc50 ! l Retrieval Sub-sequence \u2026 Reparametrize JS Divergence To incorporate temporal dimensions into prototypes, we embed prototype nodes within the graph. However, fully connecting prototype and context nodes risks creating an unwieldy graph size. To mitigate this, we suggest a filtering method that retains only edges with similarity scores above a certain threshold. However, the conventional hard-coding filtering mechanism is not differentiable, obstructing effective backpropagation training. To bypass this, we adopt the Gumbel Softmax trick from prior work [10], facilitating differentiable learning over discrete outputs. The equation for each prototype \ud835\udc56 and context \ud835\udc57 is as follows: where \ud835\udc54 \ud835\udc4f represents a noise sampled from a Gumbel distribution, and the temperature parameter \ud835\udf0f \ud835\udc5d controls its sharpness. Additionally, the similarity between each prototype-context pair is precalculated and saved in the storage, eliminating the need for realtime computation online. For the pair where P ( \ud835\udefd ) = 1, we add a bidirectional edge between the nodes. Next, we utilize Graph Attention Network [31], which could deal with directed graphs and unseen nodes, to aggregate the graph and embed temporal information into the node representation: where H \ud835\udc59 \ud835\udc56 \u2208 R \ud835\udc51 and W \ud835\udc59 \u2208 R \ud835\udc51 \u00d7 \ud835\udc51 represent the \ud835\udc56 -th node representation and the parameters in the \ud835\udc59 -th layer, respectively. \ud835\udf0e represents the ReLU activation function. N \ud835\udc56 denotes the predecessor nodes of node \ud835\udc56 .After \ud835\udc3f times of aggregation, we can obtain the \ud835\udc56 -th aggregated prototype representation e o \ud835\udc56 = H \ud835\udc3f \ud835\udc56 and \ud835\udc57 -th context preference representation e c \ud835\udc57 = H \ud835\udc3f | O |+ \ud835\udc57 integrated with temporal information.", "3.4 Learning and Discussion": "Once having the aggregated node representations, we identify the prototypes that fit with the target context by focusing on the user's most recent \ud835\udc5f activities e ( \ud835\udc5b -\ud835\udc5f ) : \ud835\udc5b = { e ( \ud835\udc5b -\ud835\udc5f ) , e ( \ud835\udc5b -\ud835\udc5f + 1 ) , \u00b7 \u00b7 \u00b7 , e \ud835\udc5b } , where \ud835\udc5f \u226a \ud835\udc5b . We encode the user's short-term behavior using GRU model [7]: where l represents the user's short-term preference. Next, we assume that the target context of the user is C \ud835\udc61 .Then, we calculate the relation between prototypes and the target context: where the dot represents inner product operation. As there is no concept of probability distribution on attributes for PoIs, we compute the similarity between prototypes and candidate PoI using the inner product. To ensure effective training, we utilize binary cross entropy loss as Eq. 15 to provide supervision for the entire process. where \ud835\udc49 denotes the training set, and \ud835\udc66 = 1 for positive samples while \ud835\udc66 = 0 otherwise. The final loss is as follows: where \ud835\udefe and \ud835\udf06 control the weights of the MSE loss and the independence loss, respectively. The overall learning algorithm for the proposed method is given in Alg. 1. In the next phase, for each prototype \ud835\udc5c \ud835\udc56 in the set O where \ud835\udc67 \ud835\udc61 \ud835\udc56 = 1, we collect the contexts that are encompassed by it. Within these contexts, we identify and select a sub-sequence of PoIs with which the user has engaged. This selected sub-sequence is then processed using target attention [39], to intricately model and capture user interest. The model is trained under the cross-entropy loss function. For the online prediction of a user's subsequent visit within the target context C \ud835\udc61 , we implement diverse strategies. For previously encountered context, we use the pre-calculated similarity between prototype-context pairs. For cold-start context, we employ the representation c \ud835\udc61 to calculate its similarity with the prototypes. Based on this representation, we emulate the learning process mentioned in this section to select sub-sequence and model user interest. The proposed CoFARS method selects a sub-sequence based on contexts with similar preferences to the target context independent of candidate items. In this section, we analyze the time complexity of inference time in the GSU stage. In the offline phase, it calculates the similarity between prototypes and context preferences with a time complexity of O(| C| \u00b7 |O |), where |C| is much smaller than the sequence length \ud835\udc5b , and |O| is a constant. The time complexity of graph aggregation is \ud835\udc42 ( \ud835\udc5b \u00b7 \ud835\udc3f ) . In the online phase, we need \ud835\udc42 ( \ud835\udc5f + |O|) to model the short-term preference to obtain the selected subsequence and use it to retrieve relevant prototypes. It is evident that our approach has a significantly better time complexity compared to other two-stage models, which have a time complexity of \ud835\udc42 ( \ud835\udc35 \u00b7 \ud835\udc5b ) , where \ud835\udc35 represents the number of candidates.", "4 EXPERIMENT": "", "4.1 Experiemental Setup": "Dataset. For offline experiments, we collect training data by log data of Meituan Waimai from April 1st to April 30th, 2023, and the validation data is constructed by the samples on May 9th, 2023. Especially, sample data contains user profile/statistical features, statistical features of PoIs, raw user-behavior features, labels (whether click or not), and so on. The raw user-behavior features include all PoIs that users have interacted with over the past three years. In addition, we use context features including geographical location, mealtime, weather, and the presence of holidays. For PoI attributes, we consider category, price, quality, and delivery time. The statistics of our dataset are shown in Tab. 1, where Recent Avg. refers to the average click records number of users in the training set, while Historical Avg. refers to the average interaction records number of users over the past three years, which are used to model user interests. Baseline. To evaluate the effectiveness of our approach, we compare our model against two types of baselines, including three traditional models, and five enhanced models for long sequences. Specifically, we consider three traditional models: \u00b7 Avg-Pooling [6] is a basic deep learning model for CTR prediction. It performs average pooling to integrate behavior embeddings. \u00b7 DIN [39] is an attention-based method that only utilizes short-term user behavior sequences to model user interests. \u00b7 DIEN [38] integrates GRU with a candidate-based attention mechanism to model user interests. For enhanced models for long sequences, we consider the following five baselines: \u00b7 MIMN[20] : utilizes a memory network to capture long-term user interest. \u00b7 SIM \u210e\ud835\udc4e\ud835\udc5f\ud835\udc51 [21] gets relevant interacted PoIs by category id. \u00b7 SIM \ud835\udc60\ud835\udc5c\ud835\udc53 \ud835\udc61 [21] searches top-k interacted PoIs by the similarity of PoI embeddings. For our experiments, the well-trained PoI embeddings are obtained from our Recall System. \u00b7 SDIM [1] aggregates and normalizes the embeddings of items with the same hash as the target item in the user behavior sequence to derive the user preference related to the target item. \u00b7 TWIN [3] is an improved version of SIM, which enhances the consistency of embeddings in GSU and ESU stages. Parameter Setting. For a fair comparison, we adopt the following setting for all methods: the number of training epochs is set to 500 and the batch size is set to 128. All embedding parameters are randomly initialized in the range of (0, 1), and the learning rate is tuned in the range of {1e-4, 5e-4, 1e-3} and the embedding size is tuned in the range of {8, 16, 32}. For other parameters, we follow the settings in the original paper or source code. For CoFARS, we implement it based on PyTorch with Adam optimizer. For Eq. 8, the \ud835\udf0f \ud835\udc5d is set to tuned in {1e-3, 1e-2, 0.1, 1, 2}. The \ud835\udefe and \ud835\udf06 in Eq. 17 are set to 5e-2 and 1e-3, respectively. And the number of prototypes is tuned in {1, 10, 20, 30, 40, 60, 80}, which is studied in the subsequent section.", "4.2 Performance Comparison": "In this section, we compare the performance of our model with the baselines. The overall performance of our proposed CoFARS and the baselines are reported in Tab. 2. We have the following observations: For traditional models, we observe that Avg-Pooling has the poorest performance. By employing target attention to assign varying weights to PoIs in the behavior sequence, we discover that DIN and DIEN outperform models without attention, demonstrating the significance of diminishing the impact of irrelevant PoIs. Furthermore, by incorporating temporal information, DIEN outperforms DIN, which validates the importance of integrating sequential information in CoFARS. In terms of enhanced recommenders for long sequences, their performance surpasses that of DIN and DIEN, which are short-term models. This indicates the critical role of long sequences in more precisely capturing user interests. We also observed that MIMN has the least effective performance among all enhanced models. This could be due to its approach of updating memory by assigning weights to PoIs in the sequence via a soft attention manner, where potential noise accumulation can significantly hinder the sequence learning process. Consequently, its performance lags behind two-stage methods that employ a hard-coding approach. A similar observation could be explored in [22, 30]. Additionally, despite a reduction in time complexity, SDIM outperforms SIM \u210e\ud835\udc4e\ud835\udc5f\ud835\udc51 and exhibits comparable performance to SIM \ud835\udc60\ud835\udc5c\ud835\udc53 \ud835\udc61 , validating the efficacy of the hashing method proposed by SDIM. Furthermore, TWIN excels over SDIM as it not only enhances the consistency of the two-stage representation but also employs target attention instead of normalizing the PoI embeddings for sequential modeling, thereby amplifying the correlation between user interest and the target PoI. Finally, CoFARS outperforms all baselines and outperforms TWIN by 0.77% on CTR AUC and 0.63% on CTCVR AUC, respectively, which proves the effectiveness of the proposed CoFARS. By introducing contextual information, CoFARS, with the proposed probability encoder and graph-based temporal aggregator, CoFARS can effectively and efficiently identify contexts with similar preferences, according to which it selects PoIs relevant to the target context.", "4.3 Ablation Study": "Our proposed CoFARS method measures the similarity between the prototypes and the context preferences based on JS divergence and constrains the latent representations through several losses while introducing temporal information into the representation through GNN.To verify the effectiveness of such a design, we consider the following variants of our model for comparison through offline experiments: \u00b7 CoFARS \ud835\udc3c \ud835\udc43 : This variant represents using the inner product instead of JS divergence as the similarity measure. \u00b7 CoFARS \u00ac \ud835\udc40\ud835\udc46\ud835\udc38 : This variant represents removing L \ud835\udc40\ud835\udc46\ud835\udc38 for comparison with CoFARS and CoFARS \ud835\udc3c \ud835\udc43 . \u00b7 CoFARS \u00ac \ud835\udc3c \ud835\udc41 \ud835\udc37 : This variant represents removing the independence constraint of prototypes. \u00b7 CoFARS \u00ac \ud835\udc3a\ud835\udc47\ud835\udc34 : This variant represents removing the graphbased temporal aggregator and directly using the original representations for subsequent operations instead of the aggregated ones. The performance comparison among CoFARS and its variants is shown in Tab. 3. From the table, we have the following observations. Firstly, regarding the similarity measure, we found that using the inner product instead of the similarity defined in Eq. 6 leads to a decrease in performance. This proves that calculating JS divergence based on the attribute distributions of the context indeed brings a more accurate similarity measure. In addition, if we only remove the alignment constraint in Eq. 5, this will lead to poor performance, which indicates that it is difficult for the model to find contexts with similar preferences by calculating JS divergence based on random and unaligned representations. Secondly, we found that removing L \ud835\udc3c \ud835\udc41 \ud835\udc37 leads to a degradation in performance as well. The underlying reason might be that without the independence constraint, the distribution of each prototype becomes concentrated, making it challenging for the model to distinguish between diverse user preferences. Thirdly, we observe a performance decline when the graph-based temporal aggregator is removed. This confirms the importance of integrating transit relationships between contexts into the representations, as its absence leads to a loss of temporal information. \u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000", "4.4 Analysis on Effect of Prototype Number": "Recall that in our proposed CoFARS, the number of prototypes is a hyperparameter, in this senction, we evaluate the effectiveness of prototypes in CoFARS by altering the number of prototypes | O |. The outcomes are depicted in Fig.4. When |O| = 1, we only retain the PoIs that have been interacted with in the target context previously. Remarkably, CoFARS consistently surpasses the baseline for various | O | values, except when |O| = 1. Optimal performance is attained when | O | is approximately 40, suggesting that contexts with similar user preferences are appropriately assigned to prototypes. On the contrary, setting | O | to 1 results in a substantial performance drop. This can be attributed to the filtered sequences are too short and monotonous to capture users' interest diversity and its evolution.", "4.5 Visualization Analysis": "In this section, to delve deeper into whether the learned representations of our model can genuinely discern contexts with similar user preferences, we have undertaken a visualization of the similarity among contexts. Specifically, we have selected a user as an example, whose mealtimes encompass breakfast, lunch, and dinner, and whose habitual locations include his home (a residential area) and his company (a business district). By utilizing the Cartesian product, we are able to generate six distinct contexts. Leveraging the well-trained embedding, we employ the Eq. 2 to compute the similarity of user preferences across these contexts and perform normalization. The visualization result of all representations is shown in Fig. 5. In the figure, we've uncovered several insights. Foremost, it is clear that within the same contextual features, users demonstrate similar preferences. For example, when mealtime is constant while the location changes, the context similarity is higher than in completely unrelated contexts. Specifically, \ud835\udc60\ud835\udc56\ud835\udc5a (\u27e8 \ud835\udc4f, \u210e \u27e9 , \u27e8 \ud835\udc4f, \ud835\udc5c \u27e9) exceeds \ud835\udc60\ud835\udc56\ud835\udc5a (\u27e8 \ud835\udc4f, \u210e \u27e9 , \u27e8 \ud835\udc59, \ud835\udc5c \u27e9) and \ud835\udc60\ud835\udc56\ud835\udc5a (\u27e8 \ud835\udc4f, \u210e \u27e9 , \u27e8 \ud835\udc51, \ud835\udc5c \u27e9) . This pattern is consistent across all mealtimes. With a static location, similar trends are observed, which is likely attributable to consistent preferences for the same mealtime or location. For instance, users may prefer heavier lunches and lighter meals for breakfast and dinner, meanwhile choose pricier PoIs at home and cheaper options at the company. Secondly, breakfast orders at home and the company are more similar than that of lunch and dinner, possibly due to the limited \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 breakfast variety compared to lunch and dinner. Lastly, the minimum similarity across all contexts exceeds 0.3, indicating basic preferences like favoring lower-priced PoIs.", "4.6 Online A/B Testing": "CoFARS is conducted in the recommender system of Meituan Waimai for online A/B testing from 2023-05 to 2023-06. During almost a month of testing, compared with the baseline, the last version of our online-serving model, CoFARS obtained 4.6% CTR and 4.2% GMV promotion. The significant improvement demonstrates the effectiveness of our proposed approach. CoFARS has been deployed online and serving the main traffic in our real system now.", "5 CONCLUSION": "In this paper, we tackle the challenge of long sequences in the Meituan Waimai recommender system. The main contribution of our work, CoFARS, lies in its integration of context information and the introduction of a fast candidate-agnostic two-stage approach to address this problem. Specifically, CoFARS introduces a similarity measure based on JS divergence, which enhances interpretability and accuracy. Moreover, it leverages GNN to incorporate temporal information into node representation. Additionally, CoFARS combines short-term interests to retrieve prototypes that match the target context and utilizes contexts with similar user preferences to obtain a relevant sub-sequence. Our experiments, conducted in both offline and online settings, demonstrate the effectiveness of our proposed model. To the best of our knowledge, we are the first to leverage context information for modeling long sequences in this domain.", "ACKNOWLEDGMENTS": "This research work was supported by General Program of the National Natural Science Foundation of China (No.62372059). We would like to thank the anonymous reviewers for their valuable comments.", "REFERENCES": "[1] Yue Cao, Xiaojiang Zhou, Jiaqi Feng, Peihao Huang, Yao Xiao, Dayao Chen, and Sheng Chen. 2022. Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR Prediction. In CIKM . ACM, 2974-2983. [2] Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng Jin, and Yong Li. 2021. Sequential Recommendation with Graph Neural Networks. In SIGIR . ACM, 378-387. [3] Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou. In KDD . ACM, 3785-3794. [4] Qiwei Chen, Changhua Pei, Shanshan Lv, Chao Li, Junfeng Ge, and Wenwu Ou. 2021. End-to-End User Behavior Retrieval in Click-Through RatePrediction Model. abs/2108.04468 (2021). [5] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions. In AAAI . AAAI Press, 3609-3616. [6] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In RecSys . ACM, 191-198. [7] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In ICLR . [8] J. Wesley Hines. 1996. A logarithmic neural network architecture for unbounded non-linear function approximation. In ICNN . IEEE, 1245-1250. [9] Fuxing Hong, Dongbo Huang, and Ge Chen. 2019. Interaction-Aware Factorization Machines for Recommender Systems. In AAAI . AAAI Press, 3804-3811. [10] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparameterization with Gumbel-Softmax. In ICLR (Poster) . OpenReview.net. [11] Yu-Chin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Fieldaware Factorization Machines for CTR Prediction. In RecSys . ACM, 43-50. Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Rec- [12] ommendation. In ICDM . 197-206. [13] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017. Neural Attentive Session-based Recommendation. In CIKM . ACM, 1419-1428. [14] Junnan Li, Pan Zhou, Caiming Xiong, and Steven C. H. Hoi. 2021. Prototypical Contrastive Learning of Unsupervised Representations. In ICLR . OpenReview.net. [15] Xiaochen Li, Jian Liang, Xialong Liu, and Yu Zhang. 2022. Adversarial Filtering Modeling on Long-term User Behavior Sequences for Click-Through Rate Prediction. In SIGIR . ACM, 1969-1973. [16] Yang Li, Tong Chen, Peng-Fei Zhang, and Hongzhi Yin. 2021. Lightweight SelfAttentive Sequential Recommendation. In CIKM . ACM, 967-977. [17] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. In KDD . ACM, 1754-1763. [18] Chang Liu, Xiaoguang Li, Guohao Cai, Zhenhua Dong, Hong Zhu, and Lifeng Shang. 2021. Non-invasive Self-attention for Side Information Fusion in Sequential Recommendation. CoRR abs/2103.03578 (2021). [19] Wantong Lu, Yantao Yu, Yongzhe Chang, Zhen Wang, Chenhui Li, and Bo Yuan. 2020. A Dual Input-aware Factorization Machine for CTR Prediction. In IJCAI . ijcai.org, 3139-3145. [20] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction. In KDD . ACM, 2671-2679. [21] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. In CIKM . ACM, 2685-2692. [22] Yuqi Qin, Pengfei Wang, and Chenliang Li. 2021. The World is Binary: Contrastive Learning for Denoising Next Basket Recommendation. In SIGIR . 859-868. [23] Ahmed Rashed, Shereen Elsayed, and Lars Schmidt-Thieme. 2022. Context and Attribute-Aware Sequential Recommendation via Cross-Attention. In RecSys . ACM, 71-80. [24] Kan Ren, Jiarui Qin, Yuchen Fang, Weinan Zhang, Lei Zheng, Weijie Bian, Guorui Zhou, Jian Xu, Yong Yu, Xiaoqiang Zhu, and Kun Gai. 2019. Lifelong Sequential Modeling with Personalized Memorization for User Response Prediction. In SIGIR . ACM, 565-574. [25] Ruiyang Ren, Zhaoyang Liu, Yaliang Li, Wayne Xin Zhao, Hui Wang, Bolin Ding, and Ji-Rong Wen. 2020. Sequential Recommendation with Self-Attentive Multi-Adversarial Network. In SIGIR . ACM, 89-98. [26] Steffen Rendle. 2010. Factorization Machines. In ICDM . IEEE Computer Society, 995-1000. [27] Uriel Singer, Haggai Roitman, Yotam Eshel, Alexander Nus, Ido Guy, Or Levi, Idan Hasson, and Eliyahu Kiperwasser. 2022. Sequential Modeling with Multiple Attributes for Watchlist Recommendation in E-Commerce. In WSDM . ACM, 937-946. [28] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In CIKM . 1441-1450. [29] Zhen Tian, Ting Bai, Wayne Xin Zhao, Ji-Rong Wen, and Zhao Cao. 2023. EulerNet: Adaptive Feature Interaction Learning via Euler's Formula for CTR Prediction. In SIGIR . ACM, 1376-1385. [30] Xiaohai Tong, Pengfei Wang, Chenliang Li, Long Xia, and Shaozhang Niu. 2021. Pattern-enhanced Contrastive Policy Learning Network for Sequential Recommendation. In IJCAI . ijcai.org, 1593-1599. [31] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR . OpenReview.net. [32] Norha M. Villegas, Cristian S\u00e1nchez, Javier D\u00edaz-Cely, and Gabriel Tamura. 2018. Characterizing context-aware recommender systems: A systematic literature review. Knowl. Based Syst. 140 (2018), 173-200. [33] Ruoxi Wang, Rakesh Shivanna, Derek Zhiyuan Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed H. Chi. 2021. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In WWW . ACM / IW3C2, 1785-1797. [34] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019. Session-Based Recommendation with Graph Neural Networks. In AAAI . AAAI Press, 346-353. [35] Yongji Wu, Lu Yin, Defu Lian, Mingyang Yin, Neil Zhenqiang Gong, Jingren Zhou, and Hongxia Yang. 2021. Rethinking Lifelong Sequential Recommendation with Incremental Multi-Interest Attention. CoRR abs/2105.14060 (2021). [36] Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor S. Sheng, Jiajie Xu, Deqing Wang, Guanfeng Liu, and Xiaofang Zhou. 2019. Feature-level Deeper Self-Attention Network for Sequential Recommendation. In IJCAI . ijcai.org, 43204326. [37] Yuren Zhang, Enhong Chen, Binbin Jin, Hao Wang, Min Hou, Wei Huang, and Runlong Yu. 2022. Clustering based Behavior Sampling with Long Sequential Data for CTR Prediction. In SIGIR . ACM, 2195-2200. [38] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep Interest Evolution Network for Click-Through Rate Prediction. In AAAI . AAAI Press, 5941-5948. [39] Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-Through Rate Prediction. In KDD . ACM, 1059-1068. [40] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. 2021. A Comprehensive Survey on Transfer Learning. Proc. IEEE 109, 1 (2021), 43-76."}
