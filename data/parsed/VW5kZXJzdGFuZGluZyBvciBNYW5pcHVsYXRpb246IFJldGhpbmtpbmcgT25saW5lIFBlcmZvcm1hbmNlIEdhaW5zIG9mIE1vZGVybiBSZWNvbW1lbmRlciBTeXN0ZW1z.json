{"Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems": "ZHENGBANG ZHU, Shanghai Jiao Tong University, China RONGJUN QIN, National Key Laboratory for Novel Software Technology, Nanjing University, China and Polixir Technologies, China JUNJIE HUANG, Shanghai Jiao Tong University, China XINYI DAI, Shanghai Jiao Tong University, China YANG YU , National Key Laboratory for Novel Software Technology, Nanjing University, China and Polixir \u2020 Technologies, China YONG YU, Shanghai Jiao Tong University, China WEINAN ZHANG \u2020 , Shanghai Jiao Tong University, China Recommender systems are expected to be assistants that help human users find relevant information automatically without explicit queries. As recommender systems evolve, increasingly sophisticated learning techniques are applied and have achieved better performance in terms of user engagement metrics such as clicks and browsing time. The increase in the measured performance, however, can have two possible attributions: a better understanding of user preferences, and a more proactive ability to utilize human bounded rationality to seduce user over-consumption. A natural following question is whether current recommendation algorithms are manipulating user preferences. If so, can we measure the manipulation level? In this paper, we present a general framework for benchmarking the degree of manipulations of recommendation algorithms, in both slate recommendation and sequential recommendation scenarios. The framework consists of four stages, initial preference calculation, training data collection, algorithm training and interaction, and metrics calculation that involves two proposed metrics, Manipulation Score and Preference Shift. We benchmark some representative recommendation algorithms in both synthetic and real-world datasets under the proposed framework. We have observed that a high online click-through rate does not necessarily mean a better understanding of user initial preference, but ends in prompting users to choose more documents they initially did not favor. Moreover, we find that the training data have notable impacts on the manipulation degrees, and algorithms with more powerful modeling abilities are more sensitive to such impacts. The experiments also verified the usefulness of the proposed metrics for measuring the degree of manipulations. We advocate that future recommendation algorithm studies should be treated as an optimization problem with constrained user preference manipulations. CCS Concepts: \u00b7 Information systems \u2192 Recommender systems . Additional Key Words and Phrases: Recommender System, User Model, Bounded Rationality", "\u2020 Corresponding authors.": "Authors' addresses: Zhengbang Zhu, Shanghai Jiao Tong University, China, zhengbangzhu@sjtu.edu.cn; Rongjun Qin, National Key Laboratory for Novel Software Technology, Nanjing University, China and Polixir Technologies, China, qinrj@polixir.ai; Junjie Huang, Shanghai Jiao Tong University, China, legend0018@sjtu.edu.cn; Xinyi Dai, Shanghai Jiao Tong University, China, daixinyi@sjtu.edu.cn; Yang Yu \u2020 , National Key Laboratory for Novel Software Technology, Nanjing University, China and Polixir Technologies, China, yuy@polixir.ai; Yong Yu, Shanghai Jiao Tong University, China, yyu@ apex.sjtu.edu.cn; Weinan Zhang \u2020 , Shanghai Jiao Tong University, China, wnzhang@sjtu.edu.cn. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM 1046-8188/2023/12-ART https://doi.org/10.1145/3637869 ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 2 Z. Zhu et al.", "ACMReference Format:": "Zhengbang Zhu, Rongjun Qin, Junjie Huang, Xinyi Dai, Yang Yu \u2020 , Yong Yu, and Weinan Zhang \u2020 . 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems. ACM Trans. Inf. Syst. 1, 1 (December 2023), 33 pages. https://doi.org/10.1145/3637869", "1 INTRODUCTION": "With the popularity of the Internet and the growth of User Generated Content (UGC) [50] during recent years, we are being overwhelmed by a massive volume of information. To save users from information overload, recommender systems have been widely applied in today's short video [55], news [87] and e-commerce [17] platforms. Different from traditional information retrieval techniques, recommender systems are considered to be more advanced by using user information to personalize the recommendations. This is accomplished through the prediction of the relevance between users and documents. There are various recommendation algorithms proposed for relevance modeling, and their development is evidenced by the progressive improvement in various offline and online metrics. Offline metrics refer to those that can be evaluated with a static dataset and without interactions with real users. For example, Normalized Discounted Cumulative Gain (NDCG) is commonly used in recommendation literature to measure how well the model ranks more relevant documents at the top of the list. On the other hand, online metrics have to be computed by deploying the trained model online and gathering feedback from real users. As a general case of online metrics, Click-Through Rate (CTR) shows how often people click on the displayed documents by a recommendation algorithm. While improved online metrics bring more traffic and revenue to the platform, it is questionable whether existing online metrics are aligned with the aspiration of recommender systems, which is to help people find relevant or favored content. As revealed by research in behavioral economics, humans make decisions with bounded rationality [77]. Bounded rationality is contrary to expectedutility models [75] where decisions made are always optimal under some expected utility function. Instead, humans are trying to make optimal decisions, but are bounded by various cognitive limitations, e.g., limited memory and decision time. Under the bounded rationality framework, a variety of phenomena have been observed and investigated, such as the decoy effect, confirmation bias, anchoring effect, etc [29, 62, 81]. Most of them are possible in online recommendations, which are already studied by prior works [1, 53, 85]. Once a recommender system is deployed online, it will actively interact with human users of bounded rationality. Therefore, we cannot distinguish whether the active participation of users is due to the system's accurate grasp of their preferences or to an over-exploitation of their psychological weaknesses. To our knowledge, there is no existing work that studies whether the performance gain of current recommendation algorithms in online metrics comes from the utilization of bounded rationality. Since intrusive evaluations directly on the production platform can have a negative impact on the company's benefits, building simulated interactive environments is used to assess the properties of algorithms when interacting with users without actually going online. Agent-based user simulations are adopted by early works in building dialogue systems [74], where user behaviors are simulated by either predefined rules [52, 67] or statistical models trained on a small amount of data [51]. In recent years, a series of simulation frameworks or specific environments have been proposed in the field of recommender systems. Recsim [42] is a general framework for simulating the interactive process of online recommendations, where the whole recommender system is decomposed into separate configurable components. Shi et.al. [80] propose Virtual-Taobao as an interactive e-commence environment, in which the user behaviors are given by a neural model adversarially trained on real logged data. However, the evaluation criteria adopted in existing works are aligned with ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 3 STAY ... ... Fig. 1. Examples of two typical scenarios in recommender systems. On the left is the search result in an ecommerce platform, referred to as slate recommendation. On the right is the illustration of a recommendation queue in music platforms, referred to as sequential recommendation. traditional online metrics, and there is no simulation framework to specifically test the extent to which recommendation algorithms manipulate user preferences. To relieve these concerns, in this paper, we provide a general simulation framework Mirror to evaluate the manipulations of user preferences from recommendation algorithms. The evaluation consists of four stages. In the first stage, each simulated user is presented with all the documents one by one to get a score for each document. Since the scoring happens before the interactions with recommender systems, and each document itself makes up a slate, such scores are not influenced by historically viewed documents or by other documents within the same recommended slate. Therefore, the obtained scores can serve as the initial preferences of users. In the second stage, we generate training datasets by using pre-defined initial recommendation strategies to interact with users. To acquire a diverse set of training datasets to study the impact of how training datasets influence trained models, we mix the datasets from multiple initial strategies with different ratios. After that, in the third stage, the recommendation model is trained on those datasets and evaluated by interacting with simulated users. At the final stage, the interaction data, as well as users' initial preferences, are jointly used to compute evaluation metrics. By incorporating the initial preferences, we define the notion of favorite documents and propose several manipulation-aware metrics. Those metrics can be used in both slate and sequential recommendations, which are two typical recommendation scenarios as illustrated in Fig. 1. Intuitively, if the users click on more documents but less proportion of favorite documents, we say the recommendation algorithms are manipulating the users' preferences. Also, the manipulations can be quantified by the long-term shift in users' unbiased preferences. Under the proposed framework, we conduct extensive studies based on four different simulation experiments, two of which are rule-based and the other two are data-driven. Those environments cover both slate and sequential recommendations. In slate recommendations, we find that compared to simple point-wise models, the reranking methods with the ability to model item relationships within one slate tend to generate recommendations that have higher clicks but a lower proportion of clicks on favorite documents. As the training data contains more slates with manipulations, all the algorithms recommend slates with increasing clicks and decreasing proportion of favorite documents clicked. Moreover, our case study reveals that the reranking method makes use of ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 4 Z. Zhu et al. comparison bias, where people tend to choose those documents if they have already seen similar but worse alternatives within the same list. In sequential recommendations, the experimental results show that sequential recommendation algorithms that take users' recent interaction history into account can lead to more significant manipulation of users' preferences. To summarize, the main contributions of our work are: \u00b7 We put forward the potential issue of manipulations on user preference in current recommendation systems, and propose a general and configurable framework named Mirror that can quantify the degree of such manipulations of recommendation algorithms. The core procedure inside the benchmark is to make simulated users view and score the entire documents one by one, through which we can access the initial preferences of each user. \u00b7 By utilizing the proposed framework, we instantiate four benchmark scenarios, ranging from slate recommendations and sequential recommendations, and including both synthetic and data-driven user behavior models. \u00b7 We benchmark several recommendation algorithms under these scenarios, and draw up a number of key findings on manipulations from recommendation systems. -In both slate and sequential recommendations, better performances on traditional online metrics are accompanied by more manipulations on user preference. -In both slate and sequential recommendations, the degree of manipulation from recommendation algorithms is overall positively correlated with the manipulation degree of the recommendation algorithm that collects the training data. -In slate recommendations, reranking methods are more actively using the mutual influence of documents within the slate to improve the overall clicks compared to point-wise ranking methods. And such influence often leads users to click on documents they do not originally favor. -In sequential recommendations, recommendation models that take the user's recent interaction behavior into account can make users choose less proportion of originally favored documents and induce greater changes to user preferences. The remaining part of this paper is organized as follows. Section 2 provides an overview of current recommender systems and the emerging concerns on negative impacts. In Section 3, we make two necessary assumptions to delimit the scope of manipulations studied in this paper. In Section 4, we introduce our benchmark framework Mirror, including its components, benchmark procedure and evaluation metrics. In Section 5, we conduct four benchmark experiments and analyze the results. Related works are summarized in Section 6. We finally conclude this paper in Section 7.", "2 AN OVERVIEW OF CURRENT RECOMMENDER SYSTEMS": "We are living in an era of information explosion. With the popularity of the internet, we have access to far more information than ever before, making it difficult to find what we need among the vast amount of content. To save people from information overload and find the most relevant content for each of us, modern recommender systems make use of rich user profiles to model user preferences. These systems utilize techniques like collaborative filtering to guess what users might favor, using not only their own but also other users' behaviors. The more you use the recommender system, and the more other people are using the system, the more you will be recommended with relevant results [49, 73]. If we look at the other side, the situation is also thriving. For companies who are deploying largescale recommender systems on their products or platforms, personalized recommendation greatly improves the retention rate of users, as well as the clicks or purchases, depending on the service ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 5 they provide. The method from a pioneer study [25] from Google achieves an average increase of 38% in click rates on its news platforms with the personalized recommendation, compared to general methods. Even more gratifyingly, the e-commerce giant e-Bay reported a 500% spike in Gross Merchandise Volume in online A/B Testing [18] with their personalized recommendation. In industry, recommender systems are continuing to iterate, taking advantage of the increasing amount of data and computing power available to deliver significant revenue for businesses. Recommender System Fig. 2. The overview of the workflow of a typical recommender system. Here we show user feedback as clicks, but in practice, user feedback can also be purchases, ratings, or just browsing, etc. Click Reranking Algorithm SetRank PRM Dataset Rerank Training Ranking Algorithm DeepFM DNN Rank Recall Algorithm (TDM NCF Candidate Set Recall Query Labelled Slates Full Document Set Feedback Start Demonstrate Users Let us briefly review the working mechanism of a standard recommender system. As illustrated in Fig. 2, we consider the multi-stage recommendation setting with user clicks as feedback. The users can interact with the recommender system R for one or multiple rounds. Each round starts with a user \ud835\udc62 proposing a query \ud835\udc5e to R . During round \ud835\udc5f , the user is recommended with a sorted list of \ud835\udc47 documents \ud835\udc37 \ud835\udc5f = { \ud835\udc51 \ud835\udc5f 1 , \ud835\udc51 \ud835\udc5f 2 , . . . , \ud835\udc51 \ud835\udc5f \ud835\udc47 } , and the user can choose to click on any number of documents, forming a click sequence \ud835\udc36 \ud835\udc5f = { \ud835\udc50 \ud835\udc5f 1 , \ud835\udc50 \ud835\udc5f 2 , . . . , \ud835\udc50 \ud835\udc5f \ud835\udc47 } . The process continues until the user exits. The recommended list \ud835\udc37 \ud835\udc5f is generated by the recommender system from a three-stage procedure. A small candidate set D \ud835\udc5e is first retrieved based on \ud835\udc5e by a recall algorithm from the full document set. Then the documents in the candidate set are scored and sorted by a ranking algorithm . Top\ud835\udc47 ranked items form the initial ranking list and are subsequently processed by a reranking algorithm to get a refined order. The reordered list is used as the recommended list \ud835\udc37 \ud835\udc5f and displayed to the ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 6 Z. Zhu et al. user. Feedback from users is logged and stored as the dataset, which is used for improving the algorithms of each stage. The ranking and reranking algorithms are the core of the recommender system and have experienced rapid growth in recent years. Since deep learning-based methods play a dominant role in the industry [20, 34, 100], here in this paper, we only discuss ranking/reranking algorithms that are based on neural networks. In the following parts, we consider two different scenarios of recommendation. In the first slate scenario, we only use the user-item information within this round as the input of the recommender. Ranking and reranking algorithms are discussed in this part. In the second sequential scenario, we use cross-round information as model input. Users' historical interaction data from previous rounds are used as model input to capture the users' preference drift. In this part, we include traditional sequential recommendation solutions and recent offline reinforcement learning-based methods.", "2.1 Slate Recommendation": "Learning to rank (LTR) methods have been widely applied in slate recommendation [98], where the most relevant items are sorted and displayed as ranked lists. Apart from simple point-wise approaches where ranking is modeled as the accurate prediction of the ground truth label for a single document, pairwise or listwise approaches have been explored. Pairwise approaches consider the relative order between a pair of items. As a representative of pairwise methods, RankNet [9] formulates the ranking problem as pairwise classification, where the loss function is constructed by the relative relationship between scores of item pairs. Listwise approaches consider the entire group of documents in the candidate set and their order in the list. LambdaRank and ListMLE are two important list-wise approaches. LambdaNet [10] uses a weighted pairwise loss function to optimize the bound of ranking metrics such as NDCG [90]. ListMLE [93] constructs the maximum likelihood loss on items in the entire list with the Plackett-Luce model assumption, which tackles the ranking list generation task in a more direct way. In real-world ranking systems, neural structures are often used to extract rich feature interactions of each user-item pair. He et al. [38] utilizes a multi-layer perceptron (MLP) structure to model the feature interactions of each user-item pair, which is extended in [37] by using convolutional neural networks (CNN) to further extract potential interactions across embedding dimensions. Wide & Deep [20] is built upon a hybrid network structure, where a linear model delivers a wide set of cross-product features along with a deep feed-forward network modeling complex and implicit interactions. DeepFM [34] combines the factorization machine and deep models to handle low-order and high-order separately, and no need for any feature engineering compared to Wide & Deep. In Multi-stage Recommender Systems (MRS), besides former deep models that encode each user-item pair separately, neural reranking models are applied after the normal ranking stage to further encode interactions between items. After candidate items are scored and ranked by ranking models, the top-ranked ones are fed into a sequence model for reranking rather than directly displayed to the user. The reranking model takes the mutual influence of items in the same list into consideration, which can further improve the quality of recommended lists. DLCM [2] employs a recurrent neural network to learn a local context embedding of all top retrieved items from the ranking process. Later, PRM [66] and SetRank [64] adopt the self-attention structure for more flexibility and efficiency, where PRM additionally utilizes a pre-trained personalized matrix to calculate the embedding.", "2.2 Sequential Recommendation": "Sequential recommendation utilizes users' historical interaction data to capture the dynamic drifts in user preferences. Sequential recommendation views users' preferences as an ever-changing process, which can be influenced by previously recommended items or other external factors, and expressed ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 7 as users' recent behaviors. Earlier works based on Markov chains [35, 36, 70] model the users' behavior as a Markov chain and predict users' next behavior based on their previous actions. Deep learning-based methods have been popular in sequential recommendation. There are RNN-based methods [6, 39, 40, 45], CNN-based methods [48, 84] and transformer-based methods [48]. Offline reinforcement learning (RL) methods are also adopted to facilitate sequential recommendations. Reinforcement learning originated from the optimal control domain, which actively learns control policies from reward signals collected through continual interactions with the environment, e.g., video games [60] or physical simulators [76]. The cost of interacting with the environment is one of the major obstacles to applying RL methods to real-world scenarios. Such cost is especially notable in the recommendation, where the randomness in the recommended items might harm the user experience and reduce the platform revenue. Offline RL methods are RL algorithms that only learn from static datasets without interaction with the user, which enjoy the advantage of RL-based methods but avoid the exploration cost. Xiao and Wang [94] propose a general framework for applying offline RL in sequential recommendations and using a stochastic actor-critic structure with several regularization techniques to stabilize training. In a study by Garg et al. [30], a variant of Batch-Constrained Q-Learning [28] with distributional objective is applied and outperforms the supervised learning methods to a large extent.", "2.3 Rising Concerns Behind the Prosperity": "We have witnessed that the evolution of existing recommendation algorithms is mainly along the lines of using increasingly complex neural networks to model, using increasingly rich interaction data, and even employing more active optimization goals (as those used in offline RL methods). On the one hand, recommender systems are developed and deployed by companies and platforms as a tool to improve revenue, and users do not have access to technical details. On the other hand, due to the black-box nature of neural networks, especially those with numerous parameters, even the developers themselves can not fully understand how the models work [27, 97]. Such a lack of transparency has led to rising concerns about the seeming success of recommender systems. There are several recent studies on the ethical issues with the commercial recommendation. A comprehensive review paper [57] discusses ethical challenges in deploying recommender systems, such as fairness, privacy, and potential negative social impacts. Paraschakis [65] has proposed a framework to guide how to promote fairness, privacy protection, and algorithmic opacity throughout the workflow of the recommendation system. However, how to completely eliminate the aforementioned ethical problems remains an open problem. We focus on another ethical question that is seldom mentioned by previous works: are recommender systems manipulating users' preferences for higher revenue? Typical recommender systems are trained with offline data and evaluated using both offline and online metrics. Offline metrics measure how well the algorithm fits the distribution of the data set, while the situation with online metrics is more complicated. Human decision-making has bounded rationality and is easily influenced by the displayed items. The behavioral economics community has studied some typical phenomena, e.g., the decoy effect, the confirmation bias, and the anchoring effect [3, 11, 14, 47]. Recommendation systems are likely to exploit these psychological weaknesses and biases to achieve higher performance in online metrics, which we call manipulation . While such manipulation can lead to an increase in user clicks, the items clicked on are not necessarily those the user preferred in the first place. The manipulations of user preferences can arise either intentionally or unintentionally. Since most recommender systems operate as a 'black box', it is hard to distinguish whether manipulations originate from intentional design or more sophisticated modeling of user behavior and biases in the data, etc. We are not trying to explore the causes or motives behind the manipulative behavior ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 8 Z. Zhu et al. of recommender systems, but rather to quantitatively measure whether such manipulation exists. From a consequentialist perspective, recommender systems that manipulate user preferences cause equal harm to users, regardless of the initial intent. In this paper, we are dedicated to proposing a general framework for evaluating representative recommendation algorithms for the degree of manipulation in user preferences and demonstrating the effectiveness of the proposed framework under various use cases.", "3 ASSUMPTIONS": "Before presenting our evaluation framework, we need to state two important assumptions. First, we expect the proposed framework to work in a fully offline manner to avoid direct interactions with real users. This is because such interactions with evaluation purposes on industriallevel recommender systems are often costly and may harm the platform's revenue. Learning user models from existing offline interaction datasets is an alternative choice. In those datasets, we only have access to the observed behavior of the users, such as clicking, but not the reasons behind the behavior. In this case, it is difficult to figure out what the user is really thinking inside their mind. Therefore, we make the following assumption. Assumption 1. The observed user's behavior is a true reflection of user preferences at that moment. Notice that the preferences under Assumption 1 are dynamic, since the clicking behavior of a user at a certain moment may be influenced by items that she has seen before [63] or other items on the same slate [93]. It is due to this dynamic nature that recommender systems have the potential to increase the platform's revenue by influencing the users' immediate preferences to deviate from their initial preferences prior to interacting with the system. As we mentioned in Section 2.3, the manipulations of users' preferences may be intentional or unintentional. Some might argue that only intentional influences on users' preferences can be called manipulation. We clarify that we do not aim to distinguish the motivations of those manipulations in this study, which is hard and unnecessary from a consequentialist perspective. Therefore, we introduce another assumption to emphasize the simplification. Assumption 2. A recommender system is considered to have manipulated a user's preferences if 1) the user's preferences change during the interaction with the system compared to those before the interaction and 2) this change results in an increase in the platform's revenue, such as clicks or purchases. With Assumption 2 in mind, as long as we can assess users' initial preferences, their immediate preferences when interacting with a recommender system, and the potential impact of users' behavior on the platform's revenue, we can measure the extent to which that recommender system manipulates users' preferences.", "4 THE GENERAL FRAMEWORK FOR MANIPULATION EVALUATION": "In this section, we introduce our evaluation framework, Mirror, in detail. The naming Mirror implies that we expect to mimic user preference dynamics and reflect the manipulations from recommender systems on user preferences by constructing the simulated user behavior models. Being modular and customizable, this framework is generic and can fit into a wide variety of specific scenarios. We start with the descriptions of the main components involved and then demonstrate the interactive process using these components. The interactive process consists of four stages: 1. initial preference calculation; 2. training data collection; 3. algorithm training and interaction; 4. metrics calculation. In this section, we illustrate each stage in an abstract manner, and later in experiments we show how each stage can be instantiated according to different scenarios. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 9 Fig. 3. The basic components and interactive process of Mirror. Document Sampler Document Set User Model Feature Initial Query Sample Extractor Strategies Ranking 0.85 0.40 0.98 Interface Action Model 0.73 Training Ranking Models 0.98 0.85 0.73 Evaluation document recall documents favorite document user Metrics click training dataset initial feedback 0.52 0.48 0.40 stage 1 stage 2 > stage 3 > stage 1 & 2 & 3 stage", "4.1 Main Components": "Here we discuss the main components used in Mirror. Each of the components can be instantiated with specific implementations, and each combination of these components forms a distinct scenario to evaluate the recommendation algorithms. Among those components, the action model M is the central part that simulates the users' responses to recommender systems. Different instantiations of the action model can reveal distinct aspects of potential manipulations from recommender systems. We start with other components involved and then elaborate on the details of the action model. Document Set. The document set D is a data storage that includes all documents used in the scenario. It should support fetching all available documents under a specific query. Note that in some recommendation scenarios, a user does not start with a query, and any documents can be recommended. In such cases, we assign all users with the same query vector and bind all documents to that single query. Document Sampler. In most cases, the complete document set with respect to a query is too large to be directly fed to ranking algorithms. Therefore, a document sampler is adopted to recall a subset of all available documents to pass to the ranking algorithms. We use random sampling or weighted sampling according to historical click-through rates as recall strategies. One can easily integrate more complex recall algorithms into the framework. User Model. A set of users U for recommendation algorithms to interact with are sampled from the user model. A user \ud835\udc56 is represented by a feature vector \ud835\udc62 0 \ud835\udc56 and a query vector \ud835\udc5e \ud835\udc56 . The feature vector not only includes the observable feature that can be used by the recommendation algorithms but can also contain unobservable information, such as the user's current satisfaction level. After viewing a document slate \ud835\udc37 in interaction round \ud835\udc5f , the user feature changes from \ud835\udc62 \ud835\udc5f -1 \ud835\udc56 to \ud835\udc62 \ud835\udc5f \ud835\udc56 following the transition model T( \ud835\udc62 \ud835\udc5f \ud835\udc56 | \ud835\udc62 \ud835\udc5f -1 \ud835\udc56 , \ud835\udc37 ) . The query vector is used in the retrieval of a subset of documents from the full document set. Feature Extractor. The feature extractor takes as input the document and user feature vector, and outputs the sparse and dense feature for each of them so that ranking models can process each part separately, e.g., use embedding vectors for sparse features. Since the output feature is then passed on to the ranking algorithms, those unobservable features are filtered out here. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 10 Z. Zhu et al. Ranking Interface. The ranking interface aims to provide a unified interface so that researchers can easily integrate new algorithms into the framework and obtain the evaluation results. The framework includes an interaction interface and a training interface. The interaction interface requires the algorithms to provide rankings to a document list when given the observable user features and document features. The training interface is similar to how a general neural networkbased recommendation model is trained, where historical interaction data is fed to the algorithm in a batch-wise manner for model updating. The interaction interface is also used by predefined strategies to collect training datasets for model learning. Evaluation Metrics. The evaluation metrics are used to describe the benchmark results. Our built-in metrics include those commonly used in recommender systems, such as CTR, NDCG, etc., as well as those needed to evaluate the level of manipulations. These metrics measure the effectiveness of the algorithm while assessing the impact on the user's initial preferences. Action Model. The action model determines how users respond to the recommended items. Modules with similar functionality are delicately discussed in other simulation-based work on recommender systems [101], and some of them are referred to by other names, such as interaction model [13] or user model [95]. Here we do not specify the concrete implementation of the action model, but rather the inputs and outputs that it needs to satisfy. Any function that satisfies the required form, either heuristic rules or data-driven models, can be used as the action model. At interaction round \ud835\udc5f , the inputs to the action model are the current user feature \ud835\udc62 \ud835\udc5f \ud835\udc56 and recommended document list with \ud835\udc5a documents \ud835\udc37 \ud835\udc5f = { \ud835\udc51 \ud835\udc5f 1 , . . . , \ud835\udc51 \ud835\udc5f \ud835\udc5a } . The outputs are probabilities of different user feedback or responses to each document in the list. Without loss of generality, we use user clicks as feedback, thus the action model outputs the click probability. It is easy to incorporate other feedback such as the purchase or like in the framework. Given the inputs and outputs, the functional form of the action model M can be written as \ud835\udc5d ( \ud835\udc50 \ud835\udc5f \ud835\udc56 \ud835\udc57 = 1 ) = M( \ud835\udc62 \ud835\udc5f -1 \ud835\udc56 , \ud835\udc51 \ud835\udc5f \ud835\udc57 , \ud835\udc37 \ud835\udc5f \ud835\udc56 ) , where \ud835\udc50 \ud835\udc5f \ud835\udc56 \ud835\udc57 = 1 means the user \ud835\udc56 clicks on the \ud835\udc57 -th item in the recommended list \ud835\udc37 \ud835\udc5f \ud835\udc56 in interaction round \ud835\udc5f . In real recommendation scenarios, the raw user click probability is not visible to the recommender system. Therefore, the binary user click is generated by sampling from the output probabilities and returned to the recommender system.", "4.2 Interactive Process": "As long as the main components of a scenario have been specified, the benchmarking of input algorithms can be decomposed as an interactive process consisting of four stages. In the following part, we describe each stage in detail. 4.2.1 Stage 1: Initial Preference Calculation. Intuitively, manipulations happen when the users' behaviors are contradictory to their initial preferences. For example, under some carefully designed rankings, the users click on less favored documents with high probabilities compared to the most favored document in the same demonstrated list. Therefore, we have to first access the initial preferences of each user to available documents. The definition of initial preferences and the set of related metrics we will later define are central to making Mirror different from existing simulation-based frameworks, allowing us to quantify manipulations from recommender systems. We are not ready in this work to propose a definition to measure manipulation from all sources. Instead, we study two types of manipulation: manipulation caused by changing the relative order of items within the same slate and manipulation caused by altering the order in which items are presented sequentially. These correspond to where recommendation algorithms take effect in two types of recommendation tasks, slate recommendation and sequential recommendation. In order to eliminate these two possible manipulations and obtain ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 11 the initial preferences, we need to take individual items out of the slate and remove the influence of historical recommendations on the user. Based on the discussion above, we define the initial preference score \ud835\udc5d 0 \ud835\udc56 \ud835\udc57 as the user \ud835\udc56 's initial feedback to the document \ud835\udc51 \ud835\udc57 when the demonstrated list { \ud835\udc51 \ud835\udc57 } only contains one document, i.e., \ud835\udc5d 0 \ud835\udc56 \ud835\udc57 = M( \ud835\udc62 0 \ud835\udc56 , \ud835\udc5e \ud835\udc56 , \ud835\udc51 \ud835\udc57 , { \ud835\udc51 \ud835\udc57 }) . At this stage, we calculate \ud835\udc5d 0 \ud835\udc56 \ud835\udc57 for each user \ud835\udc56 and each document associated with \ud835\udc5e \ud835\udc56 , which is determined by the Document Set , using the Action Model , which is later used for quantifying the deviation of users' preferences. 4.2.2 Stage 2: Training Data Collection. To train a machine-learning recommendation model, we have to build a dataset by interacting with users using some initial recommendation strategies. Note that in real-world cases, the initial strategies are usually previously deployed recommendation models, which are often trained in an offline manner and thus require another batch of interaction data. To release the burden of such a recursive data-collecting process, we use rule-based strategies or pretrained online algorithms such as reinforcement learning-based methods as the initial datacollection strategy. The round \ud835\udc5f of interaction with a user \ud835\udc56 starts from retrieving all candidate documents bound to the query \ud835\udc5e \ud835\udc56 from the Document Set and then uses the Document Sampler to recall a subset of documents { \ud835\udc51 \ud835\udc5f 1 , . . . , \ud835\udc51 \ud835\udc5f \ud835\udc5a } . After that, we use the chosen initial strategy to rank the recalled documents as { \ud835\udc51 \ud835\udc5f \ud835\udc58 1 , . . . , \ud835\udc51 \ud835\udc5f \ud835\udc58 \ud835\udc5a } , and demonstrate the top\ud835\udc3e documents { \ud835\udc51 \ud835\udc5f \ud835\udc58 1 , . . . , \ud835\udc51 \ud835\udc5f \ud835\udc58 \ud835\udc3e } to the user, where \ud835\udc3e is the slate size. The user feedback \ud835\udc50 \ud835\udc5f \ud835\udc56 \ud835\udc57 is simulated by the Action Model M( \ud835\udc62 \ud835\udc5f -1 \ud835\udc56 , \ud835\udc51 \ud835\udc5f \ud835\udc58 \ud835\udc57 , { \ud835\udc51 \ud835\udc5f \ud835\udc58 1 , . . . , \ud835\udc51 \ud835\udc5f \ud835\udc58 \ud835\udc3e }) for \ud835\udc57 = 1 , 2 , . . . , \ud835\udc3e . The recommended slate, user feature, and feedback are stored in the dataset. In sequential recommendation scenarios, the interactions take place for multiple rounds. Fig. 4. An illustration of mixing interaction data from different initial strategies into the training dataset. Initial Initial Interact Strategies 1 Data Mix Ratio 0.3 Mirror Initial Mix Ratio Initial Interact 0.2 Strategies 2 Data 2 Ranking Other Interface Components Mix Ratio Initial Initial Interact Strategies N Data Training Dataset To investigate how certain properties of the training dataset impact the trained algorithms, we need to construct a series of datasets to encompass different values of those properties. For example, suppose we want to study whether more manipulative recommendations in the dataset lead to more manipulative recommendations generated by algorithms trained on it. In that case, we have to obtain datasets with different manipulation levels. One approach is to design lots of initial strategies with different manipulation degrees, but this can be difficult. We take a more tricky approach by first designing two extreme initial strategies, i.e., one that prioritizes based purely on users' initial preferences and one that only considers maximizing user clicks. Intuitively, the latter tends to be highly manipulative. Then we just mix the data collected by these two strategies with different ratios. Such mixing of interaction data from different initial strategies can be extended to other ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 12 Z. Zhu et al. properties and mixing of more than two initial strategies. We provide an illustration of data mixing in Fig. 4. 4.2.3 Stage 3: Algorithm Training and Interaction. This stage is aimed at simulating the training and deployment of recommendation models in real industry scenarios. Thus we can access the approximate online influences of algorithms made on users by investigating the simulation results. After stage 1, we obtain a group of users with their initial feedback on all related candidate documents. To benchmark a specific algorithm, stage 3 includes training the algorithm on data collected from the users, and interacting with the same group of users using the trained model. Once the training data is collected and mixed in stage 2, we can train different recommendation algorithms. Depending on the expressive ability of the model and the form of the loss function, the final obtained model will differ in its ability to fit on offline datasets, which will reflect on differences in offline metrics. Traditional evaluations of a recommendation algorithm end up here, without further using the trained model to interact with the users. In our simulation-based framework, we evaluate the online performance of an algorithm by collecting online interacting samples. The interaction procedure is similar to the aforementioned initial data collection, except that the trained model gives the ranking of recalled documents. 4.2.4 Stage 4: Metrics Calculation. In the final stage, after we obtain the interaction data of different algorithms, we can evaluate them with respect to various metrics. Offline metrics can be calculated without interaction data and do not effectively reflect the level of manipulation, so we do not elaborate on them here. We highlight our proposed online metrics, ManiScore defined on a slate, and the Preference Shift (PS) metric used to evaluate the user preference shift during sequential recommendations. Before introducing ManiScore, we first give the definition of the CTR and FCTR on which it depends. Definition 4.1. Given the user action model M , the user feature vector \ud835\udc62 , user query vector \ud835\udc5e and a document list \ud835\udc37 = { \ud835\udc51 1 , . . . , \ud835\udc51 \ud835\udc41 } , define click-through rate (CTR) on the list \ud835\udc37 as  As mentioned before, assuming that we have access to the click model M and the user's initial preference \ud835\udc62 0 , we can assign each document \ud835\udc51 in D \ud835\udc5e with an initial preference score \ud835\udf19 ( \ud835\udc51 ) = M( \ud835\udc62 0 , \ud835\udc5e, \ud835\udc51, { \ud835\udc51 }) , which is the user click probability when the slate only consists of document \ud835\udc51 . Using the initial preference score, we can define the notion of the favorite document set. Definition 4.2. Given the user action model M , the initial preference \ud835\udc62 0 , user query vector \ud835\udc5e and a document set D , define the top\ud835\udc3e favorite document set F \ud835\udc3e as the set of documents in D with \ud835\udc3e highest initial preference score, i.e., F \ud835\udc3e = arg max D \u2032 \u2282 D , | D \u2032 | = \ud835\udc3e \u02dd \ud835\udc51 \u2208D \u2032 M( \ud835\udc62 0 , \ud835\udc5e, \ud835\udc51, { \ud835\udc51 }) . For a specific dataset, we often use a fixed \ud835\udc3e and thus abbreviate F \ud835\udc3e as F . Now we introduce favorite click-through ratio (FCTR) as a metric to measure how much portion of documents chosen by the user are favorite documents. Definition 4.3. Given the user action model M , the current user feature vector \ud835\udc62 , the user query vector \ud835\udc5e , the favorite document set F derived from the complete document set D \ud835\udc5e and a document ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 13 list \ud835\udc37 = { \ud835\udc51 1 , . . . , \ud835\udc51 \ud835\udc41 } , define FCTR on list \ud835\udc37 as  where \u2736 (\u00b7) is the indicator function.  Fig. 5. CTR and FCTR of an example document list. Favorite Docs CTR (0.5 + 0.9 + 0.4) / 3 = 0.6 Other Docs Click Probability FCTR =0.9 / (0.5 + 0.9 + 0.4) = 0.5 0.5 0.9 0.4 Intuitively, a lower FCTR means a more significant proportion of user-clicked documents are not initially favored. Note that a lower FCTR does not always relate to a higher CTR, which means such a contradiction of users' preferences may not be effective for improving the platform's income. In this study, we are interested in effective manipulations. To have a comparative anchor for manipulations, we first introduce the definition of unbiased rankings. Definition 4.4. Given the user action model M , the initial user feature vector \ud835\udc62 0 , and a document list \ud835\udc37 = { \ud835\udc51 1 , . . . , \ud835\udc51 \ud835\udc41 } , if a permutation \ud835\udc37 \u2032 = { \ud835\udc51 \ud835\udc58 1 , . . . , \ud835\udc51 \ud835\udc58 \ud835\udc41 } of \ud835\udc37 satisfies M( \ud835\udc62 0 , \ud835\udc51 \ud835\udc58 \ud835\udc56 , \ud835\udc37 \u2032 ) \u2265 M( \ud835\udc62 0 , \ud835\udc51 \ud835\udc58 \ud835\udc56 + 1 , \ud835\udc37 \u2032 ) for all \ud835\udc56 \u2265 1, then \ud835\udc37 \u2032 is an unbiased ranking of \ud835\udc37 under M and \ud835\udc62 0 . We define the Manipulation Score (ManiScore) of a ranked list to take both the relative magnitude CTR and FCTR when compared to unbiased rankings into account. Definition 4.5. Given the user action model M , the initial user feature vector \ud835\udc62 0 , and a document list \ud835\udc37 = { \ud835\udc51 1 , . . . , \ud835\udc51 \ud835\udc41 } , we denote the set of all unbiased rankings of \ud835\udc37 under M and \ud835\udc62 0 as \ud835\udc37 \ud835\udc48 . We define the Manipulation Score of \ud835\udc37 as  user satisfaction term platform revenue term where \ud835\udc39\ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc48 = min \ud835\udc37 \u2032 \u2208 \ud835\udc37 \ud835\udc48 \ud835\udc39\ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc37 \u2032 and \ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc48 = max \ud835\udc37 \u2032 \u2208 \ud835\udc37 \ud835\udc48 \ud835\udc36\ud835\udc47\ud835\udc45 \ud835\udc37 \u2032 . Note that ManiScore takes values in [ 1 , \ud835\udc52 2 ] . It has a positive correlation with FCTR and a negative correlation with CTR. Specifically, the user satisfaction term in Eq. (1) measures how much the FCTR of the current ranking is lower compared to the unbiased ranking, which indicates whether the recommended list is aligned with the users' initial preferences. The platform revenue term measures how much the CTR of the current ranking is higher compared to the unbiased ranking. Since more clicks often bring more revenue to the platform, this term indicates whether platforms benefit from the evaluated recommendation algorithm. The denominators in both terms are to normalize the overall values to a fixed interval. The exponential term is included to make the ManiScore increase slowly when FCTR and CTR do not deviate much from unbiased rankings. In particular, a ManiScore of one means the ranked list does not have a lower FCTR nor a higher CTR ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 14 Z. Zhu et al. than any unbiased rankings, which means there are no manipulations. We use ManiScore as the core metric to measure the degree of manipulations of recommender systems. During sequential recommendation, we introduce the PS (Preference Shift) metric to evaluate user preference shift across different rounds. Definition 4.6. Given the user action model M , the user preference \ud835\udc62 \ud835\udc5f , and a document set D , define the top\ud835\udc3e favorite document set of interaction round \ud835\udc5f , F \ud835\udc3e \ud835\udc5f as the set of documents in D with \ud835\udc3e highest preference score in round \ud835\udc5f , i.e., F \ud835\udc3e \ud835\udc5f = arg max D \u2032 \u2282 D , | D \u2032 | = \ud835\udc3e \u02dd \ud835\udc51 \u2208D \u2032 M( \ud835\udc62 \ud835\udc5f , \ud835\udc51, { \ud835\udc51 }) . We further sort the document in F \ud835\udc3e \ud835\udc5f in descending order according to preference score M( \ud835\udc62 \ud835\udc5f , \ud835\udc51, { \ud835\udc51 }) and get L \ud835\udc3e \ud835\udc5f . When \ud835\udc5f = 0, L \ud835\udc3e \ud835\udc5f is similar to F \ud835\udc3e in Definition 3.2, except for the sorting process. Obtaining the sequence of favorite document list in different interact rounds, L 0 , L 1 , . . . L \ud835\udc5b , we now define preference shift of round \ud835\udc5f as   where \ud835\udc51 stands for the depth of the ranking list L 0 , L \ud835\udc5f , and \ud835\udc5d is a parameter that determines how steep the decline in weights is: the smaller \ud835\udc5d , the more top-weighted the metric is. Note that rank-biased overlap [92] (RBO) is a measure that calculates the similarity between incomplete rankings, which could handle non-conjointness. As a result, preference shift falls in the range [0, 1], where zero means identical to the initial preference, and one means totally different.", "5 EXPERIMENT": "In this section, we employ our proposed benchmarking framework on both slate and sequential recommendation scenarios to investigate the manipulations of different algorithms on users' preferences. Specifically, we conduct two synthetic experiments based on users' behavioral analysis and two real-world experiments based on existing data-driven simulation environments [24, 88]. Individual conclusions are made after the presentation of the numerical results of each experiment. We conclude this section with a comprehensive analysis of the effects of all four experiments, in which we state our core findings.", "5.1 Synthetic Slate Recommendation Experiment": "5.1.1 Experiment Settings. Consider the scenario where the user has to choose from multiple forms of transportation. Each form of transportation is evaluated by the user for its traveling time \u210e and price \ud835\udc5a . We assume the probability of the user's choice of transportation \ud835\udc51 \ud835\udc56 follows a generalized Random Regret Minimization (RRM) model [22]:  where \ud835\udc37 is the set of all transportation forms examined by the user, and \ud835\udc3f is the value to model the probability of not choosing any displayed transportation. Here we set \ud835\udc3f = -1. The random regret \ud835\udc45\ud835\udc45 is given by  For simplicity, we set all \ud835\udf16 \ud835\udc56 's to zero. Note that the RRM model only takes the examined documents into account. Different documents will have the same \ud835\udc45 under the RRM model when calculating the initial preferences since there is always no regret when a slate only contains one document. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 15 Therefore, we augment the RRM action model by defining the user click probability when only one transportation \ud835\udc51 is examined as  where the utility \ud835\udc48 is given by  In this synthetic experiment, each slate demonstrated to the user consists of three items. To fit in the slate recommendation scenario and consider the user's view order, we set the examination probability of each three positions in the slate as [ 1 . 0 , 0 . 8 , 0 . 6 ] . Therefore, for a recommended slate \ud835\udc37 = { \ud835\udc51 1 , \ud835\udc51 2 , \ud835\udc51 3 } , the click probability for \ud835\udc51 \ud835\udc56 is  where \u2736 (\u00b7) is the indicator function. Intuitively, the transportation with less traveling time and a lower price has higher initial preference scores. The RRM model is shown to have the potential to induce the decoy effect [32]. Decoy effect [41, 58] refers to the following phenomenon observed in human decision-making. Suppose there are two items for the user or consumer to choose from, and the first item is of higher quality and, at the same time, more expensive than the second item. In this situation, the user needs to make a trade-off between quality and price. However, if a third item is added with its price between those two items and its quality being the lowest, the user's choice will be influenced. Now the second item is better than the newly added one in terms of both price and quality, and the user will have a higher chance to choose the second item. The third item is not added to be chosen by the user, but as a reference for comparison to make the user prefer the second item among the first two. Therefore, the third item is also called a decoy. The case that needs to be highlighted is that the first item is a better choice when considering both aspects comprehensively. Then, the introduction of the decoy can lead to irrational behavior of the user. Note that in the case of the transportation scenario, the faster transportation form can be viewed as having higher quality. Under our framework, we aim to use a synthetic dataset to study the relationship between manipulations of users' preferences and the underlying decoy effect, and how much different ranking algorithms can use the decoy effect to increase traditional online metrics. We start with a numerical example to further illustrate the decoy effect in the transportation scenario. Now we consider these four choices of transportation: \ud835\udc51 1 with \u210e 1 = 0 . 8 and \ud835\udc5a 1 = 0 . 2, \ud835\udc51 2 with \u210e 2 = 1 . 0 and \ud835\udc5a 2 = 0 . 4, \ud835\udc51 3 with \u210e 3 = 0 . 1 and \ud835\udc5a 3 = 0 . 8, and \ud835\udc51 4 with \u210e 4 = 0 . 45 and \ud835\udc5a 4 = 0 . 5. We define the user's favorite document as the one with the highest initial preference score, i.e., \ud835\udc51 3. Compared to \ud835\udc51 3, \ud835\udc51 1 has a lower initial preference score, but \ud835\udc51 2 can serve as a decoy of \ud835\udc51 1. We can see that \ud835\udc51 2 is similar to \ud835\udc51 1 but is worse in both aspects, which satisfies the requirements in the decoy effect. During each round of interaction, the document sampler will recall one of two lists of documents, either \ud835\udc51 1 , \ud835\udc51 2 , \ud835\udc51 3 or \ud835\udc51 1 , \ud835\udc51 3 , \ud835\udc51 4. In Fig. 6, we demonstrate two different ranking lists for each recall list, their corresponding user click probabilities, and online metrics (CTR and FCTR). Red arrows visualize the decoy effect. When comparing \ud835\udc37 1 and \ud835\udc37 2, we can see that placing the pair of documents with the decoy effect at the top of the slate, \ud835\udc37 2 has higher CTR and lower FCTR. On the other hand, for \ud835\udc37 3 and \ud835\udc37 4 as rankings for the other recall list, \ud835\udc51 1 and \ud835\udc51 3 are paired with the 'medium' document \ud835\udc51 4, which does not have a decoy effect on either of them. In this case, when placed at the same position, \ud835\udc51 3 receives a higher click probability compared to \ud835\udc51 1 for its advantage. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 16 Z. Zhu et al. Fig. 6. User feedback and metrics of different rankings for two recall lists under the RRM action model. D1 d3 d1 d2 d1 d2 d3 0.38 0.24 0.14 0.39 0.21 0.19 Decoy Decoy CTR 0.2559 FCTR 0.4992 CTR 0.2642 FCTR 0.2415 Favorite Docs Other Docs D3 D4 Click Probability d3 d4 d1 d1 d4 d3 0.36 0.28 0.15 0.34 0.29 0.17 CTR 0.2663 FCTR 0.4508 CTR 0.2663 FCTR 0.2140 In practical implementations, we introduce randomness in the document properties to have a richer and more diverse document set. The document set is made up of 100 groups of documents, and each group consists of four documents from \ud835\udc51 1 to \ud835\udc51 4. The traveling time and price in the first 50 document groups are sampled or set by the following rules:  where Uniform ( \ud835\udc4e, \ud835\udc4f ) is the uniform distribution defined in [ \ud835\udc4e, \ud835\udc4f ] . In most cases, the four documents in each group obtained in such a manner can produce the decoy effect described in Fig. 6. The distributions of traveling time and price dimensions are not aligned by such a sampling procedure, which makes it unreasonable to add them two together when calculating utilities. To address the issue, the remaining 50 groups of documents are generated by swapping the numerical values of the traveling time and price of those documents in the first 50 groups. Here we only introduce diversity in document attributes and do not consider the difference in preferences between different users. That is, this synthetic experiment can be seen as evaluating the interaction results of multiple recommendation algorithms on a single user. The reason is that in this part, we focus on investigating whether different recommendation algorithms can manipulate users with explicitly designed non-optimality. Such manipulations do not necessarily rely on modeling similarities between users and can be learned in the interaction data with a single non-optimal user. To keep the synthetic experiments concise, we do not add differences in user preferences. To generate training data with different degrees of manipulation, we use the following two strategies to collect the initial dataset. \u00b7 Decoy Oracle is the strategy that ranks \ud835\udc51 1 , \ud835\udc51 2 , \ud835\udc51 4 as \ud835\udc37 2 and ranks \ud835\udc51 1 , \ud835\udc51 3 , \ud835\udc51 4 as \ud835\udc37 3 and \ud835\udc37 4 for equal probability. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 17 \u00b7 Unbiased Oracle is the strategy that ranks \ud835\udc51 1 , \ud835\udc51 2 , \ud835\udc51 4 as \ud835\udc37 1 and ranks \ud835\udc51 1 , \ud835\udc51 3 , \ud835\udc51 4 as \ud835\udc37 3 and \ud835\udc37 4 for equal probability. We generate a training dataset by mixing up the interaction data collected by these two ranking strategies with different mix ratios. A mix ratio of \ud835\udefc means \ud835\udefc fraction of training data are ranked by Decoy Oracle and the rest ( 1 -\ud835\udefc ) are ranked by Unbiased Oracle. 5.1.2 Evaluated Methods. We evaluate two LTR algorithms in this scenario. \u00b7 DeepFM [34] is a neural ranking model that combines a factorization machine and a deep neural network. It is a representative work of point-wise ranking models that treat each item independently. \u00b7 SetRank [64] is a neural reranking algorithm using a self-attention structure to encode the entire slate and is trained with attention loss. We use SetRank as an instance to represent reranking algorithms, which can model the mutual interactions between items in the same slate. Fig. 7. The CTR and FCTR of algorithms trained on the synthetic dataset with different mix ratios. The metrics values of initial strategies are plotted with dashed lines. 0.0 0.2 0.4 0.6 0.8 1.0 Mix Ratio 0.250 0.252 0.254 0.256 0.258 0.260 CTR Decoy Oracle Unbiased Oracle DeepNN SetRank 0.0 0.2 0.4 0.6 0.8 1.0 Mix Ratio 0.20 0.25 0.30 0.35 0.40 FCTR Decoy Oracle Unbiased Oracle DeepNN SetRank 5.1.3 Experiment Results. From Fig. 7, we can see that when the mix ratio of training data is 1.0, ranking lists given by SetRank have higher CTR and lower FCTR than DeepFM. When the mix ratio increases from 0.0 to 1.0, both metrics of DeepFM change smoothly, while those of SetRank experience a sudden change somewhere around the mix ratio of 0.4 and 0.8. In general, as more data generated from Decoy Oracle are mixed into the dataset, the CTR of both methods becomes higher while their FCTR becomes lower. 5.1.4 Conclusions. From this experiment, we can draw the following conclusions: 1) the degree of manipulation is mainly influenced by the ranking strategy used to generate the training data; 2) When there exist manipulations in the training data, the reranking algorithms like SetRank are more likely to learn from those patterns of manipulation compared with point-wise methods like DeepFM. It is reasonable since reranking algorithms can model the mutual influences among the entire slate and thus are better aware of the decoy effects in the slate.", "5.2 Data-driven Slate Recommendation Experiment": "5.2.1 Experiment Settings. In this subsection, we conduct data-driven experiments under the slate recommendation setting. We use AICM [24] with TianGong-ST [15] dataset as our test bed. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 18 Z. Zhu et al. AICM adopts a GAN-like adversarial learning approach to imitate human users' click behavior. A generator is used as the simulated user click model, and a discriminator measures the distance between generated clicks and human clicks in the dataset. Experiments show that the generator in AICM can generalize well across different ranked list distributions when compared with other neural click models, including NCM [7] and CACM [16]. Such superior distributional coverage makes the generator quite suitable to be used as simulated human users when interacting with diverse recommendation models. Before using the trained generator in AICM as an action model in Mirror, we need to verify that it meets the input and output forms required in Section 4.1. The generator is implemented using the gated recurrent unit (GRU) [21] structure. As slate recommendations only involve a single round of interaction, we omit the superscript \ud835\udc5f for simplicity. For each document list \ud835\udc37 = { \ud835\udc51 1 , . . . , \ud835\udc51 \ud835\udc5a } , the generator processes each document \ud835\udc51 \ud835\udc57 in a top-to-bottom order, which allows the documents that are ranked on top to influence the click probabilities of the documents that follow. The current user feature \ud835\udc62 \ud835\udc56 , document feature \ud835\udc51 \ud835\udc57 and the click on the previous document \ud835\udc50 \ud835\udc56 ( \ud835\udc57 -1 ) are used to update the hidden state of GRU from \u210e \ud835\udc56 ( \ud835\udc57 -1 ) to \u210e \ud835\udc56 \ud835\udc57 . Then a linear layer followed by a Softmax operator transforms the updated hidden state to the click probability \ud835\udc5d ( \ud835\udc50 \ud835\udc56 \ud835\udc57 = 1 ) . Therefore, the computation inside the generator can be written as  We know that \u210e \ud835\udc56 ( \ud835\udc57 -1 ) is updated by all previous documents { \ud835\udc51 1 , . . . , \ud835\udc51 \ud835\udc57 -1 } and \ud835\udc62 \ud835\udc56 . Thus, the input used to compute the output probability \ud835\udc5d ( \ud835\udc50 \ud835\udc56 \ud835\udc57 = 1 ) includes { \ud835\udc51 1 , . . . , \ud835\udc51 \ud835\udc57 } and \ud835\udc62 \ud835\udc56 , which is a subset of the required form of input. That is, the generator in AICM meets the requirements of the action model in Mirror. In our experiment, we employ 35,000 queries from the TianGong-ST dataset to form our user set, and each query is associated with ten candidate documents. Since we are interested in the manipulations that happen in real-world recommendation systems, we use the same set of users for training data collection and online evaluation. The original page size of AICM is ten, and here we use five as the page size to allow randomness among interactions with the same user. During each interaction, half of the documents are randomly recalled from ten candidates and then ranked by the scores provided by the ranking or reranking algorithm. To collect offline data with different degrees of manipulations, we introduce two recommendation policies that represent the most and least manipulations possible, respectively. \u00b7 Greedy Oracle has access to the ground-truth user model and will enumerate all permutations of items on the slate and select the one with the highest click probability. \u00b7 Unbiased Oracle is the strategy that consistently ranks items according to users' initial preference scores. For a given document set \ud835\udc37 , it provides one of the unbiased rankings, as in Definition 4.4. We construct offline data with different degrees of manipulation by mixing up the user feedback of slates generated by these two oracles with different ratios. Specifically, the offline data consists of feedback for ten pages from each user, and the mix ratio is the percentage of the slates generated from Greedy Oracle. For example, a mix ratio of 1.0 means all data are generated from Greedy Oracle, 0.5 means for each user, Unbiased Oracle ranks five slates, and Greedy Oracle ranks the other five. When training all the algorithms, we use 15% of the offline data as the test set and the rest as the training set. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 19 5.2.2 Evaluated Methods. Besides two LTR algorithms already introduced in the synthetic experiment, we include one more point-wise ranking algorithm with different loss functions in this experiment. \u00b7 LambdaFM [96] shares the same encoding module with DeepFM but is trained with pairwise loss functions weighted by delta NDCG to reduce the incorrectly ordered pairs. It is a typical example of a point-wise model trained with pair-wise loss functions. 5.2.3 Experiment Results. First, we present offline and online metrics of the evaluated algorithms trained on the same data with a mixed ratio of 0.5. Before the analysis of the learning algorithms, we can see there exists a vast gap between the CTR and FCTR of two oracles, which means the significant impact of the ranking of documents in the recommended list on user behaviors. As we can see from Tab. 1, with modeling capabilities at the list level, SetRank obtains the best performance in offline metrics and online CTR compared to other learning algorithms. The simple point-wise model, DeepFM, performs the worst in these metrics. However, the FCTR of SetRank is significantly lower than that of DeepFM, which means the high CTR of SetRank is accompanied by users clicking even more items they do not favor. We also notice that LambdaFM has both lower CTR and FCTR compared to SetRank, which indicates that not all infringements of users' preferences are effective manipulations. Table 1. The evaluation results of different algorithms (strategies) on the data-driven slate recommendation scenario, where the mix ratio of training data is 0.5. We also train those three recommendation algorithms using training data with different mix ratios. The changing curve of CTR and FCTR with various mix ratios are plotted in Fig. 8(a). Overall, as the mixed ratio increases, i.e., more training data is generated by Greedy Oracle, all algorithms tend to have lower FCTR and higher CTR. It means that the degree of manipulation of the learned algorithm is easily influenced by the degree of manipulation of the recommendation strategy in the training data. As illustrated in Fig. 8(b), if we connect the corresponding points of the same algorithm from lower to higher mix ratios to form a trajectory on CTR-FCTR space, the trend of coming close to the training data point is quite clear. Another interesting finding is that when the mix ratio is very close to 1.0, SetRank instead has the highest FCTR. It indicates that SetRank can better fit the unbiased data for its stronger modeling capacity when it is clean of manipulations. 5.2.4 Case Study. We take an example from our dataset to better understand the unbiased and greedy policy. We first take a glance at the action model. For each user, we sort the candidate items according to their initial preference and choose two of them, where one item \ud835\udc51 \ud835\udc3c is of the highest initial preference to the user, implying that it is more likely to be ranked first by an unbiased policy. The other item \ud835\udc51 \ud835\udc40 is of medium preference to the user, suggesting that it is more likely to be ranked first by a greedy policy. In Fig. 9, we show the difference between unbiased and greedy ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 20 Z. Zhu et al. Fig. 8. Online metrics of different recommendation algorithms when trained with data of different mix ratios. Figure (a) shows the changing curve of CTR and FCTR in the same figure, where the arrows point from the low mix ratios to the high mix ratios. Figure (b) shows the changing curve of CTR and FCTR in the same figure, where the arrows point from the low mix ratios to the high mix ratios. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Mix Ratio 0.195 0.200 0.205 0.210 0.215 0.220 0.225 0.230 CTR DeepFM LambdaFM SetRank Unbiased Oracle Greedy Oracle 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Mix Ratio 0.55 0.60 0.65 0.70 0.75 0.80 0.85 FCTR (a) 0.195 0.200 0.205 0.210 0.215 0.220 0.225 0.230 0.235 CTR 0.55 0.60 0.65 0.70 0.75 0.80 0.85 FCTR DeepFM LambdaFM SetRank Unbiased Oracle Greedy Oracle (b) policies. The first row stands for an unbiased policy, and the second stands for a greedy policy, where the unbiased policy first shows \ud835\udc51 \ud835\udc3c to the user, and the greedy policy first shows \ud835\udc51 \ud835\udc40 . The user will re-evaluate the two items when the first item is shown to the user. For the unbiased policy, the first viewed item by the user is of the highest initial preference, so the user tends to give a lower relevant score during re-evaluations. In contrast, for the greedy policy, as the first item is of medium preference, a higher relevant score will be given to the two items after the first item is shown. When the two items are shown to the user successively, we calculate the overall user preference towards the same two items and find that the greedy policy gains a higher score. Now we have seen there do exist unbiased and greedy data in the user model, and the next question is the capability of different algorithms to manipulate. We fix the mix ratio of training data as zero, which means that the training data is purely greedy. In Table 2, the average position of the favorite item means the average position of the users' favorite item that occurs in the slate (which ranges from 1 to 5), where the favorite item means the user has the highest initial preference for the item. Similarly, we can define the average position of the least favorite item. Intuitively, the higher the degree of manipulation is, the larger the average position of the favorite item is, and the smaller the average position of the least favorite item is. Table 2 shows that SetRank manipulates more than DeepFM when the training data is manipulated. Fig. 9. Comparison of the unbiased and greedy data in user model. Total Preferences on New Preference Viewed Documents H1.25 1.00 1.17 + 0.79 1.96 Initial Preference User +1.25 DM DI DM 1.00 DI DM 0.75 1.25 DI DM 1.02 + 1.24 2.26 0.75 DM DI DM DI DI DM ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 21 Table 2. The average positions of both types of documents in the recommended slates generated by different algorithms. 5.2.5 Conclusions. From this experiment, we can draw the following conclusions: 1) real users can also be heavily manipulated by the recommendation strategy, sacrificing FCTR to achieve a higher CTR; 2) the findings in the synthetic experiment regarding the effects of training data and model choice on the degree of manipulations still apply to real users; 3) the case study reveals that manipulations on real users can be explained by placing less favored documents in front of the favorite items, similar to the decoy effect in the synthetic experiment.", "5.3 Synthetic Sequential Recommendation Experiment": "5.3.1 Experiment Settings. We start the evaluation of sequential recommendation algorithms with a synthetic scenario adopted by previous works [42, 43]. In this scenario, each document in the document set belongs to a topic, where in total, we have \ud835\udc5a topics. The user feature vector \ud835\udc62 \u2208 [ 0 , 1 ] \ud835\udc5a is the users' preference for each topic. In each round of sequential interaction, the recommendation algorithm presents users with a document from a set of candidate documents. For each document, besides the topic represented by a one-hot vector, it is also assigned with an unobservable scalar quality. The quality is sampled from Gaussian distributions with fixed variance and means determined by the topic. Initially, users have a fixed time budget, and the time budget will reduce after each round. The quality of the document specifies the reduced amount. If the document quality is high, the number of total viewed documents will be higher. Therefore, this leads to a balance between long-term and short-term gains. The recommendation algorithm can change users' preferences by influencing them in the long term so that they all move to higher-quality documents on average to achieve more clicks. In our experiment, we set \ud835\udc5a = 10, and the document set consists of 10 documents for each topic, which in total has 100 documents. The algorithm can interact with users for at most 20 rounds. During each round, the ranking algorithms select 1 document from 10 randomly recalled documents (from 100 documents) to demonstrate to the user. We use two strategies to generate a diverse training dataset: \u00b7 Unbiased Oracle always selects recommended documents based on the user's initial interest. \u00b7 Planner Oracle evaluates a document by computing the expected cumulative clicks when recommending documents from the same topic in each subsequent round using the mean quality of that topic and makes recommendations based on the evaluation. Intuitively, the Planner strategy may recommend documents from topics not currently the user's favorite in the short term, thus getting more clicks in the long term. The training data is made up of half of the data collected using Unbiased Oracle and the other half collected using Planner Oracle. 5.3.2 Evaluated Methods. In this scenario, we adopt two ranking algorithms for comparison. \u00b7 DNN uses a feedforward neural network to encode the user feature and document features and does not model the user history. The network is trained with cross-entropy loss to predict the user click probability. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 22 Z. Zhu et al. \u00b7 GRU builds upon DNN and additionally uses a GRU network to model clicked documents during the current interaction round. We use this algorithm as a representative of sequential recommendation algorithms.", "5.3.3 Experimental Results.": "Table 3. The evaluation results of different algorithms (strategies) on the synthetic sequential scenario. As we can see in Tab. 3, the two oracle strategies perform as expected. Comparing the evaluation results of the two strategies, the Planner Oracle has a higher CTR and a lower FCTR. Moreover, we demonstrate the users' Preference Shift after 20 rounds of interaction using both strategies. The Planner Oracle has an order of magnitude higher impact on user preferences than the Unbiased Oracle. Notice that the PS of both strategies is low since, in this scenario, there are ten topics of documents in total, and only the preferences of the topics that are demonstrated to the users change, the relative relationships of most topics remain the same. For the two algorithms evaluated, when trained with the same dataset, GRU behaves more closely to Planner in online metrics than DNN. GRU obtains a higher CTR at the cost of a lower FCTR and more significant impacts on user preferences. On the other hand, GRU has a higher offline AUC, which means it fits better to the dataset and is superior considering pure offline evaluations. Fig. 10. CTR and PS of evaluated algorithms and strategies across 20 interact rounds. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Round 0.0 0.1 0.2 0.3 0.4 0.5 CTR Planner Oracle Unbiased Oracle GRU DNN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Round 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200 Preference Shift Planner Oracle Unbiased Oracle GRU DNN We also show in Fig. 10 the trend of CTR and PS with the number of interaction rounds in the sequential recommendation. Note that when we calculate CTR, users who run out of time budget are included in the calculation as samples with no clicks, so as the number of rounds increases and more users leave the platform, the overall CTR will tend to decrease. Comparing the two Oracle strategies, we can see that Planner Oracle has a lower CTR than Unbiased Oracle in the first nine rounds and a higher CTR than Unbiased from 10 rounds onwards, which is due to the slower ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 23 decline of Planner Oracle's CTR. It can be explained by Planner Oracle's tendency to recommend documents that will entice users to stay on the platform longer rather than what they are most likely to click on at the moment. Documents within a user's current preferred topic may be of lower quality, and the Planner will actively change the user's preferences to achieve higher CTR throughout the interaction rounds. The higher PS corresponding to the Planner Oracle can also account for its more active influence on user preferences. Overall, similar to the table results, the CTR curve of the GRU algorithm is closer to that of the Planner Oracle compared to the DNN algorithm. 5.3.4 Conclusions. From this experiment, we can draw the following conclusions: 1) with explicit multistep planning, the recommender strategy can sacrifice some short-term clicks to get more user engagement in the long run; 2) such recommendation strategy with planning also has a greater impact on user preferences; 3) the recommendation model that utilizing the user's recent behavior is more similar to the multistep planning strategy in both dimensions of CTR and Preference Shift.", "5.4 Data-driven Sequential Recommendation Experiment": "5.4.1 Experiment Settings. For experiments of sequential recommendations, we use the user behavior model and accompanying dataset in the recently proposed simulated environment RL4RS [88] benchmark. The dataset in RL4RS is logged from the real user feedback in the most popular games released by NetEase Games . The user behavior model is trained on the dataset using DIEN [99], which we use as the action model. Similar to the data-driven slate recommendation experiment, we need to check whether the user behavior model in RL4RS meets Mirror's requirements. Before the interaction starts, each user in RL4RS is initialized with a certain length of historical clicked items, which is denoted as \ud835\udc36 0 \ud835\udc56 . At interaction round \ud835\udc5f , the input to the DIEN-based behavior model includes the historical clicked items \ud835\udc36 \ud835\udc5f -1 \ud835\udc56 until round \ud835\udc61 -1, the static user feature \ud835\udc62 \ud835\udc56 , the recommended item list \ud835\udc37 \ud835\udc5f \ud835\udc56 = { \ud835\udc51 \ud835\udc5f \ud835\udc56 1 , . . . , \ud835\udc51 \ud835\udc5f \ud835\udc56\ud835\udc5a } , and the model outputs the click probability for each document. After each round, the clicked items are appended to the \ud835\udc36 \ud835\udc5f -1 \ud835\udc56 to form \ud835\udc36 \ud835\udc5f \ud835\udc56 . If we view the historical clicked items as part of the user feature, the RL4RS behavior model can fit in the required form:  where \ud835\udc62 \u2032 \ud835\udc5f \ud835\udc56 = ( \ud835\udc62 \ud835\udc56 , \ud835\udc36 \ud835\udc5f \ud835\udc56 ) . We include two oracle strategies for training data collection. Since enumerating all permutations of possible recommendation items becomes infeasible for the large combination space in RL4RS, we use an online RL algorithm with the objective of maximizing the user clicks as the strategy that purely optimizes CTR regardless of FCTR. \u00b7 Unbiased Oracle is the strategy that always ranks items according to users' initial preference scores. \u00b7 PPO Oracle is a recommendation policy trained with a reinforcement learning algorithm PPO [76] by online interactions with the user model. At each step, the policy picks one item to the list. The reward is sparse and given as the number of clicks on the recommended list at the end of the list. 5.4.2 Evaluated Methods. We want to investigate whether modeling recent user behavior can lead to a higher Preference Shift. Therefore, we evaluate two sequential recommendation algorithms. For each of them, we implement two versions, Static and Dynamic. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 24 Z. Zhu et al. \u00b7 GRU Static uses GRU to encode the users' initial behavior history, and the clicked items during interactions are not appended to the history. \u00b7 GRU Dynamic uses GRU to encode the user behavior history, which is actively adapted according to the recently clicked items. \u00b7 DIN [100] Static utilizes an attention structure to extract information from user behavior history. Similar to GRU Static, the recent click history is not fed into the model. \u00b7 DIN Dynamic uses the same network structure and training objective as DIN Static, while the recently clicked items are included in the user behavior history. \u00b7 BERT4Rec [83] Static adopts the BERT (Bidirectional Encoder Representations from Transformers) structure to model the user behavior history. To learn the bidirectional representations, models are trained to predict the clicks on randomly masked items in the history using their left and right contexts. The recently clicked items are not appended to the history. \u00b7 BERT4Rec Dynamic uses the same network structure and training objective as BERT4Rec Static, except the recently clicked items are dynamically updated to the history. 5.4.3 Experiment Results. Tab. 4 presents the benchmarking results when the training data is collected using PPO Oracle. As we look at the offline metrics, three dynamic algorithms have better performance compared to their static counterparts. In online evaluations, results of those dynamic methods that encode the users' recent click history show a similar pattern as reranking methods in slate recommendations. The CTR of DIN Dynamic, GRU Dynamic, and BERT4Rec Dynamic is higher than DIN Static, GRU Static, and BERT4Rec Static, respectively. On the other hand, the FCTR of dynamic methods is relatively lower. Also, regarding the long-term impact on user preference after multiple interaction rounds, we witness that the Preference Shift of dynamic algorithms is also higher. Table 4. The evaluation results of different algorithms (strategies) on the data-driven sequential recommendation scenario when the mixed ratio is 1.0. In Fig. 11, we demonstrate the results when we change the mix ratio of training data from 0.2 to 1.0. When the mix ratio of training data is small, the CTR of GRU Static, DIN Static, and BERT4Rec Static, which have higher CTR and lower FCTR, are more manipulative than dynamic ones. Also, the Preference Shift induced by static algorithms is larger than that induced by dynamic algorithms. This might be due to the property of Unbiased Oracle. For the training data collected by Unbiased Oracle, the recommended slates are not determined by the users' recent history. Also, the user preferences across multiple rounds are also less affected since, from Tab. 4, the Unbiased Oracle has ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 25 Fig. 11. Online metrics of difference algorithms (strategies) when trained with data of different mix ratios. Figure (a) shows the changing curves of CTR and FCTR with different mix ratios. Figure (b) shows the changing curves of preference shifts with different mix ratios. For better presentation, the values in Figure (b) are the results of subtracting the values of Unbiased Oracle. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Mix Ratio 0.44 0.46 0.48 0.50 0.52 0.54 CTR DIN Dynamic GRU Dynamic BERT4Rec Dynamic DIN Static GRU Static BERT4Rec Static PPO Unbiased 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Mix Ratio 0.6 0.7 0.8 0.9 1.0 FCTR (a) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Mix Ratio 0.12 0.14 0.16 0.18 Preference Shift (Normalized with Unbiased Oracle) DIN Dynamic GRU Dynamic BERT4Rec Dynamic DIN Static GRU Static BERT4Rec Static PPO (b) a smaller value of Preference Shift. In such cases, algorithms that take the recent user click history into account could instead overfit the irrelevant information, degrading the online performance. When the mix ratio is close to 1.0, where most of the data is collected by PPO Oracle, the dynamic algorithms have higher CTR but also end up with lower FCTR and higher Preference Shift. 5.4.4 Conclusions. From this experiment, we can draw the following conclusions: 1) real users can be manipulated in sequential recommendations, and such manipulative recommendations cause more significant changes in user preferences; 2) the FCTR and Preference Shift of dynamic algorithms change more dramatically than static algorithms as the mix ratio changes; 3) when the training data is generated exclusively by a manipulative strategy, the dynamic algorithms are more similar to the manipulative strategy in all metrics.", "5.5 Comprehensive Analysis": "In the final part of the experiment section, we provide a comprehensive analysis of the experimental results and give our general understanding of the manipulations of recommender systems on user preferences. 5.5.1 Good performance on traditional metrics can come at the cost of manipulating user preferences. One of the main reasons we propose this framework is that traditional metrics for evaluating a recommender system, whether online or offline, are agnostic to the system's manipulation of the user's preference. In all four benchmark experiments, we can find some algorithms that are able to achieve higher CTR and perform better on offline metrics like AUC and NDCG but also make users click on a smaller proportion of favorite documents or even actively influence user preferences. For example, in the data-driven slate recommendation scenario, the NDCG and CTR of DeepFM are 0.5835 and 0.2031, respectively. Both are much higher than those of DeepFM, which are 0.5685 and 0.2010. However, when evaluated with manipulation-aware metrics, SetRank is considered to be more manipulative on users' initial preferences, which may negatively affect the reputation of the recommendation platform in the longer term. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 26 Z. Zhu et al. 5.5.2 Data greatly shapes the manipulation degrees of algorithms trained on it. In most experiments, we demonstrate the evaluated results when algorithms are trained with different mix ratios of data. The results indicate that the initial strategy or algorithm used to collect the training data significantly impacts on the manipulation behavior of the models trained afterward. For instance, in the data-driven slate commendation experiment, SetRank is always more manipulative than DeepFM, when trained using data with small mix ratios. However, from Fig. 8(a), we can see that DeepFM trained on the mix ratio of 0.1 is instead more manipulative than SetRank trained on the mix ratio of 0.2. In both synthetic and data-driven experiments, if a highly manipulative algorithm collects more training data, the same machine learning model trained on it tends to make more manipulations on users' preferences. 5.5.3 More powerful modeling abilities lead to greater sensitivity to changes in the degree of manipulation within the training data. We compare recommendation algorithms with different modeling abilities in both slate and sequential recommendation scenarios. Note that the modeling ability here does not refer to the neural model complexity, but the complexity of user and document information it takes into account. In slate recommendations, reranking models explicitly model the interactions of documents within a recommended slate, which are considered to have higher modeling ability compared to point-wise models. Similarly, in sequential recommendations, models like DIN that capture users' recent behaviors have higher modeling ability than static models. From experiments on both data-driven experiments, although the degree of manipulations of all algorithms is affected by the training data, algorithms with stronger modeling abilities undergo greater impact.", "6 RELATED WORK": "", "6.1 The Influence of Recommender Systems on Users and Related Concepts": "Some previous works have shed light on how recommender systems might affect users' internal states or preferences, which is what our research focuses on. As is mentioned in [12], some work is based on empirical study [19]; some analyze static datasets of interactions [54, 61, 71]; some are simulation-based, modeling users either using handcrafted, explicit rules [13, 31, 42, 44, 56, 95] or learning user dynamics implicitly [12]. Our work is similar to the latter one. With two synthetic and data-driven scenarios, we emphasize the potential impact of RS algorithms on users. A closely related concept is fairness in recommender systems. While improved online metrics, such as click-through rate, are the most critical metrics for recommender systems, fairness is proposed to ensure recommender systems allocate resources equitably and improve the personal experience and social good. In the survey of the fairness of recommender systems [91], it is noted that both users and items may suffer from unfairness. For users, considerable differences are found in several recommendation measurements, such as accuracy, diversity, and novelty for users of different ages and genders [26, 89]; and for items, less popular items may get less exposure opportunity [5, 102], which is similar to popularity bias [101]. However, the causal relationship is the opposite between our work and the fairness problem. Our work focuses on how recommender systems influence or manipulate users, providing several metrics to measure the manipulation degree of a particular algorithm under a specific recommendation scenario. In contrast, fairness concerns how different users or items suffer from a specific recommendation system. The former does not care about the difference between users, and in our work, we take the average score of users instead, while the latter concerns the individuals.", "6.2 User Behavior Modeling": "The modeling of user behavior is an enduring topic in recommender system research. Early attempts were made to build the user click model under the probabilistic graphical model (PGM) [46] ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 27 framework. These models rely heavily on different assumptions to specify how documents in the same slate jointly influence user behavior. The cascade model (CM) [23] adopts the assumption that users scan each document from top to bottom of the slate until the first click. Such an assumption constrains each query session to have exactly one click, and several follow-up works are proposed to overcome the limitation [33, 82]. Since these simple click models are constrained by strong assumptions, thus are not suitable for our use case of investigating human's bounded rationality in recommendations. Neural network-based methods are proposed for more flexible and realistic user behavior modeling. CTR estimation models like DeepFM use neural networks to model the feature interaction of a single item and predict the user click probability of each item independently. Neural networks are also applied in building click models that take the influences from previous items within the same slate into account. The neural click model (NCM) [7] first applies neural networks in building click models. The click sequence model (CSM) [8] employs an encoder-decoder architecture, in which the encoder computes the contextual embedding of the documents and sends it to the decoder to predict the clicked documents. Several methods further incorporate the interactions across different slates, i.e., the user behavior history, for better user modeling. DIN [100] and DIEN [99] use the attention mechanism to assign different weights to historical behaviors according to their relevance to the target document. Hierarchical Periodic Memory Network (HPMN) [69] uses a lifelong personalized memory for storing the user interest representations, which is updated incrementally. User Behavior Retrieval (UBR4CTR) [68] uses retrieval techniques to obtain the most relevant behaviors from the entire user behavior sequence. Another series of works adopt adversarial imitation learning methods to learn user models, and the learned models are then used as environments to train reinforcement-learning-based recommendation algorithms [78, 80]. Such neural network-based methods can automatically extract the real and complex dependencies within and across different slates from user behavior, including the possible irrational dependencies. Since we focus on studying the manipulations based on those existing advanced user models, the details of how the user models are obtained are beyond the scope of our discussion.", "6.3 Simulation-based Research on Recommender Systems": "Measuring algorithms purely on metrics from offline datasets is flawed [86], but not all algorithms can be validated online on real recommendation platforms in the first place. In recent years, some simulation-based platforms have been aiming to provide environments where recommendation algorithms can be evaluated or benchmarked in an online manner, and some work [4] has compared these platforms briefly. RecoGym [72] is a simulation platform that provides a unified recommendation framework for combining classical recommendation algorithms with reinforcement learning methods, allowing both offline and online experiments. Still, it can be far from the real-world dataset by just using synthetic data. PyRecGym [79] extends RecoGym by supporting multiple input data types and user-feedback functions, but its architecture makes it difficult to create reusable simulation environments. RecSim [42], like RecoGym, focuses on reinforcement learning methods for recommender systems and is suitable for simulating continuous interactive recommendation problems. It allows creating environments that can reflect certain aspects of user behavior but it is still hard to simulate a complex system. Virtual-taobao [80] can reflect properties of the real environment by generating virtual customers and interactions through GANSD and MAIL, respectively. To address the gap between simulation and real-world applications, RecSim NG [59], a scalable, modular, differentiable simulator, is proposed based on RecSim, offering a powerful, general probabilistic programming language for agent-behavior specification. However, the evaluation criteria adopted in existing works are aligned with traditional online metrics, and no simulation framework explicitly measures the degree to which recommendation algorithms manipulate user ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 28 Z. Zhu et al. preferences. Our work lies in the general area of simulation-based study of recommender systems using real-world datasets, but investigates a specific problem, manipulations of users' preferences from recommender systems, which challenges the effectiveness of traditional online metrics.", "7 CONCLUSION AND FUTURE WORK": "In this work, we put forward and investigate the problem of manipulations of users' preferences in today's recommender systems. Based on the bounded rationality of human behavior, we challenge the sources of advances in the online performance of current recommendation algorithms. We posit that the improvement in online performance may come from manipulating user preferences, and propose a general and configurable benchmark framework named Mirror to measure the manipulations. We make the simulated user behavior model to provide individual scores for documents in the entire document set one by one, which serves as the core of our manipulationaware evaluations. Key Findings. We use our proposed framework to evaluate various recommendation algorithms in both slate and sequential recommendations. We first find that in slate recommendations, more advanced reranking algorithms over-exploit the influence of the interrelationship of documents within a slate on human decisions, which causes users to choose more documents they do not favor, despite the uplift in traditional online metrics. In sequential recommendations, those algorithms that dynamically model the user's recent behavior will significantly impact on user preference, resulting in users choosing less favored documents. Moreover, in all scenarios, we found that the degree of manipulation in the training data had a significant effect on the degree of manipulation of the final recommendation model. Future Work. Our proposed framework requires access to the raw output of ranking models and the complete document set, which is not available for each individual user. In future work, we will study the approximated estimation of the manipulations from the user side, which can be used as personal supervision of recommender systems. Moreover, we are interested in proposing practical solutions to reduce manipulations in recommendation systems. We also hope that our proposed benchmark will make the academic community and the industry aware of the limitations of existing online metrics and be concerned about the negative impact of manipulations on user preferences from the recommender systems.", "ACKNOWLEDGMENTS": "The Shanghai Jiao Tong University team is supported by National Key R&D Program of China (2022ZD0114804), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (62177033). The Nanjing University team is supported by National Natural Science Foundation of China (61921006).", "REFERENCES": "[1] Gediminas Adomavicius, Jesse C Bockstedt, Shawn P Curley, and Jingjing Zhang. 2018. Effects of online recommendations on consumers' willingness to pay. Information Systems Research 29, 1 (2018), 84-102. [2] Qingyao Ai, Keping Bi, Jiafeng Guo, and W Bruce Croft. 2018. Learning a deep listwise context model for ranking refinement. In The 41st international ACM SIGIR conference on research & development in information retrieval. 135-144. [3] Oscar Bergman, Tore Ellingsen, Magnus Johannesson, and Cicek Svensson. 2010. Anchoring and cognitive ability. Economics Letters 107, 1 (2010), 66-68. [4] Lucas Bernardi, Sakshi Batra, and Cintia Alicia Bruscantini. 2021. Simulations in Recommender Systems: An industry perspective. arXiv preprint arXiv:2109.06723 (2021). ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 29 [5] Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan Hong, Ed H Chi, et al. 2019. Fairness in recommendation ranking through pairwise comparisons. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2212-2220. [6] Alex Beutel, Paul Covington, Sagar Jain, Can Xu, Jia Li, Vince Gatto, and Ed H Chi. 2018. Latent cross: Making use of context in recurrent recommender systems. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. 46-54. [7] Alexey Borisov, Ilya Markov, Maarten De Rijke, and Pavel Serdyukov. 2016. A neural click model for web search. In Proceedings of the 25th International Conference on World Wide Web. 531-541. [8] Alexey Borisov, Martijn Wardenaar, Ilya Markov, and Maarten de Rijke. 2018. A click sequence model for web search. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. 45-54. [9] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning. 89-96. [10] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81. [11] Colin Camerer. 1999. Behavioral economics: Reunifying psychology and economics. Proceedings of the National Academy of Sciences 96, 19 (1999), 10575-10577. [12] Micah Carroll, Dylan Hadfield-Menell, Stuart Russell, and Anca Dragan. 2022. Estimating and Penalizing Induced Preference Shifts in Recommender Systems. arXiv preprint arXiv:2204.11966 (2022). [13] Allison JB Chaney, Brandon M Stewart, and Barbara E Engelhardt. 2018. How algorithmic confounding in recommendation systems increases homogeneity and decreases utility. In Proceedings of the 12th ACM conference on recommender systems. 224-232. [14] Gary Charness and Chetan Dave. 2017. Confirmation bias with motivated beliefs. Games and Economic Behavior 104 (2017), 1-23. [15] Jia Chen, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2019. TianGong-ST: A new dataset with large-scale refined real-world web search sessions. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 2485-2488. [16] Jia Chen, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. A context-aware click model for web search. In Proceedings of the 13th International Conference on Web Search and Data Mining. 88-96. [17] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for ecommerce recommendation in alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data. 1-4. [18] Ye Chen and John F Canny. 2011. Recommending ephemeral items at web scale. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. 1013-1022. [19] Yanhong Chen, Yaobin Lu, Bin Wang, and Zhao Pan. 2019. How do product recommendations affect impulse buying? An empirical study on WeChat social commerce. Information & Management 56, 2 (2019), 236-248. [20] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems. 7-10. [21] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259 (2014). [22] Caspar Chorus, Sander van Cranenburgh, and Thijs Dekker. 2014. Random regret minimization for consumer choice modeling: Assessment of empirical evidence. Journal of Business Research 67, 11 (2014), 2428-2436. [23] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. 2008. An experimental comparison of click position-bias models. In Proceedings of the 2008 international conference on web search and data mining. 87-94. [24] Xinyi Dai, Jianghao Lin, Weinan Zhang, Shuai Li, Weiwen Liu, Ruiming Tang, Xiuqiang He, Jianye Hao, Jun Wang, and Yong Yu. 2021. An Adversarial Imitation Click Model for Information Retrieval. In Proceedings of the Web Conference 2021. 1809-1820. [25] Abhinandan S Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google news personalization: scalable online collaborative filtering. In Proceedings of the 16th international conference on World Wide Web. 271-280. [26] Michael D Ekstrand, Mucun Tian, Ion Madrazo Azpiazu, Jennifer D Ekstrand, Oghenemaro Anuyah, David McNeill, and Maria Soledad Pera. 2018. All the cool kids, how do they fit in?: Popularity and demographic biases in recommender evaluation and effectiveness. In Conference on fairness, accountability and transparency. PMLR, 172-186. [27] Fenglei Fan, Jinjun Xiong, and Ge Wang. 2020. On interpretability of artificial neural networks. arXiv preprint arXiv:2001.02522 2, 10 (2020). [28] Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning. PMLR, 2052-2062. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 30 Z. Zhu et al. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 31 ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. 32 Z. Zhu et al. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023. Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems 33", "85-93.": "[102] Ziwei Zhu, Jianling Wang, and James Caverlee. 2020. Measuring and mitigating item under-recommendation bias in personalized ranking systems. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 449-458. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article . Publication date: December 2023."}
