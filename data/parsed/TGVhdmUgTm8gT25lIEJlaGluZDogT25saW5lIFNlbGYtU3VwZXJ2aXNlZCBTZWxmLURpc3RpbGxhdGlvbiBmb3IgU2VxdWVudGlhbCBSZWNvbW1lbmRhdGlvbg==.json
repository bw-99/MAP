{"Leave No One Behind: Online Self-Supervised Self-Distillation for Sequential Recommendation": "Shaowei Wei Ant Group Hangzhou, China weishaowei.wsw@antgroup.com Zhengwei Wu Ant Group Hangzhou, China zejun.wzw@antgroup.com Xin Li Ant Group Hangzhou, China lixin324625@antgroup.com Qintong Wu Ant Group Hangzhou, China qintong.wqt@antgroup.com Zhiqiang Zhang Ant Group Hangzhou, China lingyao.zzq@antgroup.com Lihong Gu Ant Group Hangzhou, China lihong.glh@antgroup.com Jun Zhou \u2217 Ant Group Hangzhou, China jun.zhoujun@antgroup.com Jinjie Gu Ant Group Hangzhou, China jinjie.gujj@antgroup.com", "ABSTRACT": "Sequential recommendation methods play a pivotal role in modern recommendation systems. A key challenge lies in accurately modeling user preferences in the face of data sparsity. To tackle this challenge, recent methods leverage contrastive learning (CL) to derive self-supervision signals by maximizing the mutual information of two augmented views of the original user behavior sequence. Despite their effectiveness, CL-based methods encounter a limitation in fully exploiting self-supervision signals for users with limited behavior data, as users with extensive behaviors naturally offer more information. To address this problem, we introduce a novel learning paradigm, named Online Self-Supervised Self-distillation for Sequential Recommendation ( \ud835\udc46 4 Rec), effectively bridging the gap between self-supervised learning and self-distillation methods. Specifically, we employ online clustering to proficiently group users by their distinct latent intents. Additionally, an adversarial learning strategy is utilized to ensure that the clustering procedure is not affected by the behavior length factor. Subsequently, we employ self-distillation to facilitate the transfer of knowledge from users with extensive behaviors (teachers) to users with limited behaviors (students). Experiments conducted on four real-world datasets validate the effectiveness of the proposed method.", "CCS CONCEPTS": "\u00b7 Computing methodologies \u2192 Learning paradigms ; Neural networks ; \u00b7 Information systems \u2192 Recommender systems . Conference'17, July 2017, Washington, DC, USA Sequence 20 10 10 =20 ~15 10 12 0.0 0.2 0.4 0.6 0.8 1.0 Head Rate Figure 1: Visualization of clustering for sequence granularity and cluster granularity on an amazon dataset. The horizontal and vertical axes of Fig.1(a) represent the two-dimensional spatial coordinates of the user sequence embedding vector using the t-SNE dimensionality reduction technique.", "ACMReference Format:": "Shaowei Wei, Zhengwei Wu, Xin Li, Qintong Wu, Zhiqiang Zhang, Jun Zhou, Lihong Gu, and Jinjie Gu. 2024. Leave No One Behind: Online SelfSupervised Self-Distillation for Sequential Recommendation. In Proceedings of the ACM Web Conference 2024 (WWW '24), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 7 pages. https://doi.org/XXXXXXX. XXXXXXX", "1 INTRODUCTION": "As an important recommendation paradigm, sequential recommendation has been playing a vital role in online platforms, e.g., Amazon and Alibaba. Generally, sequential recommendation takes a sequence of user-item interactions as the input and aims to predict the subsequent user-item interactions that may happen in the near future through modelling the complex sequential dependencies embedded in the sequence of historical interactions. Early works based on Markov Chains [7, 20] focus on modelling simple low-order sequential dependencies. Afterward, deep learning networks, such as recurrent neural networks (RNN) [8, 9], convolutional neural networks (CNN) [23, 29], and memory networks [10] have drawn attention for sequential recommendations due to the powerful non-linear expressive capacity. In addition, transformerbased [11, 21, 24] models have gained popularity for sequential recommendations. They can effectively learn users' preferences by estimating an importance weight for each item. Although these methods have achieved promising results, they usually only utilize the item prediction task to optimize a huge amounts of parameters, which suffers from data sparsity problem easily. To tackle the problem, inspired by the successes of self-supervised learning in computer vision (CV) [2] and natural language processing (NLP) [5], recent works attempt to use selfsupervised learning techniques to optimize the user representation model for improving sequential recommendation systems. These methods typically derive self-supervision signals through maximizing the mutual information of two augmented views of the original user behavior sequence. Despite their effectiveness, aforementioned methods fail to further extract supervision information across historical interactions. In practice, users consume each item based on their latent intents, which can be perceived as a subjective motive for their interaction. This motivates the exploration [15, 17] to extract shared underlying intents among users, which can be utilized to guide the recommendation system in providing more relevant recommendations. Since these methods require labels to model the user's intents, ICLRec [4] learns users' underlying intent distributions from all user interaction sequences via clustering. However, clustering algorithms typically involve operations over entire datasets, which can be computationally challenging and less efficient dealing with large-scale datasets. Furthermore, these methods also encounter a limitation in fully exploiting self-supervision signals for users with limited behavior data, as users with extensive behaviors naturally offer more information. As illustrated in Figure 1, the learned representations of users with extensive behaviors (long sequences) tend to be clustered by themselves which are relatively separated from users with limited behaviors (short sequences). However, the learned user representations should be affected by users' latent intents and unaffected by the observed sparsity of behavior sequence. Many studies point that uniform representation distribution is a crucial factor for the performance of contrastive learning methods [25, 28]. Previous CL-based and intention modelling methods fail to handle the distribution discrepancy between these two types of users, which hinders the sequence recommendation performance, especially for the users with limited behaviors. To address these problems, we introduce a novel learning paradigm, named Online Self-Supervised Self-distillation for Sequential Recommendation ( \ud835\udc46 4 Rec), effectively bridging the gap between self-supervised learning and self-distillation methods. Specifically, we employ online clustering to proficiently group users by their distinct latent intents. Additionally, an adversarial learning strategy is utilized to ensure that the clustering procedure is not affected by the behavior length factor. Subsequently, we employ self-distillation to facilitate the transfer of knowledge from users with extensive behaviors (teachers) to users with limited behaviors (students). The main contributions of this paper are summarized as follows: \u00b7 We propose a novel learning paradigm for sequential recommendation, which bridges the gap between self-supervised learning and self-distillation methods. To the best of our knowledge, this is the first work to apply self-distillation techniques to the sequential recommendation. \u00b7 We propose online clustering and adversarial learning modules to learn user representation clusters which are unaffected by the sparsity of behavior. Based on the learned clusters, the cluster-aware self-distillation module is employed to transfer knowledge from users with extensive behaviors to users with limited behaviors. \u00b7 Extensive experiments are conducted on four real-world datasets, which show the state-of-the-art performance of the proposed \ud835\udc46 4 Rec model.", "2 RELATED WORK": "", "2.1 Self-Supervised Learning for Recommendation": "Self-Supervised Learning (SSL) become prevalent in different research areas, including computer vision [2], natural language processing [5], and more. The main target of SSL is to capture highquality and information-rich representations through the feature itself. There have also been some recent works to apply SSL to sequential recommendations. For example, S 3 -Rec [30] adopts a pretraining and fine-tuning strategy with four self-supervised tasks, and first proposes to maximize the mutual information between historical items and their attributes. COTREC [26] introduces a graph-based recommendation model that utilizes the session-based graph to augment two views, exhibiting the internal and external connectivities of sessions, thereby supervising each other through contrastive learning. While DCN [14] broadens its scope by incorporating user sequence data on the item side and construct a dual contrastive learning network to enhance recommendation performance, our paper narrows its focus to user-side modeling. CL4SRec [27] introduces three data-level augmentation approaches (crop/mask/reorder, referred to as invasive augmentation methods in the paper) to structure positive views. Later, CoSeRec [15] aims to produce robust augmented sequences based on item denpendencies since random item perturbations may weaken the confidence of positive pairs. ICLRec [4] conducts clustering among all user behavior sequences to obtain user's intent, and optimizes sequential recommendation model by maximizing the mutual information between sequence and corresponding intentions.", "2.2 Intention Learning for Recommendation": "Manyapproaches have been proposed to study users' intents behind each user's behavior for improving recommendations [3, 17, 22]. DSSRec [17] introduces a sequence2sequence training strategy to capture extra supervision information. An intent variable is employed to extract mutual information between an individual user's past and future interaction sequences. ICLRec [4] learns users' intent distribution from unlabeled user behavior sequences and optimize SR models with contrastive learning by considering the obtained intents. ISRec [13] extracts intentions of target user from sequential contexts, then takes complex intent transition into account through the message-passing mechanism on intention graphs.", "2.3 Sequential Recommendation": "Sequential recommendation aims to learn users' interests and forecast the next items they would most like to interact with by modeling the sequences of their historical interactions. Early works based on Markov Chains [7, 20] focus on modeling simple low-order sequential dependencies. Afterward, deep learning networks, such as recurrent neural networks (RNN)[8, 9], convolutional neural networks (CNN)[23, 29] and memory networks [10] have drawn attention for sequential recommendations due to the powerful nonlinear expressive capacity. Recently, transformer-based [24] models have gained popularity for sequential recommendations. Typically, SASRec [11] uses self-attention mechanism to dynamically assign weights to each item. BERT4Rec [21] proposes a deep bidirectional transformer model to extract both left and right-side behaviors information. ASReP [16] further solves data sparsity problem by introducing a pretrained transformer on the revised interaction sequences to augment short sequences.", "3 PRELIMINARIES": "", "3.1 Problem Formulation": "We denote U and I as the user set and item set, respectively. For each user \ud835\udc62 \u2208 U , his/her chronological interaction sequence can be represented as S \ud835\udc62 = [ \ud835\udc60 1 \ud835\udc62 , ..., \ud835\udc60 \ud835\udc59 \ud835\udc62 , ..., \ud835\udc60 \ud835\udc3f \ud835\udc62 ] , where \ud835\udc60 \ud835\udc59 \ud835\udc62 denotes the \ud835\udc59 -th item that user \ud835\udc62 interacted and \ud835\udc3f is the maximum sequence length. The goal of sequential recommendation is to predict the next item \ud835\udc60 \ud835\udc3f + 1 \ud835\udc62 which the user \ud835\udc62 will most likely interact with given the behavior sequence S \ud835\udc62 . To this end, the classical objective function for SR is usually formalized as follows: where \ud835\udf03 is the parameters of a neural network \ud835\udc53 \ud835\udf03 that encodes sequential feature into latent vectors: z \ud835\udc62 = \ud835\udc53 \ud835\udf03 (S \ud835\udc62 ) . The probability \ud835\udc5d ( \ud835\udc60 \ud835\udc59 + 1 \ud835\udc62 | z \ud835\udc59 \ud835\udc62 ) is computed based on the similarity between the encoded sequential patterns z \ud835\udc59 \ud835\udc62 and the representation of the next item \ud835\udc60 \ud835\udc59 + 1 \ud835\udc62 . In serving stage, the items with the highest probability will be recommended to the user \ud835\udc62 .", "3.2 Sequence Augmentation Operators": "Given an original behavior sequence S \ud835\udc62 , several random sequencelevel augmentation strategies can be employed [15, 27]. Following previous works, we specifically apply Mask , Crop , Reorder and Insert . The operator definitions are detailedly listed in Appendix A.", "3.3 Latent Intent Modeling in SR": "Due to subjective reasons, while users face various items in a recommendation system, they may have multiple intentions (e.g., purchasing outdoor equipment, preparing for lectures, just killing time, etc.). The intent variable can be formed as \ud835\udf07 \u2208 R \ud835\udc3e \u00d7 \ud835\udc51 . Then the probability of a user interacting with a certain item can be rewritten as E \ud835\udf07 [ \ud835\udc5d ( \ud835\udc60 \ud835\udc59 + 1 \ud835\udc62 | z \ud835\udc59 \ud835\udc62 , \ud835\udf07 )] . As users intents are usually implicit, some work [4] attempts to infer this latent intents by unsupervised approach, such as clustering.", "4 METHODOLOGY": "", "4.1 Clustering On The Fly": "Previous work learns users' implicit intents based on user interaction data typically employ clustering methods, such as ICLRec[4]. It firstly encodes all the sequences {S \ud835\udc62 } | U| \ud835\udc62 = 1 by a sequence encoder \ud835\udc53 \ud835\udf03 . Subsequently, ICLRec executes \ud835\udc3e -means clustering over all the sequence representations { z \ud835\udc62 } | U| \ud835\udc62 = 1 to obtain cluster assignment P \u2208 R | U|\u00d7 \ud835\udc3e . However, one main issue of these clustering-based methods is that they do not scale well with the dataset as they require a pass over the entire dataset to capture cluster assignments that are used as targets during training. In addition, there is no correspondence between two consecutive cluster assignments. Hence, the final prediction layer learned from an assignment may become irrelevant for the following one and thus needs to be reinitialized from scratch at each epoch, which considerably disrupts the model training. In this work, we describe an alternative [1] to mapping sequence representations to prototype latent space on the fly in order to scale to large uncurated datasets, and thus retain correspondence. Firstly, the original interaction sequence is mapped into a vector representation by an encoder as following: where \ud835\udc53 \ud835\udf03 is an alternative sequence encoder, which is set as SASRec [11] in this paper.", "4.2 Cluster-aware Self-distillation": "Once the prototypes \ud835\udf41 and corresponding clustering assignments p \ud835\udc62 are obtained, they are employed to construct the supervisory signals for the self-supervision task. More precisely, we propose cluster-aware two-fold self-distillation (CSD) modules: a sequencelevel contrastive module and a cluster-level self-distillation module. Concretely, the sequence-level contrastive module maximizes mutual information among the positive augmentation pair of the sequence itself while promoting discrimination ability to the negatives. In parallel, the cluster-level self-distillation module aligns each user's behavior sequence to its corresponding intents consistently. The detail is described as follow. The distillation loss makes use of intent representation which provides additional supervision signals to the sequence embedding and endows the generalization ability to infer the next item.", "4.3 Head-tail Adversarial Learning": "Clustering can easily group long and short sequences into separate clusters, indicating that the clusters possess semantic information regarding the sequence sparsity. While the tail sequences are clustered together, the information beyond the length of the sequence within the cluster would be very sparse, thereby impacting the efficacy of distillation on the tail sequences. Taking inspiration from the advancement in generative models [6], we propose incorporating an additional adversarial task of headtail classification. This new task aims to promote a more uniform distribution of head and tail sequences across different clusters, ultimately boosting the overall recommendation performance. With respect to the classifier, the classification loss is minimized by finding the category of sequence embeddings. While for the recommendation model, the classification loss is reversed which pushes sequence embeddings of the same category far from each other and not to form clusters. Meanwhile, the main task of minimizing the recommendation loss forces the learned embedding space to retain interest preference semantics. With the help of adversarial learning, the impact of the head-tail property sequence would be eliminated to some extent.", "4.4 Multi-task Training": "Weadopt a multi-task strategy where the main next-item prediction, the cluster assignment, the cluster-aware self-distillation and the adversarial learning task are jointly optimized.", "5 EXPERIMENTS": "", "5.1 Experimental Settings": "5.1.1 Datasets. We conduct experiments on four widely used benchmark datasets with diverse distributions: Beauty , Sports and Toys are three subcategories constructed from Amazon review datasets [18]; ML-1M is a famous movie rating dataset comprising one million ratings. We pre-process these datasets in the same manner following [11, 15, 23] by removing items and users that occur less than five times. For each dataset, we manually select the longest 20% sequence as head sequences and the rest as tail sequences. Table 1 shows dataset statistics after pre-processing. 5.1.2 Evaluation Metrics. Following previous works [4, 15, 21], we adopt two metrics evaluating the performance of SR models: top\ud835\udc58 Hit Ratio@ \ud835\udc58 (HR@ \ud835\udc58 ) and top\ud835\udc58 Normalized Discounted Cumulative Gain (NDCG@ \ud835\udc58 ) with \ud835\udc58 chosen from { 5 , 20 } . For each user's behavior sequence, we reserve the last two items for validation and test, respectively, and use the rest to train SR models. 5.1.3 Baseline Models. We compare our proposed \ud835\udc46 4 Rec with three categories of methods: \u00b7 Non-sequential models. BPR-MF [19] leverages Bayesian inference to provide personalized ranking of items for users based on implicit feedback data. \u00b7 Standard sequential models. Caser [23] is a CNN-based approach, GRU4Rec [9] is an RNN-based method, and SASRec [11] is one of the state-of-the-art Transformer-based baselines for SR. They optimize the same objective but differ in sequence encoder structures. \u00b7 Sequential models considering SSL. BERT4Rec [21] proposes a deep bidirectional transformer model to extract both left and right-side behaviors information. S 3 -Rec [30] adopts a pre-training and fine-tuning strategy with four selfsupervised tasks. CL4SRec [27] introduces three data-level augmentation approaches to construct positive views. This line of works all utilize the transformer as sequence encoder but adopt distinct constrastive learning tasks. \u00b7 Sequential models with additional latent factors. DSSRec [17] introduce an intent variable to extract mutual information between an individual user's past and future interaction sequences. ICLRec [4] leverages the clustered latent intent factor and contrastive self-supervised learning to optimize SR. Implementation details are detailed in Appendix B.", "5.2 Overall Performance": "In Table 2, we present the consistent performance gain of the proposed \ud835\udc46 4 Rec against baselines on different datasets. The major results are summarized as follows:", "5.3 Ablation Study": "Our proposed method contains two essential modules: clusteraware two-fold self-distillation (CSD) module and Gradient Reversal (GR) layer as an adversarial learning module. To understand the impact of the sub-modules of \ud835\udc46 4 Rec, we conduct ablation study by removing different sub-module. The results reported in Table 3 are based on experiments conducted in the Amazon Beauty dataset. Similar results are also achieved in other datasets.", "5.4 Complexity Analysis": "For \ud835\udc46 4 Rec, the time complexity of its main task, clustering task, self distillation task, and adversarial task are O( \ud835\udc4e\ud835\udc3f 2 |U| \ud835\udc51 ) , O( \ud835\udc4e\ud835\udc3e |U| \ud835\udc51 ) , O( \ud835\udc4e\ud835\udc35 |U| \ud835\udc51 2 ) and O( \ud835\udc4e\ud835\udc36 |U| \ud835\udc51 ) , respectively, where a is the number of epoch. Since these tasks are jointly trained in one model, the overall complexity is dominated by the term O( \ud835\udc4e\ud835\udc3f 2 |U| \ud835\udc51 + \ud835\udc4e\ud835\udc35 |U| \ud835\udc51 2 ) . On the contrary, ICLRec suffers an extra O( \ud835\udc4e\ud835\udc4f |U| \ud835\udc3e\ud835\udc51 ) time complexity in clustering E-step, where b is the maximum iteration number. Training time difference is reported in Appendix C.", "5.5 Online A/B Testing": "To further strengthen our findings, we present the results of an online A/B testing conducted on a real-world recommendation system, affecting more than 23 million users from November 2, 2023, to November 6, 2023. On the contrary, due to the entire data iteration, traditional clustering will also occupy a large amount of memory footprints, which is difficult to be deployed for real recommendation scenarios with millions of users. Comprehensive evaluation is listed in Appendix D.", "6 CONCLUSION": "In this paper, we present \ud835\udc46 4 Rec, a practical attempt to address the sparsity problem of behavior for the sequential recommendation, which bridges the gap between self-supervised learning and selfdistillation methods. \ud835\udc46 4 Rec utilizes online clustering and adversarial learning modules to optimize user intention clusters unaffected by the sparsity of behavior. Subsequently, the sequence-level contrastive learning considers negative intents augments the representation express capability, while the cluster-aware self-distillation module transfers knowledge from users with extensive behaviors to users with limited behaviors. Experimental results on benchmark datasets confirm and validate the effectiveness of the proposal.", "REFERENCES": "[1] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. 2020. Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS 33 (2020), 9912-9924. [2] Wei-Chi Chen and Wei-Ta Chu. 2023. SSSD: Self-Supervised Self Distillation. In WACV . 2769-2776. [3] WanyuChen, Pengjie Ren, Fei Cai, Fei Sun, and Maarten de Rijke. 2020. Improving end-to-end sequential recommendations with intent-aware diversification. In CIKM . 175-184. [4] Yongjun Chen, Zhiwei Liu, Jia Li, Julian J. McAuley, and Caiming Xiong. 2022. Intent Contrastive Learning for Sequential Recommendation. In WWW . 21722182. [5] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In EMNLP . 6894-6910. [6] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David WardeFarley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In NeurIPS . 2672-2680. [7] Ruining He and Julian McAuley. 2016. Fusing similarity models with markov chains for sparse sequential recommendation. In ICDM . 191-200. [8] Bal\u00e1zs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks with top-k gains for session-based recommendations. In CIKM . 843-852. [9] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. ICLR (2015). [10] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y Chang. 2018. Improving sequential recommendation with knowledge-enhanced memory networks. In SIGIR . 505-514. [11] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In ICDM . IEEE, 197-206. [12] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [13] Haoyang Li, Xin Wang, Ziwei Zhang, Jianxin Ma, Peng Cui, and Wenwu Zhu. 2023. Intention-aware Sequential Recommendation with Structured Intent Transition : (Extended Abstract). In ICDE . 3759-3760. [14] Guanyu Lin, Chen Gao, Yinfeng Li, Yu Zheng, Zhiheng Li, Depeng Jin, and Yong Li. 2022. Dual Contrastive Network for Sequential Recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22) . Association for Computing Machinery, New York, NY, USA, 2686-2691. https://doi.org/10.1145/3477495.3531918 [15] Zhiwei Liu, Yongjun Chen, Jia Li, Philip S. Yu, Julian J. McAuley, and Caiming Xiong. 2021. Contrastive Self-supervised Sequential Recommendation with Robust Augmentation. CoRR abs/2108.06479 (2021). [16] Zhiwei Liu, Ziwei Fan, Yu Wang, and Philip S Yu. 2021. Augmenting sequential recommendation with pseudo-prior items via reversely pre-training transformer. In SIGIR . 1608-1612. [17] Jianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu. 2020. Disentangled Self-Supervision in Sequential Recommenders. In KDD . 483491. [18] Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-Based Recommendations on Styles and Substitutes. In SIGIR . 43-52. [19] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In UAI . 452461. [20] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized markov chains for next-basket recommendation. In WWW . 811-820. [21] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In CIKM . 1441-1450. [22] Qiaoyu Tan, Jianwei Zhang, Jiangchao Yao, Ninghao Liu, Jingren Zhou, Hongxia Yang, and Xia Hu. 2021. Sparse-interest network for sequential recommendation. In WSDM . 598-606. [23] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In WSDM . 565-573. [24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NeurIPS . 5998-6008. [25] Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML . 99299939. [26] Xin Xia, Hongzhi Yin, Junliang Yu, Yingxia Shao, and Lizhen Cui. 2021. SelfSupervised Graph Co-Training for Session-based Recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (CIKM '21) . Association for Computing Machinery, New York, NY, USA, 2180-2190. https://doi.org/10.1145/3459637.3482388 [27] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive Learning for Sequential Recommendation. In ICDE . 1259-1273. [28] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen. 2022. Are graph augmentations necessary? simple graph contrastive learning for recommendation. In SIGIR . 1294-1303. [29] Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M Jose, and Xiangnan He. 2019. A simple convolutional generative network for next item recommendation. In WSDM . 582-590. [30] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. In CIKM . 1893-1902.", "A SEQUENCE AUGMENTATION OPERATORS": "Sequence-level augmentations aim to create multiple related views from the original user behaviors. \ud835\udc46 4 -Rec adopts the following choices of augmentation operators: Mask. It randomly masks a proportion of items in an original sequence. This mask operation can be formulated as: where \u02c6 \ud835\udc60 \ud835\udc59 \ud835\udc62 represents the masked item if \ud835\udc60 \ud835\udc59 \ud835\udc62 is selected, otherwise \u02c6 \ud835\udc60 \ud835\udc59 \ud835\udc62 = \ud835\udc60 \ud835\udc59 \ud835\udc62 . Crop. It randomly removes a continuous sub-sequence from positions \ud835\udc59 to \ud835\udc59 + \ud835\udc59 \ud835\udc50 in S \ud835\udc62 . The length to crop is set by \ud835\udc59 \ud835\udc50 = \ud835\udeff \u2217 |S \ud835\udc62 | where empirically \ud835\udeff = 0 . 8. The formulation of the cropped sequence is shown below: Reorder. It randomly shuffles a continuous sub-sequence from positions \ud835\udc59 to \ud835\udc59 + \ud835\udc59 \ud835\udc50 in S \ud835\udc62 . The length to reorder is set by \ud835\udc59 \ud835\udc50 = \ud835\udeff \u2217|S \ud835\udc62 | where empirically \ud835\udeff = 0 . 2. The formulation of the reordered sequence is as: Insert. It inserts an item chosen randomly from the interaction histories of other users into a randomly selected position within S \ud835\udc62 . This operation is employed repeatedly on the sequence to obtain an augmented view. The augmented sequence could be formulated by:", "B IMPLEMENTATION DETAILS": "For BPR-MF and GRU4Rec, we use the source code provided by Wang et al. in PyTorch. For Caser, SASRec, BERT4Rec and \ud835\udc46 3 Rec, the source code is provided by Zhao et al. in PyTorch. For DSSRec ICLRec and CL4SRec, we use the source code provided by their authors. Our method is implemented in PyTorch as well. For all models, the dimension of embedding is set as 64, and the maximum sequence length is set as 50 for alignment, following previous works [11, 21, 30]. For each baseline model, all other hyperparameters are set following the suggestions from the original papers. For our proposed \ud835\udc46 4 Rec, the optimizer is Adam [12], learning rate is 0 . 001, batch size \ud835\udc35 is 512, dropout rate is 0 . 5, number of clusters \ud835\udc3e is 128, number of hidden layers is set from { 1 , 2 , 3 } . Multi-task objective weights \ud835\udefc, \ud835\udefd 1 , \ud835\udefd 2 , \ud835\udf06 \u2208 { 0 , 0 . 01 , 0 . 1 , 1 . 0 } . The temperature parameters \ud835\udf0f 1 , \ud835\udf0f 2 , \ud835\udf0f 3 are chosen from { 0 . 1 , 1 . 0 } .", "C TRAINING EFFICIENCY ON CLUSTERING ALGORITHM": "We have present the performance of replacing online clustering of S4Rec with K-means, i.e. SR+CSD+GR. Here we further report the training time result in Table 4. We conclude that K-means clustering algorithm suffers nonnegligible training time. On the contrary, online clustering algorithm provides better training efficiency with lossless performance.", "D A/B TESTING EVALUATION": "We present the results of an online A/B test conducted on a realworld recommendation system. Due to the excessive computational cost of K-means to over 10 million samples, we deploy another wellperforming CL4SRec as the A/B test baseline, instead of ICLRec. As demonstrated in the following table, our method consistently achieves stable and statistically significant improvements in the real-world large-scale recommendation scenario, our method can achieve statistically significant improvements stably."}
