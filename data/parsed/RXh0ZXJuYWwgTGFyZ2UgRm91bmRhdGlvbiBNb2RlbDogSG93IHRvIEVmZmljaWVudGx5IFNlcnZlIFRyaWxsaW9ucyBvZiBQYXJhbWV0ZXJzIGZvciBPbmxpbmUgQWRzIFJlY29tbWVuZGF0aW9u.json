{
  "External Large Foundation Model: How to Efficiently Serve Trillions of Parameters for Online Ads Recommendation": "Mingfu Liang 1 , ∗ , Xi Liu 1 , ∗ , Rong Jin 1 , Boyang Liu 1 , Qiuling Suo 1 , Qinghai Zhou 1 , Song Zhou 1 , Laming Chen 1 , HuaZheng 1 , Zhiyuan Li 1 , Shali Jiang 1 , Jiyan Yang 1 , Xiaozhen Xia 1 , Fan Yang 1 , Yasmine Badr 1 , Ellie Wen 1 , Shuyu Xu 1 , Hansey Chen 1 , Zhengyu Zhang 1 , Jade Nie 1 , Chunzhi Yang 1 , Zhichen Zeng 2 , Weilin Zhang 1 , Xingliang Huang 1 , Qianru Li 1 , Shiquan Wang 1 , Evelyn Lyu 1 , Wenjing Lu 1 , Rui Zhang 1 , Wenjun Wang 1 , Jason Rudy 1 , Mengyue Hang 1 , Kai Wang 1 , Yinbin Ma 1 , Shuaiwen Wang 1 , Sihan Zeng 1 , Tongyi Tang 1 , Xiaohan Wei 1 , Longhao Jin 1 , Jamey Zhang 1 , Marcus Chen 1 , Jiayi Xu 1 , Angie Huang 1 , Xihuan Zeng 1 , Chi Zhang 1 , Zhengli Zhao 1 , Jared Yang 1 , Qiang Jin 1 , Xian Chen 1 , Amit Anand Amlesahwaram 1 , Lexi Song 1 , Liang Luo 1 , Yuchen Hao 1 , Nan Xiao 1 , Yavuz Yetim 1 , Luoshang Pan 1 , Gaoxiang Liu 1 , Yuxi Hu 1 , Yuzhen Huang 1 , Jackie Xu 1 , Rich Zhu 1 , Xin Zhang 1 , Yiqun Liu 1 , Hang Yin 1 , Yuxin Chen 1 , Buyun Zhang 1 , Xiaoyi Liu 1 , Xingyuan Wang 1 , Wenguang Mao 1 , Zhijing Li 1 , Zhehui Zhou 1 , Feifan Gu 1 , Qin Huang 1 , Chonglin Sun 1 , Nancy Yu 1 , Shuo Gu 1 , Shupin Mao 1 , Benjamin Au 1 , Jingzheng Qin 1 , Peggy Yao 1 , Jae-Woo Choi 1 , Bin Gao 1 , Ernest Wang 1 , Lei Zhang 1 , Wen-Yen Chen 1 , Ted Lee 1 , Jay Zha 1 , Yi Meng 1 , Alex Gong 1 , Edison Gao 1 , Alireza Vahdatpour 1 , Yiping Han 1 , Yantao Yao 1 , Toshinari Kureha 1 , Shuo Chang 1 , Musharaf Sultan 1 , John Bocharov 1 , Sagar Chordia 1 , Xiaorui Gan 1 , Peng Sun 1 , Rocky Liu 1 , BoLong 1 , Wenlin Chen 1 , Santanu Kolay 1 , Huayu Li 1 1 AI at Meta, 2 University of Illinois Urbana-Champaign ∗ Both authors contributed equally to the paper Ads recommendation is a prominent service of online advertising systems and has been actively studied. Recent studies indicate that scaling-up and advanced design of the recommendation model can bring significant performance improvement. However, with a larger model scale, such prior studies have a significantly increasing gap from industry as they often neglect two fundamental challenges in industrial-scale applications. First, training and inference budgets are restricted for the model to be served, exceeding which may incur latency and impair user experience. Second, large-volume data arrive in a streaming mode with data distributions dynamically shifting, as new users/ads join and existing users/ads leave the system. We propose the External Large Foundation Model (ExFM) framework to address the overlooked challenges. Specifically, we develop external distillation and a data augmentation system (DAS) to control the computational cost of training/inference while maintaining high performance. We design the teacher in a way like a foundation model (FM) that can serve multiple students as vertical models (VMs) to amortize its building cost. We propose Auxiliary Head and Student Adapter to mitigate the data distribution gap between FM and VMs caused by the streaming data issue. Comprehensive experiments on internal industrial-scale applications and public datasets demonstrate significant performance gain by ExFM. Date: April 24, 2025",
  "1 Introduction": "Ads recommendation is an important service provided by online advertising systems, whose model performance can impact user experience. It has been actively studied to enhance model performance by advanced designs Zhu et al. (2024); Zhang et al. (2022); Geng et al. (2022); Wang et al. (2021); Li et al.; Ying et al. (2018) and scaling-up model complexity Zhang et al. (2024a); Pan et al. (2024); Anil et al. (2022), especially after observing the remarkable performance of trillion-parameter models such as GPT-4 Achiam et al. (2023), LLaMa Touvron et al. (2023), etc. However, prior studies often neglect two fundamental challenges in industrial-scale applications: 1 · C1: Restricted training and inference latency for the serving models. Industrial platforms usually have to recursively update the models, and evaluate the prediction score of O(100) ∼ O(100K) ads for each request within a restricted latency. Exceeding latency may degrade model performance and impair user experience. · C2: Large-volume streaming data arrive with data distributions dynamically shifting. This is largely due to the evolving nature of advertising systems: new users/ads join, and existing users/ads leave the system as time passes. This implies that multi-pass training would risk over-fitting and is the most salient difference between industry and academia. Figure 1 (a) Co-Distillation vs. External Distillation; (b) Model update under streaming data; (c) Our proposed ExFM framework that enabled trillions-parameter model serving with newly designed data augmentation system (DAS) and external distillation; The proposed auxiliary head and student adapter are developed to mitigate data distribution gap between FM and VMs. (a) Co-Distillation vs. External Distillation ... ... ... ... (c) ExFM Framework: One Iteration of ExFM (b) Model Recursive Update under Streaming Data Co-Distillation Teacher Student Data Teacher Teacher Teacher Data Student Teacher Training Logged Predictions Extemal Distillation Model (t) is obtained from training Model(t-1) with Data(t) ... Model (t-1) Model (t) Model (t+1) Data (t-1) Data (t) Data (t+1) Streaming Data VM1 Data VM2 Data VMn Data ... Data Aggregation FM Data FM Model FM Model VM1 Data VM2 Data VMn Data Data Aggregation FM to VM1 Predictions Data Augmentation Service FM to VM2 Predictions FM to VMn Predictions VM1 Data VM2 Data VMn Data VMn Model VM2 Model VM1 Model VM1 Model Auxiliary Head and Student Adapter VM2 Model VMn Model ... ... ... Teacher Data Teacher Data To overcome the restricted inference latency challenge in C1, multiple prior studies employ knowledge distillation (KD) Liu et al. (2022); Kang et al. (2021); Kweon et al. (2021); Zhu et al. (2020). As shown by Figure 1(a), a large teacher model is co-trained with a compact student model to be used for serving. The teacher's knowledge is continuously distilled into the student. Unfortunately, the co-training will increase the training computational cost of the serving model, which cannot meet the industrial restriction on the training latency of the serving model. Besides, ads recommendation often involves multiple serving models, each corresponding to one specific service or ranking stage (e.g., early or later/ranking stage). Building and maintaining a dedicated large teacher model for each serving model is too inefficient and non-scalable. To sum up, the ideal teacher model (1) is separately trained with the student model, i.e., external distillation, and (2) is 1-to-N, able to benefit multiple student models, like a Foundation Model (FM). The streaming and non-stationary nature of recommendation data in C2 makes it challenging to realize the ideal teacher model. As shown by Figure 1(b), models need to be recursively trained to capture the up-to-date distribution shifting. Figure 2 illustrates an example of performance degradation under model staleness over internal datasets, where the model is trained with 74-billion examples and stops training with new examples since inference. Compared to the baseline being trained with new examples daily, its inference NE (normalized entropy) He et al. (2014) loss keeps enlarging as the delay inclines. Besides, since the teacher aggregates various students' training data from different services, the distribution of the teacher model's data will be more sophisticated. Those factors lead to two potential distribution gaps between the teacher model and the student models: (1) Cross-domain Bias. For an individual student, the teacher may carry bias due to being trained by not only that student's data but an aggregation of all students' data under distribution shifting. (2) Freshness Gap. The teacher predictions used in external distillation are from a teacher with a slight delay w.r.t. training data compared to the student under training. In light of the above limitations, we propose the External Large Foundation Model (ExFM) framework to address the challenges overlooked by prior studies. Figure 1(c) presents a big picture of the ExFM framework. To avoid additional training or inference computational costs on the serving model, ExFM employs external distillation where the teacher model is separately trained, and the teacher's predictions are offline logged as external supervision for the student training. To amortize the resources needed for training and maintenance of the teacher model, ExFM aggregates the data from multiple student traffic as the training data of the 2 teacher, like a 1-to-N Foundation Model that can serve multiple students as the vertical models (VMs), i.e. production model. To alleviate the Cross-domain Bias transferred from FM to VMs, ExFM proposes Auxiliary Head (AH) that uses an isolated task arch to receive the FM supervision instead of direct on serving head. The simple change is found effective both provably and empirically. To mitigate the Freshness Gap between FM and VMs, ExFM proposes Student Adapter (SA) that learns to transform FM predictions before being used as VMs' supervision. The main contributions of the paper are summarized as follows · We re-define the ads recommendation problem to be explored under two fundamental challenges from industrial-scale applications overlooked by prior studies: (1) restricted training and inference budget for serving model, and (2) streaming data under continuously shifting distributions. · We propose ExFM framework to overcome above challenges, where (1) the teacher is an FM to cover multiple VMs to amortize the maintaining cost; (2) external distillation (Sec. 3.2), not co-distillation, is designed with the system to avoid training or inference cost to models to be served (i.e., VM); (3) Auxiliary Head (Sec. 3.3), i.e., an isolated task instead of VM's serving head, is used to process FM' supervision, provably alleviating the bias transfer from FM to VMs, (4) Student Adapter (Sec. 3.4) with specific learning algorithms is proposed to reduce the staleness gap between FM and VMs, with mathematical guarantee. · We conducted extensive experiments (Sec. 4) on both internal industrial-scale datasets and public datasets and demonstrated that ExFM can enable efficient serving of a trillion-parameter model. We observed promising performance enhancement by employing ExFM on VMs from different domains/tasks/stages. We also provide ablation studies to reveal the impact of hyper-parameters.",
  "2 Problem Formulation": "We re-define the ads recommendation problem to match industrial-scale applications with two overlooked fundamental challenges: · Restricted training and inference latency for serving models. Ideally, the model enhancement should not incur any training or inference computational costs on serving models. · Training and inference data are large volumes of streaming data with continuously shifting data distribution. Models have to keep training on fresh examples to perform well. The new ads recommendation problem is to predict user interest in ads based on their interaction data with dynamic data distribution shift constrained by restricted training and inference latency . Formally, let Θ F ( t ) be the FM parameters after training Θ F ( t -1) on FM's new data D F ( t ) at time t , and Θ V i ( t ) be VM i 's parameters after training Θ V i ( t -1) on VM's new data D V i ( t ) , i = 1 , . . . , n .",
  "Inference NE Loss %": "1.5 1.23 1.14 0.98 0.92 0.77 0.64 0.5 0.37 0.0 2 3 5 Num Days Delay between Training and Infernce Figure 2 Click Through Rate (CTR) prediction on the internal dataset. The bigger the staleness, the larger NE Loss. 3",
  "3 Methods": "",
  "3.1 Overview": "Figure 1(c) illustrates the details of the ExFM framework with one model iteration as an example. Specifically, FM and VMs are recursively trained on streaming data D F ( t ) and { D V i ( t ) } n i =1 incrementally arrived at time t . To amortize the building and maintenance costs, the teacher model is built as an FM that can make prediction on multiple VMs' traffic. Correspondingly, the training data of FM D F ( t -1) is obtained by aggregating the cross-traffic VM's training data { D V i ( t -1) } n i =1 . After training FM model Θ F ( t -2) on D F ( t -1) , FM model parameters are updated to Θ F ( t -1) . To not incur additional training or inference costs to VMs, external distillation, as shown in Figure 1(a), is employed. Specifically, Teacher/FM is trained on D F ( t -1) separately from Students/VMs. After training, Teacher/FM inferences on Students'/VMs' training data { D V i ( t ) } n i =1 to generate predictions as external supervision to Students/VMs. The external supervision, along with the VMs' training data { D V i ( t ) } n i =1 will be used to train the Students/VMs' models { Θ V i ( t -1) } n i =1 , updating them to { Θ V i ( t ) } n i =1 . The model snapshots of Students/VMs { Θ V i ( t ) } are used to serve traffic for the next period [ t, t +1) . FM's supervision logging and the merging with Students/VMs training data are achieved by Data Augmentation Service (DAS), elaborated in Section 3.2. Since FM's training data D F ( t -1) is an aggregation of VMs' data { D V i ( t ) } n i =1 from multiple traffics, for one specific traffic, FM supervision readily carries cross-domain bias. Multiple prior studies handle this problem by carefully designing the distillation losses Gou et al. (2021). Motivated by that, we propose a flexible solution that can cover different proposals from prior studies, called Auxiliary Head, which can provably reduce the bias transfer from FM supervision to VM and significantly enhance FM's benefits on VM in experiments. Details are provided in Section 3.3. Also note that there is always a delay between FM training and FM inference used in VMs' training: FM Θ F ( t -1) inferences on data { D V i ( t ) } n i =1 to update VMs to { Θ V i ( t ) } n i =1 . Due to the streaming and distribution shifting nature of the industry data, this delay will create a Freshness Gap between the FM's supervision and the VMs' training and impair the beneficial impact of FM. Student Adapter is proposed to alleviate the gap and elaborated in Section 3.4. Figure 3 Data Augmentation Service (DAS) (b) DAS FM snapshot maintaining and update Features Vectors True Labels ... (a) DAS role in dataset generation Shared Dataset Generator Features Vectors FM predictions DAS Features Vectors FM predictions FM Model Shared Dataset VM Datasets Generator VM2 Dataset VM1 Dataset VMn Dataset Read Read Latest Snapshot (Zeus) Update Update Snapshot Publishing Database (SPD) Read Read Loader Updater Loader Updater",
  "3.2 Data Augmentation Service (DAS)": "Data Augmentation Service (DAS) logs FM's supervision on the fly when preparing training data for VMs, fulfilled in two stages. As shown by Figure 3(a), first, all VMs' features and true labels are joined together into a shared dataset. There is a large time window to wait for the feedback (i.e., true label) for this training example (e.g., 5 to 90 minutes for CTR feedback, or 1 day for CVR feedback). During this time window, DAS will call FM to get FM's inference on VM's features, which usually has a large latency budget (in seconds level) sufficient to evaluate a large-scale FM. Then each VM picks its own datasets from the shared dataset for training. This ensures that the inference of FM only needs to cover a tiny percentage of ranked ads in training data, and VMs with overlapped traffic can further amortize the inference cost of the FM. 4 As mentioned earlier, models need to be recursively trained to capture the distribution shifting of streaming data. Consequently, FM publishes model snapshots regularly. DAS has to actively detect if there is a new snapshot and switch to it if so. In practice, it may take several minutes to switch to a snapshot, as loading also takes time. Before completeness, we want the old snapshot to still maintain high availability for FM supervision generation. To avoid wasting resources, we also want the new snapshot to be loaded only once. To this end, DAS separates the FM model snapshot publishing and the latest model snapshot database. As shown by Figure 3(b), a snapshot is regularly published in the Snapshot Publishing Database (SPD), while the latest snapshot to be loaded for generating supervision is maintained in Zeus, a distributed metadata store based on Apache Zookeeper Apache (2023) with strong consistency. Zeus maintains the snapshot with a lease to do automatic garbage collection of old snapshots. The existing lease will be extended automatically at a fixed frequency until a new snapshot is discovered. For DAS tasks, Zeus is readable and writable, while SPD is only readable. The updater of the DAS task regularly queries SPD to detect if there is a new snapshot. If a new snapshot is discovered, the updater will read it from SPD and write it into Zeus. The loader of the DAS task is responsible for loading the latest snapshot to generate supervision. DAS treats all tasks equally instead of having a special primary task to be scalable to distributed settings.",
  "3.3 Auxiliary Head (AH)": "Figure 4 Auxiliary Head (AH) and Student Adapter (SA). SG means to stop the gradient from the output of SA. (b) Training VM with Student Adapter (a) Training of VM Serving Head Distillation Head Backbone Label Scaling Gradient Scaling Auxiliary Head Serving Head Distillation Head Backbone Distillation Head Student Adapter :Stop Gradient The baseline of knowledge distillation is through label smoothing of the ground-truth label y and the pseudolabel ˆ y F from FM. The model architecture of VM consists of two parts: the backbone and a serving head. We define the output vector of the backbone as x , and x will be sent to the serving head. The output vector of the serving head is defined as ˆ y S = ϕ ( x ) , where ϕ denotes the multi-layer perceptron (MLP). The training objective is binary cross-entropy and can be generally defined as  where σ ( x ) = 1 / (1 + exp ( -x )) denotes the Sigmoid activation function and ˆ y denotes the prediction. Accordingly, the training loss of VM with knowledge distillation is defined as  where ˆ y S denotes the output of the serving head. The gradients with respect to y and ˆ y F are entangled in the serving head and flowing back to the backbone. Using a single head to consume both labels may induce bias from FM to VM. Therefore, we propose the Auxiliary Head (AH) that leverages a separate head to consume the pseudo-label from FM. In this case, the model architecture of VM consists of the backbone, the serving head, and a distillation head as AH. The output of the backbone x will be sent to different heads, and the output of AH is from the distillation head ˆ y D , where ˆ y D is defined as ˆ y D = ψ ( x ) . ψ represents an additional MLP head which is built on top of the 5 backbone. By disentangling the serving head and distillation loss, the serving head is only supervised by the ground-truth label y as  The distillation head is supervised by the pseudo-label ˆ y F as  We further provide the theoretical insight that, compared to adding the auxiliary loss to the serving task for distillation, we prove that integrating the auxiliary head for knowledge distillation (KD) can alleviate the bias transfer from FM to VMs. Theorem 3.1. (Informal) The VM will contain bias from the FM when the VM is trained with KD by a single serving head. In contrast, the VM is guaranteed to find the optimal solution for predicting y gt with KD by auxiliary heads. Remark. Calibration is one of the most critical metric in ads recommendation system. When we perform knowledge distillation with single serving head, the introduced bias will lead to mis-calibration issue, while separate head can guarantee the calibration will not be effected. More specifically, if the teacher model is mis-calibrated, the bias transfer from the teacher model from a single serving head can lead to mis-calibration in the student model with high probability. In contrast, distillation with an auxiliary head mitigates this issue by reducing the propagation of teacher model's bias through the separated serving head. The proof of Theorem 3.1 in the Appendix Sec. A.2, where we prove by construction using a two-layer linear model to show that (1) the bias in the pseudo labels from the FM is partially compensated by the separated prediction head for KD, and (2) fewer 'biased' gradients will flow from the prediction head for KD to the shared backbone, leading to a better-generalized performance than the single serving head for KD. However, only isolating the supervision from ˆ y F to y is not enough to bring effective knowledge transfer in practice. This is due to the issues originating from the intrinsic properties of the training data of Ads engagement, i.e., the majority of engagement data does not convert to actions like clicking; thus, the FM primarily predicts majority of ˆ y F close to zero, making the KD task fitting to a difficult long-tailed distribution. These practical issues hinder the effectiveness of the KD with AH. To remedy the above issue, we propose amplifying the FM's distillation effect by Gradient Scaling (GS), Label Scaling (LS), and Loss Weighting (LW) . The gradient scaling scales the gradient of the distillation head to backbone with a hyperparameter β during training to enlarge the impact of the distillation from the FM. For label scaling, we multiply the ˆ y F with a constant α to enlarge its magnitude and clip the α · ˆ y F correspondingly to avoid the scaled label being out-of-bound, e.g., α · ˆ y FM needs to be smaller or equal to 1. Lastly, we apply the loss weighting to L d with a hyperparameter w . Accordingly, the final training loss L ah for VM with AH is:",
  "3.4 Student Adapter (SA)": "To further alleviate the bias from the ˆ y F , we propose to provide additional distillation using the adapted ˆ y F based on y , called Student Adapter (SA). The output of SA is defined as ˆ y SA = MLP (ˆ y F ) . As shown in Figure 4 (b), SA is trained by the loss  When training VM with SA, we first optimize L sta (ˆ y SA , y ) to update SA and get ˆ y SA . To use ˆ y SA for training VM, we stop the gradient of ˆ y SA , i.e., SG (ˆ y SA ) , to avoid the gradient of VM flowing back to SA, and we consume SG (ˆ y SA ) with another distillation head to update VM with the following loss  The algorithm is detailed in Algorithm 1. 6 We provide theoretical insight that SA can mitigate the freshness gap between FM and VM by considering the standard regression problem for analysis. The proof is provided in Appendix A.3. Theorem 3.2. With a probability 1 -δ , the underlying regression model w and the optimal solution w V achieved by training VM with Student Adapter (SA) is bounded as:  where N denotes the number of training samples, d denotes the input dimension, γ is a constant, s denotes the model dimension. Compared to the bound for the standard regression model when training with KD, the bound is reduced by the factor ( d/s ) 1 / 4 , implying that if the distribution is changing on the time scale between d and √ ds , the student adapter can quickly catch up the drift.",
  "Algorithm 1 Train VM with Student Adapter for one iteration": "Input: Pseudo-label ˆ y F from FM, ground-truth label y , learnable model parameter Θ SA of the Student Adapter SA, learnable parameters Θ V of the VM. Output: Model parameter of the Student Adapter Θ SA , and the model parameter of VM Θ V 1: Optimize L sta (ˆ y SA , y ) (Eqn. 6) to update Θ SA 3: Optimize L ah ( Eqn. 5) + L sa (Eqn. 7) to update Θ V . 2: Get SG (ˆ y SA ) by stopping gradient of the output for SA 4: Return Θ SA and Θ V Table 1 [Public] Dataset Statistics",
  "4 Experiments": "We evaluate the proposed ExFM on both internal industrial-scale and public datasets to answer the following research questions: · Q1: How effective is the proposed ExFM in elevating the performance of a single VM and multiple VMs, e.g., from different tasks, domains or stages? (Sec. 4.2) · Q2: How effective are the proposed Auxiliary Head and Student Adapter? (Sec. 4.3) · Q3: How do the values of hyperparameters impact the ExFM performance? (Sec. 4.4) For better readability, each part of discussion will first cover internal datasets, then public datasets. The caption of each figure and table starts from [Internal] if obtained on internal datasets and [Public] on public datasets.",
  "4.1 Experiment Setup": "",
  "4.1.1 Datasets": "We conduct comprehensive experiments on internal industrial-scale datasets with billions of training examples, from ads CTR prediction tasks. Each training example contains O(1k) features for a user and ad pair, and various types of feedback such as click, post-click conversion, etc. Training examples of individual VMs come 7 from the corresponding services or stages and may contain service-dedicated features and feedback. Training examples of the FM aggregate the training examples of VMs. Billions of training examples arrive daily for both VMs and FM. All training is one-pass, and training batches are ordered by arrival time. For public datasets, we use TaobaoAd Tianchi, Amazon Electronics He and McAuley (2016) and KuaiVideo Kua as they are widely adopted and have the timestamp feature that facilitates our experiments for streaming setting. The dataset statistics are briefly summarized in Table 1. For the streaming setting, we split each dataset into 8 splits (days) and divide them into the FM train set, VM train set, and VM test set based on the UNIX timestamps. For example, FM is trained on the first 4 days' data and inference on 5th day's data to generate supervision. VM is trained on the 5th day with FM's supervision and inference on the 6th day to get the AUC/LogsLoss reading. We can repeat this process for 3 times until Day 8. On TaobaoAd and Amazon Electronics, we train FM and VMs on the CTR prediction task. To validate FM's impact on different-domain VMs, we split TaobaoAd dataset by the value of the categorical feature 'new_user_class_level'. The original 'new_user_class_level' feature has five levels; we combine them into three levels as domains to ensure that each domain has almost the same training data size. To experiment the FM's impact on different-task VMs, we use KuaiVideo Kua due to its various feedbacks. We use prediction of 'Click,' 'Follow,' and 'Like,' as three different tasks, each of which has a dedicated VM to handle. Table 2 [Public] ExFM performance under different FM/VM, AH/SA and datasets choices. We choose DMIN Xiao et al. (2020), DCN Wang et al. (2017), and DCNv2 Wang et al. (2021) as FM, and FaM Rendle (2010), FmFaM Sun et al. (2021) and DeepFaM Guo et al. (2017) as VM.",
  "4.1.2 Metrics": "We adopt three widely-used evaluation metrics: · NE (normalized entropy) He et al. (2014), is the LogLoss in Eqn. 1 normalized by the entropy of the average empirical CTR of the training dataset, providing a data-insensitive evaluation. Lower the better, and an NE gain of 0.02% is considered significant for experiments on internal datasets. · AUC is an aggregate measure of model performance in correctly classifying positive and negative samples across all thresholds, used for public datasets. Higher the better. · LogLoss (cross-entropy loss) measures the distance between model prediction ˆ y and actual label y by Eqn. 1, which will be used for public datasets. Lower the better.",
  "4.1.3 Model Arch": "ExFM is model-agnostic and our purpose is to show the delta impact of FM on VM. In our problem setting, FM is assumed to have larger capacity and demand more computation resource for training and inference. To be consistent, FM we experimented with on internal datasets is often 15 to 600 times larger than the VM. FM enjoys more advanced and resource intensive model architecture such as an internal version of Interformer Zeng 8 et al. (2024), SUMZhang et al. (2024b), Wukong Zhang et al. (2024a), DHEN Zhang et al. (2022), etc., while VM adopts a shrunk version of FM or less resource intensive structure such as DLRM Naumov et al. (2019). On public datasets, we use the models that attain SOTA performance on TaobaoAd, Amazon Electronics, and KuaiVideos as FMs based on BARS Zhu et al. (2022) benchmark. Specifically, we use DMIN Xiao et al. (2020), DCN Wang et al. (2017), and DCNv2 Wang et al. (2021) as different variants of FM. Regarding VMs, Factorization Machine (FaM) Rendle (2010) and its two variants, i.e., FmFaM Sun et al. (2021) and DeepFaM Guo et al. (2017) are considered, as they are less complex and low latency to SOTA. For instance, the parameter of FaM is 4.23M on Amazon Electronics while DMIN is 5.94M, with a near 30% parameter decrease. Our purpose is to study the delta impact of FM on VMs under different choices of the two. More details of the models and training settings are in Appendix A.1. Table 3 [Public] ExFM performance under different FM/VM, AH/SA and datasets choices. We choose DMIN Xiao et al. (2020), DCN Wang et al. (2017), and DCNv2 Wang et al. (2021) as FM, and FaM Rendle (2010), FmFaM Sun et al. (2021) and DeepFaM Guo et al. (2017) as VM.",
  "4.2 Effectiveness of FM on VMs": "Figure 5 [Internal] Inference NE gain of different-size FM on VM iterations ( 1 X = 60 M training FLOPs, 1T is 1 Trillion) NE Gain % 1.5 FM on VM cum gain 24H2 FM+VM Trendline VM alone cum 0.58 VM alone Trendline 1.0 24H1 23H2 0.4 0.72 0.3 0.5 23H1 0.45 0.15 0.35 FM FLOPs, 0.2 Param Num 0.0 330X, 1.1T gain 9 Figure 6 [Internal] Inference NE gain of 1000X, 3.2T FM on cross-stage VMs.",
  "NE Gain % Contributed by FM": "0.25 0.20 0.15 0.10 0.05 0.00 Retrieval Stage VM: Early Stage VM Later Stage VM: O(1OOK) O(IK) O(IK) = 0(100) 0(100) 0(10)",
  "4.2.1 OneFMforOneVM": "On internal datasets, we evaluated the impact of different-sized FMs on elevating VM performance. As shown by Figure 5, FM model size ranges from 30X to 1800X where 1 X = 60 M training FLOPs. FM model parameter number ranges from 0.8T to 3.2T where 1T is 1 Trillion. FM scale-up is employing advanced model arches with more features and larger embedding dimensions. VMs are all 30X and from past halves' real iterations, e.g., 23H1 implies the iteration of the first half in 2023. VMs are iterated by refreshing features and adopting micro model arch changes. The baseline of both FMs and VMs is the VM at 22H2. NE Gain % refers to the percentage of NE improvement attained vs. baseline. 'VM alone' NE Gain % means the NE improvement of VM obtained without FM. Cumulative gain means the number includes the gain since 23H1. Both FM and VMs are trained with > 300 B streaming data and inference on the next-day's new arriving data. Although both VMs and FM are changing from half to half, the value of FM on VM gain is obtained by apple-to-apple comparison. We observe that (1) FMs can generate consistent NE gains on VMs from half to half, (2) compared to the trend of the world without FM (blue trendline), the one with FM has steeper slope (red trendline), implying the role of FM on bending the trending curve. On public datasets, we evaluated the impact of FM on VMs under different FM-VM pairs in Table 2 and 3. Data is described in Sec. 4.1.1 and VM is trained on the 5th day with FM's supervision and inference on the 6th day to get the AUC/LogsLoss reading. Note that the original performance of VMs without FM is limited, because the VMs' model is chosen not to have any SOTA and complex model architectures for the consistency with the problem settings. In the table, 'distill w/ L ah + L sa ' means w/ FM, and 'w/o distill' means w/o FM. From the table, we observe consistent AUC and Logloss improvement from having FM across different FM-VM choices, demonstrating that the effectiveness of ExFM can hold in general.",
  "4.2.2 OneFMfor N VMs": "Table 4 [Public] Impact of FM to cross-domain VMs Table 5 [Public] Impact of FM to cross-task VMs We evaluate the impact of one FM on multiple VMs across multi-stages in the ads system, including Retrieval, 10 Early, and Later stages, each of which reduces ads candidates, from O(100k) to O(1k), from O(1k) to O(100), and from O(100) to O(10), respectively. Specicially, we use the 1000X, 3.2T FM in Figure 5 and VMs from three stages for experiments on internal dataset. Retrieval VM and Early stage VM employ the Two-Tower Sparse Network (TTSN) Wang et al. (2019), and Later stage VM uses DLRM Naumov et al. (2019) as the corresponding VM in Figure 5. Training and inference data preparation also follow those of Figure 5. In Figure 6, we observe that the FM brings 0.11% to 0.25% additional NE gain to VMs of different stages. The earlier a stage is, the more gains the FM generates. To verify the benefits of FM to multiple VMs from different domains, we split the TaobaoAd data into different domains as described in Sec. 4.1.1. To verify the same thing for different tasks, we use the KuaiVideo dataset where multiple types of feedback are available for prediction. We use the hard-parameter shared DMIN Navon (2024) as the FM and FaM Rendle (2010) dedicatedly trained from scratch for each domain or task as the VM. Table 4 and Table 5 illustrate the results for different domains and tasks separately. In both tables, we observe that w/ FM outperforms the w/o FM in all tasks and domains, demonstrating the effectiveness of FM for multiple VMs. NE Change % Number of Streaming Training Examples Figure 7 [Internal] NE change of AH with 1000X 3.2T FM NE Change % Number of Streaming Training Examples Figure 8 [Internal] NE change of SA with 1800X, 2.2T FM",
  "4.3 Effectiveness of of AH and SA": "To understand the role of AH in the success of ExFM, on internal datasets, we compare the VM's NE after including AH (Eqn. 5) vs. w/o AH (Eqn. 2) under the same 1000X, 3.2T FM and corresponding VM from Figure 5. Figure 7 provides the comparison, where the x-axis is the number of streaming training examples and the value of NE Change at each point is obtained by using the current snapshot to inference on next-batch data, thus equivalent to the inference NE, but in a streaming mode. Since NE is the lower the better, the negative 'NE Change %' (-4%) implies that AH brings a large and stable performance gain compared to baseline. Similarly, to understand the role of SA, we compare the VM's NE after including SA vs. w/o SA under the same 1800X, 2.2T FM, and corresponding VM from Figure 5. Figure 8 shows the comparison, and we observe that with SA, the NE performance of VM on streaming data improves by 0.08%, with an enlarging trend, which is considered significant. To reproduce the results on public datasets, we split the dataset as discussed in Sec. 4.1.1 by timestamps to simulate the streaming setting. We apply AH and SA over different pairs of public models on public datasets as described in Section 4.1.3, and the performance of VMs is reported in Table 2 and 3. Specifically · distill with L kd (Eqn. 2), i.e., the distillation loss · distill with L ah (Eqn. 5), i.e., the AH · distill with L ah + L sa (Eqn. 7), i.e., AH and SA. We observe that the AH and SA are consistently superior to the baseline with L kd among different pairs of FMs and VMs on various datasets, demonstrating their strong effectiveness in enhancing FM benefits on VMs. As the breakdown, it is found that (1) the vanilla distillation w/o AH or SA has very small benefits, e.g., 11 0.35% AUC gain for DMIN as FM and FaM as VM on TaobaoAd, (2) the major leap is by AH, e.g., boosting the AUC gain from 0.35% to 1.11%, and (3) SA further brings significant gains on top of AH, e.g., lifting the AUC gan from 1.11% to 1.35%. Figure 9 [Public] Impact of SA under FM staleness. 70 1.75 1.75 1.50 1.39 1.35 1.31 @ 1.25 15 1.00 0.74 9 0.75 0 AH wlo Update FM 0.50 AH+SA wlo Update FM 0.40 0.25 0.25 AH+SA w/ Update FM 6 8 Day Day Day Day The data points in Table 2 and 3 are obtained by one-time inference on the 6th day's data. We are curious about the performance of SA with a bigger freshness gap between FM and VM. To study this, we use DMIN as FM and FaM as VM, stop the FM from being trained on the new day's data, and compare their performance w/ and w/o SA in Figure 9. We observe that (1) the benefits of FM on VM diminish as the time that FM stops training on new data is longer, (2) SA can generate additional gain no matter whether FM updates or not, but (3) SA cannot revert the diminishing trend when the FM is not updated with new day's data.",
  "4.4 Impact of hyper-parameters": "LW Impact LS Impact 0.20 Base (1) Base (1) Tuned 0.16 Tuned (5 0.15 0.15 8 0.15 0.10 0.10 E 0.05 E 0.05 0.04 0.00 0.00 Figure 10 [Internal] Impact of LW and LS on VM's NE Impact of Gradient Scale on Performance 0.4 36 35 30 0.3 Plateau 0.2 2 0.10 0.1 Rapid Growth Gradient Scale 100 Figure 11 [Internal] Impact of GS on VM's NE We notice that the benefits of FM on VM may be sensitive to the choice of GS, LS, and LW as defined in Eqn. 5. On internal datasets, Figure 10 demonstrates the positive impact of increasing LW and LS. We observed that the benefits of FM on VM are improved correspondingly when we increase the value of LW and LS with others fixed. Figure 11 shows the impact of GS, where there seems to be a sweet spot. The NE gain first increases significantly when GS increases and then tends to plateau when GS is large enough. On the internal dataset, the test for the combination effect of LW, LS, and GS is often restricted by the limited training resources. We explore that on the public dataset. Figure 12 illustrates our experiments on TaobaoAd: (1) The simple combination, i.e., ( LS = 1 , GS = 1 , LW = 1) , can only achieve moderate improvement on the VM. (2) Aggressive scaling, e.g., ( LS = 10 , GS = 10 , LW = 10) or ( LS = 1 , GS = 1 , LW = 100) , will even be detrimental to the VM's performance. Both observations imply that an appropriate combination of these 12 three scaling techniques could yield significant accuracy improvement and it is important to identify a proper combination for practical usage. Figure 12 [Public] Joint impact of different factors, i.e., LW, LS, and GS on VM performance. Impact on AUC Impact on LogLoss 0.76 0.8, 0.70 0.48 0.62 0.5 0.6 0.08 0.4 0.20 0.0 0.0 1 =1.0 LS=10, GS=1 LS=10, GS=10 =0.6 LS=1, GS=1 ~0.8 -1.44 LS=1, GS=10 LW=1 LW=10 LW=100 LW=l LW=10 LW=100 9.51",
  "5 Related Works": "Inspired by the success of large-scale model in NLP Achiam et al. (2023); Touvron et al. (2023), more and more recent studies have focused on scaling-up recommendation model capacity for better performance Zhang et al. (2024b,a); Fang et al. (2024); Pan et al. (2024); Shin et al. (2023); Anil et al. (2022). One fundamental challenge to employ them in industrial-scale applications is the restricted training and inference budget for serving models. To overcome that, multiple prior studies explored the adaption of knowledge distillation (KD) Hinton (2015) in recommendation problems, where a large teacher model continuously supervises a compact student model by co-training and finally only the student model is used for serving Kang et al. (2024, 2023); Chen et al. (2023); Liu et al. (2022); Kang et al. (2021); Kweon et al. (2021); Zhu et al. (2020); Tang and Wang (2018). One limitation of those studies is that they primarily focused on static environments, while in industrial applications, large-volume data continuously arrive. Lee et al. (2024) proposed to handle that by a continual learning framework. Another limitation is that those studies follow co-training based distillation. It will increase the training cost of the serving model, as well as the risk of model staleness due to iteration delay, implying performance loss when new incoming data has distribution shifting. Khani et al. (2024) proposed to handle that by external distillation where teacher training happens separately from supervising students. Among existing studies, ours are closest to Lee et al. (2024) and Khani et al. (2024). Compared to Lee et al. (2024), our study has the following salient differences: (1) Different settings for the teacher model. To amortize building and maintaining resources, the teacher model in ExFM is an FM that uses an aggregation of student models' feature sets. As a result, the teacher itself often has feature gap and dimension mismatch when compared to an individual VM, so ExFM does not generate student models from the teacher model like Lee et al. (2024). (2) Teacher update does not depend on students. Lee et al. (2024) instead couples the teacher model's update with students' by replay learning. Such dependency will increase overhead of teacher update and bring in staleness, as illustrated by Figure 2, implying a risk of huge performance loss. (3) Consideration of data distribution shifting in an industrial streaming setting. We develop Student Adapter to handle that and provide theoretical guarantees. Compared to Khani et al. (2024), our differences and contributions are as follows. Its auxiliary distillation looks similar to our AH, but our AH is an isolated task arch that consumes supervision from FM in a dedicated fashion, not simultaneously consuming true labels like in Khani et al. (2024). Khani et al. (2024) lacks theoretical proof to justify its proposal and does not provide a solution to mitigate the distribution gap between FM and VMs due to data distribution shifting. Instead, we develop and prove Student Adapter can achieve the goal. Moreover, Khani et al. (2024) does not contain sufficient details and lacks comprehensive benchmark experiments on external datasets. 13",
  "6 Conclusion": "In this paper, we propose the ExFM framework to address two fundamental but overlooked challenges by prior studies on scaling-up Ads recommendation models - (C1) restricted training and inference budget and (C2) streaming data with distribution shifting. To overcome C1, ExFM employs the external distillation and the data augmentation service, where teacher training separates from student training and one teacher can supervise multiple VMs, like an FM. To alleviate the distribution gap between FM and VMs caused by C2, ExFM proposes Auxiliary Head (AH) and Student Adapter (SA), with mathematical guarantees on benefits provided. ExFM, including its core techniques such as AH and SA, achieved outstanding performance on industrial-scale datasets over multiple tasks and stages. It enabled the serving of a trillion-parameter model without increasing serving latency or negatively impacting user experience, and established the capability to serve LLM-scale ads model in the future. We also experimented on public datasets and were able to reproduce ExFM's outstanding performance, further demonstrating the effectiveness of the proposed framework.",
  "References": "Kuaishou, howpublished = https://www.kuaishou.com/activity/uimc . Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. Rohan Anil, Sandra Gadanho, Da Huang, Nijith Jacob, Zhuoshu Li, Dong Lin, Todd Phillips, Cristina Pop, Kevin Regan, Gil I Shamir, et al. On the factory floor: Ml engineering for industrial-scale ads recommendation models. arXiv preprint arXiv:2209.05310 , 2022. Apache. Zookeeper, 2023. https://zookeeper.apache.org/ . Peter Bartlett, Dave Helmbold, and Philip Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In ICML , pages 521-530. PMLR, 2018. Gang Chen, Jiawei Chen, Fuli Feng, Sheng Zhou, and Xiangnan He. Unbiased knowledge distillation for recommendation. In WSDM , pages 976-984, 2023. Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, and Yiqun Liu. Scaling laws for dense retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 1339-1349, 2024. Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems , pages 299-315, 2022. Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. International Journal of Computer Vision , 129(6):1789-1819, 2021. Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-machine based neural network for ctr prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence , pages 1725-1731, 2017. Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web , pages 507-517, 2016. Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising , pages 1-9, 2014. Geoffrey Hinton. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 , 2015. SeongKu Kang, Junyoung Hwang, Wonbin Kweon, and Hwanjo Yu. Item-side ranking regularized distillation for recommender system. Information Sciences , 580:15-34, 2021. SeongKu Kang, Wonbin Kweon, Dongha Lee, Jianxun Lian, Xing Xie, and Hwanjo Yu. Distillation from heterogeneous models for top-k recommendation. In Proceedings of the ACM Web Conference 2023 , pages 801-811, 2023. 14 15 16",
  "Appendix": "",
  "A Appendix": "",
  "A.1 Details of models and training configurations": "We use the BARS benchmark Zhu et al. (2022) and the FuxiCTR Zhu et al. (2021) to implement all the public models. We follow the default training and model configurations (e.g., learning rate, training epoch, etc.) in BARS for each public dataset and model used for experiments.",
  "A.2 Proof of Theorem 3.1": "We consider a linear model to prove Theorem 3.1 by construction. Let x ∈ R D , where D denotes the input dimension. x will be first linearly mapped to a vector of d ≪ D dimension by a matrix Z ∈ R d × D . We assume that the ground-truth label y is given by y = u ⊤ 1 Zx , where u 1 ∈ R d . For convenience of discussion, let's assume that we have d -1 teacher models: for an input x , they output d -1 soft labels as w ⊤ k Zx,k = 2 , . . . , d . For simplicity, we assume that w k = u 1 + µu k , k ∈ [ d ] , with µ > 0 being a small weight and u 1 , . . . , u d form an orthonormal basis for the space of R d . Proof. Let ( α 1 , . . . , α d ) ∈ ∆ d be the weights to linearly combine the ground-truth labels with teacher labels, where ∆ is a simplex in R d . For a given input x , its smoothed label is ˜ y ( x ) = ( u 1 + µ ∑ d k =2 α k u k ) ⊤ Zx . We fit ˜ y ( x ) by a two-layer linear model, i.e., min v ∈ R d ,H ∈ R d × D 1 T ∑ T t =1 ∣ ∣ v ⊤ Hx t -˜ y ( x t ) ∣ ∣ 2 , where T denotes the number of training samples. According to the theory of deep linear model Bartlett et al. (2018), the resulting model, trained by gradient descent, will be able to find a solution with zero loss, i.e., H ⊤ v = Z ⊤ ( u 1 + µ ∑ d k =2 α k u k ) , if E [ xx ⊤ ] is non-singular, and we have enough number of training examples. The linear prediction model H ⊤ v is different from Z ⊤ u 1 , and the difference between the two linear prediction models arises from the bias introduced by the label-smoothing. Now we denote w 1 t , t = 1 , . . . , T the solution for the prediction head for the ground-truth labels evolving over time (i.e., each training sample), and by w k t , k = 2 , . . . the dynamic solutions for the d -1 teacher labels. By the gradient descent method, we have  By assuming x t ∼ N (0 , I ) , E t [ Z t +1 ] = ( I -2 η ∑ d k =1 w k t [ w k t ] ⊤ ) Z t +2 η ∑ d k =1 w k t w ⊤ k Z . As w k t is updated much more frequently than Z t , we have  implying that when Z t is close to Z , we will have all w k t being close to w t . As a result, we have  where A = ∑ d k =1 w k w ⊤ k is a full rank matrix, indicating that Z t will converge to Z and thus all w k t will converge to w k . This result indicates that by using auxiliary heads, we will guarantee to find the optimal solution w 1 and Z for predicting the ground-truth labels. 17",
  "A.3 Proof of Theorem 3.2": "We consider the standard regression problem for analysis. Let { x i ∈ R d , i = 1 , . . . , N } be the input sampled from a normal distribution N (0 , I ) . Let the observed output y i = x ⊤ i ( w + z i ) , where w is the underlying regression model and z i ∼ N ( 0 , γ 2 I ) is an independently sampled vector from a normal distribution that leads to the noise in the observed output value y i . In this study, we assume γ ≫ 1 . If we learn a regression model directly from the training data, the resulting solution w 1 is given by  Based on the Matrix Chernoff bound, with probability 1 -δ , we have ∣ ∣ ∣ 1 N ∑ N i =1 x i x ⊤ i -I ∣ ∣ ∣ 2 ≤ O (√ d N log 1 δ ) ,   implying that to achieve a recovery error ϵ , we need at least Ω ( γ 2 d ε 2 log 1 δ ) number of training samples. Now we provide the complete proof. Proof. As the teacher model will be equipped with the student adapter, i.e., an MLP, to adapt its prediction, we assume the teacher model is parameterized as ̂ y = ̂ w ⊤ x + u ⊤ Hx , where H ∈ R s × d is a given orthonormal matrix (i.e., H i, ∗ H ⊤ j, ∗ = δ i,j , ∀ i, j ∈ [ s ] ) with s ≪ d . For simplicity, we assume that w -̂ w lies in the subspace spanned by the row vectors of H , i.e., w -̂ w = P H ( w -̂ w ) , where P H = ∑ s i =1 H ⊤ i, ∗ H i, ∗ is a projection matrix. The overall objective function is given by  where sg denotes the stop gradient. We first examine the convergence of u , whose expression is given as follows  Following the standard concentration inequality, with a probability 1 -δ , we have | v | ≤ O ( γ √ s N log 1 δ ) . Thus ∣ ∣ H ⊤ u -( w -̂ w ) ∣ ∣ 2 ≤ O ( γ √ s N log 1 δ ) . Using the result of u , we obtain the optimal solution as w V = w + α ( ̂ w + H ⊤ u -w ) 1+ α + 1 1+ α ( ∑ N i =1 x i x ⊤ i ) -1 ∑ N i =1 x i x ⊤ i z i . Following the matrix Bernstein inequality, with an appropriate choice of α , we have, with a probability 1 -δ ,  Compared to the bound for the standard regression model, i.e., | w 1 -w | 2 , the bound is reduced by the factor ( d/s ) 1 / 4 . 18",
  "keywords_parsed": [
    "None"
  ]
}