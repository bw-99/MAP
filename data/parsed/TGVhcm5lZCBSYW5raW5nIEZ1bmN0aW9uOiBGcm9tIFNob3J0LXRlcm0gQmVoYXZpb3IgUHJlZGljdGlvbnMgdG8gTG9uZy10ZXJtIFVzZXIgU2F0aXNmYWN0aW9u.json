{
  "Learned Ranking Function: From Short-term Behavior Predictions to Long-term User Satisfaction": "Yi Wu âˆ— Google Inc Mountain View, USA wuyish@google.com Daryl Chang âˆ— Google Inc Mountain View, USA dlchang@google.com Jennifer She Google DeepMind Mountain View, USA jenshe@google.com",
  "Zhe Zhao": "University of California, Davis Davis, USA zao@ucdavis.edu",
  "Li Wei": "Google Inc Mountain View, California, USA liwei@google.com",
  "ABSTRACT": "Wepresent the Learned Ranking Function (LRF), a system that takes short-term user-item behavior predictions as input and outputs a slate of recommendations that directly optimizes for long-term user satisfaction. Most previous work is based on optimizing the hyperparameters of a heuristic function. We propose to model the problem directly as a slate optimization problem with the objective of maximizing long-term user satisfaction. We also develop a novel constraint optimization algorithm that stabilizes objective tradeoffs for multi-objective optimization. We evaluate our approach with live experiments and describe its deployment on YouTube.",
  "CCS CONCEPTS": "Â· Information systems â†’ Recommender systems ; Â· Computing methodologies â†’ Reinforcement learning .",
  "KEYWORDS": "Slate Optimization, Reinforcement Learning",
  "ACMReference Format:": "Yi Wu âˆ— , Daryl Chang âˆ— , Jennifer She, Zhe Zhao, Li Wei, and Lukasz Heldt. 2024. Learned Ranking Function: From Short-term Behavior Predictions to Long-term User Satisfaction. In Proceedings of (RecSys '24). ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn",
  "1 INTRODUCTION AND RELATED WORK": "Large video recommendation systems typically have the following stages: (1) Candidate Generation: The system first generates a short list of video candidates from a large corpus [6, 19]. * Equal contribution to the work. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys '24, Oct 14-18, 2024, Bari, Italy Â© 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn Lukasz Heldt Google Inc Mountain View, California, USA heldt@google.com (2) Multitask Model Scoring: A Multitask model makes predictions about user behaviors (such as CTR, watch time after click) for all the candidates [8, 21]. (3) Ranking: Multitask predictions are combined into a single ranking score to sort all candidates [1, 2, 4, 10, 17, 20]. (4) Re-ranking: Additional logic is applied to ranking score to ensure other objectives, e.g. diversity [18], taking cross-item interaction into consideration. This paper primarily focuses on the ranking stage, i.e. combining user behavior predictions to optimize long-term user satisfaction. Most existing deployed solutions (e.g. Meta [1, 2, 17], Pinterest [10] and Kuaishou [4]) use a heuristic ranking function to combine multitask model predictions. As an example, given input user behavior predictions ğ‘  1 , ğ‘  2 , . . . , ğ‘  ğ‘˜ , the ranking formula can be Ë ğ‘˜ ğ‘– = 1 ğ‘¤ ğ‘– ğ‘  ğ‘– with ğ‘¤ ğ‘– being the hyperparameters. Then these systems apply hyperparameter search methods (e.g. Bayesian optimization, policy gradient) to optimize the ranking function. Typically the complexity of optimization grows with the number of hyperparameters, making it hard to change objectives, add input signals, or increase the expressiveness of the combination function. We formulate the problem as a slate optimization instead. The goal of the optimization is learn a general ranking function to produce a slate that maximizes long-term user satisfaction. Let us take a look at related work in the area of slate optimization for long-term rewards. In [13], the authors propose the SlateQ method, which applies reinforcement learning to solve slate optimization. One limitation of the work is it assumes a simple user interaction model without considering the impact of slate position on the click probability. In [3], the authors give an efficient algorithm for the combinatorial optimization problem of reward maximization under the cascade click model [5, 9], assuming the dynamics of the system are given as input. Existing slate optimization work typically assumes that future rewards are zero when a user abandons a slate, which is unrealistic. Platforms like video streaming services have multiple recommendation systems (e.g., watch page, home page, search page), where users might abandon one and return later through engagement with another. Hence, it is important to model and optimize the lift value of a slate, i.e. its incremental value over the baseline value of the user abandoning the slate. Another less studied but important issue when applying slate optimization at the ranking stage is the stability of multi-objective RecSys '24, Oct 14-18, 2024, Bari, Italy Yi Wu et al. optimization. Most recommendation systems need to balance tradeoffs among multiple objectives. Stability here refers to maintaining consistent trade-offs among these objectives when orthogonal changes, such as adding a feature or modifying the model architecture, are made to the algorithm. The stability is crucial for system reliability and developer velocity. To address these existing limitations, we present the Learned RankingFunction (LRF) system. Our main contributions are threefold: (1) Wemodeltheuser-slate interaction as a cascade click model [9] and propose an algorithm to optimize slate-wise long-term rewards. We explicitly model the value of abandonment and optimize for the long-term rewards for entire platform. (2) Weproposeanovelconstrained optimization algorithm based on dynamic linear scalarization to ensure the stability of trade-offs for multi-objective optimization. (3) We show how the LRF is fully launched on YouTube and provide empirical evaluation results. The rest of paper is organized as follows. In Section 2, we define the Markov Decision Process(MDP) for the problem of long-term rewards slate optimization. In Section 3, we propose an optimization algorithm to solve the MDP problem. We show how we deploy the LRF to YouTube with evaluation results in Section 4.",
  "2 PROBLEM FORMATION": "",
  "2.1 MDP Formulation": "We model the problem of ranking videos using the following MDP: Â· state space S = UÃ—{ ğ‘‰ | ğ‘‰ âŠ‚ V , | ğ‘‰ | = ğ‘› } . Here U is some user state space and ğ‘‰ is a set of ğ‘› candidate videos nominated for ranking from V , the universe of all videos. Â· action space A is all permutations of ğ‘› . The system will rank ğ‘‰ = { ğ‘‰ 1 , ğ‘‰ 2 , . . . , ğ‘‰ ğ‘› } by the order of ğ‘‰ ğœ ( 1 ) , . . . , ğ‘‰ ğœ ( ğ‘› ) with action ğœ âˆˆ A . Â· P : S Ã— A Ã— S â†’ [ 0 , 1 ] is the state transition probability. Â· reward function ğ‘Ÿ ( ğ‘ , ğœ ) âˆˆ R ğ‘š is the immediate reward vector by taking action ğœ on state ğ‘  . We consider the general case that there are ğ‘š different type of rewards. Â· discounting factor ğ›¾ âˆˆ ( 0 , 1 ) and initial state distribution ğœŒ 0 . A policy ğœ‹ is a mapping from user state S to a distribution on A . Applying policy ğœ‹ on ğœŒ 0 gives a distribution on user trajectory D( ğœŒ 0 , ğœ‹ ) defined as follows. Definition 2.1. We define D( ğœŒ 0 , ğœ‹ ) as the distribution of user trajectories when applying policy ğœ‹ on initial state distribution ğœŒ 0 . Here each user trajectory is a list of tuples (( ğ‘  0 , ğœ 0 , ğ‘ 0 ) , ( ğ‘  1 , ğœ 1 , ğ‘ 1 ) , . . . , ) Here ğ‘  ğ‘– = ( ğ‘¢ ğ‘– , ğ‘‰ ğ‘– ) is the user state; ğœ ğ‘– is a permutation action applied on ğ‘‰ ; ğ‘ ğ‘– is the user click position (with a value of 0 indicating no click). We define cumulative reward for ğœ starting from timestamp ğ‘¡ as  and cumulative reward for policy ğœ‹ as  The optimization problem is to maximize cumulative reward for a primary objective subject to constraints on secondary objectives: . Problem 1.",
  "max ğœ‹ ğ½ 1 ( ğœ‹ )": "subject to ğ½ ğ‘˜ ( ğœ‹ ) â‰¥ ğ›½ ğ‘˜ for ğ‘˜ = 2 , 3 , . . . , ğ‘š . Here ğ½ ğ‘– ( ğœ‹ ) is the ğ‘– -th element of ğ½ ( ğœ‹ ) .",
  "2.2 Lift Formulation with Cascade Click model": "Let us follow the standard notation in reinforcement learning and define ğ‘„ ğœ‹ ( ğ‘ , ğœ ) as the expected cumulative reward taking action ğœ at state ğ‘  and applying policy ğœ‹ afterwards; i.e.,  Below we will factorize ğ‘„ ğœ‹ ( ğ‘ , ğœ ) into user-item-functions; i.e., functions that only depend on user and individual item. Conditional on click position ğ‘ , we can then rewrite ğ‘„ ğœ‹ ( ğ‘ , ğœ ) as  As mentioned in Section 1, the reward associated with the user abandoning the slate ( ğ‘ = 0) can be nonzero. Notice that Ë 0 â‰¤ ğ‘– â‰¤ ğ‘› Pr ( ğ‘ = ğ‘– ) = 1, so we can further rewrite ğ‘„ ğœ‹ ( ğ‘ , ğœ ) as  First, we simplify the term E [ ğ‘„ ğœ‹ ( ğ‘ , ğœ )| ğ‘ = ğ‘– ] for 0 â‰¤ ğ‘– â‰¤ ğ‘› with user-item functions. In order to do so, we make the \"Reward/transition dependence on selection\" assumption from [13] which states that future reward only depends on the item the user clicks. In other words for ğ‘  = ( ğ‘¢,ğ‘‰ ) , (1) when ğ‘– > 0, E [ ğ‘„ ğœ‹ ( ğ‘ , ğœ )| ğ‘ = ğ‘– ] can be written as ğ‘… ğœ‹ ğ‘ğ‘™ğ‘˜ ( ğ‘¢,ğ‘‰ ğœ ( ğ‘– ) ) for ğ‘… ğœ‹ ğ‘ğ‘™ğ‘˜ being a user-item function (2) E [ ğ‘„ ğœ‹ ( ğ‘ , ğœ )| ğ‘ = 0 ] can be written as ğ‘… ğœ‹ ğ‘ğ‘ğ‘‘ ( ğ‘¢ ) for ğ‘… ğœ‹ ğ‘ğ‘ğ‘‘ being a user level function. We further define ğ‘… ğœ‹ ğ‘™ğ‘– ğ‘“ ğ‘¡ as ğ‘… ğœ‹ ğ‘™ğ‘– ğ‘“ ğ‘¡ ( ğ‘¢, ğ‘£ ) = ğ‘… ğœ‹ ğ‘ğ‘™ğ‘˜ ( ğ‘¢, ğ‘£ ) -ğ‘… ğœ‹ ğ‘ğ‘ğ‘‘ ( ğ‘¢ ) , being the difference (i.e., lift) of future rewards associated with the user clicking item ğ‘£ compared to user abandoning the slate. Next, we simplify the term Pr ( ğ‘ = ğ‘– ) with user-item functions by assuming the user interacts with the slate according to a cascade click model [9]. To model the behavior of the user abandoning a slate, we consider a variant [3, 5] which also allows the user to abandon the slate, in addition to skip and click, when inspecting an item, as illustrated in Figure 1: Learned Ranking Function: From Short-term Behavior Predictions to Long-term User Satisfaction RecSys '24, Oct 14-18, 2024, Bari, Italy Figure 1: Markov Reward Process with Cascade Click Model Expected Video at Expected abandonment Abandon position Click reward given reward click Rabd (u) Scroll 1 _ Pabd(u, Video at Expected position 2 Click reward given click Abandon Pab(u, Vo(2) Definition 2.2. (Cascade Click Model) Given user state ğ‘  = ( ğ‘¢,ğ‘‰ ) , where ğ‘¢ represents the user and ğ‘‰ represents the set of items, and a ranking order ğœ on ğ‘‰ , the Cascade model describes how a user interacts with a list of items sequentially. The user's interaction with the list when inspecting an item is characterized by the following user-item functions: Â· ğ‘ ğ‘ğ‘™ğ‘˜ : A user-item function where ğ‘ ğ‘ğ‘™ğ‘˜ ( ğ‘¢, ğ‘£ ) represents the probability of user ğ‘¢ clicking on item ğ‘£ when inspecting it. Â· ğ‘ ğ‘ğ‘ğ‘‘ : A user-item function where ğ‘ ğ‘ğ‘ğ‘‘ ( ğ‘¢, ğ‘£ ) represents the probability of user ğ‘¢ abandoning the slate when inspecting item ğ‘£ . Taking ( ğ‘¢,ğ‘‰ ) , ğœ, ğ‘ ğ‘ğ‘™ğ‘˜ , ğ‘ ğ‘ğ‘ğ‘‘ as input, the Cascade model defines function ğ‘ƒ ğ‘– ğ‘ğ‘ğ‘ ğ‘ğ‘ğ‘‘ğ‘’ that outputs the probability the user clicks on the item at the ğ‘– -th position (for 1 â‰¤ ğ‘– â‰¤ ğ‘› ) with the form:  Â« â€¹ The probability that the user abandons the slate without clicking on any items is defined by the function ğ‘ƒ 0 ğ‘ğ‘ğ‘ ğ‘ğ‘ğ‘‘ğ‘’ as:  Putting everything together, we can rewrite ğ‘„ ğœ‹ ( ğ‘ , ğœ ) as  We call equation (3) the lift formulation with cascade click model . A natural question is how to order items to maximize ğ‘„ ğœ‹ ( ğ‘ , ğœ ) when there is only a single objective. Interestingly, despite the slate nature of this optimization, we prove that the problem can be solved by a user-item ranking function. Theorem 2.3. Given user-item functions ğ‘ ğ‘ğ‘™ğ‘˜ , ğ‘ ğ‘ğ‘ğ‘‘ , ğ‘… ğœ‹ ğ‘ğ‘ğ‘‘ , ğ‘… ğœ‹ ğ‘™ğ‘– ğ‘“ ğ‘¡ as input, the optimal ranking for user ğ‘¢ on candidate ğ‘‰ maximizing ğ‘„ ğœ‹ (( ğ‘¢,ğ‘‰ ) , ğœ ) for a scalar reward function is to order all items ğ‘£ âˆˆ ğ‘‰ by ğ‘ ğ‘ğ‘™ğ‘˜ ( ğ‘¢,ğ‘£ ) ğ‘ ğ‘ğ‘™ğ‘˜ ( ğ‘¢,ğ‘£ )+ ğ‘ ğ‘ğ‘ğ‘‘ ( ğ‘¢,ğ‘£ ) Â· ğ‘… ğœ‹ ğ‘™ğ‘– ğ‘“ ğ‘¡ ( ğ‘¢, ğ‘£ ) . Proof. In equation (3), ğ‘… ğœ‹ ğ‘ğ‘ğ‘‘ ( ğ‘¢ ) only depends on users. Therefore, it suffices to optimize Ë ğ‘› ğ‘– = 1 ğ‘ƒ ğ‘– ğ‘ğ‘ğ‘ ğ‘ğ‘ğ‘‘ğ‘’ ( ğ‘ ğ‘ğ‘™ğ‘˜ , ğ‘ ğ‘ğ‘ğ‘‘ , ğ‘‰ , ğœ )Â· ğ‘… ğœ‹ ğ‘™ğ‘– ğ‘“ ğ‘¡ ( ğ‘¢,ğ‘‰ ğœ ( ğ‘– ) ) . The rest of the proof follows Theorem 1 in [3]. â–¡",
  "3 OPTIMIZATION ALGORITHM": "This section outlines the optimization algorithm for solving Problem 1, initially for the special case of a single objective; i.e., ğ‘š = 1 and subsequently extending to multi-objective constraint optimization.",
  "3.1 Single Objective Optimization": "Our algorithm employs an on-policy Monte Carlo approach [16] which iteratively applies following two steps: (1) Training: Build a function approximation ğ‘„ ( ğ‘ , ğœ ; ğœƒ ) for ğ‘„ ğœ‹ ( ğ‘ , ğœ ) by separately building function approximations for ğ‘… ğœ‹ ğ‘ğ‘ğ‘‘ , ğ‘… ğœ‹ ğ‘ğ‘™ğ‘˜ , ğ‘ ğ‘ğ‘™ğ‘˜ and ğ‘ ğ‘ğ‘ğ‘‘ , using data collected by applying some initial policy ğœ‹ . (2) Inference: Modify the policy ğœ‹ to be ğ‘ğ‘Ÿğ‘” max ğœ ğ‘„ ( ğ‘ , ğœ ; ğœƒ ) (with exploration). We outline the main steps in Algorithm 1 and discuss technical details in Section 3.1.1 and 3.1.2. 3.1.1 Training. The training data is collection of user trajectories (see Definition 2.1) stored in ğ· . Each user trajectory can be written as ((( ğ‘¢ 0 , ğ‘‰ 0 ) , ğœ 0 , ğ‘ 0 ) , (( ğ‘¢ 1 , ğ‘‰ 1 ) , ğœ 1 , ğ‘ 1 ) , . . . , . ) . We apply gradient updates for ğœƒ with the following loss functions, in sequential order. Training the abandon reward network. ğ‘… ğ‘ğ‘ğ‘‘ ( ğ‘¢ ; ğœƒ ) on abandoned pages with MSE loss function  Training the lift reward network. ğ‘… ğ‘™ğ‘– ğ‘“ ğ‘¡ ( ğ‘¢, ğ‘£ ; ğœƒ ) on clicked videos with MSE loss function   Here we apply the idea from uplift modeling [12] by directly estimating the difference between ğ‘… ğœ‹ ğ‘ğ‘™ğ‘˜ ( ğ‘¢, ğ‘£ ) and ğ‘… ğœ‹ ğ‘ğ‘ğ‘‘ ( ğ‘¢ ) . Training the click network. on every page with cross-entropy loss function  using ğ‘ƒ ğ‘ ğ‘¡ ğ‘ğ‘ğ‘ ğ‘ğ‘ğ‘‘ğ‘’ in Definition 2.2. 3.1.2 Inference. Here we simply apply Theorem 2.3 using the function approximation for ğ‘ ğ‘ğ‘™ğ‘˜ , ğ‘ ğ‘ğ‘ğ‘‘ , ğ‘… ğœ‹ ğ‘ğ‘ğ‘‘ , ğ‘… ğœ‹ ğ‘™ğ‘– ğ‘“ ğ‘¡ . We randomly promote a candidate to top with small probability as exploration.",
  "3.2 Constraint optimization": "Whentherearemultiple objectives, we apply linear scalarization [15] to reduce the constraint optimization problem to a unconstrained optimization problem; i.e., we find weights ğ‘¤ 2 , ğ‘¤ 3 , . . . , ğ‘¤ ğ‘š and define the new reward function as ğ‘Ÿ 1 + Ë ğ‘š ğ‘– = 2 ğ‘¤ ğ‘– Â· ğ‘Ÿ ğ‘– . With fixed weight combination, we found it often necessary to search new weights RecSys '24, Oct 14-18, 2024, Bari, Italy Yi Wu et al.",
  "Algorithm 1 Single objective optimization": "initialize FIFO data buffer ğ· initialize network ğ‘… ğ‘ğ‘ğ‘‘ ( ğ‘¢, ğ‘£ ; ğœƒ ) , ğ‘… ğ‘™ğ‘– ğ‘“ ğ‘¡ ( ğ‘¢, ğ‘£ ; ğœƒ ) , ğ‘ ğ‘ğ‘™ğ‘˜ ( ğ‘¢, ğ‘£ ; ğœƒ ) , ğ‘ ğ‘ğ‘ğ‘‘ ( ğ‘¢, ğ‘£ ; ğœƒ ) parameterized by ğœƒ and initial policy ğœ‹ while 1 do Apply ğœ‹ to collect ğ¾ user trajectories and add it into ğ· . Update ğœƒ by using data ğ· (see Section 3.1.1 for details) Update ğœ‹ such that for user ğ‘¢ with candidate set ğ‘‰ (1) order ğ‘£ âˆˆ ğ‘‰ by ğ‘ ğ‘ğ‘™ğ‘˜ ( ğ‘¢,ğ‘£ ; ğœƒ ) ğ‘ ğ‘ğ‘™ğ‘˜ ( ğ‘¢,ğ‘£ ; ğœƒ )+ ğ‘ ğ‘ğ‘ğ‘‘ ( ğ‘¢,ğ‘£ ; ğœƒ ) Â· ğ‘… ğ‘™ğ‘– ğ‘“ ğ‘¡ ( ğ‘¢, ğ‘£ ; ğœƒ ) (2) with probability ğœ– , promote a random candidate to top end while when making changes (e.g., add features, change model architecture) to the system, which slows down our iteration velocity. We address the problem by dynamically updating ğ‘¤ as part of the training. At a high level, we make the following changes to Algorithm 1: (1) Training: apply Algorithm 1 for ğ‘… ğ‘™ğ‘– ğ‘“ ğ‘¡ ( ğ‘¢, ğ‘£ ; ğœƒ ) as a vector function for all the ğ‘š objectives separately. (2) Inference: We find a set of weights ğ‘¤ = ( 1 , ğ‘¤ 2 , ğ‘¤ 3 . . . , ğ‘¤ ğ‘š ) and use the following ranking formula at serving time:  The weights are dynamically updated with offline evaluation. The algorithm is outlined in Algorithm 2, with details below. 3.2.1 Offline evaluation on exploration candidates. We apply our offline evaluation on a data set consisting of candidates that is randomly promoted as exploration during serving. Definition 3.1. We define ğ· ğ‘’ğ‘£ğ‘ğ‘™ as {( ğ‘¢, ğ‘£, ğ‘Ÿ ğ‘£ )| ğ‘£ âˆˆ ğ‘‰ and is randomly promoted , (( ğ‘¢,ğ‘‰ ) , ğœ, ğ‘ ) âˆˆ ğœ, ğœ âˆˆ ğ·, } Here ğ‘Ÿ ğ‘£ is the reward vector ğ‘Ÿ (( ğ‘¢,ğ‘‰ ) , ğœ ) if ğ‘£ is clicked and 0 ğ‘š otherwise. We use  as the offline evaluation result for ğ‘– -th objectives. Intuitively, we are computing the correlation between weight-combined lift on ğ‘£ with the immediate rewards from showing ğ‘£ . The correlation is computed on exploration candidates to make the evaluation result less biased by the serving policy. 3.2.2 Optimization with correlation constraint. With the offline evaluation defined above, we solve the following problem to update ğ‘¤ in Algorithm 2. Problem 2.   Intuitively, we would like to minimize the change to the primary objective while satisfying offline evaluation on secondary objectives. In the case of a single constraint (i.e., ğ‘š = 2), there is a closedform solution for the problem. To see this, the optimal solution for ğ‘¤ 2 must be either 0 or be a solution that makes the constraint tight; i.e.,  It is not hard to verify that the solution for equation (4) is also the solution for a quadratic equation of ğ‘¤ 2 that can be solved with closed form. Therefore, we can set ğ‘¤ 2 be the best feasible solution from { 0 } âˆª { solution for equation (4 )} . When there is more than one constraint, we found that applying constraints sequentially works well in practice. One can also apply grid search as the offline evaluation can be done efficiently.",
  "4 DEPLOYMENT AND EVALUATION": "Figure 2: LRF deployment diagram LRF Deployment Serving Training Generate candidates Score with Model push Multitask model LRF training Update 1 Rank with LRF Model push constraints Multitask training Train Multitask Train LRF model Re-rank Logic On-policy data All traffic data User Return recs feedback User",
  "4.1 Deployment of LRF System": "The LRF was initially launched on YouTube's Watch Page, followed by the Home and Shorts pages. Below we discuss its deployment on Watch Page. Also see illustration in Figure 2. Lightweight model with on-policy training. The LRF system applies an on-policy RL algorithm. In order to enable evaluating many different LRF models with on-policy training, we make all LRF model training on a small slice (e.g. 1%) of overall traffic. By doing so,we can compare production and many experimental models that are all trained on-policy together. Training and Serving. The LRF system is continuously trained with user trajectories from the past few days. Our primary reward function is defined as the user satisfaction on watches, similar to the metric described in Section 6 of [7]. The features include the user behavior predictions from multitask models, user context features Learned Ranking Function: From Short-term Behavior Predictions to Long-term User Satisfaction RecSys '24, Oct 14-18, 2024, Bari, Italy",
  "Algorithm 2 Constraint Optimization": "initialize FIFO data buffer ğ· initialize network ğ‘… ğ‘ğ‘ğ‘‘ ( ğ‘¢, ğ‘£ ; ğœƒ ) , ğ‘… ğ‘™ğ‘– ğ‘“ ğ‘¡ ( ğ‘¢, ğ‘£ ; ğœƒ ) , ğ‘ ğ‘ğ‘™ğ‘˜ ( ğ‘¢, ğ‘£ ; ğœƒ ) , ğ‘ ğ‘ğ‘ğ‘‘ ( ğ‘¢, ğ‘£ ; ğœƒ ) parameterized by ğœƒ and initial policy ğœ‹ initialize ğ‘š -dimensional weight vectors ğ‘¤ = ( 1 , 0 , . . . , 0 ) .",
  "while 1 do": "apply ğœ‹ and add ğ¾ user trajectory into ğ· . update ğœƒ as Algorithm 1 update ğ‘¤ (See Section 3.2.1 and 3.2.2) Update ğœ‹ such that for user ğ‘¢ with candidate set ğ‘‰ (1) order all ğ‘£ âˆˆ ğ‘‰ ğ‘ ğ‘ğ‘™ğ‘˜ ( ğ‘¢,ğ‘£ ; ğœƒ ) ğ‘ ğ‘ğ‘™ğ‘˜ ( ğ‘¢,ğ‘£ ; ğœƒ )+ ğ‘ ğ‘ğ‘ğ‘‘ ( ğ‘¢,ğ‘£ ; ğœƒ ) Â· âŸ¨ ğ‘… ğ‘™ğ‘– ğ‘“ ğ‘¡ ( ğ‘¢, ğ‘£ ; ğœƒ ) , ğ‘¤ âŸ© (2) with probability ğœ– , promote a random candidate to top. end while (e.g. demographics) and video features (e.g. video topic). We use both continuous features and sparse features with small cardinality. The LRF model comprises small deep neural networks, with roughly Î˜ ( 10 4 ) parameters. The inference cost of the model is small due to the size of the model. We use the offline evaluation described in Section 3.2.1 to ensure model quality before pushing to production. At serving time, the LRF takes the aforementioned features as input and outputs a ranking score for all items.",
  "4.2 Evaluation": "We conducted A/B experiments for Î˜ ( ğ‘¤ğ‘’ğ‘’ğ‘˜ ) on YouTube to evaluate the effectiveness of the LRF. Metric trends are shown in Figure 3. Note that first three experiments describe sequential improvements to the production system; the last two experiments ablate certain components of the LRF. Evaluation Metric. Our primary objective is a metric measuring long-term cumulative user satisfaction; see Sec 6 of [7] for details. Baseline before LRF Launch: The previous system uses a heuristic ranking function optimized by Bayesian optimization [11]. Hyperparameters: Wetuned two types of hyperparameters when deploying the LRF: training parameters, such as batch size, are tuned using offline loss; reward parameters, such as constraint weights, are tuned using live experiments. Figure 3: Metrics for experiments in Section 4.2.1(top left), 4.2.2 (top right), 4.2.5 (bottom left), and 4.2.5 (bottom right). Top-line metric [% change] per experiment 0876 0.375 8 4.2.1 Initial Deployment of LRF. We initially launched a simplified version of the LRF that uses the CTR prediction from the multitask model and ranks all candidates by ğ¶ğ‘‡ğ‘… Â· ğ‘… ğ‘™ğ‘– ğ‘“ ğ‘¡ . It also uses a set of fixed weights to combine secondary objectives. The control was the previous production system, a heuristic ranking function tuned using Bayesian optimization. The LRF outperformed the production by 0.21% with 95% CI [0.04, 0.38] and was launched to production. 4.2.2 Launch Cascade Click Model. After initial deployment of the LRF, we ran an experiment to determine the efficacy of the cascade click model, i.e., replacing ğ¶ğ‘‡ğ‘… with ğ‘ ğ‘ğ‘™ğ‘˜ ğ‘ ğ‘ğ‘™ğ‘˜ + ğ‘ ğ‘ğ‘ğ‘‘ . Adding the cascade click model outperformed the control by 0.66% with 95% CI [0.60, 0.72] in the top-line metric and was launched to production. 4.2.3 Launch Constraint Optimization. We found metric trade-offs between primary and secondary objectives unstable when combining the rewards using fixed weights. To improve stability, we launched the constraint optimization. As an example of the improvement, for the same model architecture change, we saw a 13 . 15% change in the secondary objective pre-launch, compared to a 1.46% change post-launch. This post-launch fluctuation is considered small for that metric. 4.2.4 Ablating Lift Formulation. To determine the necessity of the lift formula, we ran an experiment that set ğ‘… ğ‘ğ‘ğ‘‘ to be 0. Such a change regresses top-line metrics by 0.46% with 95% CI [0.43, 0.49]. The metric contributed from watch page recommendations actually increases by 0 . 2% with 95% CI [0.14, 0.26]. This suggests the importance of lift formulation as it is sub-optimal to only maximize rewards from watch page suggestions. 4.2.5 Two Model Approach. Wemakeseparate predictions for ğ‘… ğ‘ğ‘ğ‘‘ and ğ‘… ğ‘ğ‘™ğ‘˜ . This is also known as the two-model baseline in uplift modelling [12]. The ranking formula is then ğ‘ ğ‘ğ‘™ğ‘˜ ğ‘ ğ‘ğ‘™ğ‘˜ + ğ‘ ğ‘ğ‘ğ‘‘ Â· ( ğ‘… ğ‘ğ‘™ğ‘–ğ‘ğ‘˜ -ğ‘… ğ‘ğ‘ğ‘‘ ) . The experiment results show that our production LRF outperforms this baseline in the top-line metric by 0.12% with 95% CI [0.06, 0.18].",
  "5 CONCLUSION": "We presented the Learned Ranking Function (LRF), a system that combines short-term user-item behavior predictions to optimizing slates for long-term user satisfaction. One future direction is to apply more ideas from Reinforcement Learning such as off-policy training and TD Learning [16]. Another future direction is to incorporate re-ranking algorithm (e.g., [14, 18]) into the LRF system. RecSys '24, Oct 14-18, 2024, Bari, Italy Yi Wu et al.",
  "REFERENCES": "[1] 2019. Combining online and offline tests to improve News Feed ranking. https: //ai.meta.com/blog/online-and-offline-tests-to-improve-news-feed-ranking/ [2] 2020. Efficient tuning of online systems using Bayesian optimization. https://engineering.fb.com/2018/09/17/ml-applications/bayesianoptimization-for-tuning-online-systems-with-a-b-tests/ [3] Gagan Aggarwal, Jon Feldman, Martin PÃ¡l, and S. Muthukrishnan. 2008. Sponsored Search Auctions for Markovian Users. In Fourth Workshop on Ad Auctions; Workshop on Internet and Network Economics (WINE). http://arxiv.org/abs/0805. 0766 [4] Qingpeng Cai, Shuchang Liu, Xueliang Wang, Tianyou Zuo, Wentao Xie, Bin Yang, Dong Zheng, Peng Jiang, and Kun Gai. 2023. Reinforcing user retention in a billion scale short video recommender system. In Companion Proceedings of the ACM Web Conference 2023 . 421-426. [5] Olivier Chapelle and Ya Zhang. 2009. A dynamic bayesian network click model for web search ranking. In Proceedings of the 18th international conference on World wide web . 1-10. [6] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H Chi. 2019. Top-k off-policy correction for a REINFORCE recommender system. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining . 456-464. [7] Konstantina Christakopoulou, Can Xu, Sai Zhang, Sriraj Badam, Trevor Potter, Daniel Li, Hao Wan, Xinyang Yi, Elaine Le, Chris Berg, Eric Bencomo Dixon, Ed H. Chi, and Minmin Chen (Eds.). 2021. Reward Shaping for User Satisfaction in a REINFORCE Recommender . [8] Paul Covington, , Jay Adams, and Emrin Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems . New York, NY, USA. [9] Nick Craswell, Onno Zoeter, Michael J. Taylor, and Bill Ramsey. 2008. An experimental comparison of click position-bias models. In Web Search and Data Mining . https://api.semanticscholar.org/CorpusID:2625350 [10] Pinterest Engineering. 2023. Deep multi-task learning and real-time personalization for closeup recommendations. https://medium.com/pinterestengineering/deep-multi-task-learning-and-real-time-personalization-forcloseup-recommendations-1030edfe445f [11] Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D. Sculley. 2017. Google Vizier: A Service for Black-Box Optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017 . ACM, 1487-1495. https://doi.org/10.1145/3097983.3098043 [12] Pierre Gutierrez and Jean-Yves GÃ©rardy. 2017. Causal Inference and Uplift Modelling: A Review of the Literature. In Proceedings of The 3rd International Conference on Predictive Applications and APIs (Proceedings of Machine Learning Research) , Claire Hardgrove, Louis Dorard, Keiran Thompson, and Florian Douetteau (Eds.), Vol. 67. PMLR, 1-13. https://proceedings.mlr.press/v67/gutierrez17a. html [13] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019. SlateQ: A Tractable Decomposition for Reinforcement Learning with Recommendation Sets. In Proceedings of the Twenty-eighth International Joint Conference on Artificial Intelligence (IJCAI-19) . Macau, China, 2592-2599. See arXiv:1905.12767 for a related and expanded paper (with additional material and authors). [14] Weiwen Liu, Yunjia Xi, Jiarui Qin, Fei Sun, Bo Chen, Weinan Zhang, Rui Zhang, and Ruiming Tang. 2022. Neural re-ranking in multi-stage recommender systems: A review. arXiv preprint arXiv:2202.06602 (2022). [15] Diederik M Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. 2013. A survey of multi-objective sequential decision-making. Journal of Artificial Intelligence Research 48 (2013), 67-113. [16] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction . MIT press. [17] Vladislav Vorotilov, Vladislav Vorotilov, and Ilnur Shugaepov. 2023. Scaling the Instagram explore recommendations system. https://engineering.fb.com/2023/ 08/09/ml-applications/scaling-instagram-explore-recommendations-system/ [18] Mark Wilhelm, Ajith Ramanathan, Alexander Bonomo, Sagar Jain, Ed H Chi, and Jennifer Gillenwater. 2018. Practical diversified recommendations on youtube with determinantal point processes. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management . 2165-2173. [19] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. 2019. Sampling-bias-corrected neural modeling for large corpus item recommendations. In Proceedings of the 13th ACM Conference on Recommender Systems . 269-277. [20] Qihua Zhang, Junning Liu, Yuzhuo Dai, Yiyan Qi, Yifan Yuan, Kunlun Zheng, Fan Huang, and Xianfeng Tan. 2022. Multi-Task Fusion via Reinforcement Learning for Long-Term User Satisfaction in Recommender Systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22) . ACM. https://doi.org/10.1145/3534678.3539040 [21] Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee Kumthekar, Maheswaran Sathiamoorthy, Xinyang Yi, and Ed Chi. 2019. Recommending what video to watch next: a multitask ranking system. In Proceedings of the 13th ACM Conference on Recommender Systems . 43-51.",
  "keywords_parsed": [
    "Slate Optimization",
    "Reinforcement Learning"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Combining online and offline tests to improve News Feed ranking"
    },
    {
      "ref_id": "b2",
      "title": "Efficient tuning of online systems using Bayesian optimization"
    },
    {
      "ref_id": "b3",
      "title": "Sponsored Search Auctions for Markovian Users"
    },
    {
      "ref_id": "b4",
      "title": "Reinforcing user retention in a billion scale short video recommender system"
    },
    {
      "ref_id": "b5",
      "title": "A dynamic bayesian network click model for web search ranking"
    },
    {
      "ref_id": "b6",
      "title": "Top-k off-policy correction for a REINFORCE recommender system"
    },
    {
      "ref_id": "b7",
      "title": "Reward Shaping for User Satisfaction in a REINFORCE Recommender"
    },
    {
      "ref_id": "b8",
      "title": "Deep Neural Networks for YouTube Recommendations"
    },
    {
      "ref_id": "b9",
      "title": "An experimental comparison of click position-bias models"
    },
    {
      "ref_id": "b10",
      "title": "Deep multi-task learning and real-time personalization for closeup recommendations"
    },
    {
      "ref_id": "b11",
      "title": "Google Vizier: A Service for Black-Box Optimization"
    },
    {
      "ref_id": "b12",
      "title": "Causal Inference and Uplift Modelling: A Review of the Literature"
    },
    {
      "ref_id": "b13",
      "title": "SlateQ: A Tractable Decomposition for Reinforcement Learning with Recommendation Sets"
    },
    {
      "ref_id": "b14",
      "title": "Neural re-ranking in multi-stage recommender systems: A review"
    },
    {
      "ref_id": "b15",
      "title": "A survey of multi-objective sequential decision-making"
    },
    {
      "ref_id": "b16",
      "title": "Reinforcement learning: An introduction"
    },
    {
      "ref_id": "b17",
      "title": "Scaling the Instagram explore recommendations system"
    },
    {
      "ref_id": "b18",
      "title": "Practical diversified recommendations on youtube with determinantal point processes"
    },
    {
      "ref_id": "b19",
      "title": "Sampling-bias-corrected neural modeling for large corpus item recommendations"
    },
    {
      "ref_id": "b20",
      "title": "Multi-Task Fusion via Reinforcement Learning for Long-Term User Satisfaction in Recommender Systems"
    },
    {
      "ref_id": "b21",
      "title": "Recommending what video to watch next: a multitask ranking system"
    }
  ]
}