{"JOINT AUCTION IN THE ONLINE ADVERTISING MARKET": "", "Zhen Zhang \u2217": "Yahui Lei zhangzhen2023@ruc.edu.cn Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China Weian Li \u2217 weian.li@sdu.edu.cn School of Software, Shandong University Jinan, China", "Bingzhe Wang Zhicheng Zhang": "wbz2022,mzhangzhicheng@ ruc.edu.cn Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China", "Qi Qi \u2020": "qi.qi@ruc.edu.cn Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China leiyahui@meituan.com Meituan Inc. Beijing, China", "Qiang Liu Xingxing Wang": "liuqiang43,wangxingxing04@ meituan.com Meituan Inc. Beijing, China", "ABSTRACT": "Online advertising is a primary source of income for e-commerce platforms. In the current advertising pattern, the oriented targets are the online store owners who are willing to pay extra fees to enhance the position of their stores. On the other hand, brand suppliers are also desirable to advertise their products in stores to boost brand sales. However, the currently used advertising mode cannot satisfy the demand of both stores and brand suppliers simultaneously. To address this, we innovatively propose a joint advertising model termed 'Joint Auction', allowing brand suppliers and stores to collaboratively bid for advertising slots, catering to both their needs. However, conventional advertising auction mechanisms are not suitable for this novel scenario. In this paper, we propose JRegNet, a neural network architecture for the optimal joint auction design, to generate mechanisms that can achieve the optimal revenue and guarantee (near-)dominant strategy incentive compatibility and individual rationality. Finally, multiple experiments are conducted on synthetic and real data to demonstrate that our proposed joint auction significantly improves platform's revenue compared to the known baselines. K eywords Joint auction, Neural network, JRegNet", "1 Introduction": "Advertising is a primary source of income for internet companies such as Google, Amazon, and Facebook, bringing in significant amounts of revenue and supporting the development and prosperity of the internet. One widely-used method for assigning advertising positions is sponsored search auction, where advertisers submit bids to the platform, which then executes preset auction mechanisms to decide on advertising positions and charged fees. The monetization efficiency of advertising traffic directly affects the development of internet companies. How to improve the monetization efficiency of advertising has become a core research issue for enterprises. In the context of e-commerce platform as an example, such as Instacart, when an inquiry arrives, the platform returns a list of stores that sell the related products (as shown in Figure 1(a)). Stores can pay extra advertising fees to compete for \u2217 Both authors contribute equally to this research. \u2020 Corresponding author. higher positions in the list. From the perspective of brand suppliers, they also have a strong desire to enhance their exposure in order to drive up brand sales. Nevertheless, the current advertising model does not consider brand suppliers as advertisers, which causes that the platform may lose a portion of potential revenue from brand suppliers. Figure 1: An Example of the Traditional Advertising Model and Joint Advertising Model. View details Sparkling Ice Peach Zero (a) Traditional advertising (b) Joint advertising View all 20 items Joint Ocean Spray Diet 64 oz $4.69 Sparkling Ice Peach Zero 17 fl oz $1.79 Crystal Light Pink 10 ct $4.69 Gatora de Zero Lemon 28 fl oz $3.39 Monster Energy Zero 16 oz $3.29 View details Ocean Spray Diet Cran Pineapple 64 oz $4.69 Store Brand Store Brand View all 20+ items Sparkling Ice Peach Zero 17 fl oz $1.69 17 fl oz $1.69 17 fl oz $1.79 Sparkling Ice Peach Zero View details milk sugar free drinks Cart \u00b7 0 sugar free drinks Cart \u00b7 0 Non-participation in the auction Platform revenue Platform revenue Advertising payment Advertising payment Advertising payment Safeway Dollvery by 8.45am EBT In-store ptices CVS? CVS Delivory by 10,45am- Safeway 4 Delivery by 8.45am Safeway EBT Motivated by this phenomenon and to further increase platform revenue, we set up a novel online advertisement model called 'joint advertising' catering for the demand of brand suppliers, as shown in Figure 1(b). In this joint advertising model, both stores and brands participate in the auction, a sponsored advertising item is contributed by a bundle consisting of a store and a brand supplier, and we name the auction as 'joint auction'. In contrast to the traditional advertising auction, joint auction not only provides the chance for brand suppliers to boost brand sales but also enhance the revenue of platform since it now charges for both stores and brand suppliers. In addition, joint advertising also benefits for users, because it directly presents the searched products rather than the related stores. Therefore, the joint advertising scenario essentially aligns with the vested interests of multiple stakeholders, including platforms, brands, and users, which achieves a triple-win. Joint auction enriches the practicability of traditional auction theory, potentially exerting a significant impact in both academia and industry. However, from the perspective of mechanism design, the new model of joint auction also brings new technical challenges. For example, how to overcome the dependency of bundle bids and how to decide the payments of two parties within a bundle. It leads that most of the classic and common-used mechanisms may not apply for the joint advertising. Compared to the difficulties encountered in theory, in recent years, with the rapid development of machine learning, the concept of automated mechanism design has been proposed in computing the optimal mechanism [1, 2, 3]. However, the current neural network architectures are not suitable for joint auctions, mainly due to that they do not deal with the allocation of bundles. Therefore, how to deal with the problem of designing the optimal joint auctions is deserved to be investigated.", "1.1 Main Contributions": "The advertising system's most pivotal technology is its sales mechanism. The industry has seen significant maturity in the traditional advertising auction mechanism, with most studies emphasizing revenue enhancement through the introduction of innovative variations to existing frameworks. Our study uniquely presents a practical and revenueboosting advertising sales model known as the 'joint auction'. Nonetheless, many standard and widely-used mechanisms may not be applicable to this novel joint advertising model. To identify the optimal mechanisms that are both dominant strategy incentive compatible (DSIC) and individually rational (IR), we introduce JRegNet, a neural network architecture to generate the optimal satisfactory mechanisms. Subsequently, we validate our proposed architecture through numerous experiments on both synthetic and real-world datasets. These experiments demonstrate the superior performance of the generated mechanisms compared to established baselines. In summary, our primary contributions are as follows: Joint auctions. In this paper, we introduce a novel joint advertising model that satisfies the marketing requirements of both stores and brands simultaneously, leading to an increase in revenue. Within this framework, we present the innovative 'joint auction' mechanism, where both stores and brand suppliers submit bids and compete for advertising slots. A distinct feature of the joint auction is to display a bundled advertisement consisting of a store and a brand within a single advertising slot, rather than a single advertiser. However, not all stores and brands can form such a bundle, as it is based on an established sales relationship. To design an optimal joint auction, we frame it as a learning problem and employ automated mechanism design techniques to generate the optimal mechanism. 2 JRegNet. To overcome the technical challenges in finding optimal joint auctions, we introduce an innovative neural network architecture called J oint Reg ret Net work (JRegNet). This architecture leverages the concept of regret to generate mechanisms that not only adhere to the IR and DSIC conditions but also maximize revenue. Importantly, JRegNet is capable of addressing the unique challenges posed by joint auctions, which are not effectively handled by current popular neural network architectures (e.g., [4, 2, 3]): (1) Joint relationship graph. Not all stores and brands can be combined into bundles, and the joint auction relationship can vary depending on different search queries within the same auction scenario. To tackle this issue, JRegNet incorporates the joint relationship graph of stores and brands as an input. This relationship graph plays a crucial role throughout all stages of JRegNet, influencing the formation of bundles. (2) Implementation of bundle constraints. Joint auctions impose certain constraints on bundles, such as limiting each advertising slot to a single bundle and vice versa. To address these constraints, JRegNet initially computes an initial allocation probability matrix for the bundles. Subsequently, it employs two softmax functions to ensure compliance with the bundle constraints. (3) Correlated bids of different bundles. Since the advertising slot is ultimately allocated to a bundle, from the perspective of bundles, bids are dependent due to the potential presence of a single store or brand in multiple bundles. JRegNet deals with the problem of correlated bids by directly incorporating the independent bids of each store and brand at the input stage. This approach allows JRegNet to directly calculate the allocation probabilities of bundles based on these independent bids, rather than relying on the bids of bundles themselves within the neural network. (4) Calculation of separate payment. Determining the payment for each bidder within a bundle can be a sophisticated problem, especially when the mechanism assigns a bundle to a specific slot. To solve this challenge, JRegNet introduces a parameter that scales the total expected value of each bidder based on the advertiser's allocation probability matrix (which is calculated by the bundle's allocation probability matrix) and defines the scaled value as the payment. This parameter is optimized through training, while ensuring the condition of IR for all bidders.", "1.2 Related Work": "Traditional auction mechanisms have found extensive application within the advertising auction domain. The classic Myerson auction [5] endeavors to maximize revenue in the single-parameter setting, which can apply to sponsored advertising directly. On the other hand, Vickrey-Clarke-Groves (VCG) mechanism [6, 7, 8] is engineered to maximize social welfare of advertisers and does not depend on prior distributions, while keeping IR and DSIC. In recent two decades, generalized second price (GSP) auction [9, 10, 11, 12, 13] is the most popular mechanism in industry, owing to its simplicity and comprehensibility. In pursuit of augmenting revenue of GSP, numerous researchers contribute to studying the variants of GSP. Lahaie and Pennock [14] introduce the concept of 'squashing' to GSP, which assesses bidder rankings using a compression score during the allocation phase. Thompson and Leyton-Brown [15] integrate the idea of 'squashing' and 'reserve price' into GSP. Roberts et al. [16] propose a mechanism that ranks bidders based on the discrepancy between their bids and the reserve price. Charles et al. [17] conduct an exhaustive investigation into the impact of the squashing factor on revenue and CTR. However, as far as we know, except the VCG mechanism, all the other common mechanisms may not be applicable to the context of joint auctions (e.g., difficulty to define the equilibrium of GSP or dependent bids making Myerson auction invalid). In this paper, we design a novel mechanism for the joint auctions, while maintaining DSIC and IR, and yielding the optimal revenue. With the development of the field of machine learning, the approach of 'automated mechanism design' (AMD) [18, 19, 20] is proposed for auction design, particularly for multi-item auctions. Different from the theoretical methods, AMD focuses more on using the techniques of machine learning to compute the approximate optimal auction mechanisms [21, 22, 23]. As a milestone work of AMD, D\u00fctting et al. [1] model the problem of truthful auction design into a mathematical programming, transferring DSIC condition into the constraints on regrets and come up with the first neural network framework, RegretNet, which can achieve the optimal revenue. Later, there have been many works that extend RegretNet to handle different constraints and objectives, such as budget constraints [4], fairness objective [24] and human preferences [25]. Rahme et al. [2] propose a permutation-equivariant architecture, EquivariantNet, for designing the symmetric auctions. A transformer-based neural network architecture, CITransNet, catering for the contextual auctions is investigate by Duan et al. [3]. Ivanov et al. [26] modify the loss function and propose the RegretFormer architecture based on attention layers. Most existing results of AMD are under the assumptions that one item is allocated to at most one bidder and the bids on different items and from different bidders are independent, which are not suitable for joint auction, since one item is assigned to a bundle (consisting of two bidders) and the bids of different bundles are dependent. Additionally, in the same joint auction scenario, the joint auction relationship varies across different search queries, making it difficult for the existing AMD architecture to be applied to joint auction 3 scenarios. Our proposed architecture, JRegNet, can overcome these difficulties and generate the near-optimal revenue in the joint auction.", "2 Joint Auction Design": "In this section, we first formally introduce the model of joint auction. Then, we transfer the optimal mechanism design problem into a learning problem.", "2.1 Joint Auction": "In the context of joint auctions, whenever a user searches for a keyword, the platform returns an interface containing K advertising slots, where each advertising slot displays the information of an advertising bundle consisting of a store and a brand. Denote by \u03b1 k the CTR of the k -th slot for any k \u2208 { 1 , . . . , K } . W.l.o.g., we assume that 1 > \u03b1 1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b1 K > 0 . There are m stores and n brands participating in a joint auction. For store i \u2208 M = { 1 , . . . , m } and brand j \u2208 N = { 1 , . . . , n } , we define an indicator, 1 ij , to represent whether store i and brand j can form a bundle. Specifically, 1 ij = 1 means that store i and brand j have a cooperative relationship. Moreover, we use a matrix to demonstrate the bundle relationship between all stores and all brands, where each entity is an indicator defined on a pair of store and brand (see Figure 2 as an example). Within the same joint auction scenario, the joint auction relationship in the matrix varies across different search query samples. Figure 2: The bundle relationship between stores and brands. We use different beverage icons to represent different brands, and an edge linking a store and a brand means that there is a sales relationship between the two. For each store i (or brand j ), the value for each click is denoted by v i \u00b7 (or v \u00b7 j ) which is private information of store i (or brand j ). Suppose v i \u00b7 (or v \u00b7 j ) is drawn from a commonly known distribution over the domain set V i \u00b7 (or V \u00b7 j ). The value profile can be expressed as v = ( v 1 \u00b7 , . . . , v m \u00b7 , v \u00b7 1 , . . . , v \u00b7 n ) \u2208 V , where V = V 1 \u00b7 \u00d7\u00b7\u00b7\u00b7 \u00d7 V m \u00b7 \u00d7 V \u00b7 1 \u00d7\u00b7\u00b7\u00b7 \u00d7 V \u00b7 n . Additionally, we use v -i \u00b7 to stand for the value profile except for store i , i.e., v -i \u00b7 = ( v 1 \u00b7 , . . . , v ( i -1) \u00b7 , v ( i +1) \u00b7 , . . . , v \u00b7 n ) . The similar notations also can be defined on brand j . Since the store (or brand) may have a strategic bidding, we exploit b i \u00b7 (or b \u00b7 j ) to represent the bid price. Similarly, denote b , b -i \u00b7 and b \u00b7-j by the related bid profiles. In the setting of the joint auction, an auction mechanism, M ( g, p ) , comprises two parts: the allocation rule g and payment rule p . In detail, g = ( ( g i \u00b7 ) i \u2208 M , ( g \u00b7 j ) j \u2208 N ) where g i \u00b7 (or g \u00b7 j ) : V \u2192 R + \u222a { 0 } represents the expected CTR the store i (or brand j ) can derive. That is, g i \u00b7 ( b ) = \u2211 K k =1 s i \u00b7 k ( b ) \u03b1 k , where s i \u00b7 k ( b ) indicates the probability that store i is assigned to k -th slot. The payment rule p = ( ( p i \u00b7 ) i \u2208 M , ( p \u00b7 j ) j \u2208 N ) means that how much the store i or brand j needs to charge. Different from the conventional advertising auctions, in a joint auction, each slot can be allocated to at most one bundle, and each bundle can get at most one slot. In other words, one store and one brand form a bundle to be displayed in a slot. Each store i (or brand j ) aims to maximize its own utility, which is defined in a quasi-linear form, i.e., u i \u00b7 ( v i \u00b7 ; b ) = v i \u00b7 ( g i \u00b7 ( b )) -p i \u00b7 ( b ) , for all i \u2208 M (the utility of brand j has the similar form). In this paper, we focus on the mechanisms satisfying dominant strategy incentive compatibility and individual rationality. Roughly speaking, dominant strategy incentive compatibility guarantees that for any bidder, truthful bid can bring the maximum utility, i.e., Definition 1 (DSIC) A joint auction is dominant strategy incentive compatible if any store i 's (or brand j 's) utility is maximized by reporting truthfully no matter what the others report. In other words, it holds that    4 Individual rationality means that any participant of auction can get a non-negative utility, defined as follows, Definition 2 (IR) A joint auction is ex-post individually rational if any store i (or brand j ) receives a nonzero utility when participating truthfully, i.e.,  and  In a joint auction which is DSIC and IR, the expected revenue of platform is equal to the total payment of all stores and brands, defined as  where F is the joint distribution of all stores' and brands' values. Optimal joint auction design aims to find an auction mechanism that maximizes the expected revenue while satisfying the DSIC and IR conditions.", "2.2 Joint Auction Design as a Learning Problem": "We formulate the problem of designing the optimal joint auction as a learning problem. First, to satisfy the DSIC condition, we introduce the concept of ex-post regret. Fixed others' bids, the ex-post regret for store i (similar definition for brand j ) is the highest increase on utility that she can obtain by misreporting, i.e.,  Thus, an auction mechanism satisfies the DSIC condition if and only if rgt i \u00b7 ( v ) = 0 and rgt \u00b7 j ( v ) = 0 are fulfilled, for all stores and brands. Moreover, we can formulate the problem of designing the optimal joint auction as a constrained optimization:  where M is the set of all joint auction mechanisms that satisfy IR. Due to the complex constraints, this optimization is generally intractable to solve [18, 19]. To solve this optimization problem, we parameterize the auction mechanisms as M w ( g w , p w ) \u2286 M ( g, p ) through parameter w \u2208 R d (where d is the dimension of parameters w ). Then, we move to compute the mechanism M w ( g w , p w ) which maximizes the expected revenue: E v \u223c F [ \u2211 m i =1 p w i \u00b7 ( v ) + \u2211 n j =1 p w \u00b7 j ( v )] and satisfies DSIC and IR, by optimizing the parameters w . Given a sample L , consisting of L value profiles drawn from the joint distribution F , the empirical ex-post regret of store i (similar notation for brand j ) for the mechanism M w ( g w , p w ) is estimated by:  Rewrite the constrained optimization (1) into:  In addition, through the network architecture, we ensure the IR condition and we will describe it in detail in Section 3.1. 5 Allocation  Network Figure 3: The JRegNet architecture, including the allocation network part and the payment network part, for a setting of m stores, n brands, K advertising slots, and Q bundles of stores and brands. ' reshape reshape Store bidders Brand bidders Advertising slots Getting the input of JRegNet Input layer Computing the allocation probability matrix A of bundles Hidden layer Conversion of A to S Payment  Network Input layer Hidden layer Output layer Computing the payment matrix P Output layer Output Output C R Joint relationship graph Joint relationship graph", "3 JRegNet": "In this section, after modeling the optimal joint auction design as a learning problem, we propose a neural network architecture called J oint Reg ret Net work (JRegNet) for the optimal joint auction design.", "3.1 The JRegNet Architecture": "In this subsection, we introduce the network architecture of JRegNet designed for joint auction settings, as shown in Figure 3. The JRegNet architecture consists of two parts: the allocation network encoding allocation rules and the payment network encoding payment rules. The outputs of both networks are used to calculate the utility of bidders, the revenue, the regret, and the loss function value of the entire network. Next, we introduce the main components of JRegNet in detail. Input and output of JRegNet. In JRegNet, the neural network takes 1 ij , e i \u00b7 k and e \u00b7 jk as inputs, defined as:  where e i \u00b7 k and e \u00b7 jk represent the expected bid for store i and brand j on slot k , respectively. { 1 ij } i \u2208 M,j \u2208 N represents the joint relationship matrix between stores and brands. Because the joint relationship varies across different search request samples, even under the same joint auction scenario (i.e., the same stores and brands), JRegNet takes { 1 ij } i \u2208 M,j \u2208 N as one of the inputs. In JRegNet, to adapt to the joint auction settings, the allocation network firstly outputs the allocation probability matrix A of bundles. Matrix A contains the allocation probabilities for each bundle consisting of a store and a brand on each advertising slot. After obtaining the matrix A , the allocation network then outputs the corresponding allocation probability matrix S of bidders. Matrix S contains the allocation probabilities for each store and brand on each advertising slot. Afterwards, the payment network outputs the payment matrix P of all bidders. Matrix P contains the payments of each store bidder and brand bidder. Computing the allocation probability matrix A of bundles. To calculate the allocation probability matrix S of bidders, we need to firstly calculate the allocation probability matrix A of bundles, because we need to implement several constraints through A . And then, S is computed based on A . Besides, matrix A can be used to allocate advertising slots to bundles consisting of stores and brands. The input for this part is matrices C and R obtained through forward propagation, as shown by the yellow and purple neurons in Figure 3. We use Q to represent the total number of bundles, where bundle q \u2208 { 1 , . . . , Q } corresponds to a row-by-row traversal of all the entries in the matrix consisting of 1 ij , with the bundle corresponding to the q -th entry with value 1 . The allocation probability of bundle q on slot k in the allocation probability matrix A is defined as a qk . In our model, the output matrix A must satisfy 6 two constraints: (a), each advertising slot can only be assigned to one bundle, i.e., \u2211 Q q =1 a qk \u2264 1 , \u2200 k \u2208 { 1 , ..., K } , and (b), each bundle can be assigned to at most one advertising slot, i.e., \u2211 K k =1 a qk \u2264 1 , \u2200 q \u2208 { 1 , ..., Q } . We enforce constraints (a) and (b) by designing the allocation probability matrix A as a bi-stochastic matrix. To construct A as a bi-stochastic matrix, some operations are required on C and R . The matrix C is column-normalized to matrix \u02dc C and the matrix R is row-normalized to matrix \u02dc R , where the normalization is achieved by the softmax function. Because in joint auction settings, a specific bundle consisting of a store and a brand may not be assigned to any advertising slot, we include a dummy neuron for each bundle in the normalization of the softmax function. The outputs of dummy neurons represent the probabilities of bundles not being assigned any advertising slot. The entries in matrices \u02dc C and \u02dc R are defined as \u02dc c qk and \u02dc r qk , respectively. Then, we compute allocation probability of each bundle in the matrix A by: a qk = min { \u02dc c qk , \u02dc r qk } , \u2200 q \u2208 { 1 , ..., Q } , \u2200 k \u2208 { 1 , ..., K +1 } . According to Lemma 1, the matrix A constructed in this way is a bi-stochastic matrix. The specific formula for computing the allocation probability matrix A is:  where e c qk \u2211 Q t =1 e c tk and e r qk \u2211 K +1 t =1 e r qt are the row normalization of c qk and the column normalization of r qk , respectively, and the index K +1 corresponds to the situation where the bundle is not allocated any advertising slot. This part can be found in Figure 3. Lemma 1 ([1]) The matrix \u03c6 BS qk ( c, r ) is bi-stochastic \u2200 c, r \u2208 R QK . For any bi-stochastic matrix a \u2208 [0 , 1] QK , \u2203 c, r \u2208 R QK for which a = \u03c6 BS qk ( c, r ) . Conversion of allocation probability matrix A to allocation probability matrix S . In allocation probability matrix S of bidders, the allocation probabilities of store i and brand j on slot k are denoted as s i \u00b7 k and s \u00b7 jk , respectively. After obtaining the allocation probability matrix A of bundles, the allocation probabilities in S are computed through:  where Q i \u00b7 and Q \u00b7 j denote the sets of all bundles that include store i and brand j , respectively, i.e., the allocation probability of store i on slot k is the sum of the allocation probabilities on slot k for all bundles that include store i and the calculation of the allocation probability of brand j on slot k is similar to this. Computing the payment matrix P . After obtaining the allocation probability matrix S of bidders, we propagate it to the payment network to calculate the payment matrix P , as shown in Figure 3. In the matrix P \u2208 R I + J \u2265 0 , the first m rows and the last n rows represent the payments for stores and brands, respectively. The payments of store i and brand j in P are denoted as p i \u00b7 and p \u00b7 j , respectively. The payments in P are computed through:  where \u02dc p i \u00b7 \u2208 [0 , 1] and \u02dc p \u00b7 j \u2208 [0 , 1] are the normalization factors calculated by the sigmoid function, as shown by the green squares in Figure 3. Under the condition of satisfying DSIC, because of \u02dc p i \u00b7 \u2208 [0 , 1] , and the utility of the store u i \u00b7 = \u2211 K k =1 s i \u00b7 k e i \u00b7 k -p i \u00b7 , the utility of each store must be greater than or equal to zero, satisfying IR. The proof of IR for brand is similar.", "3.2 Training of JRegNet": "In this subsection, we introduce the training process of JRegNet, which includes converting the objective function, dividing training samples, finding the optimal misreports, and updating the model parameters and the Lagrange multipliers. Conversion from constrained optimization problems to unconstrained optimization problems. In order to train JRegNet, we firstly need an objective function. The constrained optimization objective is shown in (3). We use the augmented Lagrangian method over the space of the neural network parameters w to transform the constrained optimization problem into an unconstrained optimization problem, so that we can obtain the following objective 7 function:  where \u03bb \u2208 R n is the Lagrange multiplier and \u03c1 > 0 is the penalty factor. After obtaining the objective function required for training, we need to divide the samples in order to train JRegNet. Division of training samples. The training sample L is randomly divided into minibatches of size B . We use T to represent the total number of iterations. For each iteration t \u2208 { 1 , ..., T } , we sample a minibatch L t , which is denoted by L t = { v (1) , . . . , v ( B ) } and input it into JRegNet for training, until all the minibatches in this partition have been used. Afterwards, the training samples L are randomly partitioned again into new minibatches of size B . The above process is repeated until all iterations have been completed. Afterwards, during training, to calculate the regret in C \u03c1 ( w ; \u03bb ) , we need to find the optimal misreports that maximize the regret. Finding the optimal misreports. To calculate the optimal misreports, we use the method of gradient ascent. For each minibatch, the misreport v \u2032 ( \u2113 ) i \u00b7 or v \u2032 ( \u2113 ) \u00b7 j is calculated, for each store i or brand j and each valuation profile \u2113 , by taking multiple gradient ascents, and all the misreports are maintained to initialize the misreports in the next epoch. The formula for gradient ascent is as follows:  for some \u03b3 > 0 . Updating the model parameters and the Lagrange multipliers. Afterwards, we can calculate the value of C \u03c1 ( w ; \u03bb ) to perform backpropagation and update the neural network parameters w to minimize C \u03c1 ( w ; \u03bb ) . In addition, during the training process of the model, the update of the Lagrange multipliers is performed every fixed number of iterations:  where t represents the number of iterations, and \u02dc rgt i \u00b7 ( w ) and \u02dc rgt \u00b7 j ( w ) represents the empirical regret calculated on the minibatch S t based on (2). The updates of the model parameters and the Lagrange multipliers are performed alternately. The specific and complete algorithm process of training JRegNet is shown in Algorithm 1 . For fixed \u03bb t , the gradient of C \u03c1 w.r.t. w can be written as:  8", "Algorithm 1 JRegNet Training": "1: Input: Minibatches L 1 , ..., L T of size B 2: Parameters: \u2200 t \u2208 { 1 , . . . , T } , \u03c1 t > 0 , \u03b3 > 0 , \u03b7 > 0 , \u0393 \u2208 N , T \u2208 N , H \u2208 N 3: Initialize: w 0 \u2208 R d , \u03bb 0 \u2208 R m + n 4: for t = 0 to T do 5: Receive minibatch L t = { v (1) , . . . , v ( B ) } 6: Initialize misreport v \u2032 ( \u2113 ) i \u00b7 \u2208 V i \u00b7 , v \u2032 ( \u2113 ) \u00b7 j \u2208 V \u00b7 j , \u2200 \u2113 \u2208 { 1 , . . . , B } , i \u2208 M,j \u2208 N 7: for r = 0 to \u0393 do 8: \u2200 \u2113 \u2208 { 1 , . . . , B } , i \u2208 M,j \u2208 N : 9: v \u2032 ( \u2113 ) i \u00b7 = v \u2032 ( \u2113 ) i \u00b7 + \u03b3 \u2207 v \u2032 i \u00b7 [ u w i \u00b7 ( v ( \u2113 ) i \u00b7 ; ( v \u2032 i \u00b7 , v ( \u2113 ) -i \u00b7 ))]\u2223 \u2223 \u2223 v \u2032 i \u00b7 = v \u2032 ( \u2113 ) i \u00b7 10: v \u2032 ( \u2113 ) \u00b7 j = v \u2032 ( \u2113 ) \u00b7 j + \u03b3 \u2207 v \u2032 \u00b7 j [ u w \u00b7 j ( v ( \u2113 ) \u00b7 j ; ( v \u2032 \u00b7 j , v ( \u2113 ) -\u00b7 j ))]\u2223 \u2223 \u2223 v \u2032 \u00b7 j = v \u2032 ( \u2113 ) \u00b7 j 11: end for 12: Compute regret gradient: \u2200 \u2113 \u2208 [ B ] , i \u2208 M,j \u2208 N : 13: g t \u2113,i \u00b7 = \u2207 w [ u w i \u00b7 ( v ( \u2113 ) i \u00b7 ; ( v \u2032 ( \u2113 ) i \u00b7 , v ( \u2113 ) -i \u00b7 )) -u w i \u00b7 ( v ( \u2113 ) i \u00b7 ; v ( \u2113 ) )]\u2223 \u2223 \u2223 w = w t 14: g t \u2113, \u00b7 j = \u2207 w [ u w \u00b7 j ( v ( \u2113 ) \u00b7 j ; ( v \u2032 ( \u2113 ) \u00b7 j , v ( \u2113 ) -\u00b7 j )) -u w \u00b7 j ( v ( \u2113 ) \u00b7 j ; v ( \u2113 ) )]\u2223 \u2223 \u2223 w = w t 15: Compute Lagrangian gradient using Formula (5) and update w t : 16: w t +1 \u2190 w t -\u03b7 \u2207 w C \u03c1 t ( w t , \u03bb t ) 17: Update Lagrange multipliers once in H iterations: 18: if t is a multiple of H then 19: \u03bb t +1 i \u00b7 \u2190 \u03bb t i \u00b7 + \u03c1 t \u02dc rgt i \u00b7 ( w t +1 ) , \u2200 i \u2208 M 20: \u03bb t +1 \u00b7 j \u2190 \u03bb t \u00b7 j + \u03c1 t \u02dc rgt \u00b7 j ( w t +1 ) , \u2200 j \u2208 N 21: else 22: \u03bb t +1 \u2190 \u03bb t 23: end if 24: end for where", "4 Experiments": "In this section, empirical experiments are conducted in order to evaluate the joint advertising model and the effectiveness of JRegNet. Our experiments are run on a compute cluster with NVIDIA Graphics Processing Unit (GPU) cores. Baseline methods. We compare JRegNet with the following baselines: \u00b7 RegretNet [1], a neural network architecture for near DSIC mechanism design in the traditional advertising auction settings (only include stroes) which can achieve the optimal revenue. \u00b7 I ndependent Reg ret Net work (IRegNet), a neural network architecture for the optimal mechanism design in the advertising auction settings, where both stores and brands are independent candidates, competing for different slots. I.e., each slot either displays a store advertising or a brand advertising (this scenario does not exist in reality). \u00b7 VCG [6, 7, 8], a classic mechanism satisfying DSIC and IR. In our experiments, we apply VCG mechanism to the joint advertising settings directly. Note that, while implementing the RegretNet, the bidders are only the stores. For other mechanisms, the bidders are both stores and brands. Other attributes for all mechanisms are the same. 9 Table 1: The results of experiments for Settings A to D. ' \u2020 ' indicates that the revenue has a significant improvement over other methods in paired t-test at p < 0 . 05 level. Table 2: The results of experiments for different value distributions. The setting is 3 stores and 5 brands with 3 advertising slots. ' \u2020 ' indicates that the revenue has a significant improvement over other methods in paired t-test at p < 0 . 05 level. Evaluation. To assess the performance of each method, we utilize the average empirical ex-post regret of all bidders: \u0302 rgt := 1 n + m ( \u2211 m i =1 \u0302 rgt i \u00b7 + \u2211 n j =1 \u0302 rgt \u00b7 j ) , the empirical revenue: rev := 1 L \u2211 L \u2113 =1 [ \u2211 m i =1 p w i \u00b7 ( v ( \u2113 ) ) + \u2211 n j =1 p w \u00b7 j ( v ( \u2113 ) ) ] and the empirical social welfare: sw := 1 L \u2211 L \u2113 =1 ( \u2211 m i =1 \u2211 K k =1 s ( \u2113 ) i \u00b7 k \u03b1 k v ( \u2113 ) i \u00b7 + \u2211 n j =1 \u2211 k k =1 s ( \u2113 ) \u00b7 jk \u03b1 k v ( \u2113 ) \u00b7 j ) .", "4.1 Synthetic Data": "Implementation details. We create training sets and the test sets for each experiment. In the training sets, the size B of minibatches is 128 and the numbers of minibatches and iterations are 5000 and 200000 , respectively. In the test sets, the size of minibatches is 128 and the number of minibatches is 100 . Thus, the size of the training sample L is 640000 and the size of the testing sample is 12800 . In all synthetic data experiments, the joint relationship matrix { 1 ij } i \u2208 M,j \u2208 N between stores and brands is randomly generated for each search request sample. Furthermore, within a search request sample, if a store (or brand) has no joint relationship with any brand (or store), we default its bid to be zero, since such stores (or brands) cannot form a bundle to be displayed. In the testing of JRegNet, for each bidder, we perform gradient ascent on the 100 different initial misreports for 2000 iterations, to obtain 100 empirical regrets. Then, we take the maximum value of these 100 empirical regrets as the empirical regret of this bidder. All experimental results of RegretNet, IRegNet and JRegNet are obtained by taking the average of 3 different runs on the testing samples. Table 3: The results of experiments for real data on the test set. In experiments E1 to E3, we train JRegNet using the data from Figures 5(a) to 5(c) respectively, and test JRegNet and the baseline methods using the corresponding data in Figure 6. There are two groups of experimental data in JRegNet section, where the first row with a very small ru is compared with VCG, and the second row is used for comparison with GSP. ' \u2020 ' indicates that the revenue has a significant improvement over other methods in paired t-test at p < 0 . 05 level. 10 The comparison between JRegNet and the baselines. To demonstrate the superiority of the joint advertising model and the effectiveness of JRegNet, we conduct comparative experiments between mechanism generated by JRegNet and baselines in the following settings: (A) 3 stores, 4 brands and 1 advertising slot with CTR \u03b1 = (0 . 7) . The value of each store and brand is independently drawn from U [0 , 1] . (B) 3 stores, 5 brands and 3 advertising slots with CTRs \u03b1 = (0 . 5 , 0 . 3 , 0 . 15) . The value of each store and brand is independently drawn from U [0 , 1] . (C) 3 stores, 5 brands and 5 advertising slots with CTRs \u03b1 = (0 . 5 , 0 . 3 , 0 . 15 , 0 . 1 , 0 . 03) . The value of each store and brand is independently drawn from U [0 , 1] . (D) 4 stores, 5 brands and 3 advertising slots with CTRs \u03b1 = (0 . 5 , 0 . 3 , 0 . 15) . The value of each store and brand is independently drawn from U [0 , 1] . We show the results of experiments under Settings A to D in Table 1. First, regarding to all mechanisms generated by the methods of automated mechanism design, the mechanisms generated by JRegNet achieve a significantly higher revenue, compared to the mechanisms generated by RegretNet and IRegNet, when the regret is less than 0.001. This demonstrates that the joint advertising model can derive more revenue than the traditional model. Additionally, as shown in Table 1, if we focus on the joint advertising settings, we can find that JRegNet can also obtain a significantly higher revenue than VCG, despite experiencing a measure of social welfare loss. It implies, for revenue, the good performance of mechanisms generated by JRegNet and the effectiveness of JRegNet. To further evaluate the joint advertising model and the performance of JRegNet, we execute the experiments under the setting with different distributions, CTRs and numbers of advertisement slots. Different value distributions . To verify the performance and stability of our proposed mechanism under different value distributions, we repeat experiments in three different value distributions on setting B. The value of each store and brand is independently drawn from three different distributions: Uniform distribution, U [0 , 1] ; Normal distribution N (0 . 5 , 0 . 0256) and v \u2208 [0 , 1] ; Lognormal distribution LN (0 . 1 , 1 . 44) and v \u2208 [0 , 1] . It can be seen from the experimental results in Table 2 that for three different value distributions, JRegNet achieves the significantly highest revenue, compared to RegretNet, IRegNet and VCG mechanism. This demonstrates the stability and robustness of our mechanisms, when facing the different value distributions. Different CTRs of slots . Based on the Setting B, we adjust the values of CTRs and repeatedly run all mechanisms in the following three settings: B1. CTRs, \u03b1 = (0 . 4 , 0 . 3 , 0 . 15) ; B2. CTRs, \u03b1 = (0 . 5 , 0 . 4 , 0 . 15) ; B3. CTRs, \u03b1 = (0 . 5 , 0 . 4 , 0 . 25) . The experimental results are shown in Figure 4(a) of Appendix A.1. From Figure 4 (a), it can be seen that as the CTR increases, the JRegNet's revenue also increases, and the impact of changing the CTR on the first slot is greater than the impact of changing the CTR on the last slot. For all settings in Figure 4 (a), the mechanisms generated by JRegNet all realize the highest revenue. This proves that even for different CTRs, JRegNet can still generate mechanisms that perform well in the joint advertising settings. Different numbers of advertisement slots . Based on the Setting D, we increase or decrease the number of advertisement slots to evaluate the performance of our mechanisms. The adjusted settings are described as follows: Setting D1. two advertising slots with CTRs \u03b1 = (0 . 5 , 0 . 3) ; Setting D2. four advertising slots with CTRs \u03b1 = (0 . 5 , 0 . 3 , 0 . 15 , 0 . 1) ; Setting D3. five advertising slots with CTRs \u03b1 = (0 . 5 , 0 . 3 , 0 . 15 , 0 . 1 , 0 . 03) . The curves of revenue as the number of advertising slots changing are shown in Figure 4 (b) of Appendix A.1. From Figure 4 (b), the mechanisms generated by JRegNet perform best among all mechanisms, which demonstrates that our mechanism can be suitable for the settings with different numbers of slots. In addition, the revenue derived by our mechanism strictly increases, with the increase on the number of slots. This phenomenon is not appeared on all mechanisms we execute.", "4.2 Real-World Dataset": "We train and evaluate our model using real online auction log data from Meituan, an e-commerce platform. Each user search query corresponds to one advertising auction. In the real scenario, after the recall, rough ranking, and fine ranking stages, the advertising system selects about 10 bundles as auction candidates and picks up at most 10 stores and at most 10 brands from candidates. Then, platform runs an auction mechanism to allocate at most 5 slots. This is the real scenario currently being tested by the online e-commerce platform. Therefore, we focus on the setting with 10 stores, 10 brands and 5 slots. Due to the variable number of advertisers in the original data, we trim or pad some data records to ensure that we have 10 stores and 10 brands in each auction sample. The padding operation is accomplished 11 by adding advertisers whose value per click is 0 . We use 6912 real samples for testing. The log data features we use primarily include the bid and id of each store and brand, the joint relationship between stores and brands, and the predicted CTR of each advertising slot. In addition, using regret alone to measure DSIC is no longer applicable due to the different value function distributions between real data and simulated data. For the real joint auction data and our simulated data, there is a significant difference in the magnitude of bidders' utilities. If we simply use a fixed regret threshold, it will not accurately measure the degree of DSIC. Therefore, we use ru := 1 L \u2211 L \u2113 =1 [( \u2211 m i =1 rgt ( \u2113 ) i \u00b7 + \u2211 n j =1 rgt ( \u2113 ) \u00b7 j ) / ( \u2211 m i =1 \u00b5 ( \u2113 ) i \u00b7 + \u2211 n j =1 \u00b5 ( \u2113 ) \u00b7 j )] to assess DSIC where rgt i \u00b7 := max v \u2032 i \u00b7 \u2208 V i \u00b7 u i \u00b7 ( v i \u00b7 ; ( v \u2032 i \u00b7 , v -i \u00b7 )) -u i \u00b7 ( v i \u00b7 ; v ) (similar definition for brand j ). The meaning of ru is the ratio of utility growth obtained from misreporting to the utility from reporting truthfully. For GSP, we employ an enumeration method [27] during the testing phase to get the misreport, where the misreport is equal to \u03b2v , with \u03b2 \u2208 { 0 , 0 . 1 , 0 . 2 , 0 . 3 , . . . , 1 . 9 } . We use the misreport that can obtain the maximum utility from this set to calculate the GSP's regret. We train JRegNet with real data from three distinct time periods, and for each time period, we test them using data from three different time points respectively. In Figures 5 and 6 of Appendix A.2, we show the detailed number of real search query samples used for training and testing JRegNet. In Table 3, we present the performance of JRegNet, VCG, and GSP on the test set across all real data experiments, where GSP is the mechanism initially used online by Meituan in the joint auction scenario. From Table 3, it can be seen that in all real data experiments, JRegNet can achieve significantly higher revenue than VCG at a very small ru and JRegNet can also achieve significantly higher revenue than GSP when their ru are close, and all the paired t-test is at the level of p < 0 . 05 . These real data experiments demonstrate the effectiveness of JRegNet in real joint auction scenarios. Furthermore, it should be noted that GSP does not satisfy DSIC, which leads to increased complexity in advertisers' bidding strategies and difficulty to predict the equilibrium of GSP in joint auction scenario. In the experiment, the revenue of the GSP is calculated under the assumption that advertisers bid truthfully. However, in the equilibrium state, advertisers' bids are lower than their true values. Therefore, the actual revenue of GSP is lower than what is presented in the table. But since the equilibrium of GSP in the joint auction scenario is difficult to calculate, we choose to use an upper bound. JRegNet can work in real-world scenarios. If we use a server with 1 GPU and 32 CPUs to run this real setting, it will take about 2 to 4 hours to run 30000 iterations of training. However, the training of the JRegNet architecture can be performed offline. Therefore, although offline training may be relatively slow, invoking the trained JRegNet neural network model for forward propagation to obtain allocation and payment matrices is very fast (around 1 to 3 milliseconds per search query sample), which does not affect online deployment and decision-making.", "5 Conclusions": "In this paper, we propose a novel joint advertising model where stores and brands participate in a joint auction to jointly bid for advertising slots. To design the optimal joint auction mechanisms satisfying DSIC and IR, we propose JRegNet, a neural network architecture for generating the optimal joint auctions. We also conduct multiple experiments to demonstrate the superiority of mechanisms generated by JRegNet, compared to the baselines. In the future, an interesting direction is to apply joint auction to other scenarios such as live streaming sales, different brand collaborations, and so on.", "Acknowledgments": "This work was supported by the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China (No. 22XNKJ07), Meituan Inc. Fund, and Major Innovation & Planning Interdisciplinary Platform for the 'Double-First Class' Initiative, Renmin University of China. The work was partially done at Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education, Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artificial Intelligence, Renmin University of China. We would like to thank Wanzhi Zhang for helpful comments and discussions. 12", "References": "[1] Paul D\u00fctting, Zhe Feng, Harikrishna Narasimhan, David Parkes, and Sai Srivatsa Ravindranath. Optimal auctions through deep learning. In International Conference on Machine Learning , pages 1706-1715. PMLR, 2019. [2] Jad Rahme, Samy Jelassi, Joan Bruna, and S Matthew Weinberg. A permutation-equivariant neural network architecture for auction design. In Proceedings of the AAAI conference on artificial intelligence , volume 35, pages 5664-5672, 2021. [3] Zhijian Duan, Jingwu Tang, Yutong Yin, Zhe Feng, Xiang Yan, Manzil Zaheer, and Xiaotie Deng. A contextintegrated transformer-based neural network for auction design. In International Conference on Machine Learning , pages 5609-5626. PMLR, 2022. [4] Zhe Feng, Harikrishna Narasimhan, and David C Parkes. Deep learning for revenue-optimal auctions with budgets. In Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems , pages 354-362, 2018. [5] Roger B Myerson. Optimal auction design. Mathematics of operations research , 6(1):58-73, 1981. [6] William Vickrey. Counterspeculation, auctions, and competitive sealed tenders. The Journal of finance , 16(1):8-37, 1961. [7] Edward H Clarke. Multipart pricing of public goods. Public choice , pages 17-33, 1971. [8] Theodore Groves. Incentives in teams. Econometrica: Journal of the Econometric Society , pages 617-631, 1973. [9] Hal R Varian. Position auctions. international Journal of industrial Organization , 25(6):1163-1178, 2007. [10] Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz. Internet advertising and the generalized secondprice auction: Selling billions of dollars worth of keywords. American economic review , 97(1):242-259, 2007. [11] Ioannis Caragiannis, Christos Kaklamanis, Panagiotis Kanellopoulos, and Maria Kyropoulou. On the efficiency of equilibria in generalized second price auctions. In Proceedings of the 12th ACM conference on Electronic commerce , pages 81-90, 2011. [12] Renato D Gomes and Kane S Sweeney. Bayes-nash equilibria of the generalized second price auction. In Proceedings of the 10th ACM conference on Electronic Commerce , pages 107-108, 2009. [13] Brendan Lucier, Renato Paes Leme, and \u00c9va Tardos. On revenue in the generalized second price auction. In Proceedings of the 21st international conference on World Wide Web , pages 361-370, 2012. [14] S\u00e9bastien Lahaie and David M Pennock. Revenue analysis of a family of ranking rules for keyword auctions. In Proceedings of the 8th ACM Conference on Electronic Commerce , pages 50-56, 2007. [15] David RM Thompson and Kevin Leyton-Brown. Revenue optimization in the generalized second-price auction. In Proceedings of the fourteenth ACM conference on Electronic commerce , pages 837-852, 2013. [16] Ben Roberts, Dinan Gunawardena, Ian A Kash, and Peter Key. Ranking and tradeoffs in sponsored search auctions. ACM Transactions on Economics and Computation (TEAC) , 4(3):1-21, 2016. [17] Denis Charles, Nikhil R Devanur, and Balasubramanian Sivan. Multi-score position auctions. In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining , pages 417-425, 2016. [18] Vincent Conitzer and Tuomas Sandholm. Complexity of mechanism design. In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence , pages 103-110, 2002. [19] Vincent Conitzer and Tuomas Sandholm. Self-interested automated mechanism design and implications for optimal combinatorial auctions. In Proceedings of the 5th ACM Conference on Electronic Commerce , pages 132-141, 2004. [20] Tuomas Sandholm and Anton Likhodedov. Automated design of revenue-maximizing combinatorial auctions. Operations Research , 63(5):1000-1025, 2015. [21] Maria-Florina Balcan, Avrim Blum, Jason D Hartline, and Yishay Mansour. Reducing mechanism design to algorithm design via machine learning. Journal of Computer and System Sciences , 74(8):1245-1270, 2008. [22] S\u00e9bastien Lahaie. A kernel-based iterative combinatorial auction. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 25, pages 695-700, 2011. [23] Paul D\u00fctting, Felix Fischer, Pichayut Jirapinyo, John K Lai, Benjamin Lubin, and David C Parkes. Payment rules through discriminant-based classifiers, 2015. [24] Kevin Kuo, Anthony Ostuni, Elizabeth Horishny, Michael J Curry, Samuel Dooley, Ping-yeh Chiang, Tom Goldstein, and John P Dickerson. Proportionnet: Balancing fairness and revenue for auction design with deep learning. arXiv preprint arXiv:2010.06398 , 2020. 13 [25] Neehar Peri, Michael Curry, Samuel Dooley, and John Dickerson. Preferencenet: Encoding human preferences in auction design with deep learning. Advances in Neural Information Processing Systems , 34:17532-17542, 2021. [26] Dmitry Ivanov, Iskander Safiulin, Igor Filippov, and Ksenia Balabaeva. Optimal-er auctions through attention. Advances in Neural Information Processing Systems , 35:34734-34747, 2022. [27] Guogang Liao, Xuejian Li, Ze Wang, Fan Yang, Muzhi Guan, Bingqi Zhu, Yongkang Wang, Xingxing Wang, and Dong Wang. Nma: Neural multi-slot auctions with externalities for online advertising. arXiv preprint arXiv:2205.10018 , 2022. 14", "A Additional Experimental Information": "", "A.1 Detailed Experiment Results for Different CTRs and Different Numbers of Advertisement Slots.": "The results of experiments on the settings of different CTRs and different numbers of advertisement slots are shown in Figure 4. The detailed information about the experiment results is shown in Table 4 and Table 5, respectively. Figure 4: The results of experiments on the settings of different CTRs (a) and different numbers of advertisement slots (b). In all results, the regrets are less than 0.001. (a) (b) RegretNet IRegNet JRegNet 0.8 1 [0.4,0.3,0.15] [0.5,0.3,0.15] [0.5,0.4,0.25] CTRs RegretNet IRegNet VCG JRegNet 0.6 1 0.5 0.4 Number of advertising slots", "A.2 Real-World Data Experiment": "The number of real search query samples used for training JRegNet is shown in Figure 5, and the number used for testing is shown in Figure 6. 15 Figure 5: The number of real search query samples used for training JRegNet. The horizontal axis is the generation time of the search query samples. In this figure, all search query samples are obtained from logs of the year 2023. Within settings E1 to E3, we use data from (a) to (c) to train JRegNet, respectively. (a) (b) (c) Train 0913-0923 70000 60000 f 50000 40000 30000 09.13 09.14 09.15 09.16 09.17 09.18 09.19 09.20 09.21 09.22 09.23 Date 74000 Train 1015-1017 73000 72000 1 71000 70000 69000 68000 67000 66000 10.15 10.16 10.17 Date 109000 108000 107000 106000 5 105000 1 104000 103000 11.24 11.25 Date 16 Table 4: The results of experiments for different CTRs (Settings B1, B, B2 and B3). ' \u2020 ' indicates that the revenue has a significant improvement over other methods in paired t-test at p < 0 . 05 level. Table 5: The results of experiments for different number of advertising slots (Settings D1, D, D2 and D3). ' \u2020 ' indicates that the revenue has a significant improvement over other methods in paired t-test at p < 0 . 05 level. Figure 6: The number of real search query samples used for testing JRegNet and baseline methods. In this figure, all search query samples are obtained from logs of the year 2023. For settings E1 to E3, we take three days of real data for testing, respectively. 09.24 09.25 09.26 10.18 10.19 10.20 11.26 11.27 11.28 Date 2200 2250 2300 2350 2400 Number of search query samples Test E1 Test E2 Test E3 17"}
