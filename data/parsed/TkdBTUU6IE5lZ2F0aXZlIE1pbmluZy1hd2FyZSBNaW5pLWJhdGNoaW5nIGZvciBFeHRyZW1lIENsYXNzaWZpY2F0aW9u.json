{"NGAME: Negative Mining-aware Mini-batching for Extreme Classification": "Kunal Dahiya \u2217\u2020 , Nilesh Gupta \u2217\u2021 , Deepak Saini \u2217\u00a7 , Akshay Soni \u00b6 , Yajun Wang \u00b6 , Kushal Dave \u00b6 , Jian Jiao \u00b6 , Gururaj K \u00b6 , Prasenjit Dey \u00b6 , Amit Singh \u00b6 , Deepesh Hada \u00a7 , Vidit Jain \u00a7 , Bhawna Paliwal \u00a7 , Anshul Mittal \u2020 , Sonu Mehta \u00a7 , Ramachandran Ramjee \u00a7 , Sumeet Agarwal \u2020 , Purushottam Kar \u2016 , Manik Varma \u00a7 July 12, 2022", "Abstract": "Extreme Classification (XC) seeks to tag data points with the most relevant subset of labels from an extremely large label set. Performing deep XC with dense, learnt representations for data points and labels has attracted much attention due to its superiority over earlier XC methods that used sparse, hand-crafted features. Negative mining techniques have emerged as a critical component of all deep XC methods that allow them to scale to millions of labels. However, despite recent advances, training deep XC models with large encoder architectures such as transformers remains challenging. This paper identifies that memory overheads of popular negative mining techniques often force mini-batch sizes to remain small and slow training down. In response, this paper introduces NGAME, a light-weight mini-batch creation technique that offers provably accurate in-batch negative samples. This allows training with larger mini-batches offering significantly faster convergence and higher accuracies than existing negative sampling techniques. NGAME was found to be up to 16% more accurate than state-of-the-art methods on a wide array of benchmark datasets for extreme classification, as well as 3% more accurate at retrieving search engine queries in response to a user webpage visit to show personalized ads. In live A/B tests on a popular search engine, NGAME yielded up to 23% gains in click-through-rates.", "1 Introduction": "Overview : Extreme Classification (XC) requires predicting the most relevant subset of labels for a data point from an extremely large set of labels. Note that multi-label classification generalizes multi-class classification which instead aims to predict a single label from a mutually exclusive label set. In recent years, XC has emerged as a workhorse for several real-world applications such as product recommendation [1-3], document tagging [4-6], search & advertisement [2,7,8], and query recommendation [6,9]. Challenges in XC : XC tasks give rise to peculiar challenges arising from application-specific demands as well as data characteristics. For instance, to adequately serve real-time applications such as ranking and recommendation, XC routines must offer millisecond-time inference, even if selecting among several millions \u2217 Authors contributed equally \u2020 IIT Delhi \u2021 UT Austin \u00a7 Microsoft Research \u00b6 Microsoft \u2016 IIT Kanpur Author Emails: kunalsdahiya@gmail.com, nileshgupta2797@utexas.edu, {desaini, akson, yajunw, kudave, jiajia, gururajk, prdey, siamit, deepeshhada, t-viditjain, bhawna, Sonu.Mehta, ramjee, manik}@microsoft.com, me@anshulmittal.org, sumeet@ee.iitd.ac.in, purushot@cse.iitk.ac.in 1 of labels. Training models at extreme scales is similarly challenging due to the infeasibility of training on all data point-label pairs when the number of data points and labels both are in the millions. This necessitates the use of some form of negative mining technique wherein a data point is trained only with respect to its positive labels (of which there are usually a few) and a small set of carefully chosen negative labels. Training is made even more challenging due to the abundance of rare or tail labels for which few, often less than 10, training points are available. It is common for a majority of labels to be rare in ranking, recommendation, and document tagging tasks. NGAME and Our Contributions : Despite these advances, training deep XC models based on transformer encoders poses steep time and memory overheads (see Sec. 3 for a detailed discussion). In particular, popular negative mining techniques often themselves pose memory overheads when used to train large encoder architectures like transformers. This forces mini-batch sizes to remain small leading to slow convergence and sub-optimal accuracies. As a remedy, existing methods either settle for simpler encoders such as bag-of-embeddings [10] or else forego the use of label text altogether in an attempt to speed up training [14-16], both of which result in sub-optimal accuracies. This paper develops the NGAME method for training large transformer-based Siamese architectures on XC tasks at the scale of 85 million labels. Training is performed in two stages: a pre-training stage first learns an effective Siamese architecture, followed by a fine-tuning stage that freezes the Siamese embeddings and learns a per-label refinement vector to fine-tune the label embeddings to obtain the final classifiers. To summarize, this paper makes the following contributions: Deep Siamese XC : The recent years have have seen XC models become more and more accurate with the development of two specific design choices. Firstly, XC methods identified the benefits of using label metadata in various forms such as textual descriptions of labels [3,6,10] or label correlation graphs [11,12]. This is in sharp contrast to earlier work in XC that treated labels as featureless identifiers (please see Section 2 for a survey). Secondly, XC methods began reaping the benefits of deep-learnt embeddings by using deep encoder architectures to embed both data points and labels. This was yet another departure from traditional XC methods that relied on sparse, hand-crafted features such as bag-of-words. In particular, recent work has demonstrated advantages of using Siamese architectures wherein representations for both data points as well as labels are obtained using a shared embedding model [3,10,13]. Some of these techniques [3,10] demonstrate further gains in accuracy by fine-tuning the label representation offered by the shared embedding architecture to yield label classifiers. Other methods such as [14] focus on training large transformer models at extreme scales. Together, these methods constitute the state-of-the-art across a wide range of retrieval and recommendation tasks. 1. It notices that existing techniques treat mini-batching and negative mining as unrelated problems and this leads to inefficient training. In response, the NGAME method proposes a light-weight negative mining-aware technique for creating mini-batches. A curious property of the mini-batches so obtained is that in-batch sampling itself starts offering informative negative samples and faster convergence than ANNS-based negative mining techniques such as ANCE (see Section 2). This leads to much more efficient training as in-batch sampling has much smaller time and memory overheads than ANNS-based methods. 2. The above allows NGAME to operate with much larger mini-batch sizes and allows training with 85 million labels on multiple GPUs without the need of a specialized algorithm to reduce inter-GPU communication. This becomes particularly critical when training large transformer-based feature architectures such as BERT which already restrict mini-batch sizes owing to their large memory footprint. 3. Theoretical analysis is performed where NGAME is shown to offer provably-accurate negative samples with bounded error under fairly general conditions. Furthermore, under some standard, simplifying assumptions, convergence to a first-order stationary point is also assured. 4. NGAME is shown to offer 16% more accurate predictions than state-of-the-art methods including BERTbased methods such as LightXML [17], XR-Transformers [14] on a wide array of benchmark datasets. Moreover, in live A/B tests on a popular search engine, NGAME yielded up to 23% gains in click-through-rates over an ensemble of state-of-the-art generative, XC, information retrieval (IR) and two-tower models. 2", "2 Related Work": "Extreme Classification (XC) : Early extreme classification algorithms focused primarily on designing accurate and scalable classifiers for either sparse bag-of-words [4,7,8,18-37] or dense pre-trained features obtained from feature extraction models such as CDSSM [9,38] or fastText [39]. More recent work demonstrated that learning task-specific features can lead to significant gains in classification accuracy. The use of diverse feature extraction architectures including Bag-of-embeddings [1,2,40,40-43], CNN [44-46], LSTM [5], and BERT [6,15,16, 47, 48] led to the development of deep extreme classification (deep XC) techniques that offered significant gains over techniques that either used Bag-of-Words or pre-trained features. However, these techniques did not make any use of label metadata and continued to treat labels as indices devoid of features. Still more recent work [2,3,6,11,12] introduced various techniques to exploit label metadata such as textual descriptions of labels or label correlation graphs. For instance, the X-Transformer [6] and XR-Transformer [14] methods make use of label text to clusters labels in their Semantic Label Indexing phase. However, these methods employ an ensemble making them expensive to scale to large datasets. Other techniques use label metadata to inform classifier learning more intimately via label text [3,10], images [49], or label correlation graphs [11,12]. In all cases, careful use of label metadata is shown to offer better classification accuracies, especially on data-scarce tail labels. Negative Mining : Training on a small but carefully selected set of irrelevant labels per training point is critical for accurately scaling XC algorithms. This is because training on all irrelevant labels for every data point becomes infeasible when the number of training points and labels are both in the millions. Negative mining techniques come in three flavours: Siamese methods for XC : Although widely studied in areas such as information retrieval [50] and computer vision [51], Siamese methods received attention in XC more recently due to their ability to incorporate label metadata in diverse forms. The key goal in Siamese classification is to learn embeddings of labels and data points such that data points and their positive labels are embedded in close proximity whereas data points and their negative labels maintain a discernible separation. DECAF [3] and ECLARE [11] use asymmetric networks to embed data points and labels whereas SiameseXML [10] and GalaXC [12] use a symmetric network. It is notable that with respect to the type of metadata used, DECAF and SiameseXML use only label text whereas ECLARE and GalaXC use label graphs. DECAF and ECLARE struggle to scale to tasks with over a million labels due to an expensive shortlisting step that seems critical for good performance. On the other hand, GalaXC requires pre-learnt embeddings for data points and labels from models such as Astec [2] that were trained on the same task. This combined with the execution of a multi-layer graph convolutional network makes the method more expensive. Of special interest to this paper is the SiameseXML technique that yields state-of-the-art accuracies on short-text datasets and can scale to 100M labels. However, to achieve such scales SiameseXML restricts itself to a low-capacity Bag-of-embeddings feature architecture and uses expensive but suboptimal negative sampling techniques discussed below. 1. Oblivious: these include random sampling wherein negative samples are either drawn uniformly or from token/unigram distributions [52], and in-batch sampling [10,40,51,53,54] wherein negative samples are identified among positive samples of other data points within the same mini-batch. These are computationally inexpensive but can offer uninformative negatives and slow convergence [50] (please see Fig. 2). 3. Task-aware: methods such as ANCE [50] and SiameseXML [10] retrieve hard negatives using features learnt for the task at hand. Since these features keep getting updated during training, an Approximate Nearest Neighbor Search (ANNS) index has to be recomputed every few epochs. Similarly, RocketQA [58] trains an expensive cross-encoder model to eliminate false negatives. Although these methods offer better negatives, they also add substantial time and memory overhead e.g., ANCE [50] recomputes an ANNS index over label embeddings every few training epochs which can be expensive even with asynchronous training. Moreover, these strategies are also more memory intensive as discussed in Section 3. 2. Feature-aware: these techniques identify hard negatives based on either sparse raw features such as BM25 [55,56] or features from a pre-trained model like BERT [57]. However, since these features are not necessarily aligned to the task, the negative samples may offer biased training. In contrast, NGAME provides a strategy for mining provably-accurate negatives similar to task-aware methods (see Theorem 1) but with overheads comparable to those of oblivious techniques. On multiple 3 Figure 1: (Left) NGAME's model architecture. Data points and labels are embedded using a Siamese encoder model and per-label 1-vs-all classifiers are learnt. Classifier and Siamese scores are combined to offer final output. (Right) A depiction of NGAME's negative mining strategy. A light dotted line indicates a data point-relevant label pair. NGAME misses only those hard negative labels for a data point that are not relevant to any other data point in its cluster. See also Figure 3b for a real example illustrating the hard-negatives retrieved by NGAME. Legend Data point Other points Positives of Hard negatives for      by NGAME Missed hard negatives for Cluster/mini-batch boundary Relevance link (b) \u2130\ud835\udf3d \u2130\ud835\udf3d \u0394\ud835\udc30\ud835\udc59 M1 M2 \ud835\udc30\ud835\udc59 \u2131 final score (a) Tarquinio Merula (24 Nov 1595 - 10 Dec 1665) was an Italian composer, organist, and violinist of the early Baroque era. Although mainly \u2026 Italian classical composers data point label datasets, the overhead of executing NGAME's negative mining technique was up to 1% as compared to 210% for ANCE (see Tab. 5) allowing NGAME to train with powerful feature architectures such as BERT at extreme scales and offer accuracies superior to leading oblivious, feature- and task-aware negative mining techniques.", "3 NGAME: NeGative Mining-Aware Mini-batching for Extreme Classification": "Notation : L is the total number of labels (note that the same set of labels is used during training and testing). x i , z l \u2208 X denote textual representations of data point i and label l respectively. The training set D := {{ x i , y i } N i =1 , { z l } L l =1 } consists of L labels and N data points. For a data point i \u2208 [ N ], y i \u2208 {-1 , +1 } L is its ground truth vector, i.e. , y il = +1 if label l is relevant/positive for the data point i and y il = -1 otherwise. Inference Pipeline : After training E \u03b8 , W , NGAME establishes a maximum inner product search (MIPS) [61] structure over the set of learnt label classifiers { w l } l \u2208 [ L ] . Given a test data point x t its embedding E \u03b8 ( x t ) is used to invoke the MIPS structure that returns labels l \u2208 [ L ] with highest classifier scores w /latticetop l E \u03b8 ( x t ). Mild gains were observed by fusing classifier scores with Siamese scores E \u03b8 ( z l ) /latticetop E \u03b8 ( x t ) for the shortlisted labels using a simple fusion architecture F and recommending labels in decreasing order of the fused scores (see Appendix C for details). Appendix D establishes that NGAME offers O ( b + D log L ) time inference. Architecture : NGAME uses an encoder model (DistilBERT base [59,60]) denoted by E \u03b8 : X \u2192 S D -1 , with trainable parameters \u03b8 , to embed both data point and label text onto the D -dimensional unit sphere S D -1 . NGAME also uses 1-vs-all classifiers W def = { w l } l \u2208 [ L ] where w l is the classifier for label l . Joint Training and its Limitations : The encoder parameters \u03b8 and 1-vs-all classifiers W = { w l } L i =1 could be trained by minimizing a triplet loss function  In the above expression, the indices l and k run over the relevant and irrelevant labels for data point i respectively with w l , w k being their label-wise classifiers. Consequently, (1) encourages every relevant label to get a score at least margin \u03b3 higher than every irrelevant label. Other task losses such as BCE or contrastive could also have been used. For an encoder model with b parameters, a training epoch w.r.t. L ( \u03b8, W ) would take \u2126 ( NL ( b + D ) log L ) = \u2126( NL ) time (since data points usually have O (log L ) relevant labels - see Tab. 1) which is prohibitive when 4 N and L are both in the millions. Joint training also requires storing both the encoder and classifier models in memory. Along with overheads of optimizers such as Adam, this forces mini-batches to be small and slows down convergence [58]. Common workarounds such as distributed training on a GPU cluster [62] impose overheads such as inter-GPU communication. NGAME instead proposes a novel two-pronged solution based on modular training and negative mining-aware mini-batching. Benefits of Modular Training : Modular training generalizes the Siamese encoder training strategy popular in literature and makes more effective use of label text during training that benefits rare labels with scant supervision. Splitting encoder and classifier training also eases memory overheads in each module - the parameters and gradients of the encoder and classifier are not required to be stored (in the GPU memory) or computed at the same time. Please refer to DeepXML [2] that discusses the benefits of modular training in great depth. Combined with memory savings offered by NGAME's negative sampling technique (discussed below), this enables NGAME to train with 1.5 \u00d7 larger mini-batches compared to approaches such as ANCE [50] thus offering faster convergence. For instance, NGAME could be trained within 50 hours on 6 \u00d7 A100 GPUs on the PR-85M XC task with more than 85 million labels. Modular Training : Recent papers [10,11] have postulated that label embeddings can serve as surrogates for label classifiers. To effect this intuition, NGAME reparameterizes label classifiers as w l = E \u03b8 ( z l )+ \u03b7 l , where E \u03b8 ( z l ) is the label embedding and \u03b7 l is a free D -dimensional residual vector. In module M1 , NGAME sets the residual vectors to zero, i.e., using w l = E \u03b8 ( z l ). Plugging this into (1) results in a loss expression that depends only on \u03b8 and encourages label-data point embedding similarity E \u03b8 ( x i ) /latticetop E \u03b8 ( z l ) to be high when y il = +1 and low otherwise. Note that this is identical to Siamese training for encoder models [10,40,50,51,53,54,57] and allows NGAME to perform supervised training of the encoder model \u03b8 by itself in M1. In module M2 , the learnt encoder model \u02c6 \u03b8 is frozen and residual vectors \u03b7 l are implicitly learnt by initializing w l = E \u02c6 \u03b8 ( z l ) and minimizing (1) with respect to W alone. Appendix C gives details of M1 and M2 implementation. Negative Mining and its Overheads : Modular training partly lowers the memory costs of training but not its \u2126 ( NL ) computational cost. To reduce computational costs to O ( N log L ), negative mining techniques are critical. These restrict training to only the positive/relevant labels for a data point (which are usually only O (log L ) many - see Tab. 1) and a small set of the hardest-to-classify negative/irrelevant labels for that data point. However, such techniques end up adding their own memory and computational overheads. For example, ANCE [50] queries an ANNS structure over label embeddings that needs to be periodically refreshed (since label embeddings get trained in parallel) while RocketQA [58] trains a cross-encoder to weed out false negatives; these add significant overhead to training. Computing label embeddings E \u03b8 ( z l ) for retrieved hard negatives is an additional overhead faced by all these methods. For a mini-batch of, say S training data points, if a single positive and H hard-negatives are chosen per data point, then a total of S ( H +2) embeddings need to be computed. For large transformer-based encoder models, this forces batch sizes S to be very small. A notable exception is in-batch negative mining [10,40,51,53,54,63] that looks for hard negatives of a data point only among positives of other data points in the same mini-batch and requires computing only 2 S embeddings instead of 2 S + HS embeddings. However, this technique may offer uninformative negatives and slow convergence [50] if mini-batches are created randomly. Negative Mining-aware Mini-batching : NGAME notes that the inexpensive in-batch sampling technique could have offered good-quality negatives had mini-batch creation encouraged hard-negatives of a data point to exist among positives of other data points in the mini-batch. Since Siamese training encourages embeddings of data points and related labels to be close i.e., \u2016E \u03b8 ( x i ) -E \u03b8 ( z l ) \u2016 2 /lessmuch 1 if y il = 1, if mini-batches are created out of data points that lie close to each other, triangle inequality ensures that in-batch negatives are of good quality. Figure 1 depicts this pictorially and Theorem 1 assures this formally. The resulting mini-batching and negative mining strategy is presented in Algorithm 1 and offers provably accurate hard-negatives at the low computational cost of in-batch negative mining. It also imposes very little compute overhead, requiring occasional data clustering that is inexpensive compared to ANNS creation or cross-encoder training. On a dataset with 85 million labels and 240 million training data points, refreshing ANCE's ANNS index and hard-negative retrieval for all training data points took 4 hours whereas NGAME took under an hour. ANCE needed to compute embeddings for hard negatives separately causing its GPU memory requirement for label embeddings to be at least 2 \u00d7 higher than NGAME that only needed to 5", "Algorithm 1 NGAME's negative mining strategy": "Input: Init model \u03b8 0 , mini-batch size S , cluster size C , refresh interval \u03c4 , hardness threshold r 1: for t = 0 , . . . , T do 2: if t % \u03c4 = 0 then // Redo clustering at regular intervals 3: Cluster current data point embeddings E \u03b8 t ( x i ) into /ceilingleft N/C /ceilingright clusters, with each cluster containing \u2248 C data points each. 4: end if 5: Choose /ceilingleft S/C /ceilingright random clusters to create a mini-batch S t of size S 6: Take positive labels P i + for each data point i \u2208 S t and let L t def = \u22c3 i \u2208 S t P i + 7: Compute E \u03b8 t -1 ( x i ) , E \u03b8 t -1 ( z l ) for all i \u2208 S t , l \u2208 L t . 8: Select hard-negatives for data point i \u2208 S t as { l \u2208 L t : \u2016E \u03b8 ( x i ) -E \u03b8 ( z l ) \u2016 2 \u2264 r } 9: Update \u03b8 t using mini-batch SGD over S t 10: end for compute embeddings for positive labels (since NGAME's negatives are sampled from the positives of other data points in the mini-batch). Thus, NGAME imposed a mere 1% increase in epoch time over vanilla in-batch sampling whereas the same was up to 210% for ANCE (see Table 5). NGAME also significantly outperformed techniques such as TAS [57] that use task-agnostic negatives obtained by clustering static features (see Fig. 2). This is because previous works use static one-time clustering [14,57] using features that are not aligned to the given task and offer poor quality negatives and biased training as a result. On the other hand, NGAME continuously adapts its negatives to the progress made by the training algorithm, thus offering provably accurate negatives and convergent training (see Theorem 1). Moreover, NGAME accomplishes this with much smaller overheads compared to existing state-of-the-art approaches (see Table 5 in appendices). Curriculum Learning with NGAME : The use of extremely hard negatives may impact training stability in initial epochs when the model is not well-trained [64]. Thus, it is desirable to initiate training with easier negatives. Fortunately, NGAME can tweak the 'hardness' of its negatives by simply changing the cluster size C in Algorithm 1. Using C = 1 makes NGAME identical to vanilla in-batch negative mining which offers easier negatives whereas the other extreme C \u2192 N causes NGAME to behave identical to ANNS-based techniques such as ANCE [50] that offer the hardest-to-classify negatives. This allows NGAME to commence training with small values of C and gradually increase it to get progressively harder negatives in later epochs. This curriculum learning approach could lead to 25% -40% faster convergence when compared to training that used a fixed cluster size C .", "4 Theoretical Analysis": "Theorem 1 guarantees that negative samples offered by NGAME are provably accurate. The key behind this result is the intuition that NGAME's negative mining strategy should succeed if embeddings of data points and relevant labels lie in close proximity and clustering is sufficiently precise. To state this result, we define the notion of a good embedding and clustering. Definition 1 (( r, /epsilon1 )-good embedding) . An encoder model E parameterized by \u03b8 is said to be ( r, /epsilon1 ) -good if  Definition 2 (( r, /epsilon1 )-good clustering) . A clustering of a given a set of data point embedding vectors E \u03b8 ( x i ) into K = /ceilingleft N/C /ceilingright clusters C 1 , . . . , C K is said to be ( r, /epsilon1 ) -good if /negationslash  where c ( i ) \u2208 [ K ] tells us the cluster to which data point i as assigned. 6 Theorem 1 (Negative Mining Guarantee) . Suppose Algorithm 1 performs its clustering step using data point embeddings obtained using the encoder model E \u03b8 parameterized by \u03b8 and identifies a set of hard negative labels \u02c6 P i -for data point i in step 8 of the algorithm using the threshold r > 0 . For any i \u2208 [ N ] , l \u2208 [ L ] , let E r il := { y il = -1 } \u2227 {\u2016E \u03b8 ( x i ) -E \u03b8 ( z l ) \u2016 2 \u2264 r } \u2227 { l / \u2208 \u02c6 P i -} be the bad event where l is an r -hard negative label for data point i but NGAME fails to retrieve it. Then if the model \u03b8 was ( r, /epsilon1 1 ) -good and the clustering was (2 r, /epsilon1 2 ) -good, we are assured that  for some constants c 1 , c 2 that are entirely independent of the execution of the NGAME algorithm and depend only on certain dataset statistics (such as number of data points per label etc). A description of the constants c 1 , c 2 as well as the complete proof are provided in Appendix F. However, to appreciate the essence of the guarantees, it helps to look at a special case of the result as indicated in the following corollary: Corollary 2. For the special case when all data points have the same number of relevant labels and all labels are relevant to the same number of data points, we have c 1 , c 2 \u2264 1 which gives us  Upon making certain simplifying assumptions, NGAME is also able to guarantee convergence to an approximate first-order stationary point. For sake of simplicity, this result is presented for Module M1 (Encoder Training) but a similar result holds for Module M2 (Classifier Training) as well. Theorem 3 (First-order Convergence Guarantee) . Suppose NGAME is executed with full-batch gradient updates with eager clustering i.e. \u03c4 = 1 in Algorithm 1. Also suppose the loss function and architecture are smooth and offer bounded gradients. Also, let /epsilon1 t tot def = c 1 /epsilon1 t 1 + c 2 /epsilon1 t 2 denote the total error assured by Theorem 4 in terms of hard-negative terms missed by NGAME at iteration t . Then there exists a smoothed objective \u02dc L such that within T iterations, NGAME arrives at a parameter \u03b8 that either satisfies  or else for some t \u2264 T , it satisfies  We note that some of the assumptions (full batch descent, eager clustering etc) are not essential to this result and merely simplify the proof whereas other assumptions (smoothness, bounded gradients) are standard in literature. The exact statement of the assumptions, a description of the smoothed objective \u02dc L , the description of the iterate t for which /epsilon1 t tot bounds the error in the second case, and the proof, are all available in Appendix F.", "5 Experiments": "Datasets : Multiple short-text as well as full-text benchmark datasets were considered in this paper which can be downloaded from the Extreme Classification Repository [65]. Both title and detailed content were available for full-text datasets (LF-Amazon-131K, LF-WikiSeeAlso-320K and LF-Wikipedia-500K) whereas only the product/webpage titles were available for the short-text datasets (LF-AmazonTitles-131K and LF-AmazonTitles-1.3M). These datasets cover a variety of applications including product-to-product recommendation (LF-Amazon-131K, LF-AmazonTitles-131K, and LF-AmazonTitles-1.3M), predicting related 7 Table 1: Dataset Statistics. For all datasets, L = \u0398( N ) and data points typically have O (log L ) relevant labels. The public datasets can be downloaded from The Extreme Classification Repository [65]. Wikipedia pages (LF-WikiSeeAlso-320K) and predicting Wikipedia categories (LF-Wikipedia-500K). Please refer to Table 1 for data statistics. Hyper-parameters : NGAME's hyper-parameters are: (i) cluster size, (ii) batch size, (iii) interval \u03c4 between clustering updates and (iv) margin value \u03b3 . The Adam optimizer was used to learn model parameters and its hyper-parameters include learning rate and number of epochs. NGAME did not require much hyper-parameter tuning and default values were used for all hyper-parameters except for number of epochs. Please see Section C in appendices for a detailed discussion on hyper-parameters. NGAME's encoder E \u03b8 was initialized with the 6-layered DistilBERT base [59,60] to encode data points and labels. The hyper-parameters of baseline algorithms were set as suggested by their authors wherever applicable and by fine-grained validation otherwise. Baselines : Siamese methods for XC such as SiameseXML [10], DECAF [3], and ECLARE [11] are the main baselines for NGAME. Other significant baselines include non-Siamese deep XC methods such as XR-Transformer [14], LightXML [16], BERTXML [15], MACH [1], AttentionXML [5], X-Transformer [6] and Astec [2]. Note that these include methods such as XR-Transformer [14], BERTXML [15] and LightXML [16] that also rely on transformer encoders albeit those that are not trained in a Siamese fashion. For sake of completeness, results are also reported for classical XC methods Bonsai [24], DiSMEC [4], Parabel [7], XT [42], and AnnexML [27] (please refer to Section A in the appendices for all results). Implementations provided by the respective authors were used for all methods. Evaluation metrics : Algorithms were evaluated using popular metrics such as precision@ k (P@ k , k \u2208 1 , 5) and their propensity-scored variants precision@ k (PSP@ k , k \u2208 1 , 5). Results on other metrics such as nDCG@ k (N@ k ) & propensity scored nDCG@ k (PSN@ k ) are included in Section E in the appendices. Definitions of all these metrics are available at [65] and definitions of metrics used in A/B testing experiments are provided in Section E in the appendices. Table 2 presents results on full-text datasets where NGAME could also be upto 13% and 15% more accurate in P@ k and PSP@ k respectively when compared to leading deepXC methods such as LightXML and XR-Transformer. It is notable that these methods also use transformer architectures indicating the effectiveness of NGAME's training pipeline. It is also notable that NGAME outperforms a range of other negative sampling algorithms such as TAS [57], DPR [63] and ANCE [14]. Similar results were observed on short-text datasets, where NGAME could be up to 11% and 13% more accurate in P@1 and PSP@1 respectively, compared to specialized algorithms designed for short-texts including SiameseXML, Astec, DECAF, etc . NGAME also continues to outperform other negative sampling algorithms such as TAS [57], DPR [63], and ANCE [14]. Offline evaluation on benchmark XC datasets : NGAME's P@1 could be up to 16% higher than leading Siamese methods for XC including SiameseXML, ECLARE, and DECAF which are the focus of the paper. This demonstrates that NGAME's design choices lead to significant gains over these methods. Please refer to the ablation experiments in Section A for detailed discussion on impact of NGAME's components. Curiously, [2] observed that jointly training a high-capacity feature extractor such as BERT along with the classification layer (as opposed to training them in a modular manner as done by NGAME) could yield 8 Table 2: Results on full-text benchmark datasets. See the appendices for full results. TT refers to training time in hours on a single Nvidia V100 GPU. 9 Table 3: Results on short-text benchmark datasets. See the appendices for full results. TT refers to training time in hours on a single Nvidia V100 GPU. -denotes that results are unavailable. Table 4: Results on the PR-85M dataset for personalized ad recommendations inferior results, especially on short-text datasets. We observe a similar trend where NGAME's pipeline yielded 7% better P@1 as compared to the same architecture trained in an end-to-end manner (referred as BERTXML in Table 3). The two-stage training employed by NGAME where the transformer-based encoder is first trained in a Siamese fashion was found to address this challenge and yield state-of-the-art results on short-text datasets as well. Comparison to other Negative Mining Algorithms : Results from Tables 3, 2 and Figure 2 establish that NGAME offers superior accuracies as well as faster convergence compare to a range of existing negative mining algorithms such as TAS [57], DPR [63], O-SGD [66], and ANCE [14]. Analysis of Results : Tail labels contribute significantly to the performance of both components of NGAME's classifier E \u03b8 ( z l ) and \u2206 w l . Fig. 3a in the appendix presents decile-wise analysis indicating that NGAME derives its superior performance not just by predicting popular labels but predicting rare labels accurately as well. The same was corroborated by NGAME's performance in live A/B testing on Sponsored Search where it was able to predict labels that were not being predicted by the existing ensemble of methods. NGAME's final predictions which make use of both components get the best out of both leading to overall superior performance. Live A/B testing and offline evaluation on Personalized Ad Recommendation : NGAME was used to predict queries that could lead to clicks on a given webpage in the pipeline to show personalized ads to users and was compared to an existing ensemble of state-of-the-art IR, dense retrieval (DR) and XC 10 techniques. In A/B tests on live traffic, NGAME was found to increase Click-Through Rate (CTR) and Click Yield (CY) by 23% and 19% respectively. In manual labeling by expert judges, NGAME was found to increase the quality of predictions, measured in terms of fraction of excellent and good predictions, by 16% over the ensemble of baselines. The PR-85M dataset was created to capture this inverted prediction task by mining the search engine logs for a specific time period where each webpage title became a data point and search engine queries that led to a click on that webpage became labels relevant to that data point. NGAME was found to be at least 2% more accurate than leading XC as well as Siamese encoder-based methods including ANCE and SiameseXML in R@5 metric on the PR-85M dataset. Please see Table 4 for detailed results. Live A/B testing on Sponsored Search : NGAME was deployed on a popular search engine and A/B tests were performed on live search engine traffic for matching user queries to advertiser bid phrases (Query2Bid). NGAME was compared to an ensemble of leading (proprietary) IR, XC, generative and graph-based techniques. NGAME was found to increase Impression Yield (IY), CY and Query Coverage (QC) by 1.3%, 1.23% and 2.12%, respectively. The IY boost indicates that NGAME was able to discover more ads which were not being captured by the existing ensemble of algorithms. The CY boost indicates that ads surfaced by NGAME were more relevant to the end user. The QC boost indicates that NGAME impressed ads on several queries for which ads were previously not being shown.", "6 Conclusions and Future Work": "This work accelerates the training of XC architectures that use large Siamese encoders such as transformers. A key step towards doing this is the identification of negative mining techniques as a bottleneck that forces mini-batch sizes to remain small and in turn, slowing down convergence. The paper proposes the NGAME method that uses negative-mining-aware mini-batch creation to train Siamese XC methods and effectively train large encoder architectures such as transformers making effective use of label-text. There exist other forms of label-metadata that have been exploited in XC literature, for instance label hierarchies and correlation graphs. Although experiments show that NGAME outperforms these methods by use of more powerful architectures alone, it remains to be seen how NGAME variants using graph/tree metadata would perform. It would also be interesting to explore interpretability directions to better understand situations where NGAME stands to improve. In terms of theoretical results, it is a tantalizing opportunity to relate the ( /epsilon1, r )-goodness of the embedding model to the training loss of the model. This would set up a virtuous cycle and possibly offer stronger convergence proofs since progress in terms of the training loss would improve the goodness of the model, in turn offering better negatives leading to faster training, and so on.", "References": "[1] T. K. R. Medini, Q. Huang, Y. Wang, V. Mohan, and A. Shrivastava. Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products. In NeurIPS , 2019. [2] K. Dahiya, D. Saini, A. Mittal, A. Shaw, K. Dave, A. Soni, H. Jain, S. Agarwal, and M. Varma. DeepXML: A Deep Extreme Multi-Label Learning Framework Applied to Short Text Documents. In WSDM , 2021. [3] A. Mittal, K. Dahiya, S. Agrawal, D. Saini, S. Agarwal, P. Kar, and M. Varma. DECAF: Deep Extreme Classification with Label Features. In WSDM , 2021. [4] R. Babbar and B. Sch\u00f6lkopf. DiSMEC: Distributed Sparse Machines for Extreme Multi-label Classification. In WSDM , 2017. [5] R. You, S. Dai, Z. Zhang, H. Mamitsuka, and S. Zhu. AttentionXML: Extreme Multi-Label Text Classification with Multi-Label Attention Based Recurrent Neural Networks. In NeurIPS , 2019. 11 12 13 14 [65] K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y. Prabhu, and M. Varma. The Extreme Classification Repository: Multi-label Datasets & Code, 2016. [66] K. Kawaguchi and H. Lu. Ordered SGD: A New Stochastic Optimization Framework for Empirical Risk Minimization. In AISTATS , 2020. [67] L. Van der Maaten and G. Hinton. Visualizing data using t-SNE. JMLR , 2008. 15 Table 5: Breakdown of computation costs of different negative mining methods 90 Precision@1 Figure 2: Convergence with different negative mining techniques on the LF-Wikipedia-500K dataset. Note that the plot include P@1 just for M1. 0 10 20 30 40 50 Epochs 30 40 50 60 70 80 DPR ANCE O-SGD NGAME", "A Results": "Tables 6 and 7 present detailed results on all baseline methods for short and full-text datasets respectively. Table 5 considers multiple negative sampling algorithms including DPR [63], ANCE [14] and NGAME and shows the sampling overhead incurred by each method. NGAME incurs barely 1% sampling overhead and offers epoch times similar to DPR that incurs no sampling overhead. However, NGAME offers much better accuracies than DPR on a range of datasets (see Tables 2 and 3). On the other hand, ANCE incurs between 59 -210% sampling overhead slowing down the overall training procedure. Figure 2 gives convergence plots offered by various negative sampling methods. NGAME offers by far the fastest convergence of all methods. Fig 3a shows how various deciles contribute to NGAME's performance. It is notable that a bulk of NGAME's (high) P@5 performance is due to accurate retrieval of tail labels. Figure 3b gives an illustrative example of how NGAME recovered hard negatives for an actual data point. 16 Figure 3: (a) Performance of NGAME on different deciles of LF-AmazonTitles-131K. It is notable that the tail deciles (that contain rare labels) contribute significantly to NGAME's performance indicating that NGAME derives its superior performance not just by predicting popular labels better but predicting rare labels accurately as well; (b) t-SNE [67] representation of positives, random-negatives, NGAME's negatives and hard negatives missed by NGAME for a product titled 'Fearless Confessions: A Writer's Guide to Memoir' . Note that NGAME recovers most of the hard negatives for the data point. 1 2 3 4 5 6 7 8 9 10 Deciles (decreasing frequency) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Contribution to P@5 (a) label positives random negatives missed hard negatives NGAME's negatives (b)", "B Ablation Experiments": "Table 8 gives results of ablation experiments that establish that all of NGAME's architectural components are essential to obtain optimal accuracy across several metrics and datasets. In particular, making predictions using label/data point embeddings alone is suboptimal. However, so is making predictions using classifiers scores alone. Moreover, combining classifier and embedding scores naively also does not yield optimal accuracy. A combination of embedding and classifier scores using NGAME's score fusion strategy is what offers the best performance uniformly across several metrics and datasets.", "C Implementation Details and Hyper-parameters": "Balanced hierarchical k-means was used in Algorithm 1 and offered N/C balanced clusters in D -dimensions in O ( ND log( N/C )) time. Implementing competing negative sampling algorithms : Tables 2 and 3 present results on several existing negative sampling algorithms such as DPR [63], ANCE [14] and TAS [57]. Each of these methods was afforded the same encoder and classifier architecture and loss function as NGAME and the only difference in these implementations was that a different negative sampling technique was used in place of NGAME. Hyper-parameters : NGAME's hyper-parameters include (cf. Algorithm 1): (1) cluster size C , (2) batch size S , (3) clustering refresh interval \u03c4 and (4) loss margin \u03b3 (1). Table 9 presents the hyperparameter values for various datasets (most were set to default values). Both modules M1 and M2 used the Adam optimizer and hard negatives from NGAME. Hyper-parameters for Adam include learning rate and number of epochs. NGAME's encoder E \u03b8 was initialized with a 6-layered DistilBERT base [59,60] to encode the labels and data points in all experiments. The hyper-parameters of the other algorithms were set as suggested by their authors wherever applicable and by fine-grained validation otherwise. Module M1 (Encoder Training) : This model simply trains the encoder model E \u03b8 . The residual 17 vectors were fixed to zero in this module, effectively using the label embedding itself as the label classifier i.e. w l = E \u03b8 ( z l ). For a data point i \u2208 [ n ], let P i + denote its positive labels and P i -denote the hard negatives offered by NGAME. Algorithm 1 was used to train the task loss as follows:  Module M2 (Classifier Training) : Label classifiers were initialized to their embeddings, i.e., w l \u2190 E \u02c6 \u03b8 ( z l ) where \u02c6 \u03b8 is the encoder model E \u03b8 learnt by M1 (which is now frozen). Note that this is equivalent to initializing the residual vectors to zero. The original loss (1) was now minimized with respect to the residual vectors alone, effectively learning better classifiers than what the label embeddings alone could have provided. Say for a training data point i \u2208 [ N ], let P i + denote its positive labels and P i -its NGAME hard negatives. The loss function used for module M2 training was: Note that only the hard negative labels of a data point offered by NGAME and its positive labels are used for training. To accelerate training further, only one positive label was randomly sampled for each data point rather than using all positives, i.e., \u2223 \u2223 P i + \u2223 \u2223 = 1 was used - note that this continued to offer an unbiased loss estimate.  Note that since embeddings are frozen, M2 training can be distributed across multiple GPUs without any inter-GPU communication by splitting the label set into partitions and learning classifiers independently for each split. In principle, one could use standard Binary Cross Entropy or Squared Hinge loss to train the classifiers. However, using the same objective in both modules yielded superior accuracies as compared to the alternatives. Additionally, the unit normalized classifiers offered by triplet loss were found to be more suitable for ANNS retrieval. Inference (Score Fusion) : Fusing embedding and classifier scores is commonly practiced [2,8]. However, unlike previous works that simply take a fixed linear combination of scores which is sub-optimal, NGAME uses a fusion architecture F that still offers O ( b + D log L ) time inference. Once M1 and M2 training is over, an MIPS [61] data structure is created over the label classifiers { w l , l \u2208 [ L ] } . A small validation set V of around 10K data points is created and for each data point i \u2208 V , its embedding E \u03b8 ( x i ) is used to obtain a label shortlist \u02c6 N i \u2282 [ L ] of O (log L ) labels from the MIPS structure (note that this shortlist may contain both relevant and irrelevant labels - if encoder training has been accurate then this set will have high recall i.e. contain most relevant labels for this data point). Also, the set of labels relevant to this data point P i + = { l \u2208 [ L ] : y il = +1 } is obtained. For each data point-label pair ( i, l ) with i \u2208 V, l \u2208 \u02c6 N i \u222a P i + , a 3-dimensional descriptor d il is created as d il = ( E \u03b8 ( z l ) /latticetop E \u03b8 ( x i ) , w /latticetop l E \u03b8 ( x i ) , f l ), where f l is the frequency of the label l in the training set. The pair ( i, l ) is assigned a tag t il = 1 if the label l \u2208 P i + and t il = 0 otherwise. A standard regression tree with depth 7 is learnt to predict t il using d il . Let T ( d il ) denote the score returned by this regressor. At test time, given a test data point x , its embedding E \u03b8 ( x ) is computed and used to obtain a shortlist \u02c6 N of O (log L ) labels from the MIPS structure. To each shortlisted label l \u2208 \u02c6 N , a fused score is assigned as  Labels are then recommended in decreasing order of their fused scores. See Table 8 for full results.", "D Training and Inference Time Complexity for NGAME": "M1 Complexity : We first calculate the epoch complexity of M1. In any mini-batch of say S data points, computing embeddings of all S data points and one positive label per data point takes O ( bS ) time if using 18 an encoder model E \u03b8 with b parameters. Selecting hard negatives for all data points takes O ( S 2 ) time. Executing backprop takes at most O ( bS 2 ) time giving us an iteration complexity of O ( bS 2 ) . For an epoch of N/S iterations, this takes O ( NbS ) time. NGAME chooses S = O (log L ) giving us a O ( Nb log L ) epoch time. We now compute the cost of refreshing the clustering from Algorithm 1. Computing D -dimensional embeddings for every data point takes O ( ND + Nb ) time. Clustering them takes O ( ND log N ) time as mentioned above in Appendix C. Thus, the total time is O ( Nb + ND log N ) = O ( Nb log L ) since D /lessmuch b and L = \u0398( N ). This establishes a O ( Nb log L ) time complexity for M1 training. Inference : Given a test data point, obtaining its embedding takes O ( b + D ) = O ( b ) time since D /lessmuch b . Retrieving the log L top-ranked labels for this embedding from an ANNS/MIPS data structure takes O ( D log L ) time [61]. Obtaining the Siamese and classifier scores for these shortlisted labels takes an additional O ( D log L ) time bringing the total inference complexity to O ( b + D log L ). M2 Complexity : The time complexity calculation is identical here except that executing backprop takes at most O ( DS 2 ) time since only the classifiers (more specifically the residual vectors) are being optimized in M2. However, since D /lessmuch b , we get a O ( Nb log L ) time complexity for M2 training as well.", "E Metrics for Live A/B Testing": "Standard online metrics were used for A/B testing experiments. Impression Yield (IY) is defined as the rate at which an ad appears compared to the total number of searches. Similarly, Click Yield (CY) is defined as the rate at which an ad is clicked with respect to the total number of searches. Query Coverage (QC) is defined as the fraction of search queries for which at least one ad was shown. Formally, if we denote total number of searches as SRPV :   19 Table 6: Detailed results on short-text datasets. TT refers to training time in hours on a single Nvidia V100 GPU. 20 Table 7: Detailed results on long-text datasets. TT refers to training time in hours on a single Nvidia V100 GPU. Table 8: Ablation study experimenting with prediction performance from different components of NGAME. In all rows, training was done using NGAME's usual pipeline. However, predictions were made using just label embeddings in the first row, using just the label classifier in the second row, and using score fusion function in the last row. NGAME's score fusion strategy offers moderate gains in accuracy as compared to a simple sum of two scores (compare row 3 vs row 4). Table 9: Hyper-parameter values for NGAME on all datasets to enable reproducibility. NGAME code will be released publicly. Most hyperparameters were set to their default values across all datasets. LR is learning rate. Multiple clusters were chosen to form a batch hence B > C in general. Clusters were refreshed after \u03c4 epochs. Cluster size C was doubled after every 25 epochs 22", "F Theoretical Analysis and Full Proofs": "This section details the theoretical analysis of the NGAME method and also offers first-order stationarity guarantees under some standard, simplifying assumptions.", "F.1 Proof of Theorem 1: NGAME offers Provably Accurate Hard-negatives": "Intuitively, an embedding is good if for most data points, its positive labels are embedded close to it and, a clustering is good if it does not split too many closely placed data point embedding vectors into distinct clusters. To state and prove Theorem 1, we introduce the following notions of bad events E r il , F r lm , G r il . 1. For any data point i \u2208 [ N ], label l \u2208 [ L ], let E r il := { y il = -1 } \u2227 {\u2016E \u03b8 ( x i ) -E \u03b8 ( z l ) \u2016 2 \u2264 r } \u2227 { l / \u2208 \u02c6 P i -} be the bad event where label l is an r -hard negative for data point i but NGAME fails to retrieve it in its shortlist \u02c6 P i -. /negationslash 2. For any two data points i, j \u2208 [ N ], let F r ij := { \u2016E \u03b8 ( x i ) -E \u03b8 ( x j ) \u2016 2 \u2264 r } \u2227{ c ( i ) = c ( j ) } be the bad even where the embeddings for data points i, j are r -close to each other but clustering separates them into distinct clusters. 3. For any data point i \u2208 [ N ] and any of its positive/relevant labels l \u2208 [ L ] i.e. where y il = +1, let G r il := {\u2016E \u03b8 ( z l ) -E \u03b8 ( x i ) \u2016 2 \u2265 r } be the bad event where label l is relevant to data point i but the encoder embeddings for the pair are more than r distance far apart. We introduce some handy shortcuts to state the result. \u00b7 For any data point i \u2208 [ n ], let p i def = |{ l \u2208 [ L ] : y il = +1 }| be the number of relevant/positive labels for that data point. \u00b7 For any label l \u2208 [ L ], let q l def = |{ i \u2208 [ N ] : y il = +1 }| be the number of data points for which this label is relevant/positive. \u00b7 Let \u00af p def = E [ p i ] and \u00af q def = E [ q l ] denote average/expected values of the quantities p i , q l respectively. \u00b7 Let p min = min i \u2208 [ N ] p i denote the smallest number of relevant labels for any data point. \u00b7 Let q min = min l \u2208 [ L ] q l denote the smallest number of relevant data points for any label. \u00b7 Let \u00b5 1 def = E [ N -q l q l ] , \u03c3 2 1 def = Var [ N -q l q l ] , \u03c3 2 2 def = Var[ p i ] denote various handy second order moments involving the quantities p i , q l . Given this we are ready to prove Theorem 1 that establishes the negative mining guarantee for NGAME. However, first we provide a full version of the theorem statement with all the details. Theorem 4 (Theorem 1 restated with full constants.) . Suppose Algorithm 1 performs its clustering step using embeddings obtained using the encoder model parameterized by \u03b8 and identifies a set of hard negative labels \u02c6 P i -for data point i in step 8 of the algorithm using the threshold r > 0 . As defined above, for any label l \u2208 [ L ] and data point i \u2208 [ N ] , let E r il := { y il = -1 } \u2227 {\u2016E \u03b8 ( x i ) -E \u03b8 ( z l ) \u2016 2 \u2264 r } \u2227 { l / \u2208 \u02c6 Q l -} be the bad event where label l is an r -hard negative for data point i but NGAME fails to retrieve it in its shortlist \u02c6 P i -. Then if the model \u03b8 was ( r, /epsilon1 1 ) -good and the clustering was (2 r, /epsilon1 2 ) -good, we are assured that   23 As training proceeds, the quantity /epsilon1 1 is expected to get smaller and smaller as the encoder model gets fine-tuned to the task. Although the above result is presented with reference to module M1, a similar result can be shown to hold true for module M2 as well with just one change - the label representation used is w l = E \u03b8 ( z l ) + \u03b7 l instead of E \u03b8 ( z l ). Also, note that the constants c 1 , c 2 in the statement of Theorem 4 depend purely on dataset characteristics and are entirely independent of the execution of the algorithm. The following corollary helps appreciate the result better by showing that these constants are small and in fact, upper bounded by unity for a simplified case. Corollary 5 (Corollary 2 restated with full constants.) . For the special case when all data points have the same number of relevant labels i.e. p i = p for all all i \u2208 [ N ] as well as all labels are relevant to the same number of data points i.e. q l = q for all l \u2208 [ L ] , we have c 1 , c 2 \u2264 1 which gives us  The proof of Theorem 4 is given below followed by the proof of Corollary 5. Proof (of Theorem 4). Consider any label l \u2208 [ L ] and a data point i \u2208 [ N ] such that y il = -1 i.e. label l is not relevant to the data point i . Furthermore, let j \u2208 [ N ] be any other data point for which label l is indeed relevant i.e. y jl = 1. Then triangle inequality dictates that  Now note that if \u2016E \u03b8 ( x i ) -E \u03b8 ( x j ) \u2016 2 > 2 r (i.e. the two data point embeddings are far, which means the event F 2 r ij does not occur) as well as \u2016E \u03b8 ( x j ) -E \u03b8 ( z l ) \u2016 2 < r (i.e. the label l is embedded close to data point j , which means the event G r jl does not occur), then the above inequality tells us that \u2016E \u03b8 ( x i ) -E \u03b8 ( z l ) \u2016 2 > r (i.e. the label l is not a hard negative for data point i rather an easy one, which means the event E r il cannot happen). Taking the contrapositive lets us conclude that  or in other words,  Now note that the above must hold true for all data points j such that y jl = 1. In particular, if the event E r il does occur i.e. Algorithm 1 does miss the hard negative label l for data point i , then by design of Algorithm 1, this can only happen if all data points for which label l is relevant lie in clusters other than the cluster of data point i itself since otherwise the label l would have been discovered in step 8 of the algorithm. An averaging argument then lets us conclude that  where we recall that q l is the number of data points for which label l is relevant i.e. q l = |{ j : y jl = 1 }| . Summing over all data points i \u2208 [ N ] for which label l could have been a missed hard negative gives us /negationslash  /negationslash Further, summing over all data points i and applying normalization gives us /negationslash  24 Below we bound these two terms separately. To bound ( B ), we recall the terms \u00b5 1 , \u03c3 2 1 , \u00af p, \u00af q defined earlier. Then we have  where in the second step, we used the fact that /epsilon1 1 = 1 \u00af qL \u2211 l \u2208 [ L ] \u2211 j : y jl =1 I { G r jl } and \u00af qL = \u00af pN , in the third step we use the Cauchy-Schwartz inequality and in the fourth step we use the definition of \u03c3 1 , the fact that for any vector, its L 2 norm is always upper bounded by its L 1 norm, as well as the fact that the terms G r jl are non-negative so that \u2211 j : y jl =1 I { G r jl } \u2265 0 for any l \u2208 [ L ]. To bound ( A ), we recall the terms q min , \u00af p, \u03c3 2 2 defined earlier. Note that q min \u2265 1. We get /negationslash /negationslash  /negationslash Now |{ l : y jl = 1 , y il = 1 }| \u2264 |{ i : y jl = 1 }| = p j which gives us   where in the third step we use the fact that /epsilon1 2 = 1 N 2 \u2211 i,j \u2208 [ N ] I { F 2 r ij } , in the fourth step we use the CauchySchwartz inequality, in the fifth step we use the definition of \u03c3 2 . Now, if i, j, k \u2208 [ N ] are independently selected uniformly from [ N ], we have  25 This gives us  This gives us  /negationslash This finishes the proof. We note that the two of the steps taken above, that of upper bounding q l \u2265 q min and upper bounding |{ l : y jl = 1 , y il = 1 }| \u2264 p j may be suboptimal and the proof may be tightened at these points. However, as Corollary 5 shows, tightening these steps is not expected to change the essential result and is expected to only yield better values for the constants c 1 , c 2 . Proof (of Corollary 5). We continue to use the terms ( A ) , ( B ) from the proof of Theorem 4. When q l = q for all l \u2208 [ L ], then we have \u00b5 1 = N -q q , \u03c3 1 = 0 , \u00af q = q that gives us  Similarly, when p i = p for all i \u2208 [ N ] and q l = q for all l \u2208 [ L ], we have \u03c3 2 = 0 , \u00af p = p , and q min = q = \u00af q , which gives us  since qL = \u00af qL = \u00af pN = pN . This finishes the proof.", "F.2 Proof of Theorem 3: NGAME converges to a First-order Stationary Point": "In this section, we establish that under certain simplifying assumptions, NGAME provably converges to a first-order stationary point. We first list all the assumptions followed by a restatement of Theorem 3 with all constants in Theorem 6. To present the essential aspects of the proof, we will give the proof for a full-batch version of the algorithm, mini-batch extensions being straightforward: Assumption 1 (Full-batch Update with Eager Clustering) . Let NGAME be executed so that it performs descent on the entire training set at once in a single descent step (and not in multiple steps using mini-batch SGD). Also, let the clustering in Algorithm 1 be updated after each descent step i.e. \u03c4 = 1 Our results require the training objective to be smooth. We recall the definition of an H -smooth function. Definition 3 ( H -smooth objective) . A differentiable real-valued function f : X \u2192 R over any Euclidean space X \u2286 R p is said to be H -smooth if for all x , y \u2208 X we have  Assumption 2 (Smooth Objective with Bounded Gradients) . Let NGAME be executed with training functions /lscript il that are H -smooth for some finite H > 0 . Also assume that the training functions /lscript il have bounded gradients i.e. for some G > 0 , we have \u2016\u2207 /lscript il ( \u03b8 ) \u2016 2 \u2264 G for all i \u2208 [ N ] , l \u2208 [ L ] , \u03b8 . We note that Assumption 1 is made to simplify the proof and not essential to the result whereas Assumption 2 is standard in literature. We now present the formal statement. Theorem 6 (Restatement of Theorem 3 with constants) . Suppose Assumptions 1 and 2 are satisfied and let /epsilon1 t tot def = c 1 /epsilon1 t 1 + c 2 /epsilon1 t 2 denote the total error assured by Theorem 4 in terms of hard-negative terms missed by NGAME at iteration t . Then there exists a smoothed objective \u02dc L (defined below) such that for some \u03c6 \u2208 (0 , 1) we are assured that if NGAME uses a sufficiently small step size \u03b7 < 1 -\u03c6 2 H (1+ \u03c6 2 ) where H is the smoothness parameter from Assumption 2, then for any T > 0 , within T iterations, one of the following is assured 26 1. For some iterate t \u2264 T , the NGAME iterate \u03b8 t assures  2. For some iterate t \u2264 T , the NGAME iterate \u03b8 t assures  where \u03b8 \u2217 is the optimal model parameter, \u03b8 0 is the initial iterate and G is the gradient bound in Assumption 2. We note that /epsilon1 t tot can be controlled directly by performing more relaxed clustering with larger clusters so that the clustering error /epsilon1 t 2 \u2192 0. In particular, note that if a trivial clustering is done where all points lie in the same single cluster, then indeed /epsilon1 t 2 = 0. The other component /epsilon1 t 1 is controlled by the ability of the neural architecture to successfully embed data points and their positive labels in close vicinity which is also expected to continue improving as training progresses. We will follow the following proof strategy: 1. Step 1 : Given a smooth training objective L , construct a smooth surrogate \u02dc L that places negligible weight on non-hard negatives. 2. Step 2 : Show that the training updates offered by NGAME's hard-negative mining strategy implicitly perform gradient descent on \u02dc L but using biased gradient updates. Then show that under suitable assumptions, this bias is bounded. 3. Step 3 : Show that this results in NGAME converging to an approximate first-order stationary point with respect to the modified objective \u02dc L . The construction of such surrogate objectives with respect to which convergence results are established, is popular in literature for instance in [66]. It is possible to relax the above to show convergence guarantees for the mini-batch SGD version of the algorithm, as well as when the clustering is updated after every few epochs (as is done in practice) instead of after every epoch, and would require additional bookkeeping to keep track of accumulated gradient bias due to stale clustering and additional variance due to randomness mini-batch creation. We defer this more detailed analysis to a later investigation.", "F.2.1 Step 1: Constructing a Smooth Surrogate": "It will be useful for us to arrange terms in our loss function in the following canonical form  i.e. /lscript il ( \u03b8 ) absorbs all dependence on the positive labels of data point i . For instance, if using the squared triplet loss function which is indeed smooth, we would have  where \u00af p is some normalization constant such as the average number of relevant labels per data point. We note that the above expressions have been written from the point of view of module M1 that only trains the embedding model E \u03b8 . However, this is without loss of generality and similar steps can be applied to bring module M2 loss functions into such as canonical form as well. Results from Lemma 8 assure us that if the individual loss terms /lscript il are H -smooth and have G -bounded gradients, then so does any composite loss 27 function formed using their sum, as is done while defining L . Smoothness can be readily ensured by using differentiable activation functions in the neural architecture e.g. GeLU instead of ReLU, as well as using a smooth loss function e.g. using squared triplet loss i.e. ([ x -y + \u03b3 ] + ) 2 or its logistic variant ln(1 + exp( x -y )) instead of the non-differentiable triplet loss [ x -y + \u03b3 ] + . Given the above, we define three surrogate functions. The first function L r ( \u03b8 ) only considers terms corresponding to r -hard-negatives. However, L r ( \u03b8 ) is not a smooth function and our proofs will require smooth objectives to be used. The second function \u02dc L ( \u03b8 ) is a smoothed version and pays negligible attention to non-hard negative terms. The third function L NGAME ( \u03b8 ) simply considers only those terms that were correctly retrieved by NGAME's negative mining strategy.    where we define the decay function as  Clearly d \u039b ( v, r ) = 1 if v < r and d \u039b ( v, r ) \u2192 0 rapidly if v > r with the rate of decay being controlled by the temperature parameter \u039b. Using the various parts of Lemma 8, it can be shown that d \u039b ( v, r ) is a smooth function for any value of r, \u039b and so is \u02dc L ( \u03b8 ). It is easy to see that given NGAME's negative mining strategy that only consider r -hard negatives, it performs gradient updates w.r.t L r ( \u03b8 ). However, the following discussion shows that such updates offer bounded bias with respect to the actual gradients for \u02dc L ( \u03b8 ) as well.", "F.2.2 Step 2: Bounding the Gradient Bias": "The gradient updates actually used by NGAME have two sources of bias if the objective \u02dc L ( \u03b8 ) is considered: 1. Terms corresponding to not-so-hard negatives that are never considered by NGAME's negative mining strategy i.e. where \u2016E \u03b8 ( x i ) -E \u03b8 ( z l ) \u2016 2 > r but d \u039b ( v, r ) is not small enough to be negligible. 2. Terms corresponding to hard negatives missed by NGAME's negative mining strategy. The first source of bias can be handled simply by taking \u039b to be a large enough quantity. However, we stress that an execution of NGAME in practice has to never worry about setting \u039b appropriately since \u02dc L is a surrogate created purely for the purpose of establishing the convergence guarantee and is never actually used to perform training. For the second source of bias, we will appeal to Theorem 1 which assures us that the number of hardnegatives missed by NGAME are not too many. Let /epsilon1 t tot = c 1 /epsilon1 t 1 + c 2 /epsilon1 t 2 denote the total error assured by Theorem 4 in terms of hard-negative terms missed by NGAME at iteration t . Our goal is to show that the corresponding error in estimating the gradients would be small as well. Now, the absolute error in the gradients can be estimated readily. Since /lscript il ( \u03b8 ) has G -bounded gradients, a simple application of the triangle inequality allows us to show that  However, our subsequent analysis requires the relative error in gradients to be small rather than the absolute error. To overcome this problem, fix some \u03c6 < 1 and notice that if the absolute error does not translate to a \u03c6 -relative error i.e.  28 then this implies that  which seems to suggest that \u03b8 t is already an approximate first-order stationary point since /epsilon1 t tot is expected to be vanishingly small, but with respect to the effective training objective \u2207L r ( \u03b8 ). Below we show that we can convert this into an approximate stationarity guarantee w.r.t. \u02dc L ( \u03b8 ), as well as handle cases where we are able to obtained a relative error bound.", "F.2.3 Step 3: Ensuring Convergence to an Approximate First-order Stationary Point": "We are now ready to establish the convergence proof. Lemma 7 offers a generic convergence result on a smooth objective when gradient updates are biased and may have bounded errors. If \u02dc L ( \u03b8 ) is taken as the effective training objective (recall that we have already established above that it is smooth), then the bias in gradients offered by NGAME can be bounded as  We will set \u039b to be large enough (recall that this is purely for sake of analysis and does not need to be done in practice) so that the decay function d \u039b ( v, r ) dies so rapidly that we get \u2225 \u2225 \u2207L r ( \u03b8 ) -\u2207 \u02dc L ( \u03b8 ) \u2225 \u2225 2 \u2264 min { (1 -\u03c6 ) 2(1+ \u03c6 ) \u2225 \u2225 \u2207 \u02dc L ( \u03b8 ) \u2225 \u2225 2 , G/epsilon1 tot \u03c6 } for all \u03b8 . The first case we consider now is when the absolute gradient error fails to translate to a relative gradient error. As analyzed above, this means that we have  Applying triangle inequality tells us that  due to the way we set \u039b. In the other case, we do have a relative bound which allows us to bound the first term as  Yet again, due to the way we set \u039b, this gives us  which fulfills the preconditions of Lemma 7. Thus, in every iteration, either we arrive at an approximate first-order stationary point satisfying  or else we continue satisfying the bias conditions of Lemma 7. For small enough step lengths (see Lemma 7 for an exact statement), we are ensured that NGAME reaches a point \u03b8 t where \u2225 \u2225 \u2207 \u02dc L ( \u03b8 t ) \u2225 \u2225 2 \u2264 O ( 1 T ) within T iterations if the bias preconditions keep getting fulfilled in each iteration. This concludes the convergence proof.", "F.3 Supporting Results": "Lemma 7 (First-order Stationarity with a Smooth Objective) . Let f : \u0398 \u2192 R be a H -smooth objective over model parameters \u03b8 \u2208 \u0398 that is being optimized using a biased gradient oracle and the following update for some step length \u03b7 > 0 :  29 Then, if the bias of the oracle is bounded, specifically for some \u03b4 \u2208 (0 , 1) , for all t , we have g t = \u2207 f ( \u03b8 t ) + \u2206 t where \u2016 \u2206 t \u2016 2 \u2264 \u03b4 \u00b7 \u2016\u2207 f ( \u03b8 t ) \u2016 2 and if the step length satisfies \u03b7 < (1 -\u03b4 ) 2 H (1+ \u03b4 2 ) , then for any T > 0 , for some t \u2264 T we must have  The above assures that executing the descent step for \u2126 ( T ) steps must result in an iterate at which the square of the gradient norm dips to O ( 1 T ) which assures convergence to a first-order stationary point in the limit. Proof (of Lemma 7). Smoothness of the objective gives us  Since we used the update \u03b8 t +1 = \u03b8 t -\u03b7 \u00b7 g t and we have g t = \u2207 f ( \u03b8 t ) + \u2206 t , the above gives us   Now, the Cauchy-Schwartz inequality along with the bound on the bias gives us -\u03b7 \u00b7 \u3008\u2207 f ( \u03b8 t ) , \u2206 t \u3009 \u2264 \u03b7\u03b4 \u00b7 \u2016\u2207 f ( \u03b8 t ) \u2016 2 2 as well as \u2016\u2207 f ( \u03b8 t ) + \u2206 t \u2016 2 2 \u2264 2(1 + \u03b4 2 ) \u00b7 \u2016\u2207 f ( \u03b8 t ) \u2016 2 2 . Using these gives us  since we chose \u03b7 < (1 -\u03b4 ) 2 H (1+ \u03b4 2 ) . Reorganizing, taking a telescopic sum over all t , using f ( \u03b8 T +1 ) \u2265 f ( \u03b8 \u2217 ) and making an averaging argument tells us that since we set , for any T > 0, it must be the case that for some t \u2264 T , we have  Lemma 8. Given two bounded non-negative-valued functions f, g : X \u2192 R + that are w.l.o.g. 1 -smooth, 1 -Lipschitz i.e. | f ( x ) -f ( y ) | \u2264 \u2016 x -y \u2016 2 for all x , y \u2208 X (similarly for g ), 1 -bounded i.e. f ( x ) , g ( x ) \u2264 1 for all x \u2208 X as well as have 1 -bounded gradient norms i.e. \u2016\u2207 f ( x ) \u2016 2 , \u2016\u2207 g ( x ) \u2016 2 \u2264 1 for all x \u2208 X . Then for any c > 0 the functions f + g, f \u00b7 g and min { f, c } are smooth as well. Moreover, the function min { f, c } is 1 -Lipschitz, c -bounded as well as upto isolated points of non-differentiability, has 1 -bounded gradient norms as well. The smoothness, Lipschitz-ness and boundedness constants are taken to be unity here to avoid clutter and the results hold for any positive constant values for these as well. Proof. We are given that for all x , y \u2208 X , we have   We prove the results in parts 30 1. Adding (2) and (3) gives us  which establishes that f + g is 2-smooth. 2. Multiplying (2) with g ( x ) gives us  Similarly, multiplying (3) with f ( y ) gives us  Adding the two gives us  Lipschitz-ness and the Cauchy-Schwartz inequality tells us that  due to bounded gradients. This and the fact that f, g take bounded values gives us  which tells us that f \u00b7 g is 4-smooth. 3. Let us define the shortcut h ( x ) := min { f ( x ) , c } . Notice that h may have isolated points of nondifferentiability where f ( x ) = c . We will arbitrarily define \u2207 h ( x ) = 0 at such points. Consider the following 4 cases (a) Case 1 : f ( x ) > c, f ( y ) > c : in this case h ( x ) = h ( y ) and \u2207 h ( y ) = 0 and thus it is indeed true that h ( x ) \u2264 h ( y ) + \u3008 0 , x -y \u3009 + 1 2 \u2016 x -y \u2016 2 2 (c) Case 3 : f ( x ) < c, f ( y ) > c : in this case \u2207 f ( y ) = 0 but we still have h ( x ) = f ( x ) < c = h ( y ) as well as h ( y ) \u2264 h ( y ) + \u3008 0 , x -y \u3009 + 1 2 \u2016 x -y \u2016 2 2 which tells us that h ( x ) \u2264 h ( y ) + \u3008 0 , x -y \u3009 + 1 2 \u2016 x -y \u2016 2 2 (b) Case 2 : f ( x ) > c, f ( y ) < c : in this case smoothness of f gives us f ( x ) \u2264 f ( y ) + \u3008\u2207 f ( y ) , x -y \u3009 + 1 2 \u2016 x -y \u2016 2 2 but since f ( x ) > c = h ( x ) we get h ( x ) < f ( y ) + \u3008\u2207 f ( y ) , x -y \u3009 + 1 2 \u2016 x -y \u2016 2 2 . However since f ( y ) = h ( y ) , \u2207 f ( y ) = \u2207 h ( y ), we get h ( x ) \u2264 h ( y ) + \u3008\u2207 h ( y ) , x -y \u3009 + 1 2 \u2016 x -y \u2016 2 2 . (d) Case 4 : f ( x ) < c, f ( y ) < c : in this case h ( x ) = f ( x ) , \u2207 h ( y ) = \u2207 f ( y ) , h ( y ) = f ( y ) and h simply inherits the smoothness of f to give h ( x ) \u2264 h ( y ) + \u3008\u2207 h ( y ) , x -y \u3009 + 1 2 \u2016 x -y \u2016 2 2 The above cases establish that h = min { f, c } is 1 smooth as well. It is easy to see that the function h is 1-Lipschitz as well as c -bounded. Since at all points of non-differentiabilty, we defined the gradient as the zero vector, the function has 1-bounded gradient norms as well. 31"}
