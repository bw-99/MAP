{
  "Data Pricing for Graph Neural Networks without Pre-purchased Inspection": "Yiping Liu âˆ— UESTC, China The University of Auckland, New Zealand yliu823@aucklanduni.ac.nz",
  "Jiamou Liu": "The University of Auckland, New Zealand jiamou.liu@auckland.ac.nz",
  "ABSTRACT": "Machine learning (ML) models have become essential tools in various scenarios. Their effectiveness, however, hinges on a substantial volume of data for satisfactory performance. Model marketplaces have thus emerged as crucial platforms bridging model consumers seeking ML solutions and data owners possessing valuable data. These marketplaces leverage model trading mechanisms to properly incentive data owners to contribute their data, and return a well performing ML model to the model consumers. However, existing model trading mechanisms often assume the data owners are willing to share their data before being paid, which is not reasonable in real world. Given that, we propose a novel mechanism, named Structural Importance based Model Trading (SIMT) mechanism, that assesses the data importance and compensates data owners accordingly without disclosing the data. Specifically, SIMT procures feature and label data from data owners according to their structural importance, and then trains a graph neural network for model consumers. Theoretically, SIMT ensures incentive compatible, individual rational and budget feasible. The experiments on five popular datasets validate that SIMT consistently outperforms vanilla baselines by up to 40% in both MacroF1 and MicroF1.",
  "KEYWORDS": "Model marketplaces; data pricing; structural entropy",
  "ACMReference Format:": "Yiping Liu, Mengxiao Zhang, Jiamou Liu, and Song Yang. 2025. Data Pricing for Graph Neural Networks without Pre-purchased Inspection. In Proc. of the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May 19 - 23, 2025 , IFAAMAS, 13 pages.",
  "1 INTRODUCTION": "In today's digital age, data has become an essential asset, serving as the foundation for AI and machine learning advancements. To âˆ— Equal contribution â€  Corresponding authors This work is licensed under a Creative Commons Attribution International 4.0 License. Proc. of the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS2025), Y. Vorobeychik, S. Das, A. NowÃ© (eds.), May 19 - 23, 2025, Detroit, Michigan, USA . Â© 2025 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). Mengxiao Zhang â€ âˆ— UESTC, China The University of Auckland, New Zealand mengxiao.zhang@auckland.ac.nz",
  "Song Yang": "The University of Auckland, New Zealand syan382@aucklanduni.ac.nz meet the increasing demand for high-quality data, a new business paradigm known as the model marketplace has emerged [24], exemplified by platforms like Modzy. A model marketplace facilitates the exchange between model consumers , who seek AI models for various tasks, and data owners , who possess the feature and label data necessary for model training. The marketplace purchases data from data owners, uses it to train AI models, and then sells these trained models to consumers. However, a key challenge in model marketplaces is determining how to properly compensate data owners for their contributions, a problem referred to as data pricing . This problem is challenging because the importance of data is difficult to evaluate. Most existing studies assume that marketplaces acquire data from data owners before paying them and use the subsequent performance improvements as a measure of data importance. For example, [2, 11, 17, 24, 46] rely on this assumption to establish pricing mechanisms based on the marginal impact of data on model accuracy. However, this pre-purchased inspection assumption is impractical in real-world settings. Data owners are often unwilling to release their data without proper payment, fearing that the data, once disclosed, may immediately provide valuable insights to buyers, reducing the incentive to pay. This leads to a critical question: How can we measure data importance for model training without direct inspection, thereby facilitating data pricing? Several studies have attempted to address this question by introducing exogenous metrics for measuring data importance, such as data age, accuracy, volume [14], the extent of perturbations [9, 39], or data owners' reputation [48]. However, these metrics often fail to accurately reflect the contribution of data in the context of model training, particularly when dealing with complex models like Graph Neural Networks (GNNs). Graph-structured data is prevalent in many real-world scenarios, where the relationships between entities are often as important as their attributes. GNNs excel in tasks involving such data, capturing both node features and network structure. However, data ownership is often decentralized, with different entities controlling separate 'pockets' of the network. This creates a need for a marketplace where subgraphs can be purchased and integrated to enable comprehensive model training [3]. For example, in finance, each bank holds its own subset of transaction data, but detecting fraud often requires analysing transaction flows across multiple institutions. Similarly, in healthcare, patient interactions are fragmented across hospitals, clinics, and insurance companies, forming an interconnected yet distributed network. In supply chain management, companies typically have visibility into their direct suppliers and customers, but the complete supply chain network spans many interdependent organisations. In all of these cases, the full value of the network data cannot be realised without aggregating subgraphs from multiple sources. For data consumers, aggregating subgraphs from multiple sources is essential for training reliable GNN models, enabling applications like fraud detection, personalized healthcare, and supply chain risk analysis. In this paper, we advance existing research by exploring the pricing of individual data points within subgraphs for GNN training. This introduces a distinct challenge, as the value of any given node to the model's performance is highly dependent on its structural role and connectivity within the broader network. To address this, we aim to develop pricing mechanisms that capture the marginal contribution of each data point, taking into account both its local features and its position within the global network structure. Notably, this is the first work to tackle the problem of pricing graphstructured data in a model marketplace for GNNs. In the following, we list our main contributions: Â· We propose Structural Importance based Model Trading (SIMT), a novel model marketplace framework for GNNs that integrates two phases: data procurement and model training . Figure 1 shows the conceptual framework of SIMT. Â· For data procurement, we put forward a new method for assessing the importance of graph-structured data. For this we present a novel marginal structural entropy to quantify node informativeness. This method of importance assessment is integrated with an auction mechanism to select data owners and fairly compensate them based on their contributions. We prove that this mechanism is incentive compatible, individual rational, and budget feasible. Â· For model training, we introduce the method of feature propagation to impute missing feature data for unselected nodes, enabling effective learning with partial data. We also design an edge augmentation method to enhance graph structure by adding connections involving unselected nodes, improving the GNN's ability to generalize. Â· Theproposed SIMT method was evaluated on five well-established benchmark datasets, and consistently outperformed four baseline mechanisms in node classification tasks. SIMT achieved up to a 40% improvement in MacroF1 and MicroF1 scores compared to the Greedy and ASCV methods, demonstrating its superior performance under various budget constraints.",
  "2 RELATED WORK": "Data pricing has been extensively studied in two main contexts: data marketplaces and model marketplaces . Data Pricing in Data Marketplaces. In data marketplaces, pricing mechanisms revolve around trading raw datasets or simple queries. Previous work has focused on pre-purchase decisions, where data is evaluated before it is accessed, which aligns with our setting. For instance, the importance of datasets is often quantified by metrics such as size, as explored by [20], or privacy levels, as in [30]. Other studies, such as [45] and [16], assess data importance based on its utility to consumers, proposing auction mechanisms and contracts to compensate data owners accordingly. When it comes to query-based data pricing , metrics like privacy levels directly impact the accuracy of responses, thereby influencing data value. For instance, [12, 33] propose auction mechanisms that incorporate privacy in queries, while [23] introduces a take-it-orleave-it contract for count queries. Further work by [10, 49] expands these ideas to linear predictor queries and broader query settings. In data marketplaces, data importance is often easily quantifiable using metrics like size or privacy levels. However, in the context of model marketplaces , the contribution of individual data points to machine learning model performance is more complex and requires novel pricing methods. Data Pricing in Model Marketplaces. In model marketplaces, data pricing is typically based on how much a dataset improves a machine learning model's performance. [1] introduced a theoretical framework for data pricing that balances budget constraints with model performance. Subsequent works, such as those by [2] and [11], assume the model's benefit is known and focus on fairly distributing rewards among data owners. A common method for this is the Shapley value [34], which compensates each data owner based on their contribution to the model. Various studies have refined the utility function used in Shapley value calculations by incorporating additional factors. [11] and [17], for example, include ğ¾ -nearest neighbors and privacy considerations in their utility designs. [24] builds on this by extending the Shapley value framework to model marketplaces. Other research, such as [36] and [28], explores utility design in collaborative machine learning scenarios, where data owners also serve as model consumers. In these cases, utility is defined either as the sum of the model's value to the owner and its marginal value to others [28], or through metrics like information gain [36]. [46] and [15] further define utility based on the cosine similarity of parameters or the privacy leakage of shared model parameters. A common limitation of these works is that they often require training models on the entire dataset before compensating data owners. In practice, this assumption is often unrealistic, as data owners are usually hesitant to contribute their data upfront without proper guarantees or compensation. A more realistic setting, which is closer to our approach, has been explored by studies [9, 39, 48]. [9] assume that data importance is known and apply a VCG auction mechanism to select and compensate data owners. [48] propose an auction mechanism that incorporates the reputation of data owners as a reflection of their contribution, while [39] design an auction that selects data owners based on their privacy requirements. Although these approaches offer valuable insights, they rely on exogenous metrics, such as reputation or privacy, which are often difficult to obtain or may not accurately reflect the intrinsic value of data for model training. In contrast, our work proposes a novel method to measure data importance without direct data inspection. By focusing on the structural properties of graph data and using techniques like structural entropy, we aim to create a fair and effective data pricing mechanism that overcomes the limitations of previous methods. Comparison with FL and AL. While Federated Learning (FL) and Active Learning (AL) are well-known paradigms for training models with distributed data, our approach differs in key ways. (1) In FL, each data owner trains a local model on private data, which is then aggregated into a global model while preserving Figure 1: The framework of structural importance-based model trading (SIMT) mechanism. graph cluster informa- budget structure tiveness structural purchased clustering represen- importance connection feature |tativeness ffeature & label imputation GNN model importance assessment auction training edge reported valuation data laugmentation model owners Data Procurement Model Training consumer privacy [47]. SIMT, by contrast, does not require data owners to train models. Instead, data is directly provided to a central model, allowing for optimizations like data augmentation that are not possible in FL's gradient-based aggregation. This eliminates the computational burden on data owners and allows for more flexible model improvements. (2) In AL, the model iteratively queries data points to refine learning, typically in multiple rounds [31]. SIMT, however, collects data in a single round, reducing overhead and cost. Furthermore, while AL assumes access to unlabeled data with labels provided iteratively [7, 50], SIMT addresses the real-world challenge of compensating data owners, ensuring they are fairly rewarded for their contributions upfront.",
  "3 PROBLEM FORMULATION": "Each data owner ğ‘– âˆˆ ğ‘‚ attaches a valuation to her attribute and label data of a single node, denoted by ğœƒ ğ‘– âˆˆ Î˜ , where Î˜ is the set of all possible valuations. The valuation ğœƒ ğ‘– indicates the minimum payment required by the data owner to allow the use of the attribute and connection data of a single node for model training. The valuation ğœƒ ğ‘– is privately known only to the data owner, but they may report a different valuation ğœƒ â€² ğ‘– â‰  ğœƒ ğ‘– if it serves their interests. We assume that each data owner values all their data subjects equally , implying that the total valuation is linearly dependent on the number of data records. Let ğœ½ ğ‘– be ğ‘– 's valuation vector for all nodes, i.e., ğœ½ ğ‘– B ( ğœƒ ğ‘–, 1 , . . . , ğœƒ ğ‘–,ğ‘› ğ‘– ) = ğœƒ ğ‘– Â· 1 , where ğœƒ ğ‘–,ğ‘£ is the valuation of ğ‘– for node ğ‘£ . The valuation of all data owners form a valuation matrix, denoted by ğœ½ , which is the concatenation ğœ½ 1 | Â· Â· Â· | ğœ½ ğ‘œ âˆˆ Î˜ ğ‘› . The model consumer has a budget , denoted by ğ›½ âˆˆ R + , for buying the prediction model trained on structure and attribute data.",
  "3.1 Model marketplace for graph data": "Weconsider a model marketplace where a model consumer interacts with multiple data owners to trade graph-structured data. This data is distributed among the various data owners. Let the overall graph be represented as an attributed graph G B ( ğ‘‰, ğ¸, ğ‘¿ , ğ’š ) , where ğ‘‰ is the set of nodes, representing individual data subjects, ğ¸ âŠ† ğ‘‰ Ã— ğ‘‰ is the set of edges. ğ‘¿ âˆˆ R ğ‘› Ã— ğ‘š is the feature matrix, where ğ‘› is the number of nodes and ğ‘š is the dimensionality of the feature vector, and ğ’š âˆˆ R ğ‘› is the label vector, where each entry corresponds to a label for each node. The adjacency matrix and normalised adjacency matrix of G are denoted as ğ‘¨ and Ëœ ğ‘¨ , resp. For each node ğ‘£ âˆˆ ğ‘‰ , let ğ’™ ğ‘£ âˆˆ R ğ‘š and ğ‘¦ ğ‘£ represent the feature vector and label of node ğ‘£ , resp. ğ‘ ğ‘£ âŠ† ğ‘‰ represents the set of neighbours of node ğ‘£ , and ğ‘‘ ğ‘£ B | ğ‘ ğ‘£ | denote the degree of node ğ‘£ . The graph G is distributed among multiple data owners, each controlling a subgraph. Let ğ‘‚ denote the set of data owners, where ğ‘œ B | ğ‘‚ | represents the total number of data owners. For each data owner ğ‘– âˆˆ ğ‘‚ , the subgraph held by owner ğ‘– is represented as G ğ‘– B ( ğ‘‰ ğ‘– , ğ¸ ğ‘– , ğ‘¿ [ ğ‘‰ ğ‘– ] , ğ’š [ ğ‘‰ ğ‘– ]) , where ğ‘‰ ğ‘– âŠ† ğ‘‰ is the set of nodes controlled by data owner ğ‘– , ğ¸ ğ‘– = ğ¸ âˆ© ( ğ‘‰ ğ‘– Ã— ğ‘‰ ğ‘– ) is the set of edges between nodes in ğ‘‰ ğ‘– , ğ‘¿ [ ğ‘‰ ğ‘– ] , and ğ’š [ ğ‘‰ ğ‘– ] are the feature matrix, and label vector induced by the nodes in ğ‘‰ ğ‘– , resp. Let ğ‘› ğ‘– B | ğ‘‰ ğ‘– | be the number of nodes in subgraph G ğ‘– . Denote the edges within subgraphs as / ğ¸ and the edges between the subgraphs as Â¥ ğ¸ . We assume that / ğ¸ âˆ© Â¥ ğ¸ = âˆ… and then ğ¸ = / ğ¸ âˆª Â¥ ğ¸ . We also assume that the internal structure of each subgraph, including the features and labels of nodes, is private to the corresponding data owner. However, the connections between subgraphs (i.e., Â¥ ğ¸ the edges connecting nodes from different subgraphs) are known by the model consumer. Data owners are willing to sell the feature, label, and connection data for the nodes they control. The model marketplace involves designing a mechanism that procures the attribute/structure data from data owners, and train a GNN model for the model consumer.",
  "3.2 Incentive mechanism": "Definition 3.1. A mechanism ğ‘€ consists of two functions, ( ğœ‹ (Â·) , ğ‘ (Â·)) , where ğœ‹ : Î˜ ğ‘› â†’{ 0 , 1 } ğ‘› is an allocation function and ğ‘ : Î˜ ğ‘› â†’ R ğ‘› is a payment function . Given a set of data owners and a model consumer, the mechanism takes the reported valuation ğœ½ â€² âˆˆ Î˜ ğ‘› as input, and outputs allocation result and payment result. The allocation function and the payment function determine which nodes are selected for model training and how much to pay for the data owners, resp. We write the allocation result ğ… ( ğœ½ â€² ) as ( ğ… 1 ( ğœ½ â€² ) , . . . , ğ… ğ‘œ ( ğœ½ â€² )) and the payment result ğ’‘ ( ğœ½ â€² ) as ( ğ’‘ 1 ( ğœ½ â€² ) , . . . , ğ’‘ ğ‘œ ( ğœ½ â€² )) , where each ğ… ğ‘– ( ğœ½ â€² ) , ğ’‘ ğ‘– ( ğœ½ â€² ) is a ğ‘› ğ‘– -dimensional vector with each element ğœ‹ ğ‘–,ğ‘£ , ğ‘ ğ‘–,ğ‘£ being an allocation and payment for ğ‘– 's node ğ‘£ . The allocation and payment of node ğ‘£ give data owner ğ‘– a utility ğ‘¢ ğ‘–,ğ‘£ ( ğœ½ â€² ) = ( ğ‘ ğ‘–,ğ‘£ ( ğœ½ â€² ) -ğœƒ ğ‘–,ğ‘£ ) ğœ‹ ğ‘–,ğ‘£ ( ğœ½ â€² ) . The utility of data owner ğ‘– is ğ‘¢ ğ‘– = Ë ğ‘£ âˆˆ ğ‘‰ ğ‘– ğ‘¢ ğ‘–,ğ‘£ . Once a node is selected, its connection, feature and label data are used for model training. Let ğœ½ -ğ‘– denote the valuation of all data owner but ğ‘– and Î˜ -ğ‘– denote the set of all possible ğœ½ -ğ‘– . A mechanism ğ‘€ should satisfy: Â· Incentive Compatible (IC): Each data owner ğ‘– âˆˆ ğ‘‚ gains maximum utility when truthfully reporting her valuation, i.e., ğ‘¢ ğ‘– ( ğœƒ ğ‘– , ğœ½ -ğ‘– ) â‰¥ ğ‘¢ ğ‘– ( ğœƒ â€² ğ‘– , ğœ½ -ğ‘– ) , âˆ€ ğœƒ ğ‘– , ğœƒ â€² ğ‘– âˆˆ Î˜ , âˆ€ ğœ½ -ğ‘– âˆˆ Î˜ -ğ‘– . Â· Individual Rational (IR): Each data owner ğ‘– âˆˆ ğ‘‚ gains an nonnegative utility when participating in the mechanism, i.e., ğ‘¢ ğ‘– ( ğœƒ ğ‘– , ğœ½ -ğ‘– ) â‰¥ 0 , âˆ€ ğœƒ ğ‘– âˆˆ Î˜ , âˆ€ ğœ½ -ğ‘– âˆˆ Î˜ -ğ‘– . Â· Budget Feasible (BF): Total payment given to all data owners is not exceed the budget ğ›½ , i.e., Ë ğ‘– âˆˆ ğ‘‚ ğ’‘ ğ‘– ğ… ğ‘– â‰¤ ğ›½ .",
  "3.3 Graph neural network models": "We use GNN as the prediction model. Given a graph, GNN predicts the node labels by stacking multiple layers. Let ğ¿ be the number of layers in a GNN model. The main idea is to iteratively aggregate the feature information of each node from its neighbours. Specifically, given an attributed graph G = ( ğ‘‰, ğ¸, ğ‘¿ , ğ’š ) , and a GNN with ğ¿ convolution layers, at a layer â„“ â‰¤ ğ¿ , the feature embedding ğ’‰ â„“ ğ‘£ of node ğ‘£ âˆˆ ğ‘‰ is generated through aggregation and update: Â· Aggregation: aggregate the feature embeddings ğ’‰ â„“ ğ‘¢ of all neighbours ğ‘¢ of ğ‘£ by an aggregate function such as mean and sum, with trainable weights, i.e., ğ’ â„“ ğ‘£ B Aggregator â„“ GLYPH<0> { ğ’‰ â„“ ğ‘¢ , âˆ€ ğ‘¢ âˆˆ ğ‘ ğ‘£ } GLYPH<1> . Â· Update: update the feature embedding ğ’‰ â„“ + 1 ğ‘£ at the next layer by an update function of the embedding ğ’‰ â„“ ğ‘£ and the aggregated embeddings ğ’ â„“ ğ‘£ , i.e., ğ’‰ â„“ + 1 ğ‘£ B Updater â„“ GLYPH<0> ğ’‰ â„“ ğ‘£ , ğ’ â„“ ğ‘£ GLYPH<1> . Initially, the feature embedding of node ğ‘£ is its feature vector, i.e., ğ’‰ 0 ğ‘£ B ğ’™ ğ‘£ .",
  "3.4 Optimisation problem": "As discussed in the Introduction, a key issue in determining compensation for data owners in a model marketplace is assessing the importance of their data to model training without direct inspection. To summarise, the problem in this paper is: Given the model marketplace with a model consumer and several data owners, we, as a data broker , aim to design a mechanism that procures the attribute/structural data from data owners, and train a GNN model for the consumer with the following subgoals: Â· assessing the importance of data to model training without disclosing the feature and label data; Â· optimising GNN performance within the budget; and Â· ensuring the mechanism is IC, IR, and BF. More formally, let â—¦ denote the Hadamard product operator, which selectively includes elements from / ğ¸, ğ‘¿ , ğ’š according to the indicator vector ğ… = ( ğ… 1 , . . . , ğ… ğ‘œ ) . We define ğ‘“ ğºğ‘ğ‘ (Â·) as the output of a GNN model trained on a selected subset of the data with the known Â¥ ğ¸ . The problem in the paper can be formulated as:     ğ’‘ ğ‘– â‰¥ 0 , ğœ‹ ğ‘–,ğ‘£ âˆˆ { 0 , 1 } âˆ€ ğ‘– âˆˆ ğ‘‚, âˆ€ ğ‘£ âˆˆ ğ‘‰ ğ‘–",
  "4 PROPOSED METHOD": "In this section, we propose a mechanism that procures the most contributing data and trains a GNN model using the procured data. By considering the correlation between graph structure, features, and labels, we leverage the graph structure to offer insights into the contribution of the associated data. Then, we combine this contribution assessment with the data owners' valuation in an auction mechanism to select the most cost-effective data. Subsequently, we augment the procured data using feature imputation and edge augmentation and use the augmented data to train a two-layer GNN model, which is returned to the model consumer. The overall framework is shown in Figure 1.",
  "4.1 Structural importance": "We begin by evaluating the importance of data to model training without inspecting the feature and label data. Our solution is motivated by the observation that the structure of a graph often encodes valuable information about its features and labels. According to the well-known homophily assumption, nodes with similar features and labels are more likely to be closely connected [27, 41]. This is further validated by our case studies on five real-world graphs. We analyse the connections both within and between classes, and the results show that the number of edges within the same class is substantially higher than between different classes, as illustrated in Figure 2. The strong correlation between graph structure and the associated features and labels motivates our approach to leverage the graph structure as auxiliary information in the data selection process. Thus we propose to use the structural importance of data owner to represent her data importance. Figure 2: Proportion of intra-class and inter-class edges Citeseer Pubmed 40% 80% 809 60% 409 20% 20% 109 20% 096 0% Amazo Coauthor 80% average ratio of 60% intra-class neighbours per node in a class 409 average ratio of inter-class neighbours 20% 20% per node in class 0% 0% 1 2 3 The sample dataset for model training should be both informative, reducing the uncertainty of the model, and representative, capturing the overall pattern of the entire dataset [50]. Therefore, we measure the structural importance of data owners in terms of informativeness and representativeness. Nevertheless, determining the informativeness and representativeness of nodes in a graph often relies on the true classes and node features, which are not available due to the absence of true feature and label data. To address this, we first use structural clusters to approximate the true classes. With this clustering, we propose the notion of marginal structural entropy to quantifies informativeness, and deploy PageRank centrality to quantify representativeness. Structuring clustering. A crucial tool for the structural clustering is structural entropy . Let ğº = ( ğ‘‰, ğ¸ ) represent a graph without attributes. Suppose ğ‘ƒ B { ğ¶ 1 , ğ¶ 2 , . . . , ğ¶ ğ‘‡ } is a partition of ğ‘‰ , where each ğ¶ ğ‘¡ is called a cluster and ğ‘‡ is the number of clusters. Structural entropy of ğº relative to ğ‘ƒ captures the information gain due to the partition. For each cluster ğ¶ ğ‘¡ âˆˆ ğ‘ƒ , write ğ‘‘ ğ‘¡ as the sum of degrees ğ‘‘ ğ‘£ of all nodes ğ‘£ âˆˆ ğ¶ ğ‘¡ . Write ğ‘” ğ‘¡ as the number of the edges between the nodes in ğ¶ ğ‘¡ and those in the other clusters. The structural entropy [25] of ğº relative to ğ‘ƒ is  Agreater value of H ğ‘ƒ ( ğº ) means that the corresponding partition ğ‘ƒ gains more information about the graph ğº and thus ğ‘ƒ is preferable. Given that, we would like to obtain a good partition by maximising the structural entropy H ğ‘ƒ ( ğº ) . Unfortunately, maximising the structural entropy H ğ‘ƒ ( ğº ) is NP-hard [22, 43]. As an alternative, we propose an algorithm Clustering ( ğº ) that harnesses the power of unsupervised GCN models [43] to obtain a partition ğ‘ƒ . Specifically, Clustering ( ğº ) first employs the Singular Value Decomposition (SVD) [5] to generate spectral features, and a classical Variational Graph Auto-Encoder (VGAE) model [19] with reconstruction loss taking the generated spectral features as input to learn node embeddings. Using the obtained node embedding, Clustering ( ğº ) then trains a linear classifier to get a partition by maximising structural entropy. Structural informativeness. Given the learned clustering ğ‘ƒ , we propose the notion of cluster-based marginal structural entropy to measure the structural informativeness of data owners. Basically, the marginal structural entropy captures the information gain of a node to the clustering. The lower the marginal structural entropy of a node has, the more uncertainty this node has, and thus more information the node's data will capture. More formally, we define the marginal structural entropy of node ğ‘£ as the information gain due to existence of ğ‘£ , i.e., the difference between the structural entropy of graph ğº relative to ğ‘ƒ and that of ğº without node ğ‘£ relative to ğ‘ƒ â€² . Then the normalised marginal structural entropy ğœ– ğ‘£ of ğ‘£ is the normalised difference in structural entropy with partition ğ‘ƒ and another partition ğ‘ƒ â€² that moves ğ‘£ out of its cluster ğ¶ ğ‘¡ , i.e., ğœ– ğ‘£ = (H ğ‘ƒ ( ğº ) - H ğ‘ƒ â€² ( ğº ))/H ğ‘ƒ ( ğº ) . Let ğ‘› ğ‘£,ğ‘¡ be the number of nodes that are incident to ğ‘£ and belong to ğ¶ ğ‘¡ . After calculation, we have the following (see App. A for the detailed calculation): Definition 4.1. The normalised marginal structural entropy of node ğ‘£ âˆˆ ğ¶ ğ‘¡ to structural entropy H ğ‘ƒ ( ğº ) is  A lower normalised marginal structural entropy means more structural uncertainty of ğ‘£ , making ğ‘£ more informative. Structural representativeness. We use a classical structural centrality measure, PageRank [26], to quantify the structural representativeness of a node. We opt for PageRank centrality due to its superior performance compared to other centrality measures, as validated in App. D. The higher the PageRank centrality of a node has, the more representative the node is. Let ğ›¾ âˆˆ ( 0 , 1 ) denote the damping factor, which controls the probability of following links. The PageRank centrality ğœŒ ğ‘£ of node ğ‘£ is:  Structural importance score. Given the clustering ğ‘ƒ , the entropy ğœ– ğ‘£ and the PageRank centrality ğœŒ ğ‘£ of each node ğ‘£ , we define the structural importance score. Specifically, we first sort all nodes in their own cluster by their entropy and PageRank values, resp. Nodes are sorted in ascending order by entropy (as lower entropy indicates higher informativeness) and in descending order by PageRank (as higher PageRank indicates greater representativeness). This ensures that more informative and representative nodes are prioritised. For a node ğ‘£ in cluster ğ¶ ğ‘¡ , let rank entr ğ‘£ denote its rank by entropy and rank pr ğ‘£ denote its rank by PageRank. We then define node ğ‘£ 's informativeness and representativeness based on these rankings as  Finally, we define the structural importance score of a node. Following the approach in [7, 50], we introduce a parameter ğ›¼ to balance representativeness and informativeness. Intuitively, representative data helps to learn general classification patterns, while informative data is used to refine the classification boundaries. Therefore, when the budget ğ›½ is relatively small compared to the overall valuations of data owners and the partition ğ‘ƒ is complex (i.e., when ğ‘‡ is large), prioritising representative data is crucial to learning the general classification patterns. On the other hand, when the budget is relatively large and the partition is simpler, a small amount of representative data is sufficient to capture the overall pattern, allowing us to focus on acquiring more informative data to further refine the classification. More formally, given the average valuation ğœƒ , defined as the average of the upper and lower bounds of data valuations, we set ğ›¼ = 1 2 ( 1 + ğ›½ ğ‘›ğœƒ ) -ğ‘‡ . The structural importance score of node ğ‘£ is then defined as",
  "4.2 Model trading mechanism": "We propose a model trading mechanism, named Structural importance based model trading (SIMT) mechanism , which consists of two phases: (1) data procurement phase selects the most cost effective data owners, and (2) model training phase trains a GNN model on the procured data; See the workflow of SIMT in Figure 1 and the algorithm in Alg. 1. Phase 1. Data procurement. In Phase 1, SIMT takes the attributed graph G and the valuation vector ğœ½ , and the budget ğ›½ as inputs and returns an allocation result ğ… and a payment result ğ’‘ . Firstly, Clustering ( ğº ) returns a clustering ğ‘ƒ of the nodes ğ‘‰ after training. Given the clustering ğ‘ƒ with ğ‘‡ clusters, the mechanism computes the structural importance score ğœ™ ğ‘£ of each node ğ‘£ âˆˆ ğ‘‰ using Equation (1), which represents the importance of the node. Then an auction is conducted in each cluster ğ¶ ğ‘¡ âˆˆ ğ‘ƒ with budget ğ›½ / ğ‘‡ . In the auction for each ğ¶ ğ‘¡ , nodes in ğ¶ ğ‘¡ are sorted in descending order based on the ratio ğœ™ ğ‘£ ğœƒ ğ‘–,ğ‘£ , where data owner ğ‘– owns node ğ‘£ . The mechanism selects the most cost-effective ğ‘˜ data until the total payment exceeds the allocated budget. The payment to data owner ğ‘– for node ğ‘£ is ğ‘ ğ‘–,ğ‘£ = min { ğ›½ ğ‘‡ ğœ™ ğ‘£ Ë ğ‘˜ ğ‘¢ = 1 ğœ™ ğ‘¢ , ğœƒ ğ‘—,ğ‘¤ ğœ™ ğ‘¤ ğœ™ ğ‘£ } , where ğ‘— is the first data owner who has not had any data selected and ğ‘¤ is her first data in the order (if such data owner ğ‘— does not exist, we set ğ‘ ğ‘–,ğ‘£ as min { ğ›½ğœ™ ğ‘£ /( ğ‘‡ Ë ğ‘˜ ğ‘¢ = 1 ğœ™ ğ‘¢ ) , Ëœ ğœƒğœ™ ğ‘£ / ğœ™ ğ‘˜ + 1 } , where Ëœ ğœƒ is the upper bound of Î˜ ). The total payment to data owner ğ‘– is Ë ğ‘£ âˆˆ ğ‘‰ ğ‘– ,ğ‘£ â‰¤ ğ‘˜ ğ‘ ğ‘–,ğ‘£ . Phase 2. Model training. Given the allocation result ğ… , the model training phase uses the connections, features and labels of the selected data owners to train a GNN model. However, due to budget constraints, only a subset of features and connections can be purchased, which may not be sufficient for training a robust GNN model. To address this, we first impute the missing node features",
  "Algorithm 1 The SIMT mechanism": "Input: Attributed graph G , data owners ğ‘‚ , valuation vector ğœ½ , and budget ğ›½ Output: Allocation ğ… , payment ğ’‘ , and trained model ğ‘“ GNN 1: let ğº = ( ğ‘‰, Â¥ ğ¸ ) represent the known subgraph of G without attributes. 2: Phase 1: Data procurement 3: get a partition ğ‘ƒ â† Clustering ( ğº ) 4: compute the structural importance score ğœ™ ğ‘£ for each ğ‘£ âˆˆ ğ‘‰ according to ğ‘ƒ 5: initialise ğ… = 0 , ğ’‘ = 0 6: for each cluster ğ¶ ğ‘¡ âˆˆ ğ‘ƒ do 7: sort the nodes ğ‘£ âˆˆ ğ¶ ğ‘¡ by ğœ™ğ‘£ ğœƒ ğ‘–,ğ‘£ in a descending order  9: for ğ‘£ â‰¤ ğ‘˜ do 10: Let ğ‘– âˆˆ ğ‘‚ be the data owner of node ğ‘£ }  12: procure data to get ğ‘¿ ğ‘  , ğ’š ğ‘  , update ğº and normalized adjacency matrix Ëœ ğ‘¨ 13: Phase 2: Model training 14: initialise ğ‘¿ â€² â† [ ğ‘¿ ğ‘  , 0 ğ‘¢ ] âŠº 15: while ğ‘¿ â€² has not converged do 16: ğ‘¿ â€² â† Ëœ ğ‘¨ğ‘¿ â€² 17: ğ‘¿ â€² â† [ ğ‘¿ ğ‘  , ğ‘¿ â€² ğ‘¢ ] âŠº 18: do edge augmentation on ğº and get ğº 19: ğ‘“ GNN â† Train ( ğº, ğ‘¿ â€² , ğ’š ğ‘  ) and augment the missing edges before training. After acquiring the features from the selected nodes, we apply the feature propagation algorithm [32] to infer the features of the unselected nodes, producing a new feature matrix ğ‘¿ â€² . Additionally, we use the ğº (| ğ‘‰ | , | ğ¸ |) Erdos-Renyi (ER) model [4] to generate missing edges, where | ğ‘‰ | and | ğ¸ | are determined by the edge density of the known graph. Then we incorporate contrastive learning [29] to mitigate the randomness introduced by the ER model, resulting in an augmented graph ğº . The GNN training algorithm then takes the augmented graph ğº , the new feature matrix ğ‘¿ â€² and labels ğ’š ğ‘  as input and returns a GNN model to the consumer. Node feature imputation. Features are crucial when training GNN [38, 40], and we apply a feature imputation method to the procured data to address missing values. Among various feature imputation methods, we choose the Feature Propagation algorithm due to its strong convergence guarantees, simplicity, speed, and scalability [32]. We use subscripts ğ‘  and ğ‘¢ to denote the selected and unselected nodes, resp. Write ğ‘¿ = [ ğ‘¿ ğ‘  , ğ‘¿ ğ‘¢ ] âŠº and ğ’š = [ ğ’š ğ‘  , ğ’š ğ‘¢ ] âŠº . Also, we write the normalised adjacency matrix Ëœ ğ‘¨ and the graph Laplacian Î” of  imputation process, the feature matrix ğ‘¿ â€² is initialised with the known feature ğ‘¿ ğ‘  and a zero matrix 0 ğ‘¢ for the unselected nodes. The feature matrix ğ‘¿ â€² is then iteratively updated as follows: ğ‘¿ ( ğ‘¡ ) =  matrix converges. The steady status of the feature matrix is [32]:  Edge Augmentation. Given the critical role of message passing in GNNs, the absence of certain edges may impede this process, leading to sub-optimal model performance. To alleviate this issue, we introduce augmented edges to enhance message passing. Specifically, we employ the ER model [4] to generate edges for data owners with multiple unselected nodes. However, the introduction of augmented edges may inadvertently introduce noise, which could mislead the model by learning from incorrect connections. To counteract this, we integrate contrastive loss [29], denoted by ğ¿ ctr , into the GNN training process. This loss function encourages the model to maximise the similarity between the augmented graph and the original (non-augmented) graph views. Given a graph G and an augmented graph G , let ğ’‰ ğ‘£ and ğ’‰ â€² ğ‘£ be the feature embeddings in G and G , resp. The contrastive loss of a node ğ‘£ is:  where ğœ represents the temperature parameter, which scales the similarities between the embeddings ğ’‰ ğ‘£ and ğ’‰ â€² ğ‘£ .",
  "4.3 Analysis": "Now we show that SIMT satisfies IC, IR and BF properties. Additionally, we analyse its time complexity. Theorem 4.2. The SIMT mechanism is incentive compatible, individual rational and budget feasible. Proof. We first show that the mechanism is IR. In each auction of cluster ğ¶ ğ‘¡ , the utility that a data owner ğ‘– obtains from a node ğ‘£ > ğ‘˜ when she truthfully reports is ğ‘¢ ğ‘–,ğ‘£ ( ğœ½ ) = ğœƒ ğ‘–,ğ‘£ ğœ‹ ğ‘–,ğ‘£ ( ğœƒ ) -ğ‘ ğ‘–,ğ‘£ ( ğœƒ ) = 0 -0 â‰¥ 0; the utility that a data owner ğ‘– obtains from ğ‘£ â‰¤ ğ‘˜ is ğ‘¢ ğ‘–,ğ‘£ ( ğœƒ ) = ğ‘ ğ‘–,ğ‘£ ( ğœƒ ) -ğœƒ ğ‘–,ğ‘£ ğœ‹ ğ‘–,ğ‘£ ( ğœƒ ) = min { ( ğ›½ğœ™ ğ‘£ ) /( ğ‘‡ Ë ğ‘˜ ğ‘¢ = 1 ğœ™ ğ‘¢ ) , ( ğœ™ ğ‘£ ğœƒ ğ‘—,ğ‘¤ ) / ğœ™ ğ‘¤ } -ğœƒ ğ‘–,ğ‘£ Ã— 1 } â‰¥ 0. Therefore, the utility of data owner ğ‘– is Ë ğ‘£ âˆˆ ğ‘‰ ğ‘– ğ‘¢ ğ‘–,ğ‘£ â‰¥ 0. Also, SIMT satisfies BF. The total payment in cluster ğ¶ ğ‘¡ is Ë ğ‘˜ ğ‘£ = 1 min { ( ğ›½ğœ™ ğ‘£ ) /( ğ‘‡ Ë ğ‘˜ ğ‘¢ = 1 ğœ™ ğ‘¢ ) , ( ğœƒ ğ‘—,ğ‘¤ ğœ™ ğ‘£ / ğœ™ ğ‘¤ } â‰¤ Ë ğ‘˜ ğ‘¢ = 1 ğœ™ ğ‘¢ Ã— ğ›½ /( ğ‘‡ Ë ğ‘˜ ğ‘¢ = 1 ğœ™ ğ‘¢ ) = ğ›½ / ğ‘‡ . Then the total payment in ğ‘‡ clusters is â‰¤ ğ›½ , which shows BF. Next we show IC. Each of data subject is assigned to an auction associated with a cluster ğ¶ ğ‘¡ . As the assignment is independent of the reported valuation of the data owner, we just need to show that in the auction for each cluster is IC. In one auction, we consider an arbitrary data owner. When she report truthfully, there are two cases regarding each of her node, either being selected or not. We discuss the two cases separately. (1) Consider an arbitrary node ğ‘£ that is selected. Assume that in the ranking, the ( ğ‘˜ + 1 ) -th node is possessed by data owner ğ‘— , i.e., the ( ğ‘˜ + 1 ) -th ratio is ğœ™ ğ‘˜ + 1 / ğœƒ ğ‘—,ğ‘˜ + 1 . Note that the data owner ğ‘— could be ğ‘– . If ğ‘– reports a lower valuation ğœƒ â€² ğ‘–,ğ‘£ < ğœƒ ğ‘–,ğ‘£ or a higher valuation ğœƒ ğ‘–,ğ‘£ < ğœƒ â€² ğ‘–,ğ‘£ < ğœƒ ğ‘—,ğ‘˜ + 1 ğœ™ ğ‘£ / ğœ™ ğ‘˜ + 1 , as the marginal contribution ğœ™ ğ‘£ is independent from the reported valuation, the ratio ğœ™ ğ‘£ / ğœƒ â€² ğ‘–,ğ‘£ â‰¥ ğœ™ ğ‘˜ + 1 / ğœƒ ğ‘—,ğ‘˜ + 1 and her ranking is still in the top ğ‘˜ . Further, the payment of ğ‘– for ğ‘£ is independent from ğ‘– 's report. As a consequence, her utility of ğ‘£ is ğ‘¢ ğ‘–,ğ‘£ ( ğœ½ â€² ğ‘– , ğœ½ -ğ‘– ) = ğ‘¢ ğ‘–,ğ‘£ ( ğœ½ ğ‘– , ğœ½ -ğ‘– ) . If she reports a even higher valuation ğœƒ â€² ğ‘–,ğ‘£ â‰¥ ğœƒ ğ‘—,ğ‘˜ + 1 ğœ™ ğ‘£ / ğœ™ ğ‘˜ + 1 , the ratio ğœ™ ğ‘˜ + 1 / ğœƒ ğ‘—,ğ‘˜ + 1 > ğœ™ ğ‘£ / ğœƒ â€² ğ‘–,ğ‘£ . Then her allocation becomes 0 and her utility of ğ‘£ is ğ‘¢ ğ‘–,ğ‘£ ( ğœ½ â€² ğ‘– , ğœ½ -ğ‘– ) = 0 â‰¤ ğ‘¢ ğ‘–,ğ‘£ ( ğœ½ ğ‘– , ğœ½ -ğ‘– ) . (2) Consider a node ğ‘£ that is not selected. Assume that in the ranking, the ğ‘˜ -th node is possessed by data owner ğ‘— , i.e, the ğ‘˜ -th ratio is ğœ™ ğ‘˜ / ğœƒ ğ‘—,ğ‘˜ . Here, ğ‘— could also be ğ‘– . If ğ‘– reports a higher valuation ğœƒ â€² ğ‘–,ğ‘£ > ğœƒ ğ‘–,ğ‘£ or a lower valuation ğœƒ ğ‘—,ğ‘˜ ğœ™ ğ‘£ / ğœ™ ğ‘˜ â‰¤ ğœƒ â€² ğ‘–,ğ‘£ < ğœƒ ğ‘–,ğ‘£ , the ratio ğœ™ ğ‘˜ / ğœƒ ğ‘—,ğ‘˜ â‰¥ ğœ™ ğ‘£ / ğœƒ â€² ğ‘–,ğ‘£ , and ğ‘£ 's ranking is still not among the first ğ‘˜ . Then ğ‘– 's utility of ğ‘£ is ğ‘¢ ğ‘–,ğ‘£ ( ğœ½ â€² ğ‘– , ğœ½ -ğ‘– ) = ğ‘¢ ğ‘–,ğ‘£ ( ğœ½ ğ‘– , ğœ½ -ğ‘– ) = 0. If ğ‘– reports a much lower valuation ğœƒ â€² ğ‘–,ğ‘£ < ğœƒ ğ‘—,ğ‘˜ ğœ™ ğ‘£ / ğœ™ ğ‘˜ , and her ranking is among the first ğ‘˜ . Her  Time complexity. Given a GNN, recall ğ‘š is the dimensions of the input and let ğ‘š h be the hidden layers. The computational complexity of a typical GNN is ğ‘‚ (| ğ¸ | ğ‘š + | ğ‘‰ | ğ‘šğ‘š h ) . The computational complexity of SIMT is ğ‘‚ ((| ğ¸ | ğ‘š +| ğ‘‰ | ğ‘šğ‘š h )+(| ğ¸ | ğ‘š +| ğ‘‰ | ğ‘šğ‘š h )+(| ğ‘‰ |+| ğ¸ |)) , where the first one terms correspond to the complexity of training the GNN, while the last two terms account for the computation of clustering and PageRank centrality resp.",
  "5 EXPERIMENT": "We conduct experiments to validate the performance of proposed SIMT mechanism in terms of node classification accuracy. (1) To demonstrate the overall performance of SIMT, we compare it with multiple baselines under different budgets. (2) To underscore the impact of each component, we perform a detailed ablation study.",
  "5.1 Experiment setup": "Dataset. Five widely-used datasets are included in our experiments: Cora, Citeseer, Pubmed, Amazon and Coauthor [7, 18, 32, 35]. The dataset statistics are listed in Table 3 in App. B. For each dataset, we randomly sample 15% of the data as test set, which remains untouched during data procurement. This set is consistent across all baselines. Once getting the selected data from auction, we further split 80% as training data and remaining 20% for validation. To accommodate different real-world scenarios, we follow the setup in existing studies [15, 17, 24, 28, 36, 46] to validate SIMT on various datasets and varying hyperparameters. Data valuations. We generate a set of random numbers to represent the data valuations. The valuations are sampled at random i.i.d. following a series of normal distributions N( ğœ‡, ğœ 2 ) . We get a ğœ‡ drawn from U[ 0 . 8 , 1 . 2 ] for each class to capture the difference in valuations between classes. Then for each data owner, we set the valuation of each data subject as the mean of the generated valuations of her data subjects. We set ğœ = 0 . 1. The effect of different ğœ s on performance is investigated and the results are in App. C. To ensure all valuations are non-negative, we use a resample trick [6]. The generated valuations are in the range [ 0 , 2 ] . Note that when the domain is different, we could scale it into [ 0 , 2 ] . Budget. We set the budget in { 50 , 100 , 150 , 200 , 250 , 300 } . Given that the data valuation range is [ 0 , 2 ] , the number of selected data is approximately from 50 to 300, which is aligned with the setup of the studies on label selection e.g. [7, 50]. Structural clustering. Here, we give the configuration of the model used for Clustering ( ğº ) . We deploy SVD [5] to generate spectral node features, VGAE [19] to learn node embeddings followed by a linear classifier to learn the partition. We set the hidden size as 32, the learning rate as 0 . 01, the L2 regularisation as 5 Ã— 10 -4 . The total training budget is 400 epochs. The clustering model is initialised to solely minimise the reconstruction loss. We repeat this process 100 epochs to comprehensively capture the graph structure information. Using the obtained node embeddings, we train the linear classifier to learn a partition, maximising the structural entropy. This process is repeated 300 epochs to obtain a robust partition. GNN model. We employ classical GNN models as ğ‘“ GNN to learn node classification. Following the configurations of [18], we set the hidden size as 32, the total training budget as 200, the learning rate as 0 . 01 and the L2 regularisation as 5 Ã— 10 -4 . The models are optimised with minimising both reconstruction loss ğ¿ recon and classification loss ğ¿ class on the train data. We repeat 10 training iterations with different random seeds and report the average performance. To mitigate the impact of randomness in train-validation splitting, each training iteration creates 10 train-validation splits, trains 10 independent models according to the split and reports the best model according to their performance on the validation set. Ultimately, we evaluate the model performance using the test data. In other words, each experimental result is derived from 100 runs. Wepresent the results using a GCN model and defer the exploration on the effects of different GNN architectures in App. F. Subgraph. Each data owner possesses a subgraph with at least one node. We vary both the number ğ‘œ of data owners and the size ğ‘› ğ‘– of theire subgraphs. We first fix the number of data owners at 10, and vary the subgraph size within { 20 , 40 , 60 , 80 } to investigate the effect of subgraph size. Next, we fix the subgraph size at 80, and vary the number of data owners within { 5 , 10 , 15 , 20 } to investigate the effect of the number of data owners. The comparison results are presented in App. E. Baselines. To validate the overall performance of the SIMT, we benchmark it against four baseline mechanisms. These baselines incorporate different methods for assessing data importance within our proposed model trading framework. The baselines are: Â· Greedy [37]: The Greedy mechanism treats all data as equally important and procures data based solely on the valuations of data owners. No feature propagation is applied. Â· ASCV[8]: The ASCV mechanism first trains a VGAE model on the graph to learn node embeddings with optimising reconstruction loss, and evaluates data importance by the nodes' contribution to the reconstruction loss. The greater the contribution of the node, the more important the corresponding owner's data is. Then the auction procures data according to the ratio of data importance to valuation. No feature propagation is applied. Â· Greedy(P): The Greedy(P) mechanism is the same as the Greedy except for that a feature imputation is applied. Â· ASCV(P): The ASCP(P) mechanism is the same as the ASCV except for that a feature imputation is applied. Note that ASCV is originally designed using various techniques to evaluate data importance. However, in the absence of features, only the VGAE technique can be directly applied in our scenario. For fair comparison, we redesign all baselines to avoid pre-purchase inspection of data, and set same seeds for all places involving randomness, including edge augmentation and model initialisation. Implementation. All experiments are conducted on a single machine with AMD Ryzen 9 5900X 12-Core CPU, and NVIDIA GeForce RTX 3090 GPU. The code is implemented using Python 3 . 10 and Pytorch 2 . 0 . 1. Our code is available in the supplementary material.",
  "5.2 Overall performance": "The experiment results are presented in Table 1. Here, we fix the number of data owners at 10, with each owner holding 80 data subjects. As shown in the table, SIMT consistently outperforms all Table 1: Node classification performance under different budgets baselines under all budgets. Compared to the vanilla Greedy and ASCV, SIMT improves up to 40% in both MacroF1 and MicroF1. Also, the last column shows the contribution per node, i.e., calculated as the average accuracy divided by the number of purchased nodes. The results consistently show that the contribution per node of SIMT is higher than that of all baselines, demonstrating the data selected by SIMT is more valuable. This validates the effectiveness of our structural importance assessment method in Sec. 4.1. Table 1 also shows that the ASCV/ASCV(P) mechanism outperforms the Greedy/Greedy(P) mechanism. This could be attributed to that both ASCV and ASCV(P) assess the importance of data based on their structural contribution to the reconstruction loss, which, in a way, reflects structural uncertainty. However, ASCV and ASCV(P) do not perform as well as SIMT. This discrepancy underscores the effectiveness of structural importance score. Lastly, Table 1 shows that the ASCV(P) and Greedy(P) outperform their vanilla versions by up to 20% in both MacroF1 and MicroF1. This validate the need of feature imputation. Same trend is observed in the scenario with ğ‘› ğ‘– = 1 , âˆ€ ğ‘– âˆˆ ğ‘‚ . See more details in Table 10 of App. F.",
  "5.3 Ablation study": "To explore the impact of each component in SIMT on its performance, we conduct ablation studies across the five datasets and present the average test accuracy. As shown in Table 2, the four components, i.e., structuring clustering (clust), structural informativeness (info), structural representativeness (rep), and edge augmentation (edge aug), distinctly enhances SIMT's performance. In general, clust plays contributes the most among all components, which underscores the crucial role of structural clustering. Without clustering, there is a high probability that the procured data are unevenly distributed across the classes, leading to a biased training dataset. The second contributor is edge augmentation, which highlights the role of missing edge augmentation in the training Table 2: The impact of each component process. Without edge augmentation, the message passing process is likely hindered, resulting in sub-optimal performance. We also compare the effect of data valuations deviation (in App. C), graph centrality metrics (in App. D), subgraph parameters (in App. E), and GNN architectures (in App. F).",
  "6 CONCLUSION": "In this paper, we aim to design a mechanism that properly incentives data owners to contribute their data, and returns a well performing GNN model to the model consumer for model marketplaces. In particular, we focus on the question of how we can measure data importance for model training without direct inspection. We propose SIMT, which consists of a data procurement phase and a model training phase. For data procurement, we incorporate a structure-based importance assessment method into an auction mechanism. For model training, we introduce and design two effective methods to impute missing data. As a result, SIMT ensures no data disclosure and incentive properties. Experimental results demonstrate that SIMT outperforms the baselines by up to 40% in accuracy. To the best of our knowledge, SIMT is the first model trading mechanism addressing the data disclosure problem. In the future, we will further consider the potential privacy leakage in the trained model.",
  "ACKNOWLEDGMENTS": "This work is supported by National Natural Science Foundation of China #62172077 and China Scholarship Council Grant #201906030067.",
  "REFERENCES": "[1] Jacob Abernethy, Yiling Chen, Chien-Ju Ho, and Bo Waggoner. 2015. Lowcost learning via active data procurement. In Proceedings of the Sixteenth ACM Conference on Economics and Computation . 619-636. [2] Anish Agarwal, Munther Dahleh, and Tuhin Sarkar. 2019. A marketplace for data: An algorithmic solution. In Proceedings of the 2019 ACM Conference on Economics and Computation . 701-726. [3] Dimitrios Bechtsis, Naoum Tsolakis, Eleftherios Iakovou, and Dimitrios Vlachos. 2022. Data-driven secure, resilient and sustainable supply chains: gaps, opportunities, and a new generalised data sharing and data monetisation framework. International Journal of Production Research 60, 14 (2022), 4397-4417. [4] BÃ©la BollobÃ¡s. 1998. Random graphs . Springer. [5] Steven L Brunton and J Nathan Kutz. 2022. Data-driven science and engineering: Machine learning, dynamical systems, and control . Cambridge University Press. [6] John Burkardt. 2014. The truncated normal distribution. Department of Scientific Computing Website, Florida State University 1 (2014), 35. [7] Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. 2017. Active learning for graph embedding. arXiv preprint arXiv:1705.05085 (2017). [8] Akshay L Chandra, Sai Vikas Desai, Chaitanya Devaguptapu, and Vineeth N Balasubramanian. 2021. On initial pools for deep active learning. In NeurIPS 2020 Workshop on Pre-registration in Machine Learning . PMLR, 14-32. [9] Mingshu Cong, Han Yu, Xi Weng, Jiabao Qu, Yang Liu, and Siu Ming Yiu. 2020. A VCG-based Fair Incentive Mechanism for Federated Learning. (2020). [10] Pranav Dandekar, Nadia Fawaz, and Stratis Ioannidis. 2012. Privacy auctions for recommender systems. In Proceedings of the 8th international conference on Internet and Network Economics . Springer-Verlag, 309-322. [11] Amirata Ghorbani and James Zou. 2019. Data shapley: Equitable valuation of data for machine learning. In International conference on machine learning . PMLR, 2242-2251. [12] Arpita Ghosh and Aaron Roth. 2011. Selling privacy at auction. In Proceedings of the 12th ACM conference on Electronic commerce . 199-208. [13] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances in neural information processing systems 30 (2017). [14] Judd Randolph Heckman, Erin Laurel Boehmer, Elizabeth Hope Peters, Milad Davaloo, and Nikhil Gopinath Kurup. 2015. A pricing model for data markets. IConference 2015 Proceedings (2015). [15] Rui Hu and Yanmin Gong. 2020. Trading data for learning: Incentive mechanism for on-device federated learning. In GLOBECOM 2020-2020 IEEE Global Communications Conference . IEEE, 1-6. [16] Jeevan Jaisingh, Jack Barron, Shailendra Mehta, and Alok Chaturvedi. 2008. Privacy and pricing personal information. European Journal of Operational Research 187, 3 (2008), 857-870. [17] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang, Costas J. Spanos, and Dawn Song. 2019. Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms. Proceedings of the VLDB Endowment (2019). [18] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [19] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308 (2016). [20] Avanish Kushal, Sharmadha Moorthy, and Vikash Kumar. 2012. Pricing for data markets. online] https://courses. cs. washington. edu/courses/cse544/11wi/projects/kumar_kushal_moorthy. pdf (2012). [21] Vito Latora, Vincenzo Nicosia, and Giovanni Russo. 2017. Complex networks: principles, methods and applications . Cambridge University Press. [22] Angsheng Li and Yicheng Pan. 2016. Structural information and dynamical complexity of networks. IEEE Transactions on Information Theory 62, 6 (2016), 3290-3339. [23] Katrina Ligett and Aaron Roth. 2012. Take it or leave it: Running a survey when privacy comes at a cost. In International Workshop on Internet and Network Economics . Springer, 378-391. [24] Jinfei Liu. 2020. Dealer: end-to-end data marketplace with model-based pricing. arXiv preprint arXiv:2003.13103 (2020). [25] Yiwei Liu, Jiamou Liu, Zijian Zhang, Liehuang Zhu, and Angsheng Li. 2019. REM: From structural entropy to community structure deception. Advances in Neural Information Processing Systems 32 (2019). [26] Nan Ma, Jiancheng Guan, and Yi Zhao. 2008. Bringing PageRank to the citation analysis. Information Processing & Management 44, 2 (2008), 800-810. [27] Miller McPherson, Lynn Smith-Lovin, and James M Cook. 2001. Birds of a feather: Homophily in social networks. Annual review of sociology 27, 1 (2001), 415-444. [28] Olga Ohrimenko, Shruti Tople, and Sebastian Tschiatschek. 2019. Collaborative machine learning markets with data-replication-robust payments. arXiv preprint arXiv:1911.09052 (2019). [29] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [30] Javier Parra-Arnau. 2018. Optimized, direct sale of privacy in personal data marketplaces. Information Sciences 424 (2018), 354-384. [31] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. 2021. A survey of deep active learning. ACM computing surveys (CSUR) 54, 9 (2021), 1-40. [32] Emanuele Rossi, Henry Kenlay, Maria I Gorinova, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein. 2022. On the unreasonable effectiveness of feature propagation in learning on graphs with missing node features. In Learning on Graphs Conference . PMLR, 11-1. [33] Aaron Roth and Grant Schoenebeck. 2012. Conducting truthful surveys, cheaply. In Proceedings of the 13th ACM Conference on Electronic Commerce . 826-843. [34] Lloyd S Shapley. 1951. Notes on the n-person game-ii: The value of an n-person game. (1951). [35] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868 (2018). [36] Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, and Bryan Kian Hsiang Low. 2020. Collaborative machine learning with incentive-aware model rewards. In International conference on machine learning . PMLR, 89278936. [37] Yaron Singer. 2010. Budget feasible mechanisms. In 2010 IEEE 51st Annual Symposium on foundations of computer science . IEEE, 765-774. [38] Indro Spinelli, Simone Scardapane, and Aurelio Uncini. 2020. Missing data imputation with adversarially-trained graph convolutional networks. Neural Networks 129 (2020), 249-260. [39] Peng Sun, Xu Chen, Guocheng Liao, and Jianwei Huang. 2022. A profitmaximizing model marketplace with differentially private federated learning. In IEEE INFOCOM 2022-IEEE Conference on Computer Communications . IEEE, 1439-1448. [40] Hibiki Taguchi, Xin Liu, and Tsuyoshi Murata. 2021. Graph convolutional networks for graphs containing missing features. Future Generation Computer Systems 117 (2021), 155-168. [41] Bisheng Tang, Xiaojun Chen, Shaopu Wang, Yuexin Xuan, and Zhendong Zhao. 2023. Generalized heterophily graph data augmentation for node classification. Neural Networks (2023). https://doi.org/10.1016/j.neunet.2023.09.021 [42] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017). [43] Yifei Wang, Yupan Wang, Zeyu Zhang, Song Yang, Kaiqi Zhao, and Jiamou Liu. 2023. User: Unsupervised structural entropy-based robust graph neural network. arXiv preprint arXiv:2302.05889 (2023). [44] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826 (2018). [45] Lei Xu, Chunxiao Jiang, Yan Chen, Yong Ren, and K. J. Ray Liu. 2015. Privacy or Utility in Data Collection? A Contract Theoretic Approach. IEEE Journal of Selected Topics in Signal Processing 9, 7 (2015), 1256-1269. [46] Xinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao, Chuan Sheng Foo, and Bryan Kian Hsiang Low. 2021. Gradient driven rewards to guarantee fairness in collaborative machine learning. Advances in Neural Information Processing Systems 34 (2021), 16104-16117. [47] Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao. 2021. A survey on federated learning. Knowledge-Based Systems 216 (2021), 106775. [48] Jingwen Zhang, Yuezhou Wu, and Rong Pan. 2021. Incentive mechanism for horizontal federated learning based on reputation and reverse auction. In Proceedings of the Web Conference 2021 . 947-956. [49] Mengxiao Zhang, Fernando Beltran, and Jiamou Liu. 2020. Selling Data at an Auction under Privacy Constraints. In Conference on Uncertainty in Artificial Intelligence . PMLR, 669-678. [50] Wentao Zhang, Yu Shen, Yang Li, Lei Chen, Zhi Yang, and Bin Cui. 2021. Alg: Fast and accurate active learning framework for graph convolutional networks. In Proceedings of the 2021 International Conference on Management of Data . 23662374. Table 4: The impact of different standard deviations for ğ‘› ğ‘– = 1",
  "APPENDIX": "",
  "A NORMALISED MARGINAL STRUCTURAL ENTROPY": "The calculation of the normalised marginal structural entropy is as follows. The marginal structural entropy is:  We normalise this value and get",
  "B DATASET STATISTICS": "Table 3: Dataset statistics",
  "C COMPARISON OF DATA VALUATIONS DEVIATION": "We conduct an experiment to evaluate the effect of the standard deviation in data valuation distribution on scenario with ğ‘› ğ‘– = 1 , ğ‘– âˆˆ ğ‘‚ . The experiment is evaluated on Cora, Citeseer and Pubmed datasets with standard deviation ğœ ranging from 0 . 01 to 0 . 2. In Table 4, we list the MacroF1 and MicroF1 performance of our mechanism running on varying standard deviations of data valuations. We also list the number of bought data to compare the effect of ğœ on data quantity. We observe that the number of bought data increases as ğœ increases. The increasing data quantity brings in more information  for training GNN model and thus improves the model performance. To balance the effect of data quantity, we fix ğœ = 0 . 1 for all the other experiments. To eliminate the randomness induced by unknown subgraphs, we conduct experiments on the case where all data owners are individuals.",
  "D COMPARISON OF GRAPH CENTRALITY METRICS": "Weevaluate the effectiveness of various graph centrality metrics on Cora dataset. To eliminate the randomness induced by subgraphs, we conduct experiments on the scenario with ğ‘› ğ‘– = 1 for all ğ‘– âˆˆ ğ‘‚ . Specifically, we substitute the PageRank centrality used in our structural representativeness with three other widely used centrality metrics: degree centrality, closeness centrality, and betweenness centrality [21]. We report the test accuracy with varying budgets. As shown in Table 5, PageRank centrality consistently surpasses the other metrics. Consequently, our mechanism employs PageRank centrality as structural representativeness. Table 5: Comparison of graph centrality metrics ( ğ‘› ğ‘– = 1 )",
  "E COMPARISON OF SUBGRAPH PARAMETERS": "Here we explore the effect of the subgraph size and the number of data owners. To reduce the effect of randomness, we use ğ‘› ğ‘– = 1 as a specific scenario to measure the effect of other parameters. The experimental results on the effects due to subgraph size and number of subgraphs are shown in Tables 6 and 7. When fixing the budget as 1500 and the number of data owners as 10, we find that the larger the subgraph size is, the more edges within the subgraph are missing, which hinders the message passing process and results in worse model performance, as shown in Table 6. Then when fixing the budget at 1500 and the subgraph size at 80, Table 7 shows that the more subgraphs exist, the more edges within subgraphs are missing, resulting in worse model performance. These experiments underscore the negative effect of missing edges during model training. Consequently, we utilise an edge augmentation technique to alleviate the impact of these missing edges. Table 6: The impact of subgraph size ( ğ‘› ğ‘– â‰¥ 1 ) Table 7: The impact of subgraph numbers ( ğ‘› ğ‘– â‰¥ 1 ) Table 8: The number of bought data under different budgets for ğ‘› ğ‘– = 1 Table 9: The impact of each component ( ğ‘› ğ‘– = 1 )",
  "F COMPARISON OF GNN ARCHITECTURES": "To test the effect of GNN architectures, we also conduct experiments on scenario with ğ‘› ğ‘– = 1 , ğ‘– âˆˆ ğ‘‚ using three representative GNN architectures: GIN [44], GraphSage [13], and GAT [42]. These architectures have emerged as alternatives to the traditional GCN [18] and have gained widespread acceptance within the GNN community. The configuration of these architectures are the same as that of GCN as shown in GNN models part in Sec. 5.1. To eliminate the randomness induced by subgraphs, we conduct experiments on the case where all data owners are individuals. The experiment results are shown in Tables 8, 10, 11, 12, and 13. In general, SIMT outperforms its benchmarks in all experiments. When compared to the Greedy mechanism, SIMT exhibits an improvement ranging from 10% to 40% in both MacroF1 and MicroF1, consistently confirming the effectiveness of SIMT. However, a minor fluctuation is observed when compared to ASCV(P) within the GraphSage architecture on the Citeseer dataset. More specifically, when the budget is 1500, SIMT demonstrates a 1 . 1% improvement in MacroF1, albeit a 0 . 2% decrease in MicroF1. Furthermore, when the budget is increased to 3000, SIMT shows a slight decrease by 0 . 2% in MacroF1, but a notable increase by 1 . 3% in MicroF1. Consequently, this fluctuation does not detract from the overall effectiveness of SIMT. When comparing the effects of different architectures, it is observed that the GraphSage architecture generally outperforms the others. This superior performance is particularly noticeable when the budget is low. Additionally, the GraphSage architecture exhibits lower variance, which implies robust performance. This could potentially be attributed to the fact that GraphSage does not heavily depend on node features and is well suited to spectral features [42]. This compatibility allows it to effectively leverage the spectral properties of the data, potentially contributing to its robust performance across various experiments. Furthermore, when the budget is set to 3000, the GraphSage architecture underperforms the GIN architecture on the Cora dataset. This could potentially be attributed to the theoretically stronger learnability of GIN compared to GraphSage [44]. Last, the GAT architecture performs the worst among the four architectures. This could be due to the fact that GAT employs an attention mechanism, which has significantly more parameters and thus requires a sufficient amount of feature and label data for parameter training.",
  "G ABLATION STUDY FOR ğ‘› ğ‘– = 1": "We conduct ablation studies across the three datasets and present the average test accuracy for ğ‘› ğ‘– = 1. As shown in Table 9, (1) the accuracy of SIMT without clustering diminishes by 8 . 3% in Cora, 3 . 7% in Citeseer, and 13 . 7% in Pubmed. This significant reduction underscores the crucial role of structural clustering. Without clustering, there is a high probability that the procured data are unevenly distributed across the classes, leading to a biased training dataset. (2) The accuracy of SIMT without structural informativeness drops by 1 . 8% in Cora, 2 . 1% in Citeseer, and 2 . 9% in Pubmed. (3) The accuracy reduction for SIMT without structural representativeness is 1 . 8% in Cora, 1 . 5% in Citeseer, and 2 . 2% in Pubmed. Overall, each component distinctly enhances SIMT's performance. In the scenario with ğ‘› ğ‘– = 1 , ğ‘– âˆˆ ğ‘‚ , the results show similar trends as the scenario with ğ‘› ğ‘– â‰¥ 1 , ğ‘– âˆˆ ğ‘‚ : SIMT performs the best under all budgets, followed by ASCV/ASCV(P), and Greedy/Greedy(P) performs the worst. It is worth to notice that SIMT performs even better when ğ‘› ğ‘– = 1 , ğ‘– âˆˆ ğ‘‚ than when ğ‘› ğ‘– â‰¥ 1 , ğ‘– âˆˆ ğ‘‚ . In particular, SIMT improves 20% âˆ¼ 40% in both MacroF1 and MicroF1 in this case; See more details in Table 10 of App. F. This can be attributed to that when ğ‘› ğ‘– = 1 , ğ‘– âˆˆ ğ‘‚ , fewer edges are missing within subgraphs, allowing the structural importance scores calculated by our mechanism to more accurately reflect the true structural importance of each node.",
  "H LIMITATIONS AND FUTURE WORK": "Even though our SIMT mechanism shows the incentive properties theoretically and the accuracy empirically, we acknowledge certain limitations that offer avenues for future research. Â· This paper addresses issues related to pre-purchase inspections. However, there are concerns about privacy breaches, as the raw data shared with data brokers and the trained models returned to data consumers could expose private information of the data owners. Future work could enhance the SIMT mechanism by incorporating considerations of these privacy issues Â· In the design of the SIMT mechanism, we assess the structural importance once and subsequently make a single purchasing decision for all data owners. An alternative approach could involve an adaptive purchasing method, where data points are bought in several times, each time reassessing the structural importance in light of newly acquired data. The primary rationale for the existing design is efficiency compared to this adaptive method. Nonetheless, developing Table 10: Node classification performance under different budgets using GCN architecture for ğ‘› ğ‘– = 1 Table 11: Node classification performance under different budgets using GIN architecture for ğ‘› ğ‘– = 1 Table 12: Node classification performance under different budgets using GraphSage architecture for ğ‘› ğ‘– = 1 an efficient adaptive model training mechanism presents an exciting avenue for future research. Â· The efficacy of the SIMT mechanism heavily depends on the quality of clustering; more accurate structural importance assessment is achieved when clusters are closer to the true classes. Our SIMT mechanism employs a classical VGAE model [19] as the Clustering . However, our findings indicate that this approach is not always robust. Exploring a broader range of GNN models for clustering might yield more stable results and enhance the performance of the SIMT mechanism, although such investigations fall outside the scope of this paper Table 13: Node classification performance under different budgets using GAT architecture for ğ‘› ğ‘– = 1",
  "keywords_parsed": [
    "Model marketplaces",
    " data pricing",
    " structural entropy"
  ]
}