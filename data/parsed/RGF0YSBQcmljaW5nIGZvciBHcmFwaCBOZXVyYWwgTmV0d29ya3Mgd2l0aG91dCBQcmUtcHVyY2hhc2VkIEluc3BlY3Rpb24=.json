{"Data Pricing for Graph Neural Networks without Pre-purchased Inspection": "Yiping Liu \u2217 UESTC, China The University of Auckland, New Zealand yliu823@aucklanduni.ac.nz", "Jiamou Liu": "The University of Auckland, New Zealand jiamou.liu@auckland.ac.nz", "ABSTRACT": "Machine learning (ML) models have become essential tools in various scenarios. Their effectiveness, however, hinges on a substantial volume of data for satisfactory performance. Model marketplaces have thus emerged as crucial platforms bridging model consumers seeking ML solutions and data owners possessing valuable data. These marketplaces leverage model trading mechanisms to properly incentive data owners to contribute their data, and return a well performing ML model to the model consumers. However, existing model trading mechanisms often assume the data owners are willing to share their data before being paid, which is not reasonable in real world. Given that, we propose a novel mechanism, named Structural Importance based Model Trading (SIMT) mechanism, that assesses the data importance and compensates data owners accordingly without disclosing the data. Specifically, SIMT procures feature and label data from data owners according to their structural importance, and then trains a graph neural network for model consumers. Theoretically, SIMT ensures incentive compatible, individual rational and budget feasible. The experiments on five popular datasets validate that SIMT consistently outperforms vanilla baselines by up to 40% in both MacroF1 and MicroF1.", "KEYWORDS": "Model marketplaces; data pricing; structural entropy", "ACMReference Format:": "Yiping Liu, Mengxiao Zhang, Jiamou Liu, and Song Yang. 2025. Data Pricing for Graph Neural Networks without Pre-purchased Inspection. In Proc. of the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May 19 - 23, 2025 , IFAAMAS, 13 pages.", "1 INTRODUCTION": "In today's digital age, data has become an essential asset, serving as the foundation for AI and machine learning advancements. To This work is licensed under a Creative Commons Attribution International 4.0 License. Proc. of the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS2025), Y. Vorobeychik, S. Das, A. Now\u00e9 (eds.), May 19 - 23, 2025, Detroit, Michigan, USA . \u00a9 2025 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). Mengxiao Zhang \u2020\u2217 UESTC, China The University of Auckland, New Zealand mengxiao.zhang@auckland.ac.nz", "Song Yang": "The University of Auckland, New Zealand syan382@aucklanduni.ac.nz meet the increasing demand for high-quality data, a new business paradigm known as the model marketplace has emerged [24], exemplified by platforms like Modzy. A model marketplace facilitates the exchange between model consumers , who seek AI models for various tasks, and data owners , who possess the feature and label data necessary for model training. The marketplace purchases data from data owners, uses it to train AI models, and then sells these trained models to consumers. However, a key challenge in model marketplaces is determining how to properly compensate data owners for their contributions, a problem referred to as data pricing . This problem is challenging because the importance of data is difficult to evaluate. Most existing studies assume that marketplaces acquire data from data owners before paying them and use the subsequent performance improvements as a measure of data importance. For example, [2, 11, 17, 24, 46] rely on this assumption to establish pricing mechanisms based on the marginal impact of data on model accuracy. However, this pre-purchased inspection assumption is impractical in real-world settings. Data owners are often unwilling to release their data without proper payment, fearing that the data, once disclosed, may immediately provide valuable insights to buyers, reducing the incentive to pay. This leads to a critical question: How can we measure data importance for model training without direct inspection, thereby facilitating data pricing? Several studies have attempted to address this question by introducing exogenous metrics for measuring data importance, such as data age, accuracy, volume [14], the extent of perturbations [9, 39], or data owners' reputation [48]. However, these metrics often fail to accurately reflect the contribution of data in the context of model training, particularly when dealing with complex models like Graph Neural Networks (GNNs). Graph-structured data is prevalent in many real-world scenarios, where the relationships between entities are often as important as their attributes. GNNs excel in tasks involving such data, capturing both node features and network structure. However, data ownership is often decentralized, with different entities controlling separate 'pockets' of the network. This creates a need for a marketplace where subgraphs can be purchased and integrated to enable comprehensive model training [3]. For example, in finance, each bank holds its own subset of transaction data, but detecting fraud often requires analysing transaction flows across multiple institutions. Similarly, in healthcare, patient interactions are fragmented across hospitals, clinics, and insurance companies, forming an interconnected yet distributed network. In supply chain management, companies typically have visibility into their direct suppliers and customers, but the complete supply chain network spans many interdependent organisations. In all of these cases, the full value of the network data cannot be realised without aggregating subgraphs from multiple sources. For data consumers, aggregating subgraphs from multiple sources is essential for training reliable GNN models, enabling applications like fraud detection, personalized healthcare, and supply chain risk analysis. In this paper, we advance existing research by exploring the pricing of individual data points within subgraphs for GNN training. This introduces a distinct challenge, as the value of any given node to the model's performance is highly dependent on its structural role and connectivity within the broader network. To address this, we aim to develop pricing mechanisms that capture the marginal contribution of each data point, taking into account both its local features and its position within the global network structure. Notably, this is the first work to tackle the problem of pricing graphstructured data in a model marketplace for GNNs. In the following, we list our main contributions: \u00b7 We propose Structural Importance based Model Trading (SIMT), a novel model marketplace framework for GNNs that integrates two phases: data procurement and model training . Figure 1 shows the conceptual framework of SIMT. \u00b7 For data procurement, we put forward a new method for assessing the importance of graph-structured data. For this we present a novel marginal structural entropy to quantify node informativeness. This method of importance assessment is integrated with an auction mechanism to select data owners and fairly compensate them based on their contributions. We prove that this mechanism is incentive compatible, individual rational, and budget feasible. \u00b7 For model training, we introduce the method of feature propagation to impute missing feature data for unselected nodes, enabling effective learning with partial data. We also design an edge augmentation method to enhance graph structure by adding connections involving unselected nodes, improving the GNN's ability to generalize. \u00b7 Theproposed SIMT method was evaluated on five well-established benchmark datasets, and consistently outperformed four baseline mechanisms in node classification tasks. SIMT achieved up to a 40% improvement in MacroF1 and MicroF1 scores compared to the Greedy and ASCV methods, demonstrating its superior performance under various budget constraints.", "2 RELATED WORK": "Data pricing has been extensively studied in two main contexts: data marketplaces and model marketplaces . Data Pricing in Data Marketplaces. In data marketplaces, pricing mechanisms revolve around trading raw datasets or simple queries. Previous work has focused on pre-purchase decisions, where data is evaluated before it is accessed, which aligns with our setting. For instance, the importance of datasets is often quantified by metrics such as size, as explored by [20], or privacy levels, as in [30]. Other studies, such as [45] and [16], assess data importance based on its utility to consumers, proposing auction mechanisms and contracts to compensate data owners accordingly. When it comes to query-based data pricing , metrics like privacy levels directly impact the accuracy of responses, thereby influencing data value. For instance, [12, 33] propose auction mechanisms that incorporate privacy in queries, while [23] introduces a take-it-orleave-it contract for count queries. Further work by [10, 49] expands these ideas to linear predictor queries and broader query settings. In data marketplaces, data importance is often easily quantifiable using metrics like size or privacy levels. However, in the context of model marketplaces , the contribution of individual data points to machine learning model performance is more complex and requires novel pricing methods. Data Pricing in Model Marketplaces. In model marketplaces, data pricing is typically based on how much a dataset improves a machine learning model's performance. [1] introduced a theoretical framework for data pricing that balances budget constraints with model performance. Subsequent works, such as those by [2] and [11], assume the model's benefit is known and focus on fairly distributing rewards among data owners. A common method for this is the Shapley value [34], which compensates each data owner based on their contribution to the model. Various studies have refined the utility function used in Shapley value calculations by incorporating additional factors. [11] and [17], for example, include \ud835\udc3e -nearest neighbors and privacy considerations in their utility designs. [24] builds on this by extending the Shapley value framework to model marketplaces. Other research, such as [36] and [28], explores utility design in collaborative machine learning scenarios, where data owners also serve as model consumers. In these cases, utility is defined either as the sum of the model's value to the owner and its marginal value to others [28], or through metrics like information gain [36]. [46] and [15] further define utility based on the cosine similarity of parameters or the privacy leakage of shared model parameters. A common limitation of these works is that they often require training models on the entire dataset before compensating data owners. In practice, this assumption is often unrealistic, as data owners are usually hesitant to contribute their data upfront without proper guarantees or compensation. A more realistic setting, which is closer to our approach, has been explored by studies [9, 39, 48]. [9] assume that data importance is known and apply a VCG auction mechanism to select and compensate data owners. [48] propose an auction mechanism that incorporates the reputation of data owners as a reflection of their contribution, while [39] design an auction that selects data owners based on their privacy requirements. Although these approaches offer valuable insights, they rely on exogenous metrics, such as reputation or privacy, which are often difficult to obtain or may not accurately reflect the intrinsic value of data for model training. In contrast, our work proposes a novel method to measure data importance without direct data inspection. By focusing on the structural properties of graph data and using techniques like structural entropy, we aim to create a fair and effective data pricing mechanism that overcomes the limitations of previous methods. Comparison with FL and AL. While Federated Learning (FL) and Active Learning (AL) are well-known paradigms for training models with distributed data, our approach differs in key ways. (1) In FL, each data owner trains a local model on private data, which is then aggregated into a global model while preserving graph cluster informa- budget structure tiveness structural purchased clustering represen- importance connection feature |tativeness ffeature & label imputation GNN model importance assessment auction training edge reported valuation data laugmentation model owners Data Procurement Model Training consumer privacy [47]. SIMT, by contrast, does not require data owners to train models. Instead, data is directly provided to a central model, allowing for optimizations like data augmentation that are not possible in FL's gradient-based aggregation. This eliminates the computational burden on data owners and allows for more flexible model improvements. (2) In AL, the model iteratively queries data points to refine learning, typically in multiple rounds [31]. SIMT, however, collects data in a single round, reducing overhead and cost. Furthermore, while AL assumes access to unlabeled data with labels provided iteratively [7, 50], SIMT addresses the real-world challenge of compensating data owners, ensuring they are fairly rewarded for their contributions upfront.", "3 PROBLEM FORMULATION": "Each data owner \ud835\udc56 \u2208 \ud835\udc42 attaches a valuation to her attribute and label data of a single node, denoted by \ud835\udf03 \ud835\udc56 \u2208 \u0398 , where \u0398 is the set of all possible valuations. The valuation \ud835\udf03 \ud835\udc56 indicates the minimum payment required by the data owner to allow the use of the attribute and connection data of a single node for model training. The valuation \ud835\udf03 \ud835\udc56 is privately known only to the data owner, but they may report a different valuation \ud835\udf03 \u2032 \ud835\udc56 \u2260 \ud835\udf03 \ud835\udc56 if it serves their interests. We assume that each data owner values all their data subjects equally , implying that the total valuation is linearly dependent on the number of data records. Let \ud835\udf3d \ud835\udc56 be \ud835\udc56 's valuation vector for all nodes, i.e., \ud835\udf3d \ud835\udc56 B ( \ud835\udf03 \ud835\udc56, 1 , . . . , \ud835\udf03 \ud835\udc56,\ud835\udc5b \ud835\udc56 ) = \ud835\udf03 \ud835\udc56 \u00b7 1 , where \ud835\udf03 \ud835\udc56,\ud835\udc63 is the valuation of \ud835\udc56 for node \ud835\udc63 . The valuation of all data owners form a valuation matrix, denoted by \ud835\udf3d , which is the concatenation \ud835\udf3d 1 | \u00b7 \u00b7 \u00b7 | \ud835\udf3d \ud835\udc5c \u2208 \u0398 \ud835\udc5b . The model consumer has a budget , denoted by \ud835\udefd \u2208 R + , for buying the prediction model trained on structure and attribute data.", "3.1 Model marketplace for graph data": "Weconsider a model marketplace where a model consumer interacts with multiple data owners to trade graph-structured data. This data is distributed among the various data owners. Let the overall graph be represented as an attributed graph G B ( \ud835\udc49, \ud835\udc38, \ud835\udc7f , \ud835\udc9a ) , where \ud835\udc49 is the set of nodes, representing individual data subjects, \ud835\udc38 \u2286 \ud835\udc49 \u00d7 \ud835\udc49 is the set of edges. \ud835\udc7f \u2208 R \ud835\udc5b \u00d7 \ud835\udc5a is the feature matrix, where \ud835\udc5b is the number of nodes and \ud835\udc5a is the dimensionality of the feature vector, and \ud835\udc9a \u2208 R \ud835\udc5b is the label vector, where each entry corresponds to a label for each node. The adjacency matrix and normalised adjacency matrix of G are denoted as \ud835\udc68 and \u02dc \ud835\udc68 , resp. For each node \ud835\udc63 \u2208 \ud835\udc49 , let \ud835\udc99 \ud835\udc63 \u2208 R \ud835\udc5a and \ud835\udc66 \ud835\udc63 represent the feature vector and label of node \ud835\udc63 , resp. \ud835\udc41 \ud835\udc63 \u2286 \ud835\udc49 represents the set of neighbours of node \ud835\udc63 , and \ud835\udc51 \ud835\udc63 B | \ud835\udc41 \ud835\udc63 | denote the degree of node \ud835\udc63 . The graph G is distributed among multiple data owners, each controlling a subgraph. Let \ud835\udc42 denote the set of data owners, where \ud835\udc5c B | \ud835\udc42 | represents the total number of data owners. For each data owner \ud835\udc56 \u2208 \ud835\udc42 , the subgraph held by owner \ud835\udc56 is represented as G \ud835\udc56 B ( \ud835\udc49 \ud835\udc56 , \ud835\udc38 \ud835\udc56 , \ud835\udc7f [ \ud835\udc49 \ud835\udc56 ] , \ud835\udc9a [ \ud835\udc49 \ud835\udc56 ]) , where \ud835\udc49 \ud835\udc56 \u2286 \ud835\udc49 is the set of nodes controlled by data owner \ud835\udc56 , \ud835\udc38 \ud835\udc56 = \ud835\udc38 \u2229 ( \ud835\udc49 \ud835\udc56 \u00d7 \ud835\udc49 \ud835\udc56 ) is the set of edges between nodes in \ud835\udc49 \ud835\udc56 , \ud835\udc7f [ \ud835\udc49 \ud835\udc56 ] , and \ud835\udc9a [ \ud835\udc49 \ud835\udc56 ] are the feature matrix, and label vector induced by the nodes in \ud835\udc49 \ud835\udc56 , resp. Let \ud835\udc5b \ud835\udc56 B | \ud835\udc49 \ud835\udc56 | be the number of nodes in subgraph G \ud835\udc56 . Denote the edges within subgraphs as / \ud835\udc38 and the edges between the subgraphs as \u00a5 \ud835\udc38 . We assume that / \ud835\udc38 \u2229 \u00a5 \ud835\udc38 = \u2205 and then \ud835\udc38 = / \ud835\udc38 \u222a \u00a5 \ud835\udc38 . We also assume that the internal structure of each subgraph, including the features and labels of nodes, is private to the corresponding data owner. However, the connections between subgraphs (i.e., \u00a5 \ud835\udc38 the edges connecting nodes from different subgraphs) are known by the model consumer. Data owners are willing to sell the feature, label, and connection data for the nodes they control. The model marketplace involves designing a mechanism that procures the attribute/structure data from data owners, and train a GNN model for the model consumer.", "3.2 Incentive mechanism": "Definition 3.1. A mechanism \ud835\udc40 consists of two functions, ( \ud835\udf0b (\u00b7) , \ud835\udc5d (\u00b7)) , where \ud835\udf0b : \u0398 \ud835\udc5b \u2192{ 0 , 1 } \ud835\udc5b is an allocation function and \ud835\udc5d : \u0398 \ud835\udc5b \u2192 R \ud835\udc5b is a payment function . Given a set of data owners and a model consumer, the mechanism takes the reported valuation \ud835\udf3d \u2032 \u2208 \u0398 \ud835\udc5b as input, and outputs allocation result and payment result. The allocation function and the payment function determine which nodes are selected for model training and how much to pay for the data owners, resp. We write the allocation result \ud835\udf45 ( \ud835\udf3d \u2032 ) as ( \ud835\udf45 1 ( \ud835\udf3d \u2032 ) , . . . , \ud835\udf45 \ud835\udc5c ( \ud835\udf3d \u2032 )) and the payment result \ud835\udc91 ( \ud835\udf3d \u2032 ) as ( \ud835\udc91 1 ( \ud835\udf3d \u2032 ) , . . . , \ud835\udc91 \ud835\udc5c ( \ud835\udf3d \u2032 )) , where each \ud835\udf45 \ud835\udc56 ( \ud835\udf3d \u2032 ) , \ud835\udc91 \ud835\udc56 ( \ud835\udf3d \u2032 ) is a \ud835\udc5b \ud835\udc56 -dimensional vector with each element \ud835\udf0b \ud835\udc56,\ud835\udc63 , \ud835\udc5d \ud835\udc56,\ud835\udc63 being an allocation and payment for \ud835\udc56 's node \ud835\udc63 . The allocation and payment of node \ud835\udc63 give data owner \ud835\udc56 a utility \ud835\udc62 \ud835\udc56,\ud835\udc63 ( \ud835\udf3d \u2032 ) = ( \ud835\udc5d \ud835\udc56,\ud835\udc63 ( \ud835\udf3d \u2032 ) -\ud835\udf03 \ud835\udc56,\ud835\udc63 ) \ud835\udf0b \ud835\udc56,\ud835\udc63 ( \ud835\udf3d \u2032 ) . The utility of data owner \ud835\udc56 is \ud835\udc62 \ud835\udc56 = \u02dd \ud835\udc63 \u2208 \ud835\udc49 \ud835\udc56 \ud835\udc62 \ud835\udc56,\ud835\udc63 . Once a node is selected, its connection, feature and label data are used for model training. Let \ud835\udf3d -\ud835\udc56 denote the valuation of all data owner but \ud835\udc56 and \u0398 -\ud835\udc56 denote the set of all possible \ud835\udf3d -\ud835\udc56 . A mechanism \ud835\udc40 should satisfy: \u00b7 Incentive Compatible (IC): Each data owner \ud835\udc56 \u2208 \ud835\udc42 gains maximum utility when truthfully reporting her valuation, i.e., \ud835\udc62 \ud835\udc56 ( \ud835\udf03 \ud835\udc56 , \ud835\udf3d -\ud835\udc56 ) \u2265 \ud835\udc62 \ud835\udc56 ( \ud835\udf03 \u2032 \ud835\udc56 , \ud835\udf3d -\ud835\udc56 ) , \u2200 \ud835\udf03 \ud835\udc56 , \ud835\udf03 \u2032 \ud835\udc56 \u2208 \u0398 , \u2200 \ud835\udf3d -\ud835\udc56 \u2208 \u0398 -\ud835\udc56 . \u00b7 Individual Rational (IR): Each data owner \ud835\udc56 \u2208 \ud835\udc42 gains an nonnegative utility when participating in the mechanism, i.e., \ud835\udc62 \ud835\udc56 ( \ud835\udf03 \ud835\udc56 , \ud835\udf3d -\ud835\udc56 ) \u2265 0 , \u2200 \ud835\udf03 \ud835\udc56 \u2208 \u0398 , \u2200 \ud835\udf3d -\ud835\udc56 \u2208 \u0398 -\ud835\udc56 . \u00b7 Budget Feasible (BF): Total payment given to all data owners is not exceed the budget \ud835\udefd , i.e., \u02dd \ud835\udc56 \u2208 \ud835\udc42 \ud835\udc91 \ud835\udc56 \ud835\udf45 \ud835\udc56 \u2264 \ud835\udefd .", "3.3 Graph neural network models": "We use GNN as the prediction model. Given a graph, GNN predicts the node labels by stacking multiple layers. Let \ud835\udc3f be the number of layers in a GNN model. The main idea is to iteratively aggregate the feature information of each node from its neighbours. Specifically, given an attributed graph G = ( \ud835\udc49, \ud835\udc38, \ud835\udc7f , \ud835\udc9a ) , and a GNN with \ud835\udc3f convolution layers, at a layer \u2113 \u2264 \ud835\udc3f , the feature embedding \ud835\udc89 \u2113 \ud835\udc63 of node \ud835\udc63 \u2208 \ud835\udc49 is generated through aggregation and update: \u00b7 Aggregation: aggregate the feature embeddings \ud835\udc89 \u2113 \ud835\udc62 of all neighbours \ud835\udc62 of \ud835\udc63 by an aggregate function such as mean and sum, with trainable weights, i.e., \ud835\udc8f \u2113 \ud835\udc63 B Aggregator \u2113 GLYPH<0> { \ud835\udc89 \u2113 \ud835\udc62 , \u2200 \ud835\udc62 \u2208 \ud835\udc41 \ud835\udc63 } GLYPH<1> . \u00b7 Update: update the feature embedding \ud835\udc89 \u2113 + 1 \ud835\udc63 at the next layer by an update function of the embedding \ud835\udc89 \u2113 \ud835\udc63 and the aggregated embeddings \ud835\udc8f \u2113 \ud835\udc63 , i.e., \ud835\udc89 \u2113 + 1 \ud835\udc63 B Updater \u2113 GLYPH<0> \ud835\udc89 \u2113 \ud835\udc63 , \ud835\udc8f \u2113 \ud835\udc63 GLYPH<1> . Initially, the feature embedding of node \ud835\udc63 is its feature vector, i.e., \ud835\udc89 0 \ud835\udc63 B \ud835\udc99 \ud835\udc63 .", "3.4 Optimisation problem": "As discussed in the Introduction, a key issue in determining compensation for data owners in a model marketplace is assessing the importance of their data to model training without direct inspection. To summarise, the problem in this paper is: Given the model marketplace with a model consumer and several data owners, we, as a data broker , aim to design a mechanism that procures the attribute/structural data from data owners, and train a GNN model for the consumer with the following subgoals: \u00b7 assessing the importance of data to model training without disclosing the feature and label data; \u00b7 optimising GNN performance within the budget; and \u00b7 ensuring the mechanism is IC, IR, and BF. More formally, let \u25e6 denote the Hadamard product operator, which selectively includes elements from / \ud835\udc38, \ud835\udc7f , \ud835\udc9a according to the indicator vector \ud835\udf45 = ( \ud835\udf45 1 , . . . , \ud835\udf45 \ud835\udc5c ) . We define \ud835\udc53 \ud835\udc3a\ud835\udc41\ud835\udc41 (\u00b7) as the output of a GNN model trained on a selected subset of the data with the known \u00a5 \ud835\udc38 . The problem in the paper can be formulated as: \ud835\udc91 \ud835\udc56 \u2265 0 , \ud835\udf0b \ud835\udc56,\ud835\udc63 \u2208 { 0 , 1 } \u2200 \ud835\udc56 \u2208 \ud835\udc42, \u2200 \ud835\udc63 \u2208 \ud835\udc49 \ud835\udc56", "4 PROPOSED METHOD": "In this section, we propose a mechanism that procures the most contributing data and trains a GNN model using the procured data. By considering the correlation between graph structure, features, and labels, we leverage the graph structure to offer insights into the contribution of the associated data. Then, we combine this contribution assessment with the data owners' valuation in an auction mechanism to select the most cost-effective data. Subsequently, we augment the procured data using feature imputation and edge augmentation and use the augmented data to train a two-layer GNN model, which is returned to the model consumer. The overall framework is shown in Figure 1.", "4.1 Structural importance": "We begin by evaluating the importance of data to model training without inspecting the feature and label data. Our solution is motivated by the observation that the structure of a graph often encodes valuable information about its features and labels. According to the well-known homophily assumption, nodes with similar features and labels are more likely to be closely connected [27, 41]. This is further validated by our case studies on five real-world graphs. We analyse the connections both within and between classes, and the results show that the number of edges within the same class is substantially higher than between different classes, as illustrated in Figure 2. The strong correlation between graph structure and the associated features and labels motivates our approach to leverage the graph structure as auxiliary information in the data selection process. Thus we propose to use the structural importance of data owner to represent her data importance. Citeseer Pubmed 40% 80% 809 60% 409 20% 20% 109 20% 096 0% Amazo Coauthor 80% average ratio of 60% intra-class neighbours per node in a class 409 average ratio of inter-class neighbours 20% 20% per node in class 0% 0% 1 2 3 The sample dataset for model training should be both informative, reducing the uncertainty of the model, and representative, capturing the overall pattern of the entire dataset [50]. Therefore, we measure the structural importance of data owners in terms of informativeness and representativeness. Nevertheless, determining the informativeness and representativeness of nodes in a graph often relies on the true classes and node features, which are not available due to the absence of true feature and label data. To address this, we first use structural clusters to approximate the true classes. With this clustering, we propose the notion of marginal structural entropy to quantifies informativeness, and deploy PageRank centrality to quantify representativeness. Structuring clustering. A crucial tool for the structural clustering is structural entropy . Let \ud835\udc3a = ( \ud835\udc49, \ud835\udc38 ) represent a graph without attributes. Suppose \ud835\udc43 B { \ud835\udc36 1 , \ud835\udc36 2 , . . . , \ud835\udc36 \ud835\udc47 } is a partition of \ud835\udc49 , where each \ud835\udc36 \ud835\udc61 is called a cluster and \ud835\udc47 is the number of clusters. Structural entropy of \ud835\udc3a relative to \ud835\udc43 captures the information gain due to the partition. For each cluster \ud835\udc36 \ud835\udc61 \u2208 \ud835\udc43 , write \ud835\udc51 \ud835\udc61 as the sum of degrees \ud835\udc51 \ud835\udc63 of all nodes \ud835\udc63 \u2208 \ud835\udc36 \ud835\udc61 . Write \ud835\udc54 \ud835\udc61 as the number of the edges between the nodes in \ud835\udc36 \ud835\udc61 and those in the other clusters. The structural entropy [25] of \ud835\udc3a relative to \ud835\udc43 is Agreater value of H \ud835\udc43 ( \ud835\udc3a ) means that the corresponding partition \ud835\udc43 gains more information about the graph \ud835\udc3a and thus \ud835\udc43 is preferable. Given that, we would like to obtain a good partition by maximising the structural entropy H \ud835\udc43 ( \ud835\udc3a ) . Unfortunately, maximising the structural entropy H \ud835\udc43 ( \ud835\udc3a ) is NP-hard [22, 43]. As an alternative, we propose an algorithm Clustering ( \ud835\udc3a ) that harnesses the power of unsupervised GCN models [43] to obtain a partition \ud835\udc43 . Specifically, Clustering ( \ud835\udc3a ) first employs the Singular Value Decomposition (SVD) [5] to generate spectral features, and a classical Variational Graph Auto-Encoder (VGAE) model [19] with reconstruction loss taking the generated spectral features as input to learn node embeddings. Using the obtained node embedding, Clustering ( \ud835\udc3a ) then trains a linear classifier to get a partition by maximising structural entropy. Structural informativeness. Given the learned clustering \ud835\udc43 , we propose the notion of cluster-based marginal structural entropy to measure the structural informativeness of data owners. Basically, the marginal structural entropy captures the information gain of a node to the clustering. The lower the marginal structural entropy of a node has, the more uncertainty this node has, and thus more information the node's data will capture. More formally, we define the marginal structural entropy of node \ud835\udc63 as the information gain due to existence of \ud835\udc63 , i.e., the difference between the structural entropy of graph \ud835\udc3a relative to \ud835\udc43 and that of \ud835\udc3a without node \ud835\udc63 relative to \ud835\udc43 \u2032 . Then the normalised marginal structural entropy \ud835\udf16 \ud835\udc63 of \ud835\udc63 is the normalised difference in structural entropy with partition \ud835\udc43 and another partition \ud835\udc43 \u2032 that moves \ud835\udc63 out of its cluster \ud835\udc36 \ud835\udc61 , i.e., \ud835\udf16 \ud835\udc63 = (H \ud835\udc43 ( \ud835\udc3a ) - H \ud835\udc43 \u2032 ( \ud835\udc3a ))/H \ud835\udc43 ( \ud835\udc3a ) . Let \ud835\udc5b \ud835\udc63,\ud835\udc61 be the number of nodes that are incident to \ud835\udc63 and belong to \ud835\udc36 \ud835\udc61 . After calculation, we have the following (see App. A for the detailed calculation): Definition 4.1. The normalised marginal structural entropy of node \ud835\udc63 \u2208 \ud835\udc36 \ud835\udc61 to structural entropy H \ud835\udc43 ( \ud835\udc3a ) is A lower normalised marginal structural entropy means more structural uncertainty of \ud835\udc63 , making \ud835\udc63 more informative. Structural representativeness. We use a classical structural centrality measure, PageRank [26], to quantify the structural representativeness of a node. We opt for PageRank centrality due to its superior performance compared to other centrality measures, as validated in App. D. The higher the PageRank centrality of a node has, the more representative the node is. Let \ud835\udefe \u2208 ( 0 , 1 ) denote the damping factor, which controls the probability of following links. The PageRank centrality \ud835\udf0c \ud835\udc63 of node \ud835\udc63 is: Structural importance score. Given the clustering \ud835\udc43 , the entropy \ud835\udf16 \ud835\udc63 and the PageRank centrality \ud835\udf0c \ud835\udc63 of each node \ud835\udc63 , we define the structural importance score. Specifically, we first sort all nodes in their own cluster by their entropy and PageRank values, resp. Nodes are sorted in ascending order by entropy (as lower entropy indicates higher informativeness) and in descending order by PageRank (as higher PageRank indicates greater representativeness). This ensures that more informative and representative nodes are prioritised. For a node \ud835\udc63 in cluster \ud835\udc36 \ud835\udc61 , let rank entr \ud835\udc63 denote its rank by entropy and rank pr \ud835\udc63 denote its rank by PageRank. We then define node \ud835\udc63 's informativeness and representativeness based on these rankings as Finally, we define the structural importance score of a node. Following the approach in [7, 50], we introduce a parameter \ud835\udefc to balance representativeness and informativeness. Intuitively, representative data helps to learn general classification patterns, while informative data is used to refine the classification boundaries. Therefore, when the budget \ud835\udefd is relatively small compared to the overall valuations of data owners and the partition \ud835\udc43 is complex (i.e., when \ud835\udc47 is large), prioritising representative data is crucial to learning the general classification patterns. On the other hand, when the budget is relatively large and the partition is simpler, a small amount of representative data is sufficient to capture the overall pattern, allowing us to focus on acquiring more informative data to further refine the classification. More formally, given the average valuation \ud835\udf03 , defined as the average of the upper and lower bounds of data valuations, we set \ud835\udefc = 1 2 ( 1 + \ud835\udefd \ud835\udc5b\ud835\udf03 ) -\ud835\udc47 . The structural importance score of node \ud835\udc63 is then defined as", "4.2 Model trading mechanism": "We propose a model trading mechanism, named Structural importance based model trading (SIMT) mechanism , which consists of two phases: (1) data procurement phase selects the most cost effective data owners, and (2) model training phase trains a GNN model on the procured data; See the workflow of SIMT in Figure 1 and the algorithm in Alg. 1. Phase 1. Data procurement. In Phase 1, SIMT takes the attributed graph G and the valuation vector \ud835\udf3d , and the budget \ud835\udefd as inputs and returns an allocation result \ud835\udf45 and a payment result \ud835\udc91 . Firstly, Clustering ( \ud835\udc3a ) returns a clustering \ud835\udc43 of the nodes \ud835\udc49 after training. Given the clustering \ud835\udc43 with \ud835\udc47 clusters, the mechanism computes the structural importance score \ud835\udf19 \ud835\udc63 of each node \ud835\udc63 \u2208 \ud835\udc49 using Equation (1), which represents the importance of the node. Then an auction is conducted in each cluster \ud835\udc36 \ud835\udc61 \u2208 \ud835\udc43 with budget \ud835\udefd / \ud835\udc47 . In the auction for each \ud835\udc36 \ud835\udc61 , nodes in \ud835\udc36 \ud835\udc61 are sorted in descending order based on the ratio \ud835\udf19 \ud835\udc63 \ud835\udf03 \ud835\udc56,\ud835\udc63 , where data owner \ud835\udc56 owns node \ud835\udc63 . The mechanism selects the most cost-effective \ud835\udc58 data until the total payment exceeds the allocated budget. The payment to data owner \ud835\udc56 for node \ud835\udc63 is \ud835\udc5d \ud835\udc56,\ud835\udc63 = min { \ud835\udefd \ud835\udc47 \ud835\udf19 \ud835\udc63 \u02dd \ud835\udc58 \ud835\udc62 = 1 \ud835\udf19 \ud835\udc62 , \ud835\udf03 \ud835\udc57,\ud835\udc64 \ud835\udf19 \ud835\udc64 \ud835\udf19 \ud835\udc63 } , where \ud835\udc57 is the first data owner who has not had any data selected and \ud835\udc64 is her first data in the order (if such data owner \ud835\udc57 does not exist, we set \ud835\udc5d \ud835\udc56,\ud835\udc63 as min { \ud835\udefd\ud835\udf19 \ud835\udc63 /( \ud835\udc47 \u02dd \ud835\udc58 \ud835\udc62 = 1 \ud835\udf19 \ud835\udc62 ) , \u02dc \ud835\udf03\ud835\udf19 \ud835\udc63 / \ud835\udf19 \ud835\udc58 + 1 } , where \u02dc \ud835\udf03 is the upper bound of \u0398 ). The total payment to data owner \ud835\udc56 is \u02dd \ud835\udc63 \u2208 \ud835\udc49 \ud835\udc56 ,\ud835\udc63 \u2264 \ud835\udc58 \ud835\udc5d \ud835\udc56,\ud835\udc63 . Phase 2. Model training. Given the allocation result \ud835\udf45 , the model training phase uses the connections, features and labels of the selected data owners to train a GNN model. However, due to budget constraints, only a subset of features and connections can be purchased, which may not be sufficient for training a robust GNN model. To address this, we first impute the missing node features", "Algorithm 1 The SIMT mechanism": "Input: Attributed graph G , data owners \ud835\udc42 , valuation vector \ud835\udf3d , and budget \ud835\udefd Output: Allocation \ud835\udf45 , payment \ud835\udc91 , and trained model \ud835\udc53 GNN 1: let \ud835\udc3a = ( \ud835\udc49, \u00a5 \ud835\udc38 ) represent the known subgraph of G without attributes. 2: Phase 1: Data procurement 3: get a partition \ud835\udc43 \u2190 Clustering ( \ud835\udc3a ) 4: compute the structural importance score \ud835\udf19 \ud835\udc63 for each \ud835\udc63 \u2208 \ud835\udc49 according to \ud835\udc43 5: initialise \ud835\udf45 = 0 , \ud835\udc91 = 0 6: for each cluster \ud835\udc36 \ud835\udc61 \u2208 \ud835\udc43 do 7: sort the nodes \ud835\udc63 \u2208 \ud835\udc36 \ud835\udc61 by \ud835\udf19\ud835\udc63 \ud835\udf03 \ud835\udc56,\ud835\udc63 in a descending order 9: for \ud835\udc63 \u2264 \ud835\udc58 do 10: Let \ud835\udc56 \u2208 \ud835\udc42 be the data owner of node \ud835\udc63 } 12: procure data to get \ud835\udc7f \ud835\udc60 , \ud835\udc9a \ud835\udc60 , update \ud835\udc3a and normalized adjacency matrix \u02dc \ud835\udc68 13: Phase 2: Model training 14: initialise \ud835\udc7f \u2032 \u2190 [ \ud835\udc7f \ud835\udc60 , 0 \ud835\udc62 ] \u22ba 15: while \ud835\udc7f \u2032 has not converged do 16: \ud835\udc7f \u2032 \u2190 \u02dc \ud835\udc68\ud835\udc7f \u2032 17: \ud835\udc7f \u2032 \u2190 [ \ud835\udc7f \ud835\udc60 , \ud835\udc7f \u2032 \ud835\udc62 ] \u22ba 18: do edge augmentation on \ud835\udc3a and get \ud835\udc3a 19: \ud835\udc53 GNN \u2190 Train ( \ud835\udc3a, \ud835\udc7f \u2032 , \ud835\udc9a \ud835\udc60 ) and augment the missing edges before training. After acquiring the features from the selected nodes, we apply the feature propagation algorithm [32] to infer the features of the unselected nodes, producing a new feature matrix \ud835\udc7f \u2032 . Additionally, we use the \ud835\udc3a (| \ud835\udc49 | , | \ud835\udc38 |) Erdos-Renyi (ER) model [4] to generate missing edges, where | \ud835\udc49 | and | \ud835\udc38 | are determined by the edge density of the known graph. Then we incorporate contrastive learning [29] to mitigate the randomness introduced by the ER model, resulting in an augmented graph \ud835\udc3a . The GNN training algorithm then takes the augmented graph \ud835\udc3a , the new feature matrix \ud835\udc7f \u2032 and labels \ud835\udc9a \ud835\udc60 as input and returns a GNN model to the consumer. Node feature imputation. Features are crucial when training GNN [38, 40], and we apply a feature imputation method to the procured data to address missing values. Among various feature imputation methods, we choose the Feature Propagation algorithm due to its strong convergence guarantees, simplicity, speed, and scalability [32]. We use subscripts \ud835\udc60 and \ud835\udc62 to denote the selected and unselected nodes, resp. Write \ud835\udc7f = [ \ud835\udc7f \ud835\udc60 , \ud835\udc7f \ud835\udc62 ] \u22ba and \ud835\udc9a = [ \ud835\udc9a \ud835\udc60 , \ud835\udc9a \ud835\udc62 ] \u22ba . Also, we write the normalised adjacency matrix \u02dc \ud835\udc68 and the graph Laplacian \u0394 of imputation process, the feature matrix \ud835\udc7f \u2032 is initialised with the known feature \ud835\udc7f \ud835\udc60 and a zero matrix 0 \ud835\udc62 for the unselected nodes. The feature matrix \ud835\udc7f \u2032 is then iteratively updated as follows: \ud835\udc7f ( \ud835\udc61 ) = matrix converges. The steady status of the feature matrix is [32]: Edge Augmentation. Given the critical role of message passing in GNNs, the absence of certain edges may impede this process, leading to sub-optimal model performance. To alleviate this issue, we introduce augmented edges to enhance message passing. Specifically, we employ the ER model [4] to generate edges for data owners with multiple unselected nodes. However, the introduction of augmented edges may inadvertently introduce noise, which could mislead the model by learning from incorrect connections. To counteract this, we integrate contrastive loss [29], denoted by \ud835\udc3f ctr , into the GNN training process. This loss function encourages the model to maximise the similarity between the augmented graph and the original (non-augmented) graph views. Given a graph G and an augmented graph G , let \ud835\udc89 \ud835\udc63 and \ud835\udc89 \u2032 \ud835\udc63 be the feature embeddings in G and G , resp. The contrastive loss of a node \ud835\udc63 is: where \ud835\udf0f represents the temperature parameter, which scales the similarities between the embeddings \ud835\udc89 \ud835\udc63 and \ud835\udc89 \u2032 \ud835\udc63 .", "4.3 Analysis": "Now we show that SIMT satisfies IC, IR and BF properties. Additionally, we analyse its time complexity. Theorem 4.2. The SIMT mechanism is incentive compatible, individual rational and budget feasible. Proof. We first show that the mechanism is IR. In each auction of cluster \ud835\udc36 \ud835\udc61 , the utility that a data owner \ud835\udc56 obtains from a node \ud835\udc63 > \ud835\udc58 when she truthfully reports is \ud835\udc62 \ud835\udc56,\ud835\udc63 ( \ud835\udf3d ) = \ud835\udf03 \ud835\udc56,\ud835\udc63 \ud835\udf0b \ud835\udc56,\ud835\udc63 ( \ud835\udf03 ) -\ud835\udc5d \ud835\udc56,\ud835\udc63 ( \ud835\udf03 ) = 0 -0 \u2265 0; the utility that a data owner \ud835\udc56 obtains from \ud835\udc63 \u2264 \ud835\udc58 is \ud835\udc62 \ud835\udc56,\ud835\udc63 ( \ud835\udf03 ) = \ud835\udc5d \ud835\udc56,\ud835\udc63 ( \ud835\udf03 ) -\ud835\udf03 \ud835\udc56,\ud835\udc63 \ud835\udf0b \ud835\udc56,\ud835\udc63 ( \ud835\udf03 ) = min { ( \ud835\udefd\ud835\udf19 \ud835\udc63 ) /( \ud835\udc47 \u02dd \ud835\udc58 \ud835\udc62 = 1 \ud835\udf19 \ud835\udc62 ) , ( \ud835\udf19 \ud835\udc63 \ud835\udf03 \ud835\udc57,\ud835\udc64 ) / \ud835\udf19 \ud835\udc64 } -\ud835\udf03 \ud835\udc56,\ud835\udc63 \u00d7 1 } \u2265 0. Therefore, the utility of data owner \ud835\udc56 is \u02dd \ud835\udc63 \u2208 \ud835\udc49 \ud835\udc56 \ud835\udc62 \ud835\udc56,\ud835\udc63 \u2265 0. Also, SIMT satisfies BF. The total payment in cluster \ud835\udc36 \ud835\udc61 is \u02dd \ud835\udc58 \ud835\udc63 = 1 min { ( \ud835\udefd\ud835\udf19 \ud835\udc63 ) /( \ud835\udc47 \u02dd \ud835\udc58 \ud835\udc62 = 1 \ud835\udf19 \ud835\udc62 ) , ( \ud835\udf03 \ud835\udc57,\ud835\udc64 \ud835\udf19 \ud835\udc63 / \ud835\udf19 \ud835\udc64 } \u2264 \u02dd \ud835\udc58 \ud835\udc62 = 1 \ud835\udf19 \ud835\udc62 \u00d7 \ud835\udefd /( \ud835\udc47 \u02dd \ud835\udc58 \ud835\udc62 = 1 \ud835\udf19 \ud835\udc62 ) = \ud835\udefd / \ud835\udc47 . Then the total payment in \ud835\udc47 clusters is \u2264 \ud835\udefd , which shows BF. Next we show IC. Each of data subject is assigned to an auction associated with a cluster \ud835\udc36 \ud835\udc61 . As the assignment is independent of the reported valuation of the data owner, we just need to show that in the auction for each cluster is IC. In one auction, we consider an arbitrary data owner. When she report truthfully, there are two cases regarding each of her node, either being selected or not. We discuss the two cases separately. (1) Consider an arbitrary node \ud835\udc63 that is selected. Assume that in the ranking, the ( \ud835\udc58 + 1 ) -th node is possessed by data owner \ud835\udc57 , i.e., the ( \ud835\udc58 + 1 ) -th ratio is \ud835\udf19 \ud835\udc58 + 1 / \ud835\udf03 \ud835\udc57,\ud835\udc58 + 1 . Note that the data owner \ud835\udc57 could be \ud835\udc56 . If \ud835\udc56 reports a lower valuation \ud835\udf03 \u2032 \ud835\udc56,\ud835\udc63 < \ud835\udf03 \ud835\udc56,\ud835\udc63 or a higher valuation \ud835\udf03 \ud835\udc56,\ud835\udc63 < \ud835\udf03 \u2032 \ud835\udc56,\ud835\udc63 < \ud835\udf03 \ud835\udc57,\ud835\udc58 + 1 \ud835\udf19 \ud835\udc63 / \ud835\udf19 \ud835\udc58 + 1 , as the marginal contribution \ud835\udf19 \ud835\udc63 is independent from the reported valuation, the ratio \ud835\udf19 \ud835\udc63 / \ud835\udf03 \u2032 \ud835\udc56,\ud835\udc63 \u2265 \ud835\udf19 \ud835\udc58 + 1 / \ud835\udf03 \ud835\udc57,\ud835\udc58 + 1 and her ranking is still in the top \ud835\udc58 . Further, the payment of \ud835\udc56 for \ud835\udc63 is independent from \ud835\udc56 's report. As a consequence, her utility of \ud835\udc63 is \ud835\udc62 \ud835\udc56,\ud835\udc63 ( \ud835\udf3d \u2032 \ud835\udc56 , \ud835\udf3d -\ud835\udc56 ) = \ud835\udc62 \ud835\udc56,\ud835\udc63 ( \ud835\udf3d \ud835\udc56 , \ud835\udf3d -\ud835\udc56 ) . If she reports a even higher valuation \ud835\udf03 \u2032 \ud835\udc56,\ud835\udc63 \u2265 \ud835\udf03 \ud835\udc57,\ud835\udc58 + 1 \ud835\udf19 \ud835\udc63 / \ud835\udf19 \ud835\udc58 + 1 , the ratio \ud835\udf19 \ud835\udc58 + 1 / \ud835\udf03 \ud835\udc57,\ud835\udc58 + 1 > \ud835\udf19 \ud835\udc63 / \ud835\udf03 \u2032 \ud835\udc56,\ud835\udc63 . Then her allocation becomes 0 and her utility of \ud835\udc63 is \ud835\udc62 \ud835\udc56,\ud835\udc63 ( \ud835\udf3d \u2032 \ud835\udc56 , \ud835\udf3d -\ud835\udc56 ) = 0 \u2264 \ud835\udc62 \ud835\udc56,\ud835\udc63 ( \ud835\udf3d \ud835\udc56 , \ud835\udf3d -\ud835\udc56 ) . (2) Consider a node \ud835\udc63 that is not selected. Assume that in the ranking, the \ud835\udc58 -th node is possessed by data owner \ud835\udc57 , i.e, the \ud835\udc58 -th ratio is \ud835\udf19 \ud835\udc58 / \ud835\udf03 \ud835\udc57,\ud835\udc58 . Here, \ud835\udc57 could also be \ud835\udc56 . If \ud835\udc56 reports a higher valuation \ud835\udf03 \u2032 \ud835\udc56,\ud835\udc63 > \ud835\udf03 \ud835\udc56,\ud835\udc63 or a lower valuation \ud835\udf03 \ud835\udc57,\ud835\udc58 \ud835\udf19 \ud835\udc63 / \ud835\udf19 \ud835\udc58 \u2264 \ud835\udf03 \u2032 \ud835\udc56,\ud835\udc63 < \ud835\udf03 \ud835\udc56,\ud835\udc63 , the ratio \ud835\udf19 \ud835\udc58 / \ud835\udf03 \ud835\udc57,\ud835\udc58 \u2265 \ud835\udf19 \ud835\udc63 / \ud835\udf03 \u2032 \ud835\udc56,\ud835\udc63 , and \ud835\udc63 's ranking is still not among the first \ud835\udc58 . Then \ud835\udc56 's utility of \ud835\udc63 is \ud835\udc62 \ud835\udc56,\ud835\udc63 ( \ud835\udf3d \u2032 \ud835\udc56 , \ud835\udf3d -\ud835\udc56 ) = \ud835\udc62 \ud835\udc56,\ud835\udc63 ( \ud835\udf3d \ud835\udc56 , \ud835\udf3d -\ud835\udc56 ) = 0. If \ud835\udc56 reports a much lower valuation \ud835\udf03 \u2032 \ud835\udc56,\ud835\udc63 < \ud835\udf03 \ud835\udc57,\ud835\udc58 \ud835\udf19 \ud835\udc63 / \ud835\udf19 \ud835\udc58 , and her ranking is among the first \ud835\udc58 . Her Time complexity. Given a GNN, recall \ud835\udc5a is the dimensions of the input and let \ud835\udc5a h be the hidden layers. The computational complexity of a typical GNN is \ud835\udc42 (| \ud835\udc38 | \ud835\udc5a + | \ud835\udc49 | \ud835\udc5a\ud835\udc5a h ) . The computational complexity of SIMT is \ud835\udc42 ((| \ud835\udc38 | \ud835\udc5a +| \ud835\udc49 | \ud835\udc5a\ud835\udc5a h )+(| \ud835\udc38 | \ud835\udc5a +| \ud835\udc49 | \ud835\udc5a\ud835\udc5a h )+(| \ud835\udc49 |+| \ud835\udc38 |)) , where the first one terms correspond to the complexity of training the GNN, while the last two terms account for the computation of clustering and PageRank centrality resp.", "5 EXPERIMENT": "We conduct experiments to validate the performance of proposed SIMT mechanism in terms of node classification accuracy. (1) To demonstrate the overall performance of SIMT, we compare it with multiple baselines under different budgets. (2) To underscore the impact of each component, we perform a detailed ablation study.", "5.1 Experiment setup": "Dataset. Five widely-used datasets are included in our experiments: Cora, Citeseer, Pubmed, Amazon and Coauthor [7, 18, 32, 35]. The dataset statistics are listed in Table 3 in App. B. For each dataset, we randomly sample 15% of the data as test set, which remains untouched during data procurement. This set is consistent across all baselines. Once getting the selected data from auction, we further split 80% as training data and remaining 20% for validation. To accommodate different real-world scenarios, we follow the setup in existing studies [15, 17, 24, 28, 36, 46] to validate SIMT on various datasets and varying hyperparameters. Data valuations. We generate a set of random numbers to represent the data valuations. The valuations are sampled at random i.i.d. following a series of normal distributions N( \ud835\udf07, \ud835\udf0e 2 ) . We get a \ud835\udf07 drawn from U[ 0 . 8 , 1 . 2 ] for each class to capture the difference in valuations between classes. Then for each data owner, we set the valuation of each data subject as the mean of the generated valuations of her data subjects. We set \ud835\udf0e = 0 . 1. The effect of different \ud835\udf0e s on performance is investigated and the results are in App. C. To ensure all valuations are non-negative, we use a resample trick [6]. The generated valuations are in the range [ 0 , 2 ] . Note that when the domain is different, we could scale it into [ 0 , 2 ] . Budget. We set the budget in { 50 , 100 , 150 , 200 , 250 , 300 } . Given that the data valuation range is [ 0 , 2 ] , the number of selected data is approximately from 50 to 300, which is aligned with the setup of the studies on label selection e.g. [7, 50]. Structural clustering. Here, we give the configuration of the model used for Clustering ( \ud835\udc3a ) . We deploy SVD [5] to generate spectral node features, VGAE [19] to learn node embeddings followed by a linear classifier to learn the partition. We set the hidden size as 32, the learning rate as 0 . 01, the L2 regularisation as 5 \u00d7 10 -4 . The total training budget is 400 epochs. The clustering model is initialised to solely minimise the reconstruction loss. We repeat this process 100 epochs to comprehensively capture the graph structure information. Using the obtained node embeddings, we train the linear classifier to learn a partition, maximising the structural entropy. This process is repeated 300 epochs to obtain a robust partition. GNN model. We employ classical GNN models as \ud835\udc53 GNN to learn node classification. Following the configurations of [18], we set the hidden size as 32, the total training budget as 200, the learning rate as 0 . 01 and the L2 regularisation as 5 \u00d7 10 -4 . The models are optimised with minimising both reconstruction loss \ud835\udc3f recon and classification loss \ud835\udc3f class on the train data. We repeat 10 training iterations with different random seeds and report the average performance. To mitigate the impact of randomness in train-validation splitting, each training iteration creates 10 train-validation splits, trains 10 independent models according to the split and reports the best model according to their performance on the validation set. Ultimately, we evaluate the model performance using the test data. In other words, each experimental result is derived from 100 runs. Wepresent the results using a GCN model and defer the exploration on the effects of different GNN architectures in App. F. Subgraph. Each data owner possesses a subgraph with at least one node. We vary both the number \ud835\udc5c of data owners and the size \ud835\udc5b \ud835\udc56 of theire subgraphs. We first fix the number of data owners at 10, and vary the subgraph size within { 20 , 40 , 60 , 80 } to investigate the effect of subgraph size. Next, we fix the subgraph size at 80, and vary the number of data owners within { 5 , 10 , 15 , 20 } to investigate the effect of the number of data owners. The comparison results are presented in App. E. Baselines. To validate the overall performance of the SIMT, we benchmark it against four baseline mechanisms. These baselines incorporate different methods for assessing data importance within our proposed model trading framework. The baselines are: \u00b7 Greedy [37]: The Greedy mechanism treats all data as equally important and procures data based solely on the valuations of data owners. No feature propagation is applied. \u00b7 ASCV[8]: The ASCV mechanism first trains a VGAE model on the graph to learn node embeddings with optimising reconstruction loss, and evaluates data importance by the nodes' contribution to the reconstruction loss. The greater the contribution of the node, the more important the corresponding owner's data is. Then the auction procures data according to the ratio of data importance to valuation. No feature propagation is applied. \u00b7 Greedy(P): The Greedy(P) mechanism is the same as the Greedy except for that a feature imputation is applied. \u00b7 ASCV(P): The ASCP(P) mechanism is the same as the ASCV except for that a feature imputation is applied. Note that ASCV is originally designed using various techniques to evaluate data importance. However, in the absence of features, only the VGAE technique can be directly applied in our scenario. For fair comparison, we redesign all baselines to avoid pre-purchase inspection of data, and set same seeds for all places involving randomness, including edge augmentation and model initialisation. Implementation. All experiments are conducted on a single machine with AMD Ryzen 9 5900X 12-Core CPU, and NVIDIA GeForce RTX 3090 GPU. The code is implemented using Python 3 . 10 and Pytorch 2 . 0 . 1. Our code is available in the supplementary material.", "5.2 Overall performance": "The experiment results are presented in Table 1. Here, we fix the number of data owners at 10, with each owner holding 80 data subjects. As shown in the table, SIMT consistently outperforms all baselines under all budgets. Compared to the vanilla Greedy and ASCV, SIMT improves up to 40% in both MacroF1 and MicroF1. Also, the last column shows the contribution per node, i.e., calculated as the average accuracy divided by the number of purchased nodes. The results consistently show that the contribution per node of SIMT is higher than that of all baselines, demonstrating the data selected by SIMT is more valuable. This validates the effectiveness of our structural importance assessment method in Sec. 4.1. Table 1 also shows that the ASCV/ASCV(P) mechanism outperforms the Greedy/Greedy(P) mechanism. This could be attributed to that both ASCV and ASCV(P) assess the importance of data based on their structural contribution to the reconstruction loss, which, in a way, reflects structural uncertainty. However, ASCV and ASCV(P) do not perform as well as SIMT. This discrepancy underscores the effectiveness of structural importance score. Lastly, Table 1 shows that the ASCV(P) and Greedy(P) outperform their vanilla versions by up to 20% in both MacroF1 and MicroF1. This validate the need of feature imputation. Same trend is observed in the scenario with \ud835\udc5b \ud835\udc56 = 1 , \u2200 \ud835\udc56 \u2208 \ud835\udc42 . See more details in Table 10 of App. F.", "5.3 Ablation study": "To explore the impact of each component in SIMT on its performance, we conduct ablation studies across the five datasets and present the average test accuracy. As shown in Table 2, the four components, i.e., structuring clustering (clust), structural informativeness (info), structural representativeness (rep), and edge augmentation (edge aug), distinctly enhances SIMT's performance. In general, clust plays contributes the most among all components, which underscores the crucial role of structural clustering. Without clustering, there is a high probability that the procured data are unevenly distributed across the classes, leading to a biased training dataset. The second contributor is edge augmentation, which highlights the role of missing edge augmentation in the training process. Without edge augmentation, the message passing process is likely hindered, resulting in sub-optimal performance. We also compare the effect of data valuations deviation (in App. C), graph centrality metrics (in App. D), subgraph parameters (in App. E), and GNN architectures (in App. F).", "6 CONCLUSION": "In this paper, we aim to design a mechanism that properly incentives data owners to contribute their data, and returns a well performing GNN model to the model consumer for model marketplaces. In particular, we focus on the question of how we can measure data importance for model training without direct inspection. We propose SIMT, which consists of a data procurement phase and a model training phase. For data procurement, we incorporate a structure-based importance assessment method into an auction mechanism. For model training, we introduce and design two effective methods to impute missing data. As a result, SIMT ensures no data disclosure and incentive properties. Experimental results demonstrate that SIMT outperforms the baselines by up to 40% in accuracy. To the best of our knowledge, SIMT is the first model trading mechanism addressing the data disclosure problem. In the future, we will further consider the potential privacy leakage in the trained model.", "ACKNOWLEDGMENTS": "This work is supported by National Natural Science Foundation of China #62172077 and China Scholarship Council Grant #201906030067.", "REFERENCES": "[1] Jacob Abernethy, Yiling Chen, Chien-Ju Ho, and Bo Waggoner. 2015. Lowcost learning via active data procurement. In Proceedings of the Sixteenth ACM Conference on Economics and Computation . 619-636. [2] Anish Agarwal, Munther Dahleh, and Tuhin Sarkar. 2019. A marketplace for data: An algorithmic solution. In Proceedings of the 2019 ACM Conference on Economics and Computation . 701-726. [3] Dimitrios Bechtsis, Naoum Tsolakis, Eleftherios Iakovou, and Dimitrios Vlachos. 2022. Data-driven secure, resilient and sustainable supply chains: gaps, opportunities, and a new generalised data sharing and data monetisation framework. International Journal of Production Research 60, 14 (2022), 4397-4417. [4] B\u00e9la Bollob\u00e1s. 1998. Random graphs . Springer. [5] Steven L Brunton and J Nathan Kutz. 2022. Data-driven science and engineering: Machine learning, dynamical systems, and control . Cambridge University Press. [6] John Burkardt. 2014. The truncated normal distribution. Department of Scientific Computing Website, Florida State University 1 (2014), 35. [7] Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. 2017. Active learning for graph embedding. arXiv preprint arXiv:1705.05085 (2017). [8] Akshay L Chandra, Sai Vikas Desai, Chaitanya Devaguptapu, and Vineeth N Balasubramanian. 2021. On initial pools for deep active learning. In NeurIPS 2020 Workshop on Pre-registration in Machine Learning . PMLR, 14-32. [9] Mingshu Cong, Han Yu, Xi Weng, Jiabao Qu, Yang Liu, and Siu Ming Yiu. 2020. A VCG-based Fair Incentive Mechanism for Federated Learning. (2020). [10] Pranav Dandekar, Nadia Fawaz, and Stratis Ioannidis. 2012. Privacy auctions for recommender systems. In Proceedings of the 8th international conference on Internet and Network Economics . Springer-Verlag, 309-322. [11] Amirata Ghorbani and James Zou. 2019. Data shapley: Equitable valuation of data for machine learning. In International conference on machine learning . PMLR, 2242-2251. [12] Arpita Ghosh and Aaron Roth. 2011. Selling privacy at auction. In Proceedings of the 12th ACM conference on Electronic commerce . 199-208. [13] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances in neural information processing systems 30 (2017). [14] Judd Randolph Heckman, Erin Laurel Boehmer, Elizabeth Hope Peters, Milad Davaloo, and Nikhil Gopinath Kurup. 2015. A pricing model for data markets. IConference 2015 Proceedings (2015). [15] Rui Hu and Yanmin Gong. 2020. Trading data for learning: Incentive mechanism for on-device federated learning. In GLOBECOM 2020-2020 IEEE Global Communications Conference . IEEE, 1-6. [16] Jeevan Jaisingh, Jack Barron, Shailendra Mehta, and Alok Chaturvedi. 2008. Privacy and pricing personal information. European Journal of Operational Research 187, 3 (2008), 857-870. [17] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang, Costas J. Spanos, and Dawn Song. 2019. Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms. Proceedings of the VLDB Endowment (2019). [18] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [19] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308 (2016). [20] Avanish Kushal, Sharmadha Moorthy, and Vikash Kumar. 2012. Pricing for data markets. online] https://courses. cs. washington. edu/courses/cse544/11wi/projects/kumar_kushal_moorthy. pdf (2012). [21] Vito Latora, Vincenzo Nicosia, and Giovanni Russo. 2017. Complex networks: principles, methods and applications . Cambridge University Press. [22] Angsheng Li and Yicheng Pan. 2016. Structural information and dynamical complexity of networks. IEEE Transactions on Information Theory 62, 6 (2016), 3290-3339. [23] Katrina Ligett and Aaron Roth. 2012. Take it or leave it: Running a survey when privacy comes at a cost. In International Workshop on Internet and Network Economics . Springer, 378-391. [24] Jinfei Liu. 2020. Dealer: end-to-end data marketplace with model-based pricing. arXiv preprint arXiv:2003.13103 (2020). [25] Yiwei Liu, Jiamou Liu, Zijian Zhang, Liehuang Zhu, and Angsheng Li. 2019. REM: From structural entropy to community structure deception. Advances in Neural Information Processing Systems 32 (2019). [26] Nan Ma, Jiancheng Guan, and Yi Zhao. 2008. Bringing PageRank to the citation analysis. Information Processing & Management 44, 2 (2008), 800-810. [27] Miller McPherson, Lynn Smith-Lovin, and James M Cook. 2001. Birds of a feather: Homophily in social networks. Annual review of sociology 27, 1 (2001), 415-444. [28] Olga Ohrimenko, Shruti Tople, and Sebastian Tschiatschek. 2019. Collaborative machine learning markets with data-replication-robust payments. arXiv preprint arXiv:1911.09052 (2019). [29] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [30] Javier Parra-Arnau. 2018. Optimized, direct sale of privacy in personal data marketplaces. Information Sciences 424 (2018), 354-384. [31] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. 2021. A survey of deep active learning. ACM computing surveys (CSUR) 54, 9 (2021), 1-40. [32] Emanuele Rossi, Henry Kenlay, Maria I Gorinova, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein. 2022. On the unreasonable effectiveness of feature propagation in learning on graphs with missing node features. In Learning on Graphs Conference . PMLR, 11-1. [33] Aaron Roth and Grant Schoenebeck. 2012. Conducting truthful surveys, cheaply. In Proceedings of the 13th ACM Conference on Electronic Commerce . 826-843. [34] Lloyd S Shapley. 1951. Notes on the n-person game-ii: The value of an n-person game. (1951). [35] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868 (2018). [36] Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, and Bryan Kian Hsiang Low. 2020. Collaborative machine learning with incentive-aware model rewards. In International conference on machine learning . PMLR, 89278936. [37] Yaron Singer. 2010. Budget feasible mechanisms. In 2010 IEEE 51st Annual Symposium on foundations of computer science . IEEE, 765-774. [38] Indro Spinelli, Simone Scardapane, and Aurelio Uncini. 2020. Missing data imputation with adversarially-trained graph convolutional networks. Neural Networks 129 (2020), 249-260. [39] Peng Sun, Xu Chen, Guocheng Liao, and Jianwei Huang. 2022. A profitmaximizing model marketplace with differentially private federated learning. In IEEE INFOCOM 2022-IEEE Conference on Computer Communications . IEEE, 1439-1448. [40] Hibiki Taguchi, Xin Liu, and Tsuyoshi Murata. 2021. Graph convolutional networks for graphs containing missing features. Future Generation Computer Systems 117 (2021), 155-168. [41] Bisheng Tang, Xiaojun Chen, Shaopu Wang, Yuexin Xuan, and Zhendong Zhao. 2023. Generalized heterophily graph data augmentation for node classification. Neural Networks (2023). https://doi.org/10.1016/j.neunet.2023.09.021 [42] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017). [43] Yifei Wang, Yupan Wang, Zeyu Zhang, Song Yang, Kaiqi Zhao, and Jiamou Liu. 2023. User: Unsupervised structural entropy-based robust graph neural network. arXiv preprint arXiv:2302.05889 (2023). [44] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826 (2018). [45] Lei Xu, Chunxiao Jiang, Yan Chen, Yong Ren, and K. J. Ray Liu. 2015. Privacy or Utility in Data Collection? A Contract Theoretic Approach. IEEE Journal of Selected Topics in Signal Processing 9, 7 (2015), 1256-1269. [46] Xinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao, Chuan Sheng Foo, and Bryan Kian Hsiang Low. 2021. Gradient driven rewards to guarantee fairness in collaborative machine learning. Advances in Neural Information Processing Systems 34 (2021), 16104-16117. [47] Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao. 2021. A survey on federated learning. Knowledge-Based Systems 216 (2021), 106775. [48] Jingwen Zhang, Yuezhou Wu, and Rong Pan. 2021. Incentive mechanism for horizontal federated learning based on reputation and reverse auction. In Proceedings of the Web Conference 2021 . 947-956. [49] Mengxiao Zhang, Fernando Beltran, and Jiamou Liu. 2020. Selling Data at an Auction under Privacy Constraints. In Conference on Uncertainty in Artificial Intelligence . PMLR, 669-678. [50] Wentao Zhang, Yu Shen, Yang Li, Lei Chen, Zhi Yang, and Bin Cui. 2021. Alg: Fast and accurate active learning framework for graph convolutional networks. In Proceedings of the 2021 International Conference on Management of Data . 23662374.", "APPENDIX": "", "A NORMALISED MARGINAL STRUCTURAL ENTROPY": "The calculation of the normalised marginal structural entropy is as follows. The marginal structural entropy is: We normalise this value and get", "B DATASET STATISTICS": "", "C COMPARISON OF DATA VALUATIONS DEVIATION": "We conduct an experiment to evaluate the effect of the standard deviation in data valuation distribution on scenario with \ud835\udc5b \ud835\udc56 = 1 , \ud835\udc56 \u2208 \ud835\udc42 . The experiment is evaluated on Cora, Citeseer and Pubmed datasets with standard deviation \ud835\udf0e ranging from 0 . 01 to 0 . 2. In Table 4, we list the MacroF1 and MicroF1 performance of our mechanism running on varying standard deviations of data valuations. We also list the number of bought data to compare the effect of \ud835\udf0e on data quantity. We observe that the number of bought data increases as \ud835\udf0e increases. The increasing data quantity brings in more information for training GNN model and thus improves the model performance. To balance the effect of data quantity, we fix \ud835\udf0e = 0 . 1 for all the other experiments. To eliminate the randomness induced by unknown subgraphs, we conduct experiments on the case where all data owners are individuals.", "D COMPARISON OF GRAPH CENTRALITY METRICS": "Weevaluate the effectiveness of various graph centrality metrics on Cora dataset. To eliminate the randomness induced by subgraphs, we conduct experiments on the scenario with \ud835\udc5b \ud835\udc56 = 1 for all \ud835\udc56 \u2208 \ud835\udc42 . Specifically, we substitute the PageRank centrality used in our structural representativeness with three other widely used centrality metrics: degree centrality, closeness centrality, and betweenness centrality [21]. We report the test accuracy with varying budgets. As shown in Table 5, PageRank centrality consistently surpasses the other metrics. Consequently, our mechanism employs PageRank centrality as structural representativeness.", "E COMPARISON OF SUBGRAPH PARAMETERS": "Here we explore the effect of the subgraph size and the number of data owners. To reduce the effect of randomness, we use \ud835\udc5b \ud835\udc56 = 1 as a specific scenario to measure the effect of other parameters. The experimental results on the effects due to subgraph size and number of subgraphs are shown in Tables 6 and 7. When fixing the budget as 1500 and the number of data owners as 10, we find that the larger the subgraph size is, the more edges within the subgraph are missing, which hinders the message passing process and results in worse model performance, as shown in Table 6. Then when fixing the budget at 1500 and the subgraph size at 80, Table 7 shows that the more subgraphs exist, the more edges within subgraphs are missing, resulting in worse model performance. These experiments underscore the negative effect of missing edges during model training. Consequently, we utilise an edge augmentation technique to alleviate the impact of these missing edges.", "F COMPARISON OF GNN ARCHITECTURES": "To test the effect of GNN architectures, we also conduct experiments on scenario with \ud835\udc5b \ud835\udc56 = 1 , \ud835\udc56 \u2208 \ud835\udc42 using three representative GNN architectures: GIN [44], GraphSage [13], and GAT [42]. These architectures have emerged as alternatives to the traditional GCN [18] and have gained widespread acceptance within the GNN community. The configuration of these architectures are the same as that of GCN as shown in GNN models part in Sec. 5.1. To eliminate the randomness induced by subgraphs, we conduct experiments on the case where all data owners are individuals. The experiment results are shown in Tables 8, 10, 11, 12, and 13. In general, SIMT outperforms its benchmarks in all experiments. When compared to the Greedy mechanism, SIMT exhibits an improvement ranging from 10% to 40% in both MacroF1 and MicroF1, consistently confirming the effectiveness of SIMT. However, a minor fluctuation is observed when compared to ASCV(P) within the GraphSage architecture on the Citeseer dataset. More specifically, when the budget is 1500, SIMT demonstrates a 1 . 1% improvement in MacroF1, albeit a 0 . 2% decrease in MicroF1. Furthermore, when the budget is increased to 3000, SIMT shows a slight decrease by 0 . 2% in MacroF1, but a notable increase by 1 . 3% in MicroF1. Consequently, this fluctuation does not detract from the overall effectiveness of SIMT. When comparing the effects of different architectures, it is observed that the GraphSage architecture generally outperforms the others. This superior performance is particularly noticeable when the budget is low. Additionally, the GraphSage architecture exhibits lower variance, which implies robust performance. This could potentially be attributed to the fact that GraphSage does not heavily depend on node features and is well suited to spectral features [42]. This compatibility allows it to effectively leverage the spectral properties of the data, potentially contributing to its robust performance across various experiments. Furthermore, when the budget is set to 3000, the GraphSage architecture underperforms the GIN architecture on the Cora dataset. This could potentially be attributed to the theoretically stronger learnability of GIN compared to GraphSage [44]. Last, the GAT architecture performs the worst among the four architectures. This could be due to the fact that GAT employs an attention mechanism, which has significantly more parameters and thus requires a sufficient amount of feature and label data for parameter training.", "G ABLATION STUDY FOR \ud835\udc5b \ud835\udc56 = 1": "We conduct ablation studies across the three datasets and present the average test accuracy for \ud835\udc5b \ud835\udc56 = 1. As shown in Table 9, (1) the accuracy of SIMT without clustering diminishes by 8 . 3% in Cora, 3 . 7% in Citeseer, and 13 . 7% in Pubmed. This significant reduction underscores the crucial role of structural clustering. Without clustering, there is a high probability that the procured data are unevenly distributed across the classes, leading to a biased training dataset. (2) The accuracy of SIMT without structural informativeness drops by 1 . 8% in Cora, 2 . 1% in Citeseer, and 2 . 9% in Pubmed. (3) The accuracy reduction for SIMT without structural representativeness is 1 . 8% in Cora, 1 . 5% in Citeseer, and 2 . 2% in Pubmed. Overall, each component distinctly enhances SIMT's performance. In the scenario with \ud835\udc5b \ud835\udc56 = 1 , \ud835\udc56 \u2208 \ud835\udc42 , the results show similar trends as the scenario with \ud835\udc5b \ud835\udc56 \u2265 1 , \ud835\udc56 \u2208 \ud835\udc42 : SIMT performs the best under all budgets, followed by ASCV/ASCV(P), and Greedy/Greedy(P) performs the worst. It is worth to notice that SIMT performs even better when \ud835\udc5b \ud835\udc56 = 1 , \ud835\udc56 \u2208 \ud835\udc42 than when \ud835\udc5b \ud835\udc56 \u2265 1 , \ud835\udc56 \u2208 \ud835\udc42 . In particular, SIMT improves 20% \u223c 40% in both MacroF1 and MicroF1 in this case; See more details in Table 10 of App. F. This can be attributed to that when \ud835\udc5b \ud835\udc56 = 1 , \ud835\udc56 \u2208 \ud835\udc42 , fewer edges are missing within subgraphs, allowing the structural importance scores calculated by our mechanism to more accurately reflect the true structural importance of each node.", "H LIMITATIONS AND FUTURE WORK": "Even though our SIMT mechanism shows the incentive properties theoretically and the accuracy empirically, we acknowledge certain limitations that offer avenues for future research. \u00b7 This paper addresses issues related to pre-purchase inspections. However, there are concerns about privacy breaches, as the raw data shared with data brokers and the trained models returned to data consumers could expose private information of the data owners. Future work could enhance the SIMT mechanism by incorporating considerations of these privacy issues \u00b7 In the design of the SIMT mechanism, we assess the structural importance once and subsequently make a single purchasing decision for all data owners. An alternative approach could involve an adaptive purchasing method, where data points are bought in several times, each time reassessing the structural importance in light of newly acquired data. The primary rationale for the existing design is efficiency compared to this adaptive method. Nonetheless, developing an efficient adaptive model training mechanism presents an exciting avenue for future research. \u00b7 The efficacy of the SIMT mechanism heavily depends on the quality of clustering; more accurate structural importance assessment is achieved when clusters are closer to the true classes. Our SIMT mechanism employs a classical VGAE model [19] as the Clustering . However, our findings indicate that this approach is not always robust. Exploring a broader range of GNN models for clustering might yield more stable results and enhance the performance of the SIMT mechanism, although such investigations fall outside the scope of this paper"}
