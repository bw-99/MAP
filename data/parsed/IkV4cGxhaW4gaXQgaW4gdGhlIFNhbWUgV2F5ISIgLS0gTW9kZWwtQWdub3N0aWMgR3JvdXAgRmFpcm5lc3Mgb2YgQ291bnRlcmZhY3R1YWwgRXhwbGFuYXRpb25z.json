{"title": "\"Explain it in the Same Way!\" -Model-Agnostic Group Fairness of Counterfactual Explanations *", "authors": "Andr\u00e9 Artelt; Barbara Hammer", "pub_date": "2022-11-27", "abstract": "Counterfactual explanations are a popular type of explanation for making the outcomes of a decision making system transparent to the user. Counterfactual explanations tell the user what to do in order to change the outcome of the system in a desirable way. However, it was recently discovered that the recommendations of what to do can differ significantly in their complexity between protected groups of individuals. Providing more difficult recommendations of actions to one group leads to a disadvantage of this group compared to other groups. In this work we propose a model-agnostic method for computing counterfactual explanations that do not differ significantly in their complexity between protected groups.", "sections": [{"heading": "Introduction", "text": "Transparency and fairness are essential building blocks of modern trustworthy decision making systems deployed in the real world [33,23]. Transparency is often realized by means of explanations -i.e. explanations of the system's behavior are provided to the user [24]. A lot of different explanation methodologies have been developed by researchers establishing the field of eXplainable Artificial Intelligence (XAI) [24,3]. Developed explanation methodologies include feature relevance/importance methods [15] and examples based methods [1]. Instances of popular example based methods are contrasting explanations like counterfactual explanations [36,34] and prototypes & criticisms [19] -these methods use a set or a single example for explaining the behavior of the system. The aspect of fairness [23] usually refers to the requirement that the output (i.e. predictions) of the systems must not discriminate single individuals or groups of individuals. Individual fairness states that similar individuals must be treated similarly [13] -i.e. the behavior or output of a fair system should not differ significantly between similar individuals. The key problem here is to come up with a suitable definition of similarity. On the other hand, group fairness states that the system must not behave significantly different between individuals from different protected groups, which are created based on protected attributes such as gender, race, etc., -i.e. the behavior/output of the system must be independent of the protected attribute [26,23]. Here the challenges is to model independence of the protected attribute -different modelings give rise to different fairness criteria which contradict each other [26].\nBoth aspects, transparency/explanations and fairness, have been extensively studied in the past [2,29,3,16,26]. Only recently, the research community started to investigated the fairness of different explanation methodologies and discovered that fairness issues might exist there as well [9,35]. In particular, it was observed that provided explanations might differ significantly between similar individuals or between protected groups of individuals [17,31,7]. This becomes problematic especially in case of actionable recourse -i.e. explanations that tell the user what to do in order to observe a different outcome. A common way of achieving actionable recourse is to provide recommendations as explanations such as counterfactual explanations -as some recommendations are easier to full fill than others, significant differences in these recommendations across groups or similar individuals can therefore disadvantage certain individuals or groups of individuals.", "publication_ref": ["b32", "b22", "b23", "b23", "b2", "b14", "b0", "b35", "b33", "b18", "b22", "b12", "b25", "b22", "b25", "b1", "b28", "b2", "b15", "b25", "b8", "b34", "b16", "b30", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Our contributions", "text": "We propose a novel model-agnostic method for computing counterfactual explanations, a very popular instance of contrasting explanations, that satisfy a particular notion of group fairness -i.e. the counterfactual explanations should not differ significantly between different protected groups. We also extensively empirically evaluate the performance of our proposed method in several experiments.\nThe remainder of this work is structured as follows: In Section 2 we review counterfactual explanations and (re-)introduce the group fairness problem of counterfactual explanations with which we are dealing in this work. Next, in Section 3, we propose our methodology for computing group fair counterfactual explanations, which we evaluate empirically in Section 4. Finally, after discussing important characteristics of our proposed method and its broader impact in Section 5, the work closes with a conclusion and summary in Section 6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Counterfactual Explanations", "text": "Counterfactual explanations (often just called counterfactuals) are an instance of contrasting explanations, stating changes to the input that lead to a different outcome (i.e. behavior of the system). Counterfactuals can be interpreted as a recommendation of actions that lead to a different specific behavior or output of the system. Since this mimics the way humans explain things [10] -i.e. humans often ask questions like \"What needs to be changed in order to observe a different outcome?\" -, counterfactuals became quite popular in the XAI community [24,34]. For illustrative purposes, consider the example of loan application: Imagine you applied for a credit at a bank. Unfortunately, the bank rejects your application. Now, you would like to know why. In particular, you would like to know what would have to be different so that your application would have been accepted. A possible explanation might be that you would have been accepted if you had earned 500$ more per month and if you had not had a second credit card. In order to keep the explanations \"simple\" -i.e. easy to understand -, a common approach is to minimize the number of recommended actions by staying as close as possible to the original input. This is formalized in the following definition Definition 1.\nDefinition 1 ((Closest) Counterfactual Explanation [36]). Assume a prediction function h : R d \u2192 Y is given. Computing a counterfactual x cf \u2208 R d for a given input x orig \u2208 R d is phrased as the following optimization problem: arg min\nx cf \u2208 R d h( x cf ), y cf + C \u2022 \u03b8( x cf , x orig )(1)\nwhere (\u2022) denotes a loss function that penalizes deviation of the prediction h( x cf ) from the requested target prediction y cf , \u03b8(\u2022) denotes a penalty for dissimilarity of x cf and x orig \u2021 , and C > 0 denotes the regularization strength.\nNote that the actual explanation -i.e. recommendation of actions to change some features in a particular way -is given by the difference \u03b4 = x cf -x orig . Furthermore, \u03b8(\u2022) can also be interpreted as the complexity of the counterfactual x cf -i.e. measuring how difficult it is to execute the recommended changes \u03b4, since changing some features might be easier or more difficult than changing others, and also the amount of change might contribute to the complexity & difficultiness of the explanation. For illustrative purposes, we again consider the previous example of a loan application: While a valid explanation might be to increase the monthly income by 500$, another valid explanation might be to increase monthly income by 400$, canceling a second credit card and having some property as a collateral security. Clearly, the latter one is much more difficult to full fill \u00a7 than the first one -therefore, one would assign a higher score \u03b8(\u2022) to the latter.\nIn the remainder of this work, we assume a binary classification problem: In this case, we denote a (closest) counterfactual x cf (according to Definition 1) of a given sample x orig under a prediction function h(\u2022) simply as x cf = CF( x orig , h) and drop the target label y cf because it is uniquely determined.\nThe counterfactuals x cf from Definition 1 are also called closest counterfactuals [18] because they try stay as close as possible to the original input x orig . However, closest counterfactual are not necessarily plausible & actionable which might hinder they use in practice [37] -there exist numerous extensions [5,22,6,32,28], that try to improve plausibility & actionability of counterfactuals from Definition 1. The most simple approach for generating plausible, and therefore hopefully actionable, counterfactuals is to limit the feasible set of solutions to Eq. ( 1) to a set of training samples [28] -i.e. simply picking the most similar sample from the training set as a counterfactual explanation. While this option is easily implementable and guarantees plausible counterfactuals, it comes however at the price of potentially leaking sensitive information and might therefore pose a privacy issue [25]. Nevertheless, this is the method we use whenever we refer to plausible counterfactuals.", "publication_ref": ["b9", "b23", "b33", "b35", "b17", "b36", "b4", "b21", "b5", "b31", "b27", "b27", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Fairness of Counterfactual Explanations", "text": "In this work, we assume that the individuals are grouped into two so called protected groups D 1 and D 2 based on some sensitive attribute like sex, race, etc. While fairness of the predictions of AI systems is a well known challenge that has been studied for quite some time [23,27,11], potential fairness issues of explanations were only recently discovered by the community [35,7,31,17]. In particular, it was observed that closest counterfactual explanations (Definition 1) might differ significantly between individuals from different groups [17] or also between similar individuals [7].\nIn this work, we follow the common fairness notion of counterfactuals as introduced in [17], that is based on the complexity & difficultiness of the explanation, which is measured by the distance \u03b8(\u2022) between the original sample x orig and the corresponding counterfactual explanation x cf -we refer to this quantity as the cost of a counterfactual: Definition 2 (Cost of a Counterfactual Explanation). For a given sample x orig \u2208 R d and a classifier h(\u2022), we define the cost of the corresponding closest counterfactual Definition 1 x cf as the distance between the original sample and the counterfactual:\n\u03b8( x orig , CF( x orig , h))(2)\nwhere we substituted x cf with CF( x orig , h) to stress the dependencies on x orig , h(\u2022) and the explanation generation method CF(\u2022, \u2022) itself.\nNote that in Definition 2 we completely rely on \u03b8(\u2022) for measuring the complexity and costs of the recommended actions by the counterfactual explanation -note that in practice, \u03b8(\u2022) must be chosen carefully and might require some domain knowledge.\nIt can be perceived as problematic if the cost of the counterfactuals (i.e. recommended actions) differs significantly between protected groups of individuals (e.g. males vs. females) [17,9,31] or between similar individuals [7], since achieving the algorithmic recourse would be much easier for the individuals getting low-cost explanations than those getting high-cost (i.e. more difficult to execute) explanations -this would disadvantage certain groups of individuals which violates social norms of fairness [14,21]. For illustrative purposes, we again come back to our running example of loan applications: For instance, it can be considered as unfair if the counterfactual explanations presented to males on average ask only for a minor increase in their monthly income while those presented to females ask for a major increase in their monthly income. We formalize this intuition of group fairness of counterfactual explanations by using the definition of a cost (Definition 2) of a counterfactual: Definition 3 (Group Fair Counterfactual Explanation). We say that counterfactual explanations are fair iff their costs (Definition 2) does not differ significantly between the different groups [17,31] -i.e. the distributions of \u03b8(\u2022) are \"sufficiently similar\":\np xorig\u223cD1 \u03b8( x orig , CF( x orig , h)) \u2248 p xorig\u223cD2 \u03b8( x orig , CF( x orig , h))(3)\nSince dealing with the entire distribution p xorig\u223cD ? might be too complicated, we relax and reduce Eq. ( 3) to the expected value [31]:\nE xorig\u223cD1 \u03b8( x orig , CF( x orig , h)) \u2248 E xorig\u223cD2 \u03b8( x orig , CF( x orig , h))(4)\nwhich can be easily estimated using given sets of samples.\nExisting work from literature In order to achieve group fairness of counterfactual explanations -i.e. satisfying Eq. ( 3) -in case of a support vector machine (SVM), the authors of [17] propose to change the training of the SVM such that the counterfactual explanations satisfy the group fairness criterion Eq. ( 4). More specifically, they propose an iterative method where they add recourse constraints to the original SVM optimization problem. In addition, they also propose a model-agnostic method where they iteratively generate samples using LIME [30] that are used to retrain the original classifier h(\u2022) using these samples. However, to the best of our knowledge, all existing methods either focus on specific models and/or do not consider changing the model-agnostic algorithm CF(\u2022, \u2022) for computing the final counterfactuals that satisfy Eq. (3). In this work, we aim to address this research gap by proposing a model-agnostic algorithm CF(\u2022, \u2022) for computing group fair counterfactual explanations (see Section 3) without retraining or changing the given model h(\u2022).", "publication_ref": ["b22", "b26", "b10", "b34", "b6", "b30", "b16", "b16", "b6", "b16", "b16", "b8", "b30", "b6", "b13", "b20", "b16", "b30", "b30", "b16", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Computing Group Fair Counterfactual Explanations", "text": "In order to achieve the relaxed group fairness of counterfactual explanations (Definition 3) as stated in Eq. ( 4), we propose to change the method CF(\u2022, \u2022) for computing counterfactuals. More specifically, we propose to extend Eq. ( 1) as follows: min\nx cf \u2208 R d \u03b8( x orig , x cf ) + C 0 \u2022 (h( x cf ), y cf ) + C 1 \u2022 max (z -\u03b8( x orig , x cf ))\nwhere z \u223c p(\u03b8( X orig , X cf )) (5) where the only difference to Eq. ( 1) is the addition of the last regularization term C 1 \u2022 max (z -\u03b8( x orig , x cf )) in Eq. ( 5) which adds a penalty if the distance (i.e.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 Computing Group Fair Counterfactual Explanations", "text": "Input: A classifier h(\u2022); Samples grouped into two groups D1 and D2 based on their binary sensitive attribute; Regularization strengths C0, C1 > 0 Output: Group fair counterfactuals x cf 1:\nInitialization step 2:\nCompute the cost of counterfactual explanation for all samples in both sets 3:\nZ1 = {\u03b8( xorig, CF( xorig, h)) \u2200 xorig \u2208 D1} 4: Z2 = {\u03b8( xorig, CF( xorig, h)) \u2200 xorig \u2208 D2} 5:\nCheck which of the two groups is disadvantaged compared to the other 6: For all new samples ( xorig, yorig) we compute a group fair counterfactual as follows: 15: z = random.choice(Z)\nZ = \u2205 7: if 1 |Z 1 | z i \u2208Z 1 zi > 1 |Z 2 | z j \u2208Z 2 zj\nSelect a random cost from the disadvantaged group 16:\nCompute the group fair counterfactual by solving Eq. ( 5) 17: x cf = arg min\nx cf \u2208 R d \u03b8( xorig, x cf ) + C0 \u2022 (h( x cf ), y cf ) + C1 \u2022 max (z -\u03b8( xorig, x cf ))\ncost/complexity) of the counterfactual x cf is larger than some particular value z sampled from the distribution of distances (i.e. costs) \u03b8(\u2022) of the disadvantaged group -by this we make sure that the difference between the advantaged and disadvantage group disappears on average. Note that for this, we need access to a set of samples (including their counterfactual explanation) from the disadvantaged group, so that we can estimate E \u03b8( X orig , X cf ) since we do not know the true distribution. For practical purposes, we suggest to simply select the distance (i.e. cost) of a random sample from the disadvantaged group as a value of z -by this we obtain a randomized algorithm/method for computing counterfactual explanations and the counterfactuals are no longer deterministic. After we obtained a value of z, we can solve Eq. ( 5) either by using any black-box solver, like for instance Downhill-Simplex, resulting in a closest counterfactual or we could obtain a plausible counterfactual by simply select a sample from the training set D 1 \u222a D 2 that minimizes Eq. ( 5). The entire proposed methodology for computing group fair counterfactual explanations is described in full detail in Algorithm 1.\nNote that the major advantage of this approach, compared to existing methods [17], is that the model h(\u2022) remains untouched -i.e. we do not assume that we have access to model internals, or more crucially, that we are able to retrain the entire model which might be quite costly in practice -which perfectly aligns with the model-agnostic paradigm of counterfactual explanations [36].", "publication_ref": ["b16", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We empirically evaluate our proposed method (Algorithm 1) for generating group fair counterfactual explanations on a diverse set of data sets and classifiers. We also consider and compare our proposed method (Algorithm 1) for two different types of counterfactuals: Closest counterfactuals (Definition 1) and plausible counterfactuals (i.e. samples from the training set). The Python implementation, including all necessary details for reproduction, of the experiments is publicly available on GitHub \u00b6 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "Credit card clients data set The Credit card clients data set [39] (Credit) contains 30000 data records of customers used for default payment prediction. Each record is described by 23 attributes (8 categorical, 14 numerical and 1 binary). The sensitive attribute is \"sex\" but we also remove \"age\" and \"marriage\".\nCommunities & Crime data set The Communities & Crime data set [12] (Crime) contains 1994 socio-economic data records from 46 states in the USA. Following the preprocessing as suggested in [20], we are left with 100 encoded attributes and that are used to predict the crime rate (low crime rate vs. high crime rate). The sensitive attribute is \"race\".\nLaw school data set The Law school data set [38] (Law ) contains 20798 law school admission records. Each record (student) is described by 12 attributes (3 categorical, 3 binary and 6 numerical). The binary target attribute describes whether the student as admitted or not. The sensitive attribute is \"sex\" but we also remove the attribute \"race\".", "publication_ref": ["b38", "b11", "b19", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Setup", "text": "We run the following procedure, for every data set and classifier -we consider logistic regression (Logreg), decision trees (Dectree) and Gaussian naive Bayes (GNB ) -, in a 3-fold cross validation:\n1. Build the classifier using all available training data -i.e. ignore any group assignments based on sensitive attributes -but sensitive attributes are NOT included as features.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Compute two counterfactual explanations for each sample of all test samples", "text": "that are classified correctly -the closest counterfactual (Definition 1) is computed by using [8] and for a plausible counterfactual we simply select the sample from the training set that minimizes Eq. (1). We then group these samples and their counterfactuals according to the sensitive attribute. \u00b6 https://github.com/HammerLabML/ModelAgnosticGroupFairnessCounterfactuals 3. Use Algorithm 1 to compute group fair counterfactuals of all correctly classified samples. Again, we solve Eq. ( 5) in two different ways: A closest counterfactual by solving Eq. ( 5) using the Downhill-Simplex method, and a plausible counterfactual by limiting the set of feasible solutions of Eq. ( 5) to the training set.\nFor the purpose of evaluating our proposed method Algorithm 1, we compare the costs per group of the closest counterfactuals (Definition 1) with those of the group fair counterfactuals (Definition 3).", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 1. Results for closest counterfactuals -we report the median cost of counterfactuals (Definition 2), all number are rounded to two decimal points.\nClf Data set Group-0 Group-1 Group-0-Fair Group-  two things: First, not all scenarios show group fairness issues -i.e. in some cases there is no significant difference in the costs across the different groups. Second, if there is a significant difference, our method almost always improves the group fairness significantly -i.e. the difference in the costs across different groups is much smaller, which we additionally illustrate in Figure 1 for the particular scenario of closest counterfactuals of a decision tree classifier on the Commu-nitiesCrime data set. We do not observer any difference between closest and plausible counterfactuals. These findings demonstrate the strong performance of our proposed model-agnostic method for computing group fair counterfactual explanations.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Discussion & Broader Impact", "text": "Advantages. The key advantage of our proposed method over existing work is that it is completely model-agnostic -i.e. we do not assume access to any model internals or that we are able to retrain the model.\nComputational complexity. Before we can compute group fair counterfactuals, we first have to compute counterfactual explanations of all samples from the training set that is used to estimate the distribution of costs. The computational complexity of this step mainly depends on the number of samples and the classifier h(\u2022), since the computational complexity of computing a counterfactual varies significantly between different classifiers and how many assumptions we can make on the classifier [4]. However, this initial step has to be done only once and does not need to be repeated if the data distribution and model do not change. The computation of the group fair counterfactual is then done by solving Eq. ( 5) using a black-box method like Downhill-Simplex method.\nLimitations. There exist two potential limitations of our proposed method: First, the initialization step of Algorithm 1 assumes access to a sufficiently large set of training samples that can be used to estimate the costs of counterfactuals across the two protected groups. In practice, it might be challenging to obtain enough samples from a minority group in order to reliable estimate the cost of the counterfactuals. Furthermore, the computation of these counterfactuals induces some computational costs as discussed above. Second, the advantage of being model-agnostic, and not considering any special cases where we might have access to model internals, comes at the price that we can not exploit any structure of h(\u2022) for efficiently computing a group fair counterfactual by solving Eq. ( 5). As a consequence, we have to rely on computationally expensive methods for solving Eq. ( 5). Note that this is in contrast to the initialization step of Algorithm 1, where we have to compute normal counterfactuals and can exploit any structure or access to model internals if those are available.\nBroader impact. Counterfactual explanations are widely used in practice to provide users with actionable feedback how to change the outcome of a decision making system. Potential fairness issues, as described in this work, are therefore harmful to individuals and must be eliminated as much as possible. We therefore expect that our proposed method have a high impact in practice as it can significantly improve the fairness of counterfactual explanations. Because it is completely model-agnostic, it can be applied to any models used by practitioners, and our publicly available Python implementation of Algorithm 1 is easy to use and therefore allows a direct and seamless integration into existing XAI pipelines.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Summary & Conclusion", "text": "In this paper, we looked into the issue of group fairness of counterfactual explanations, and proposed a model-agnostic method for computing group fair counterfactuals. We empirically evaluated our proposed algorithm on a diverse set of experiments and observed a strong performance. The key advantage over existing methods from literature is that our method does not need access to the model or have to retrain the model -i.e. our method is completely model-agnostic and can be applied to any model that is going to be explained by counterfactuals.\nBased on this work, there emerge a couple of potential directions for future research:\n-In this work we ignored any potential access to model internals or structure of h(\u2022) for computing group fair counterfactuals. However, as it was shown in [4], exploiting model internals can lead to much more efficient algorithms for computing normal closest counterfactuals (Definition 1). In this context it would interesting and relevant to study how such available knowledge could be exploited for computing group fair counterfactuals by solving Eq. ( 5). -As discussed, the initialization step of Algorithm 1 has to be recomputed if the data distribution or h(\u2022) changes. Since the initialization step can be computationally expensive, it would be relevant to study if and how an adaptation of this initialization step is possible in case of any changes in the data distribution or the model h(\u2022).", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Case-based reasoning: Foundational issues, methodological variations, and systemapproaches", "journal": "AI communications", "year": "1994", "authors": "A Aamodt; E Plaza"}, {"ref_id": "b1", "title": "Peeking inside the black-box: a survey on explainable artificial intelligence (xai)", "journal": "IEEE access", "year": "2018", "authors": "A Adadi; M Berrada"}, {"ref_id": "b2", "title": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai", "journal": "Information fusion", "year": "2020", "authors": "A B Arrieta; N D\u00edaz-Rodr\u00edguez; J Del Ser; A Bennetot; S Tabik; A Barbado; S Garc\u00eda; S Gil-L\u00f3pez; D Molina; R Benjamins"}, {"ref_id": "b3", "title": "On the computation of counterfactual explanations -A survey", "journal": "", "year": "2019", "authors": "A Artelt; B Hammer"}, {"ref_id": "b4", "title": "Convex density constraints for computing plausible counterfactual explanations", "journal": "", "year": "2020", "authors": "A Artelt; B Hammer"}, {"ref_id": "b5", "title": "Convex optimization for actionable\\& plausible counterfactual explanations", "journal": "", "year": "2021", "authors": "A Artelt; B Hammer"}, {"ref_id": "b6", "title": "Evaluating robustness of counterfactual explanations", "journal": "IEEE", "year": "2021", "authors": "A Artelt; V Vaquet; R Velioglu; F Hinder; J Brinkrolf; M Schilling; B Hammer"}, {"ref_id": "b7", "title": "Ceml: Counterfactuals for explaining machine learning models -a python toolbox", "journal": "", "year": "2019", "authors": "A Artelt"}, {"ref_id": "b8", "title": "The road to explainability is paved with bias: Measuring the fairness of explanations", "journal": "", "year": "2022", "authors": "A Balagopalan; H Zhang; K Hamidieh; T Hartvigsen; F Rudzicz; M Ghassemi"}, {"ref_id": "b9", "title": "Counterfactuals in explainable artificial intelligence (xai): Evidence from human reasoning", "journal": "", "year": "2019", "authors": "R M J Byrne"}, {"ref_id": "b10", "title": "The measure and mismeasure of fairness: A critical review of fair machine learning", "journal": "", "year": "2018", "authors": "S Corbett-Davies; S Goel"}, {"ref_id": "b11", "title": "Uci machine learning repository", "journal": "", "year": "2017", "authors": "D Dheeru; E K Taniskidou"}, {"ref_id": "b12", "title": "Fairness through awareness", "journal": "ACM", "year": "2012", "authors": "C Dwork; M Hardt; T Pitassi; O Reingold; R S Zemel"}, {"ref_id": "b13", "title": "A theory of fairness, competition, and cooperation", "journal": "The quarterly journal of economics", "year": "1999", "authors": "E Fehr; K M Schmidt"}, {"ref_id": "b14", "title": "All Models are Wrong but many are Useful: Variable Importance for Black-Box, Proprietary, or Misspecified Prediction Models, using Model Class Reliance", "journal": "", "year": "2018-01", "authors": "A Fisher; C Rudin; F Dominici"}, {"ref_id": "b15", "title": "A comparative study of fairness-enhancing interventions in machine learning", "journal": "Association for Computing Machinery", "year": "2019", "authors": "S A Friedler; C Scheidegger; S Venkatasubramanian; S Choudhary; E P Hamilton; D Roth"}, {"ref_id": "b16", "title": "Equalizing recourse across groups", "journal": "", "year": "2019", "authors": "V Gupta; P Nokhiz; C D Roy; S Venkatasubramanian"}, {"ref_id": "b17", "title": "A survey of algorithmic recourse: contrastive explanations and consequential recommendations", "journal": "ACM Computing Surveys (CSUR)", "year": "2021", "authors": "A H Karimi; G Barthe; B Sch\u00f6lkopf; I Valera"}, {"ref_id": "b18", "title": "Examples are not enough, learn to criticize! criticism for interpretability", "journal": "Advances in Neural Information Processing Systems", "year": "2016", "authors": "B Kim; O Koyejo; R Khanna"}, {"ref_id": "b19", "title": "A survey on datasets for fairness-aware machine learning", "journal": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "year": "2022", "authors": "Le Quy; T Roy; A Iosifidis; V Zhang; W Ntoutsi; E "}, {"ref_id": "b20", "title": "Social utility and decision making in interpersonal contexts", "journal": "Journal of Personality and Social psychology", "year": "1989", "authors": "G F Loewenstein; L Thompson; M H Bazerman"}, {"ref_id": "b21", "title": "Interpretable counterfactual explanations guided by prototypes", "journal": "", "year": "2021", "authors": "A V Looveren; J Klaise"}, {"ref_id": "b22", "title": "A survey on bias and fairness in machine learning", "journal": "ACM Computing Surveys (CSUR)", "year": "2021", "authors": "N Mehrabi; F Morstatter; N Saxena; K Lerman; A Galstyan"}, {"ref_id": "b23", "title": "Interpretable Machine Learning", "journal": "", "year": "2019", "authors": "C Molnar"}, {"ref_id": "b24", "title": "On the privacy risks of algorithmic recourse", "journal": "", "year": "2022", "authors": "M Pawelczyk; H Lakkaraju; S Neel"}, {"ref_id": "b25", "title": "A review on fairness in machine learning", "journal": "ACM Comput. Surv", "year": "2022-02", "authors": "D Pessach; E Shmueli"}, {"ref_id": "b26", "title": "A review on fairness in machine learning", "journal": "ACM Computing Surveys (CSUR)", "year": "2022", "authors": "D Pessach; E Shmueli"}, {"ref_id": "b27", "title": "Face: feasible and actionable counterfactual explanations", "journal": "", "year": "2020", "authors": "R Poyiadzi; K Sokol; R Santos-Rodriguez; T De Bie; P Flach"}, {"ref_id": "b28", "title": "Recent advances in trustworthy explainable artificial intelligence: Status, challenges and perspectives", "journal": "IEEE Transactions on Artificial Intelligence", "year": "2021", "authors": "A Rawal; J Mccoy; D B Rawat; B Sadler; R Amant"}, {"ref_id": "b29", "title": "Model-agnostic interpretability of machine learning", "journal": "", "year": "2016", "authors": "M T Ribeiro; S Singh; C Guestrin"}, {"ref_id": "b30", "title": "Counterfactual explanations can be manipulated", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "D Slack; A Hilgard; H Lakkaraju; S Singh"}, {"ref_id": "b31", "title": "A few good counterfactuals: generating interpretable, plausible and diverse counterfactual explanations", "journal": "Springer", "year": "2022", "authors": "B Smyth; M T Keane"}, {"ref_id": "b32", "title": "The relationship between trust in ai and trustworthy machine learning technologies", "journal": "", "year": "2020", "authors": "E Toreini; M Aitken; K Coopamootoo; K Elliott; C G Zelaya; A Van Moorsel"}, {"ref_id": "b33", "title": "Counterfactual explanations for machine learning: A review", "journal": "", "year": "2020", "authors": "S Verma; J Dickerson; K Hines"}, {"ref_id": "b34", "title": "On the fairness of causal algorithmic recourse", "journal": "", "year": "2022", "authors": "J Von K\u00fcgelgen; A H Karimi; U Bhatt; I Valera; A Weller; B Sch\u00f6lkopf"}, {"ref_id": "b35", "title": "Counterfactual explanations without opening the black box: Automated decisions and the gdpr", "journal": "Harv. JL & Tech", "year": "2017", "authors": "S Wachter; B Mittelstadt; C Russell"}, {"ref_id": "b36", "title": "better\" counterfactuals, ones people can understand: Psychologically-plausible case-based counterfactuals using categorical features for explainable ai (xai)", "journal": "Springer", "year": "2022", "authors": "G Warren; B Smyth; M T Keane"}, {"ref_id": "b37", "title": "Lsac national longitudinal bar passage study", "journal": "", "year": "1998", "authors": "L F Wightman"}, {"ref_id": "b38", "title": "The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients", "journal": "Expert systems with applications", "year": "2009", "authors": "I C Yeh; C H Lien"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 .1Fig. 1. Normal vs. group fairness constrained counterfactuals: Group wise cost of closest counterfactuals for a decision tree classifier on the Communities & Crime data set.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Results for plausible counterfactuals -we report the median cost of counterfactuals (Definition 2), all number are rounded to two decimal points. summary of results for the closest counterfactuals are shown in Table1and the results for plausible counterfactuals are shown in Table2. We observe", "figure_data": "1-Fair Diff Diff-Fair"}], "formulas": [{"formula_id": "formula_0", "formula_text": "x cf \u2208 R d h( x cf ), y cf + C \u2022 \u03b8( x cf , x orig )(1)", "formula_coordinates": [3.0, 224.88, 262.64, 255.72, 17.88]}, {"formula_id": "formula_1", "formula_text": "\u03b8( x orig , CF( x orig , h))(2)", "formula_coordinates": [4.0, 263.92, 451.57, 216.67, 9.65]}, {"formula_id": "formula_2", "formula_text": "p xorig\u223cD1 \u03b8( x orig , CF( x orig , h)) \u2248 p xorig\u223cD2 \u03b8( x orig , CF( x orig , h))(3)", "formula_coordinates": [5.0, 161.4, 222.28, 319.19, 9.74]}, {"formula_id": "formula_3", "formula_text": "E xorig\u223cD1 \u03b8( x orig , CF( x orig , h)) \u2248 E xorig\u223cD2 \u03b8( x orig , CF( x orig , h))(4)", "formula_coordinates": [5.0, 170.51, 280.7, 310.09, 14.83]}, {"formula_id": "formula_4", "formula_text": "x cf \u2208 R d \u03b8( x orig , x cf ) + C 0 \u2022 (h( x cf ), y cf ) + C 1 \u2022 max (z -\u03b8( x orig , x cf ))", "formula_coordinates": [5.0, 162.31, 599.28, 290.74, 15.94]}, {"formula_id": "formula_5", "formula_text": "Initialization step 2:", "formula_coordinates": [6.0, 138.66, 167.16, 341.93, 18.85]}, {"formula_id": "formula_6", "formula_text": "Z1 = {\u03b8( xorig, CF( xorig, h)) \u2200 xorig \u2208 D1} 4: Z2 = {\u03b8( xorig, CF( xorig, h)) \u2200 xorig \u2208 D2} 5:", "formula_coordinates": [6.0, 138.66, 189.1, 180.05, 29.78]}, {"formula_id": "formula_7", "formula_text": "Z = \u2205 7: if 1 |Z 1 | z i \u2208Z 1 zi > 1 |Z 2 | z j \u2208Z 2 zj", "formula_coordinates": [6.0, 138.66, 221.98, 150.24, 22.24]}, {"formula_id": "formula_8", "formula_text": "x cf \u2208 R d \u03b8( xorig, x cf ) + C0 \u2022 (h( x cf ), y cf ) + C1 \u2022 max (z -\u03b8( xorig, x cf ))", "formula_coordinates": [6.0, 176.68, 354.62, 271.26, 16.9]}], "doi": "10.1145/2090236.2090255"}
