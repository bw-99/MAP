{
  "Improving Scientific Document Retrieval with Concept Coverage-based Query Set Generation": "SeongKu Kang Korea University Seoul, South Korea seongkukang@korea.ac.kr Bowen Jin University of Illinois at Urbana-Champaign Champaign, United States bowenj4@illinois.edu Wonbin Kweon Pohang University of Science and Technology Pohang, South Korea kwb4453@postech.ac.kr Yu Zhang Texas A&M University College Station, United States yuzhang@tamu.edu Dongha Lee Yonsei University Seoul, South Korea donalee@yonsei.ac.kr Jiawei Han University of Illinois at Urbana-Champaign Champaign, United States hanj@illinois.edu",
  "Abstract": "In specialized fields like the scientific domain, constructing largescale human-annotated datasets poses a significant challenge due to the need for domain expertise. Recent methods have employed large language models to generate synthetic queries, which serve as proxies for actual user queries. However, they lack control over the content generated, often resulting in incomplete coverage of academic concepts in documents. We introduce C oncept C overagebased Q uery set Gen eration ( CCQGen ) framework, designed to generate a set of queries with comprehensive coverage of the document's concepts. A key distinction of CCQGen is that it adaptively adjusts the generation process based on the previously generated queries. We identify concepts not sufficiently covered by previous queries, and leverage them as conditions for subsequent query generation. This approach guides each new query to complement the previous ones, aiding in a thorough understanding of the document. Extensive experiments demonstrate that CCQGen significantly enhances query quality and retrieval performance.",
  "CCS Concepts": "¬∑ Information systems ‚Üí Information retrieval ; Contentanalysis and feature selection ; Information retrieval query processing .",
  "Keywords": "Information retrieval; Query generation; Scientific document search",
  "ACMReference Format:": "SeongKu Kang, Bowen Jin, Wonbin Kweon, Yu Zhang, Dongha Lee, Jiawei Han, and Hwanjo Yu. 2025. Improving Scientific Document Retrieval with Concept Coverage-based Query Set Generation. In Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining (WSDM '25), March 10-14, 2025, Hannover, Germany. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3701551.3703544 ‚àó Corresponding author This work is licensed under a Creative Commons Attribution International 4.0 License. WSDM '25, March 10-14, 2025, Hannover, Germany ¬© 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1329-3/25/03 https://doi.org/10.1145/3701551.3703544 Hwanjo Yu ‚àó Pohang University of Science and Technology Pohang, South Korea hwanjoyu@postech.ac.kr ¬∑ Title: Improving Scientific Document Retrieval with Concept coverage-based Query Set Generation Figure 1: A conceptual comparison of (a) the existing approach for query set generation and (b) our concept coveragebased query set generation. Best viewed in color. (a) Query set generation using LLM (     ) document prompt uncovered concepts concept extractor document concepts covered by the queries? relevant queries LLM document prompt (b) Concept coverage-based query set generation (             ) concept-conditioning module previously generated queries a relevant query w.r.t. the concepts LLM +",
  "¬∑ Method: 1 Introduction": "Concept coverage-based query set generation (CCQGen) Scientific document retrieval is a fundamental task that accelerates scientific innovations and access to technical solutions [17]. Recently, pre-trained language models (PLMs) have largely enhanced various ad-hoc searches [12, 18]. PLM-based retrievers are initially pre-trained on massive textual corpora to develop language understanding. They are then fine-tuned using vast datasets of annotated query-document pairs, enabling the models to accurately assess the relevance between queries and documents. However, in specialized domains like scientific document retrieval, constructing large-scale annotated datasets is challenging due to the need for domain expertise [4, 14, 22]. While there are a few general domain datasets (e.g., web search [2, 19]), they often fail to generalize to specialized domains [4, 47]. This remains a major obstacle for applications. Recently, large language models (LLMs) [5, 40, 48, 51] have been actively utilized to generate synthetic data. Given a document and a prompt including an instruction such as ' generate five relevant queries to the document ' [9, 41], LLMs generate synthetic queries for each document (Figure 1a). The generated queries serve as proxies for actual user queries. Recent developments in prompting schemes have largely improved the quality of these queries. [4, 9] show that incorporating a few examples of actual query-document pairs in the prompt leads to the generation of queries with similar WSDM'25, March 10-14, 2025, Hannover, Germany SeongKu Kang et al. Table 1: An example of synthetic queries. The queries are generated sequentially from ùëû 1 to ùëû 3 . CCQGen is applied after generating ùëû 1 . Repeated keywords are denoted in red, while newly covered concepts are denoted in blue. Details of the generation process are illustrated in Figure 2.",
  "Document": "Automated music playlist generation is a specific form of music recommendation. Collaborative filtering methods can be used to ... However, the scarcity of thoroughly curated playlists and the bias towards popular songs ... we propose an alternative model based on a song-to-playlist classifier, ... while leveraging song features derived from audio, ... robust performance when recommending rare and out-of-set songs. ...",
  "Generated queries for the document": "ùëû 1 How can song-to-playlist classifiers enhance automated music playlist generation? ùëû 2 How can automated playlist creation be boosted through song-toplaylist classification and feature exploitation ? ( w/o any condition ) ùëû 3 How does song-to-playlist classifier differ from traditional collabora- tive filtering for music recommendation? ( w/o any condition ) ùëû 2 ‚Ä≤ What techniques can be used to overcome filter bubbles and recommend out-of-set songs ? ( w/ CCQGen ) ùëû 3 ‚Ä≤ How to leverage mel-spectrogram features to mitigate the coldstart problem in playlist recommendation? ( w/ CCQGen ) distributions (e.g., expression styles) to actual queries. The state-ofthe-art method [6] employs a pair-wise generation that instructs LLMs to generate relevant queries first and then relatively less relevant ones. These less relevant queries serve as natural 'hard negatives', further improving the efficacy of fine-tuning [6]. Though effective in generating plausible queries, the existing methods lack control over the content generated, which can lead to incomplete coverage of the academic concepts in a document. Academic concepts refer to fundamental ideas, theories, and methodologies that form the contents of scientific documents. A scientific document typically explores various concepts. For example, in Table 1, the document addresses the primary task of music playlist recommendation, along with the design of classification-based models, solutions for popularity biases and data scarcity, and the utilization of audio features. For a thorough understanding of the document, training queries should comprehensively cover these concepts. However, in the absence of control over the content generated, the queries often repeatedly cover similar aspects of the document, showing high redundancy. For example, in Table 1, the generated queries ( ùëû 2 , ùëû 3 ) repeat keywords such as 'automated playlist creation' and 'song-to-playlist classification' already present in the previous query ( ùëû 1 ). While these concepts are undoubtedly relevant to the document, such redundant queries cannot effectively bring new training signals. Furthermore, the queries exhibit a particularly higher lexical overlap with the document, compared to the actual user queries (¬ß4.2.1). We observe that the queries tend to repeat only a few terms extracted from the document. Given that users express the same concepts using various expressions in their queries, this limited term usage may not effectively simulate actual queries, reducing the efficacy of fine-tuning. As a naive solution, one might consider adding more instructions to the prompt, such as ' use various terms and reduce redundancy among the queries '. However, this still lacks systematic control over the generation and fails to bring consistent improvements (¬ß4.1.1); the improved term usage often appears in common expressions (e.g., advance, enhance, and reinforce), not necessarily enhancing concept coverage. Wepropose C oncept C overage-based Q uery set Gen eration (CCQGen) framework to meet two desiderata for training queries: (1) The queries should cover complementary aspects, enabling comprehensive coverage of the document's concepts, and (2) The queries should articulate the concepts in various related terms, rather than merely echoing a few phrases from the document. A key distinction of CCQGen is that it adaptively adjusts the generation process based on the concept coverage of previously generated queries (Figure 1b). We introduce a concept extractor to (1) identify the core concepts of each text and (2) uncover concept-related terms not explicitly mentioned in the document. Using this information, we discern the concepts not sufficiently covered by previous queries, and leverage them as conditions for the subsequent query generation. Table 1 shows that the queries generated with CCQGen ( ùëû 2 ‚Ä≤ , ùëû 3 ‚Ä≤ ) cover complementary concepts using more various related terms. Furthermore, we introduce new techniques to filter out low-quality queries and enhance retrieval accuracy using the obtained concept information. Our primary contributions are: ¬∑ We show that existing query generation methods often fail to comprehensively cover academic concepts in documents, leading to suboptimal training and retrieval performance. ¬∑ We propose CCQGen framework, which adaptively imposes conditions for subsequent generation based on the concept coverage. CCQGen can be flexibly integrated with existing prompting schemes to enhance concept coverage of generated queries. ¬∑ We validate the effectiveness of CCQGen by extensive experiments. CCQGen brings significant improvements in query quality and retrieval performance over existing prompting schemes.",
  "2 Preliminaries": "",
  "2.1 Fine-tuning Retrieval Model": "To perform retrieval on a new corpus, a PLM-based retriever is fine-tuned using a training set of annotated query-document pairs. For each query ùëû , the contrastive learning loss is typically applied:  where ùëë + and ùëë -denote the relevant and irrelevant documents. ùë† ùë°ùëíùë•ùë° ( ùëû, ùëë ) represents the similarity score between the query and a document, computed by the retriever. For effective fine-tuning, a substantial amount of training data is required. However, in specialized domains such as scientific document search, constructing vast human-annotated datasets is challenging due to the need for domain expertise, which remains an obstacle for applications [14, 22].",
  "2.2 Prompt-based Query Generation": "Several attempts have been made to generate synthetic queries using LLMs. Recent advancements have centered on advancing prompting schemes to enhance the quality of these queries. We summarize recent methods in terms of their prompting schemes. Few-shot examples. Several methods [4, 6, 7, 9, 13, 38] incorporate a few examples of relevant query-document pairs in the prompt. The prompt comprises the following components: ùëÉ = Improving Scientific Document Retrieval with Concept Coverage-based Query Set Generation WSDM'25, March 10-14, 2025, Hannover, Germany { ùëñùëõùë†ùë°, ( ùëë ùëñ , ùëû ùëñ ) ùëò ùëñ = 1 , ùëë ùë° } , where ùëñùëõùë†ùë° is the textual instruction 1 , ( ùëë ùëñ , ùëû ùëñ ) ùëò ùëñ = 1 denotes ùëò examples of the document and its relevant query, and ùëë ùë° is the new document we want to generate queries for. By providing actual examples of the desired outputs, this technique effectively generates queries with distributions similar to actual queries (e.g., expression styles and lengths) [9]. It is worth noting that this technique is also utilized in subsequent prompting schemes. Label-conditioning. Relevance label ùëô (e.g., relevant and irrelevant) has been utilized to enhance query generation [4, 7, 38]. The prompt comprises ùëÉ = { ùëñùëõùë†ùë°, ( ùëô ùëñ , ùëë ùëñ , ùëû ùëñ ) ùëò ùëñ = 1 , ( ùëô ùë° , ùëë ùë° )} , where ùëò labeldocument-query triplets are provided as examples. ùëô ùëñ represents the relevance label for the document ùëë ùëñ and its associated query ùëû ùëñ . To generate queries, the prompt takes the desired relevance label ùëô ùë° along with the document ùëë ùë° . This technique incorporates knowledge of different relevance, which aids in improving query quality and allows for generating both relevant and irrelevant queries [7]. Pair-wise generation. To further enhance the query quality, the state-of-the-art method [6] introduces a pair-wise generation of relevant and irrelevant queries. It instructs LLMs to first generate relevant queries and then generate relatively less relevant ones. The prompt comprises ùëÉ = { ùëñùëõùë†ùë°, ( ùëë ùëñ , ùëû ùëñ , ùëû -ùëñ ) ùëò ùëñ = 1 , ùëë ùë° } , where ùëû ùëñ and ùëû -ùëñ denote relevant and irrelevant query for ùëë ùëñ , respectively. The generation of irrelevant queries is conditioned on the previously generated relevant ones, allowing for generating thematically similar rather than completely unrelated queries. These queries can serve as natural 'hard negative' samples for training [6]. Remarks. Though effective in generating plausible queries, there remains substantial room for improvement. We observe that existing techniques often generate queries with limited coverage of the document's concepts. That is, the queries frequently cover similar aspects of the document, exhibiting high redundancy and failing to add new training signals. Furthermore, the queries show a high lexical overlap with the document, often repeating a few keywords from the document (¬ß4.2.1). Considering that the same concepts are expressed using diverse terms in actual user queries, merely repeating a few keywords may limit the efficacy of fine-tuning.",
  "3 Methodology": "We present C oncept C overage-based Q uery set Gen eration (CCQGen) framework, designed to meet two desiderata of training queries: (1) The concepts covered by queries should be complementary to each other, enabling a comprehensive coverage of the document's concepts. (2) The queries should articulate the document concepts in various related terms, rather than merely repeating phrases from the document. CCQGen consists of two major stages: ¬∑ Concept identification and enrichment (¬ß3.1): We first identify the core academic concepts of each document. Then, we enrich the identified concepts by assessing their importance and adding related concepts not explicitly mentioned in the document. This information serves as the basis for generating queries. ¬∑ Concept coverage-based query generation (¬ß3.2): Given the previously generated queries ùëÑ ùëö -1 ùëë = { ùëû 1 ùëë , ..., ùëû ùëö -1 ùëë } , we compare the concepts of the document ùëë with those covered by ùëÑ ùëö -1 ùëë 1 For example, ' Given a document, generate five search queries for which the document can be a perfect answer '. The instructions vary slightly across methods, typically in terms of word choice. In this work, we follow the instructions used in [6]. to identify uncovered concepts. These uncovered concepts are then leveraged as conditions for generating the subsequent query ùëû ùëö ùëë , allowing ùëû ùëö ùëë to cover complementary aspects of ùëÑ ùëö -1 ùëë . Moreover, we propose a new technique, concept similarity-enhanced retrieval (CSR), that leverages the obtained concept information for filtering out low-quality queries and for improving retrieval accuracy (¬ß3.2.3) . Figure 2 provides an overview of CCQGen.",
  "3.1 Concept Identification and Enrichment": "To measure concept coverage, we first identify the core academic concepts of each document. We represent the concepts using a combination of two different granularities: topic and phrase levels (Figure 2a). Topic level provides broader categorizations of research, such as 'collaborative filtering' or 'machine learning', while phrase level includes specific terms in the document, such as 'playlist continuation' or 'song-to-playlist classifier', complementarily revealing the document concepts. A tempting way to obtain these topics and phrases is to simply instruct LLMs to find them in each document. However, this approach has several limitations: the results may contain concepts not covered by the document, and there is always a potential risk of hallucination. As a solution, we propose a new approach that first constructs a candidate set, and then uses LLMs to pinpoint the most relevant ones from the given candidates, instead of directly generating them. By doing so, the output space is restricted to the predefined candidate space, greatly reducing the risk of hallucinations while effectively leveraging the language-understanding capability of LLMs. 3.1.1 Core topics identification . To identify the core topics of documents, we propose using an academic topic taxonomy [44]. In the scientific domain, academic taxonomies are widely used for categorizing studies in various institutions and can be easily obtained from the web. 2 A taxonomy refers to a hierarchical tree structure outlining academic topics (Figure 2a). Each node represents a topic, with child nodes corresponding to its sub-topics. Leveraging taxonomy allows for exploiting domain knowledge of topic hierarchy and reflecting researchers' tendency to classify studies. Candidate set construction. One challenge in finding candidate topics is that the taxonomy obtained from the web is often very large and contains many irrelevant topics.To effectively narrow down the candidates, we employ a top-down traversal technique that recursively visits the child nodes with the highest similarities at each level. For each document, we start from the root node and compute its similarity to each child node. We then visit child nodes with the highest similarities. 3 This process recurs until every path reaches leaf nodes, and all visited nodes are regarded as candidates. The document-topic similarity ùë† ( ùëë, ùëê ) can be defined in various ways. As a topic encompasses its subtopics, we collectively consider the subtopic information for each topic node. Let N ùëê denote the set of nodes in the sub-tree having ùëê as a root node. We compute the similarity as: ùë† ( ùëë, ùëê ) = 1 | N ùëê | Àù ùëó ‚ààN ùëê cos ( e ùëë , e ùëó ) , where e ùëë and e ùëó 2 E.g., IEEE Taxonomy (link), ACM Computing Classification System (link). 3 We visit multiple child nodes and create multiple paths, as a document usually covers various topics. For a node at level ùëô , we visit ùëô + 2 nodes to reflect the increasing number of nodes at deeper levels of the taxonomy. The root node is level 0. WSDM'25, March 10-14, 2025, Hannover, Germany SeongKu Kang et al. (b) Concept coverage-based query set generation playlist continuation (0.18), song-to-playlist classifier (0.14), out-of-set songs (0.1), data scarcity (0.12), playlist coherence (0.12), cold-start problem (0.08), popularity bias (0.08), filter bubble (0.06), audio features (0.05), listening logs (0.04), mel-spectrogram (0.03) ‚Ä¶ collaborative filtering (0.18), machine learning (0.12), recommender system (0.28), classification (0.16), data scarcity (0.14), audio feature analysis (0.07), item-based filtering (0.05) (a) Concept identification and enrichment Phrase set Target corpus ‚Ä¢ data scarcity ‚Ä¢ audio features ‚Ä¢ mel-spectogram ‚Ä¶ Academic topic taxonomy biology computer science machine learning data mining reinforcement learning recommender system * ‚Ä¶ computer vision topic model ‚Ä¶ rating prediction CTR prediction collaborative filtering 'Automated music playlist generation is a form of music recommendation. ‚Ä¶ However, the scarcity of curated playlists and the bias towards popular songs. ‚Ä¶ We propose a model based on a song-to-playlist classifier, ‚Ä¶ leveraging song features derived from audio ‚Ä¶ ' Topics Phrases red: enrichment results (importance) document prompt Previously generated queries LLM 'How to leverage mel-spectrogram features to mitigate the cold-start problem in playlist recommendation?' 'What techniques can be used to overcome filter bubble and recommend out-of-set songs?' 'How can song-to-playlist classifiers enhance automated music playlist generation?' playlist continuation (‚Üì), song-to-playlist classifier (‚Üì), out-of-set songs (‚Üì), data scarcity (‚Üë), playlist coherence (‚Üì), cold-start problem (‚Üë), popularity bias (‚Üì), filter bubble (‚Üì), audio features (‚Üë), listening logs (‚Üë), mel-spectrogram (‚Üë) ‚Ä¶ Concept comparison adjusting sampling probability based on query coverage Concept conditioning 'How can automated playlist creation be boosted through song-to-playlist classification and feature exploitation?' (w/o any condition) (concept- based) Concept coverage-based consistency filtering Figure 2: The overview of Concept Coverage-based Query set Generation (CCQGen) framework. Best viewed in color. denote representations from PLM for a document ùëë and the topic name of node ùëó , respectively. 4 Core topic selection. Weinstruct LLMs to select the most relevant topics from the candidates. An example of an input prompt is: 31 Core phrase selection. We instruct LLMs to select the most relevant phrases (up to ùëò ùëù phrases) from the candidates, using the same instruction format used for the topic selection. We set ùëò ùëù = 15. The core phrases are denoted by y ùëù ùëë ‚àà { 0 , 1 } | P | , where ùë¶ ùëù ùëëùëó = 1 indicates ùëó is a core phrase of ùëë , otherwise 0. You will receive a document along with a set of candidate topics. Your task is to select the topics that best align with the core theme of the document. Exclude topics that are too broad or less relevant. You may list up to [ ùëò ùë° ] topics, using only the topic names in the candidate set. Document : [Document], Candidate topic set : [Candidates] In this work, we set ùëò ùë° = 10. For each document ùëë , we obtain core topics as y ùë° ùëë ‚àà { 0 , 1 } | T | , where ùë¶ ùë° ùëëùëñ = 1 indicates ùëñ is a core topic of ùëë , otherwise 0. T denotes the topic set obtained from the taxonomy. 3.1.2 Core phrases identification . From each document, we identify core phrases used to describe its concepts. These phrases offer fine-grained details not captured at the topic level. We note that not all phrases in the document are equally important. Core phrases should describe concepts strongly relevant to the document but not frequently covered by other documents with similar topics. For example, among documents about 'recommender system' topic, the phrase 'user-item interaction' is very commonly used, and less likely to represent the most important concepts of the document. Candidate set construction. Given the phrase set P of the corpus 5 , we measure the distinctiveness of phrase ùëù in document ùëë . Inspired by recent phrase mining methods [20, 46], we compute the distinctiveness as: exp ( BM25 ( ùëù, ùëë ))/ ( 1 + Àù ùëë ‚Ä≤ ‚ààD ùëë exp ( BM25 ( ùëù, ùëë ‚Ä≤ ))) . This quantifies the relative relevance of ùëù to the document ùëë compared to other topically similar documents D ùëë . D ùëë is simply retrieved using Jaccard similarity of core topics y ùë° ùëë . We set |D ùëë | = 100. We select phrases with top-20% distinctiveness score as candidates. 4 We use BERT with mean pooling as the simplest choice. 5 The phrase set is obtained using an off-the-shelf phrase mining tool [42]. 3.1.3 Enriching concept information . We have identified core topics and phrases representing each document's concepts. We further enrich this information by (1) measuring their relative importance, and (2) incorporating strongly related concepts (i.e., topics and phrases) not explicitly revealed in the document. This enriched information serves as the basis for generating queries. Concept extractor. We employ a small model called a concept extractor . For a document ùëë , the model is trained to predict its core topics y ùë° ùëë and phrases y ùëù ùëë from the PLM representation e ùëë . We formulate this as a two-level classification task: topic and phrase levels. Topics and phrases represent concepts at different levels of granularity, and learning one task can aid the other by providing a complementary perspective. To exploit their complementarity, we employ a multi-task learning model with two heads [28]. Each head has a Softmax output layer, producing probabilities for topics ÀÜ y ùë° ùëë and phrases ÀÜ y ùëù ùëë , respectively. The cross-entropy loss is then applied for classification learning: -Àù | T | ùëñ = 1 ùë¶ ùë° ùëëùëñ log ÀÜ ùë¶ ùë° ùëëùëñ -Àù | P | ùëó = 1 ùë¶ ùëù ùëëùëó log ÀÜ ùë¶ ùëù ùëëùëó . Concept enrichment. Using the trained concept extractor, we compute ÀÜ y ùë° ùëë and ÀÜ y ùëù ùëë , which reveal their importance in describing the document's concepts. Also, we identify strongly related topics and phrases that are expressed differently or not explicitly mentioned, by incorporating those with the highest prediction probabilities. For example, in Figure 2, we identify phrases 'cold-start problem', 'filter bubble', and 'mel-spectrogram', which are strongly relevant to the document's concepts but not explicitly mentioned, along with their detailed importance. These phrases are used to aid in articulating the document's concepts in various related terms. Improving Scientific Document Retrieval with Concept Coverage-based Query Set Generation WSDM'25, March 10-14, 2025, Hannover, Germany We obtain ùëò ùë° ‚Ä≤ enriched topics and ùëò ùëù ‚Ä≤ enriched phrases for each document with their importance from ÀÜ y ùë° ùëë and ÀÜ y ùëù ùëë . We set the probabilities for the remaining topics and phrases as 0, and normalize the probabilities for selected topics and phrases, denoted by ¬Ø y ùë° ùëë and ¬Ø y ùëù ùëë .",
  "3.2 Concept Coverage-based Query Generation": "We present how we generate a set of queries that comprehensively cover the various concepts of a document. We first identify concepts insufficiently covered by the previously generated queries (¬ß3.2.1) and leverage them as conditions for subsequent generation (¬ß3.2.2). Then, a filtering step is applied to ensure the query quality (¬ß3.2.3). This process is repeated until a predefined number ( ùëÄ ) of queries per document is achieved. ùëÄ is empirically determined, considering available training resources such as GPU memory and training time. For the first query of each document, we impose no conditions, thus it is identical to the results obtained from existing methods. 3.2.1 Concept sampling based on query coverage . The enriched information ¬Ø y ùëë reveals the core concepts and their importance within the document. Our key idea is to generate queries that align with this distribution to ensure comprehensive coverage of the document's concepts. Let ùëÑ ùëö -1 ùëë = { ùëû 1 ùëë , ..., ùëû ùëö -1 ùëë } denote the previously generated queries. Using the concept extractor, which is trained to predict core concepts from the text, we identify the concepts covered by the queries, i.e., ¬Ø y ùë° ùëÑ and ¬Ø y ùëù ùëÑ . We use the concatenation of queries as input, denoted as ùëÑ . A high value in ¬Ø y ùëë coupled with a low value in ¬Ø y ùëÑ indicates that the existing queries do not sufficiently cover the corresponding concepts. Based on the concept coverage information, we identify concepts that need to be more emphasized in the subsequently generated query. We opt to leverage phrases as explicit conditions for generation, as topics reveal concepts at a broad level, making them less effective for explicit inclusion in the query. Note that topics are implicitly reflected in identifying and enriching core phrases. We define a probability distribution to identify less covered concepts as:  Weset ùúñ = 10 -3 as a minimal value to the core phrases for numerical stability. We sample ‚åä ùëò ùëù ‚Ä≤ ùëÄ ‚åã different phrases from Multinomial ( ùùÖ ) , where ùëÄ is the total number of queries per document. Note that ¬Ø y ùëù ùëÑ is dynamically adjusted during the construction of the query set. 3.2.2 Concept conditioning for query generation . The sampled phrases are leveraged as conditions for generating the next query ùëû ùëö ùëë . There have been active studies to control the generation of LLMs for various tasks. Recent methods [23, 57] have specified conditions for the desired outputs, such as sentiment, keywords, and an outline, directly in the prompts. Following these studies, we impose a condition by adding a simple textual instruction ùê∂ : ' Generate a relevant query based on the following keywords : [Sampled phrases]'. While more sophisticated instruction could be employed, we obtained satisfactory results with our choice. The final prompt is constructed as [ ùëÉ ; ùê∂ ] , where ùëÉ is an existing prompting scheme discussed in ¬ß2.2. This integration allows us to inherit the benefits of existing techniques (e.g., few-shot examples), while generating queries that comprehensively cover the document's concepts. For example, in Figure 2, ùê∂ includes phrases like 'cold-start problem' and 'audio features', which are not well covered by the previous queries. Based on this concept condition, we guide LLMs to generate a query that covers complementary aspects to the previous ones. It is important to note that ùê∂ adds an additional condition for ùëÉ ; the query is still about playlist recommendation, the main task of the document. 3.2.3 Concept coverage-based consistency filtering . After generating a query, we apply a filtering step to ensure its quality. A critical criterion for this process is round-trip consistency [1]; a query should be answerable by the document from which it was generated. Existing work [7, 9] employs a retriever to assess this consistency. Given a generated pair ( ùëû ùëë , ùëë ) , the retriever retrieves documents for ùëû ùëë . Then, ùëû ùëë is retained only if ùëë ranks within the topùëÅ results. The accuracy of the retriever is crucial in this step; a weak retriever may fail to filter out low-quality queries and also only retain queries that are too easy (e.g., high lexical overlap with the document), thereby limiting the effectiveness of training. We note that relying on the existing retriever is insufficient for measuring relevance. While it is effective at capturing similarities of surface texts, the retriever often fails to match underlying concepts. For example, in Figure 2, the generated query includes phrases 'coldstart problem' and 'mel-spectrogram', which are highly pertinent to 'data scarcity' and 'audio features' discussed in the document. Nevertheless, as these phrases are not directly used in the document, the retriever struggles to assess the relevance and ranks the document low. Consequently, the query is considered unreliable and removed during the filtering process. Concept similarity-enhanced retrieval (CSR). We propose a simple and effective technique to enhance retrieval by using concept information. For relevance prediction, we consider both textual similarity from the retriever ùë† ùë°ùëíùë•ùë° ( ùëû, ùëë ) , and concept similarity ùë† ùëêùëúùëõùëêùëíùëùùë° ( ùëû, ùëë ) . We measure concept similarity using core phrase distributions, i.e., ùë† ùëêùëúùëõùëêùëíùëùùë° ( ùëû, ùëë ) = ùë†ùëñùëö ( ¬Ø y ùëù ùëû , ¬Ø y ùëù ùëë ) , which reveals related concepts at a fine-grained level. 6 ùë†ùëñùëö (¬∑ , ¬∑) is the similarity function, for which use inner-product. The relevance score is defined as:  where ùëì (¬∑ , ¬∑) is a function that combines the two scores. We use a simple addition after rescaling them via z-score normalization. We denote this technique as Concept Similarity-enhanced Retrieval (CSR). For filtering process , we assess the round-trip consistency using CSR. By directly matching underlying concepts not apparent from the surface text, we can more accurately measure relevance and distinguish low-quality queries. Additionally, for search with test queries (i.e., after fine-tuning using the generated data), CSR can be used as a supplementary technique to further enhance retrieval. It helps to understand test queries, which contain highly limited contexts and jargon not included in the training queries, by associating them with pre-organized concept information. 6 Here, we compute the similarity for top-10% phrases (instead of ùëò ùëù ‚Ä≤ ) to consider concepts having a certain degree of relevance. We also tried using core topics. However, it proved less effective as topics reveal concepts only at a broad level. WSDM'25, March 10-14, 2025, Hannover, Germany SeongKu Kang et al.",
  "4 Experiments": "Datasets. We conduct a thorough review of the literature to find retrieval datasets in the scientific domain, specifically those where relevance has been assessed by skilled experts or annotators. We select two recently published datasets: CSFCube [31] and DORISMAE [49]. They offer test query collections annotated by human experts and LLMs, respectively, and embody two real-world search scenarios: query-by-example and human-written queries. For both datasets, we conduct retrieval from the entire corpus, including all candidate documents. CSFCube dataset consists of 50 test queries, with about 120 candidates per query drawn from approximately 800,000 papers in the S2ORC corpus [26]. DORIS-MAE dataset consists of 165,144 test queries, with candidates drawn similarly to CSFCube. We consider annotation scores above '2', which indicate documents are 'nearly identical or similar' (CSFCube) and 'directly answer all key components' (DORIS-MAE), as relevant. Note that training queries are not provided in both datasets. Academic topic taxonomy. Weutilize the field of study taxonomy from Microsoft Academic [44], which contains 431 , 416 nodes with a maximum depth of 4. After the concept identification step (¬ß3.1), we obtain 1 , 164 topics and 18 , 440 phrases for CSFCube, and 1 , 498 topics and 34 , 311 phrases for DORIS-MAE. Metrics. Following [14, 29], we employ Recall@ ùêæ (R@ ùêæ ) for a large retrieval size ( ùêæ ), and NDCG@ ùêæ (N@ ùêæ ) and MAP@ ùêæ (M@K) for a smaller ùêæ ( ‚â§ 20). Recall@ ùêæ measures the proportion of relevant documents in the top ùêæ results, while NDCG@ ùêæ and MAP@ ùêæ assign higher weights to relevant documents at higher ranks. Backbone retrievers. We employ two representative models: (1) Contriever-MS [12] is a widely used retriever fine-tuned using vast labeled data from general domains (i.e., MS MARCO). (2) SPECTER-v2 [45] is a PLM specifically developed for the scientific domain. It is trained using metadata (e.g., citation relations) of scientific papers. For both models, we use public checkpoints: facebook/contriever-msmarco and allenai/specter2_base . Baselines. We compare various query generation methods. For all LLM-based methods, we use gpt-3.5-turbo-0125 . Additionally, we explore the results with a smaller LLM ( Llama-3-8B ) in ¬ß4.2.3. For each document, we generate five relevant queries [47]. ¬∑ GenQ [47] employs a specialized query generation model, trained with massive document-query pairs from the general domains. We use T5-base, trained using approximately 500 , 000 pairs from MSMARCOdataset[32]: BeIR/query-gen-msmarco-t5-base-v1 . CCQGen can be flexibly integrated with existing LLM-based methods to enhance the concept coverage of the generated queries. We apply CCQGen to two recent approaches, discussed in ¬ß2.2. ¬∑ Promptgator [9] is a recent LLM-based query generation method that leverages few-shot examples within the prompt. ¬∑ Pair-wise generation [6] is the state-of-the-art method that generates relevant and irrelevant queries in a pair-wise manner. Additionally, we devise a new competitor that adds more instruction in the prompt to enhance the quality of queries: Promptga-tor_diverse is a variant of Promptgator, where we add the instruction ' use various terms and reduce redundancy among the queries '. Implementation details. We conduct all experiments using 4 NVIDIA RTX A5000 GPUs, 512 GB memory, and a single Intel Xeon Gold 6226R processor. For fine-tuning, we use top-50 BM25 hard negatives for each query [10]. We use 10% of training data as a validation set. The learning rate is set to 10 -6 for Contriever-MS and 10 -7 for SPECTER-v2, after searching among { 10 -7 , 10 -6 , ..., 10 -3 } . We set the batch size as 64 and the weight decay as 10 -4 . We report the average performance over five independent runs. For all methods, we generate five synthetic queries for each document ( ùëÄ = 5). For the few-shot examples in the prompt, we randomly select five annotated examples, which are then excluded in the evaluation process [9]. We follow the textual instruction used in [6]. For other baseline-specific setups, we adhere to the configurations described in the original papers. For the concept extractor, we employ a multi-gate mixture of expert architecture [28], designed for multi-task learning. We use three experts, each being a two-layer MLP. For the consistency filtering, we set ùëÅ = 5. We set the number of enriched topics and phrases to ùëò ùë° ‚Ä≤ = 15 and ùëò ùëù ‚Ä≤ = 20, respectively.",
  "4.1 Performance Comparison": "4.1.1 Effectiveness of CCQGen . Table 2 presents retrieval performance after fine-tuning with various query generation methods. CCQGen consistently outperforms all baselines, achieving significant improvements across various metrics with both backbone models. We observe that GenQ underperforms compared to LLM-based methods, showing the advantages of leveraging the text generation capability of LLMs. Also, existing methods often fail to improve the backbone model (i.e., no Fine-Tune), particularly Contriever-MS. As it is trained on labeled data from general domains, it already captures overall textual similarities well, making further improvements challenging. The consistent improvements by CCQGen support its efficacy in generating queries that effectively represent the scientific documents. Notably, Promptgator_diverse struggles to produce consistent improvements. We observe that it often generates redundant queries covering similar aspects, despite increased diversity in their expressions (further analysis provided in ¬ß4.2.1). This underscores the importance of proper control over generated content and supports the validity of our approach. Impact of amount of training data. In Figure 3, we further explore the retrieval performance by limiting the amount of training data, using Contriever-MS as the backbone model. The existing LLM-based generation method (i.e., Pair-wise gen.) shows limited performance under restricted data conditions and fails to fully benefit from an increasing volume of training data. This supports our claim that the generated queries are often redundant and do not effectively introduce new training signals. Conversely, CCQGen consistently delivers considerable improvements, even with a limited number of queries. CCQGen guides each new query to complement the previous ones, allowing for reducing redundancy and fully leveraging the limited number of queries. 4.1.2 Effectiveness of CCQGen with CSR . In ¬ß3.2.3, we introduce CSR, designed to enhance retrieval using concept information from CCQGen. This technique aligns with the ongoing research direction of enhancing retrieval by integrating additional context not directly revealed from the queries and document [14, 29, 52, 56]. We compare CSR with two recent methods: (1) GRF [29] generates relevant contexts by LLMs. For a fair comparison, we generate both topics and keywords, as used in CCQGen. (2) ToTER [14] uses the Improving Scientific Document Retrieval with Concept Coverage-based Query Set Generation WSDM'25, March 10-14, 2025, Hannover, Germany Table 2: Retrieval performance comparison after fine-tuning with the generated queries. Red color denotes results that fail to show improvements over no Fine-Tune. ‚Ä† and * indicate a statistically significant difference ( ùëù < 0 . 05 ) from no Fine-Tune (one-sample t-test) and the applied query generation method (paired t-test), respectively. Table 3: Retrieval performance comparison with various enhancement methods. * indicates a significant difference (paired t-test, ùëù < 0 . 05 ) from the best baseline (i.e., the combination of the best existing query generation and enhancement methods). Figure 3: Results with varying amounts of training data. x% denotes setups using a random x% of generated queries. CSFCube CSFCube 0.38 Pair-wise gen_ Pair-wise gen_ WI ccQGen 0.775 w/ CCQGen 0.36 8 0.750 0.34 0.725 0 2-Tun 0.32 0.700 109 1009 109 25% 100% DORIS-MAE DORIS-MAE 0.28 Pair-wise gen_ 0.64 Pair-wise gen WI ccQGen WI ccQGen 9 0.62 0.26 no Fine-Tune 0.60 0.58 109 25% 509 1009 109 25% 1009 topic distributions between queries and documents, with topics provided by the taxonomy. Contriever-MS is used as the backbone. Table 3 presents the retrieval performance with various enhancement methods. CSR significantly improves the retrieval performance. Notably, the combination of proposed concept-based query generation (CCQGen) and enhancement (CSR) methods achieves significant improvements over the best existing solutions (i.e., Pairwise gen. combined with ToTER). GRF often degrades performance because the LLM-generated contexts are not tailored to target documents; these contexts may be related but often not covered by documents in the corpus, potentially causing discrepancies in focused aspects. Lastly, ToTER only considers topic-level information, which may be insufficient for providing find-grained details necessary to distinguish between topically-similar documents.",
  "4.2 Study of CCQGen": "4.2.1 Analysis of generated queries . We analyze whether CCQGen indeed reduces redundancy among the queries and includes a variety of related terms. We introduce two criteria: (1) redundancy , measured as the average cosine similarity of term frequency vectors of queries. 7 A high redundancy indicates that queries tend to cover similar aspects of the document. (2) lexical overlap , measured as the average BM25 score between the queries and the document. A higher lexical overlap indicates that queries tend to reuse terms from the document. In Table 4, the generated queries show higher lexical overlap with the document compared to the actual user queries. This shows that the generated queries tend to use a limited range of terms already present in the document, whereas actual user queries include a broader variety of terms. With the 'diverse condition' (i.e., Promptgator_diverse), the generated queries exhibit reduced lexical overlap and redundancy. However, this does not consistently 7 We use CountVectorizer from the SciKit-Learn library. WSDM'25, March 10-14, 2025, Hannover, Germany SeongKu Kang et al. Table 4: Analysis of generated queries. (a) Statistics of queries generated by different methods. (b) Retriever performance (SPECTER-v2 on NDCG@10) after fine-tuning using the queries. The average lexical overlap of actual queries is 13 . 32 for CSFCube and 20 . 42 for DORIS-MAE. CSFCube",
  "DORIS-MAE": "Figure 4: Improvements by concept coverage-based filtering. lead to performance improvements. The improved term usage often appears in common expressions, not necessarily enhancing concept coverage. Conversely, CCQGen directly guides each new query to complement the previous ones. Also, CCQGen incorporate concept-related terms not explicitly mentioned in the document via enrichment step (¬ß3.1.3). This provides more systematic controls over the generation, leading to consistent improvements. 4.2.2 Effectiveness of concept coverage-based filtering . Figure 4 presents the improvements achieved through the filtering step, which aims to remove low-quality queries that the document does not answer (¬ß3.2.3). As shown in Table 3, CSR largely enhances retrieval accuracy by incorporating concept information. This enhanced accuracy helps to accurately measure round-trip consistency, effectively improving the effects of fine-tuning. 4.2.3 Results with a smaller LLM . In Table 5, we explore the effectiveness of the proposed approach using a smaller LLM, Llama3-8B, with Contriever-MS as the backbone model. Consistent with the trends observed in Table 2 and Table 3, the proposed techniques (CCQGen and CSR) consistently improve the existing method. We expect CCQGen to be effective with existing LLMs that possess a certain degree of capability. Since comparing different LLMs is not the focus of this work, we leave further investigation on more various LLMs and their comparison for future study.",
  "5 Related Work": "Weprovide a detailed survey of LLM-based query generation in ¬ß2.2. PLM-based retrieval models. The advancement of PLMs has led to significant progress in retrieval. Recent studies have introduced retrieval-targeted pre-training [11, 12], distillation from crossencoders [54], and advanced negative mining methods [36, 53] Table 5: Retrieval performance with Llama-3-8B. We report improvements over no Fine-Tune. ‚àó denotes ùëù < 0 . 05 from paired t-test with pair-wise generation. There is also an increasing emphasis on pre-training methods specifically designed for the scientific domain. In addition to pre-training on academic corpora [3], researchers have exploited metadata associated with scientific papers. [37] uses journal class, [8, 34] use citations, [30] uses co-citation contexts, and [25] utilizes venues, affiliations, and authors. [45, 55] devise multi-task learning of related tasks such as citation prediction and paper classification. Very recently, [14, 17] leverage corpus-structured knowledge (e.g., core topics and phrases) for academic concept matching. Synthetic query generation. Earlier studies [24, 27, 32, 33, 50] have employed dedicated query generation models, trained using massive document-query pairs from general domains. Recently, there has been a shift towards replacing these generation models with LLMs [4, 6, 9, 13, 38, 39], as discussed in ¬ß2.2. On the other hand, many recent studies have focused on developing query generation tailored to specific retrieval domains. [41] focuses on entity search for virtual assistants, [43] improves the diversity of queries for news article searches guided by a knowledge graph, [35] focuses on enhancing the retrievability of entities on online content (e.g., Podcast) platforms. However, a dedicated method for scientific document retrieval has not been studied well in the literature.",
  "6 Conclusion": "We propose CCQGen framework to generate a set of queries that comprehensively cover the document concepts. CCQGen identifies concepts not sufficiently covered by previous queries, and leverages them as conditions for subsequent query generation. This approach guides each new query to complement the previous ones, aiding in a comprehensive understanding of the document. Extensive experiments show that CCQGen significantly improves both query quality and retrieval performance, even with limited training data. Future work may explore its applicability across various domains. In particular, e-commerce [15, 16, 21] presents a promising opportunity, as users often express multi-faceted needs involving desired attributes, characteristics, or specific use cases. We expect CCQGen to better simulate user queries in such scenarios and leave further investigation for future work. Acknowledgements. This work was supported IITP grant funded by MSIT (No.2018-0-00584, No.2019-0-01906), NRF grant funded by the MSIT (No.RS-2023-00217286, No.2020R1A2B5B03097210). It was also in part by US DARPA INCAS Program No. HR0011-21C0165 and BRIES Program No. HR0011-24-3-0325, National Science Foundation IIS-19-56151, the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, and the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE) by NSF under Award No. 2118329. Improving Scientific Document Retrieval with Concept Coverage-based Query Set Generation WSDM'25, March 10-14, 2025, Hannover, Germany",
  "References": "[1] Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. 2019. Synthetic QA Corpora Generation with Roundtrip Consistency. In ACL . 6168-6173. [2] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 (2016). [3] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: Pretrained Language Model for Scientific Text. In EMNLP . 3615-3620. [4] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Unsupervised dataset generation for information retrieval. In SIGIR . 23872392. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In NeurIPS , Vol. 33. 1877-1901. [6] Aditi Chaudhary, Karthik Raman, and Michael Bendersky. 2024. It's All Relative!A Synthetic Query Generation Approach for Improving Zero-Shot Relevance Prediction. In Findings of NAACL . [7] Aditi Chaudhary, Karthik Raman, Krishna Srinivasan, Kazuma Hashimoto, Mike Bendersky, and Marc Najork. 2023. Exploring the viability of synthetic query generation for relevance prediction. In The SIGIR 2023 Workshop on eCommerce . [8] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S. Weld. 2020. SPECTER: Document-level Representation Learning using Citationinformed Transformers. In ACL . [9] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot dense retrieval from 8 examples. In ICLR . [10] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and St√©phane Clinchant. 2022. From distillation to hard negative sampling: Making sparse neural ir models more effective. In SIGIR . 2353-2359. [11] Luyu Gao and Jamie Callan. 2022. Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. In ACL . 2843-2853. [12] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118 (2021). [13] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and Rodrigo Nogueira. 2023. Inpars-v2: Large language models as efficient dataset generators for information retrieval. arXiv preprint arXiv:2301.01820 (2023). [14] SeongKu Kang, Shivam Agarwal, Bowen Jin, Dongha Lee, Hwanjo Yu, and Jiawei Han. 2024. Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy. In WWW . 1497-1508. [15] SeongKu Kang, Junyoung Hwang, Wonbin Kweon, and Hwanjo Yu. 2020. DERRD: A Knowledge Distillation Framework for Recommender System. In CIKM . [16] SeongKu Kang, Junyoung Hwang, Dongha Lee, and Hwanjo Yu. 2019. Semisupervised learning for cross-domain recommendation to cold-start users. In CIKM . [17] SeongKu Kang, Yunyi Zhang, Pengcheng Jiang, Dongha Lee, Jiawei Han, and Hwanjo Yu. 2024. Taxonomy-guided Semantic Indexing for Academic Paper Search. In EMNLP . 7169-7184. [18] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for OpenDomain Question Answering. In EMNLP . 6769-6781. [19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics 7 (2019), 453-466. [20] Dongha Lee, Jiaming Shen, SeongKu Kang, Susik Yoon, Jiawei Han, and Hwanjo Yu. 2022. Taxocom: Topic taxonomy completion with hierarchical discovery of novel topic clusters. In WWW . 2819-2829. [21] Youngjune Lee, Yeongjong Jeong, Keunchan Park, and SeongKu Kang. 2023. MvFS: Multi-view Feature Selection for Recommender System. In CIKM . [22] Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Yueyue Wu, Yiqun Liu, Chong Chen, and Qi Tian. 2023. SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval. In SIGIR . [23] Yunzhe Li, Qian Chen, Weixiang Yan, Wen Wang, Qinglin Zhang, and Hari Sundaram. 2024. Advancing Precise Outline-Conditioned Text Generation with Task Duality and Explicit Outline Control. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics . 2362-2377. [24] Davis Liang, Peng Xu, Siamak Shakeri, Cicero Nogueira dos Santos, Ramesh Nallapati, Zhiheng Huang, and Bing Xiang. 2020. Embedding-based zero-shot retrieval through query generation. arXiv preprint arXiv:2009.10270 (2020). [25] Xiao Liu, Da Yin, Jingnan Zheng, Xingjian Zhang, Peng Zhang, Hongxia Yang, Yuxiao Dong, and Jie Tang. 2022. OAG-BERT: Towards a Unified Backbone Language Model for Academic Knowledge Services. In KDD . 3418-3428. [26] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S Weld. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In ACL . 4969-4983. [27] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume . 1075-1088. [28] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts. In KDD . 1930-1939. [29] Iain Mackie, Shubham Chatterjee, and Jeffrey Dalton. 2023. Generative Relevance Feedback with Large Language Models. In SIGIR . 2026-2031. [30] Sheshera Mysore, Arman Cohan, and Tom Hope. 2022. Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity. In NAACL . 4453-4470. [31] Sheshera Mysore, Tim O'Gorman, Andrew McCallum, and Hamed Zamani. 2021. CSFCube-A Test Collection of Computer Science Research Articles for Faceted Query by Example. NeurIPS 2021 Track on Datasets and Benchmarks (2021). [32] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to docTTTTTquery. Online preprint 6, 2 (2019). [33] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document expansion by query prediction. arXiv preprint arXiv:1904.08375 (2019). [34] Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein, Bela Gipp, and Georg Rehm. 2022. Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings. In EMNLP . [35] Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, and Hugues Bouchard. 2023. Improving content retrievability in search with controllable query generation. In WWW . 3182-3192. [36] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. In NAACL-HLT . 5835-5847. [37] Anastasiia Razdaibiedina and Aleksandr Brechalov. 2023. MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents. In ACL . 530-539. [38] Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Sultan, and Christopher Potts. 2023. UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers. In EMNLP . 11265-11279. [39] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval with Zero-Shot Question Generation. In EMNLP . 3781-3797. [40] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207 (2021). [41] Sonal Sannigrahi, Thiago Fraga-Silva, Youssef Oualil, and Christophe Van Gysel. 2024. Synthetic query generation using large language models for virtual assistants. In SIGIR . 2837-2841. [42] Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, and Jiawei Han. 2018. Automated phrase mining from massive text corpora. IEEE Transactions on Knowledge and Data Engineering 30, 10 (2018), 1825-1837. [43] Xinyao Shen, Jiangjie Chen, Jiaze Chen, Chun Zeng, and Yanghua Xiao. 2022. Diversified query generation guided by knowledge graph. In WSDM . 897-907. [44] Zhihong Shen, Hao Ma, and Kuansan Wang. 2018. A Web-scale system for scientific knowledge exploration. In Proceedings of ACL 2018, System Demonstrations . 87-92. [45] Amanpreet Singh, Mike D'Arcy, Arman Cohan, Doug Downey, and Sergey Feldman. 2023. SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. In EMNLP . 5548-5566. [46] Fangbo Tao, Honglei Zhuang, Chi Wang Yu, Qi Wang, Taylor Cassidy, Lance M Kaplan, Clare R Voss, and Jiawei Han. 2016. Multi-Dimensional, Phrase-Based Summarization in Text Cubes. IEEE Data Eng. Bull. 39, 3 (2016), 74-84. [47] Nandan Thakur, Nils Reimers, Andreas R√ºckl√©, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In NeurIPS Datasets and Benchmarks Track . [48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [49] Jianyou Wang, Kaicheng Wang, Xiaoyue Wang, Prudhviraj Naidu, Leon Bergen, and Ramamohan Paturi. 2023. Scientific Document Retrieval using Multi-level Aspect-based Queries. In NeurIPS Datasets and Benchmarks Track . [50] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022. GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval. In NAACL . 2345-2360. [51] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models WSDM'25, March 10-14, 2025, Hannover, Germany SeongKu Kang et al. are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021). [52] HongChien Yu, Chenyan Xiong, and Jamie Callan. 2021. Improving query representations for dense retrieval with pseudo relevance feedback. In CIKM . 35923596. [53] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Optimizing dense retrieval model training with hard negatives. In SIGIR . 1503-1512. [54] Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2022. Adversarial Retriever-Ranker model for Dense Retrieval. In ICLR . [55] Yu Zhang, Hao Cheng, Zhihong Shen, Xiaodong Liu, Ye-Yi Wang, and Jianfeng Gao. 2023. Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding. In Findings of EMNLP . 12259-12275. [56] Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. 2020. BERTQE: Contextualized Query Expansion for Document Re-ranking. In Findings of EMNLP . 4718-4728. [57] Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, and Mrinmaya Sachan. 2023. Controlled text generation with natural language instructions. In ICML . 42602-42613.",
  "keywords_parsed": [
    "Information retrieval",
    "Query generation",
    "Scientific document search"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Synthetic QA Corpora Generation with Roundtrip Consistency"
    },
    {
      "ref_id": "b2",
      "title": "Ms marco: A human generated machine reading comprehension dataset"
    },
    {
      "ref_id": "b3",
      "title": "SciBERT: Pretrained Language Model for Scientific Text"
    },
    {
      "ref_id": "b4",
      "title": "Inpars: Unsupervised dataset generation for information retrieval"
    },
    {
      "ref_id": "b5",
      "title": "Language models are few-shot learners"
    },
    {
      "ref_id": "b6",
      "title": "It's All Relative!A Synthetic Query Generation Approach for Improving Zero-Shot Relevance Prediction"
    },
    {
      "ref_id": "b7",
      "title": "Exploring the viability of synthetic query generation for relevance prediction"
    },
    {
      "ref_id": "b8",
      "title": "SPECTER: Document-level Representation Learning using Citationinformed Transformers"
    },
    {
      "ref_id": "b9",
      "title": "Promptagator: Few-shot dense retrieval from 8 examples"
    },
    {
      "ref_id": "b10",
      "title": "From distillation to hard negative sampling: Making sparse neural ir models more effective"
    },
    {
      "ref_id": "b11",
      "title": "Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval"
    },
    {
      "ref_id": "b12",
      "title": "Unsupervised dense information retrieval with contrastive learning"
    },
    {
      "ref_id": "b13",
      "title": "Inpars-v2: Large language models as efficient dataset generators for information retrieval"
    },
    {
      "ref_id": "b14",
      "title": "Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy"
    },
    {
      "ref_id": "b15",
      "title": "DERRD: A Knowledge Distillation Framework for Recommender System"
    },
    {
      "ref_id": "b16",
      "title": "Semisupervised learning for cross-domain recommendation to cold-start users"
    },
    {
      "ref_id": "b17",
      "title": "Taxonomy-guided Semantic Indexing for Academic Paper Search"
    },
    {
      "ref_id": "b18",
      "title": "Dense Passage Retrieval for OpenDomain Question Answering"
    },
    {
      "ref_id": "b19",
      "title": "Natural questions: a benchmark for question answering research"
    },
    {
      "ref_id": "b20",
      "title": "Taxocom: Topic taxonomy completion with hierarchical discovery of novel topic clusters"
    },
    {
      "ref_id": "b21",
      "title": "MvFS: Multi-view Feature Selection for Recommender System"
    },
    {
      "ref_id": "b22",
      "title": "SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval"
    },
    {
      "ref_id": "b23",
      "title": "Advancing Precise Outline-Conditioned Text Generation with Task Duality and Explicit Outline Control"
    },
    {
      "ref_id": "b24",
      "title": "Embedding-based zero-shot retrieval through query generation"
    },
    {
      "ref_id": "b25",
      "title": "OAG-BERT: Towards a Unified Backbone Language Model for Academic Knowledge Services"
    },
    {
      "ref_id": "b26",
      "title": "S2ORC: The Semantic Scholar Open Research Corpus"
    },
    {
      "ref_id": "b27",
      "title": "Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation"
    },
    {
      "ref_id": "b28",
      "title": "Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts"
    },
    {
      "ref_id": "b29",
      "title": "Generative Relevance Feedback with Large Language Models"
    },
    {
      "ref_id": "b30",
      "title": "Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity"
    },
    {
      "ref_id": "b31",
      "title": "CSFCube-A Test Collection of Computer Science Research Articles for Faceted Query by Example"
    },
    {
      "ref_id": "b32",
      "title": "From doc2query to docTTTTTquery"
    },
    {
      "ref_id": "b33",
      "title": "Document expansion by query prediction"
    },
    {
      "ref_id": "b34",
      "title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings"
    },
    {
      "ref_id": "b35",
      "title": "Improving content retrievability in search with controllable query generation"
    },
    {
      "ref_id": "b36",
      "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering"
    },
    {
      "ref_id": "b37",
      "title": "MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents"
    },
    {
      "ref_id": "b38",
      "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers"
    },
    {
      "ref_id": "b39",
      "title": "Improving Passage Retrieval with Zero-Shot Question Generation"
    },
    {
      "ref_id": "b40",
      "title": "Multitask prompted training enables zero-shot task generalization"
    },
    {
      "ref_id": "b41",
      "title": "Synthetic query generation using large language models for virtual assistants"
    },
    {
      "ref_id": "b42",
      "title": "Automated phrase mining from massive text corpora"
    },
    {
      "ref_id": "b43",
      "title": "Diversified query generation guided by knowledge graph"
    },
    {
      "ref_id": "b44",
      "title": "A Web-scale system for scientific knowledge exploration"
    },
    {
      "ref_id": "b45",
      "title": "SciRepEval: A Multi-Format Benchmark for Scientific Document Representations"
    },
    {
      "ref_id": "b46",
      "title": "Multi-Dimensional, Phrase-Based Summarization in Text Cubes"
    },
    {
      "ref_id": "b47",
      "title": "BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models"
    },
    {
      "ref_id": "b48",
      "title": "Llama: Open and efficient foundation language models"
    },
    {
      "ref_id": "b49",
      "title": "Scientific Document Retrieval using Multi-level Aspect-based Queries"
    },
    {
      "ref_id": "b50",
      "title": "GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval"
    },
    {
      "ref_id": "b51",
      "title": "Finetuned language models are zero-shot learners"
    },
    {
      "ref_id": "b52",
      "title": "Improving query representations for dense retrieval with pseudo relevance feedback"
    },
    {
      "ref_id": "b53",
      "title": "Optimizing dense retrieval model training with hard negatives"
    },
    {
      "ref_id": "b54",
      "title": "Adversarial Retriever-Ranker model for Dense Retrieval"
    },
    {
      "ref_id": "b55",
      "title": "Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding"
    },
    {
      "ref_id": "b56",
      "title": "BERTQE: Contextualized Query Expansion for Document Re-ranking"
    },
    {
      "ref_id": "b57",
      "title": "Controlled text generation with natural language instructions"
    }
  ]
}