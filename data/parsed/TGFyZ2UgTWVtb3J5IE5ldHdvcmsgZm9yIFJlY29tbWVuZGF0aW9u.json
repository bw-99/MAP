{"Large Memory Network for Recommendation": "Hui Lu luhui.xx@bytedance.com ByteDance Hangzhou, China", "Zheng Chai \u2020": "chaizheng.cz@bytedance.com ByteDance Hangzhou, China Yuchao Zheng zhengyuchao.yc@bytedance.com ByteDance Beijing, China Zhe Chen chenzhe.john@bytedance.com ByteDance Beijing, China Deping Xie deping.xie@bytedance.com ByteDance San Jose, USA Xun Zhou zhouxun@bytedance.com ByteDance Beijing, China Peng Xu xupeng@bytedance.com ByteDance San Jose, USA Di Wu di.wu@bytedance.com ByteDance Beijing, China", "Abstract": "Modeling user behavior sequences in recommender systems is essential for understanding user preferences over time, enabling personalized and accurate recommendations for improving user retention and enhancing business values. Despite its significance, there are two challenges for current sequential modeling approaches. From the spatial dimension, it is difficult to mutually perceive similar users' interests for a generalized intention understanding; from the temporal dimension, current methods are generally prone to forgetting long-term interests due to the fixed-length input sequence. In this paper, we present Large Memory Network (LMN), providing a novel idea by compressing and storing user history behavior information in a large-scale memory block. With the elaborated online deployment strategy, the memory block can be easily scaled up to million-scale in the industry. Extensive offline comparison experiments, memory scaling up experiments, and online A/B test on Douyin E-Commerce Search (ECS) are performed, validating the superior performance of LMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of users each day.", "CCS Concepts": "", "\u00b7 Information systems \u2192 Recommender systems .": "", "Keywords": "Large Recommender Models, Memory Learning, Personalized Recommender System, Sequential Modeling \u2020 Corresponding Author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWWCompanion '25, Sydney, NSW, Australia \u00a9 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1331-6/2025/04 https://doi.org/10.1145/3701716.3715514", "ACMReference Format:": "Hui Lu, Zheng Chai \u2020 , Yuchao Zheng, Zhe Chen, Deping Xie, Peng Xu, Xun Zhou, and Di Wu. 2025. Large Memory Network for Recommendation. In Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3701716.3715514", "1 Introduction": "User behavior sequences are a crucial means of depicting user interests and preferences [3, 5]. Currently, many outstanding sequential modeling methods have been approached, which can be divided into two branches. The first one is to adopt a target-attention mechanism for short sequence modeling, for example, DIN [19], DIEN [18], CAN [1]. As such methods are mostly developed for short sequences, the other branch develops search-based mechanism to perceive longer sequences in the model, like SIM [13], TWIN [4], etc. However, much efforts are generally needed to deploy such complex two-stage models for online serving, and the behavior discarding further results in incomplete interest estimation. Therefore, another type of approach that is emerging as a hotspot is to directly model long sequence in the recommendation model [2, 10, 15, 16], which inevitably increases the computational cost of model training and inference, introducing a substantial online overhead for industrial recommenders. Currently, scaling laws have guided the language model design by continuously scaling up the model parameters, showing promising performance gains. The discovery has also spurred research in other fields towards increasing model complexity, for example, increasing the dense network parameters [17] and sparse embedding parameters [7] in industrial recommenders. However, in practice, simply scaling model parameters can directly lead to decreased training and inference efficiency, and the computational FLOPs increment further limits the model scalability. To mitigate the computational and GPU memory overhead, the memory network structure can amplify model parameters through memory mechanism to enhance model capacity, which has been applied in the field of natural language modeling. The Key-Value Memory Network [11] proposed by Miller et al. abstracts the basic structure of memory network mechanism, which mainly consists of query, keys, and values, WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Hui Lu et al. representing an early form of attention mechanism. However, with the increase of memory space, both the computational complexity and memory access overhead of such basic key-value memory structure will also increase. Under hardware constraints, the memory space cannot be infinitely enlarged, limiting the expressive power and scaling laws of memory. To this end, Lample et al. [9] developed large memory layers by decomposing keys to decrease the complexity. Despite the popularity and effectiveness, such memory mechanism is rarely used in the field of recommendation. Pi et al. proposed a multi-channel user interest memory network (MIMN) by introducing neural Turing machine and memory induction unit [12], while the complex serial modeling structure with GRU made the model training inefficient, and the limited memory space further restricts its effectiveness. To solve the above problems, in this paper, we present Large Memory Network (LMN), a lightweight yet effective sequential modeling framework for user intention understanding. Generally, the overall contributions can be summarized as follows: \u00b7 We present a novel lightweight large memory network, by which the user sequences can be effectively compressed into memory parameters and achieve large-scale expansion of model parameter capacity with little computational cost. \u00b7 The designed LMN builds a large global-shared table for different users, enabling spatial perception among users and temporal memory of long-term user interests. Both a product quantization-based memory decomposition and an industrial online framework are provided for efficient deployment. \u00b7 Extensive comparison and scaling up experiments, and online A/B experiment illustrate the superiority. LMN has been fully deployed in Douyin E-Commerce, serving millions of users.", "2 Methodology": "", "2.1 Preliminaries": "2.1.1 Problem Formulation. This paper focuses on the click-through rate (CTR) prediction tasks in recommendation, and the overall learning objective can be formulated as follows:  where \ud835\udc96 , \ud835\udc8a , \ud835\udc84 denote user-side, item-side, and user-item cross feature embeddings, respectively, and \ud835\udc94 represents the user sequence representation. \ud835\udc53 denotes the recommendation model for predicting the CTR \u02c6 \ud835\udc66 .", "2.2 The Proposed LMN": "The structure of the proposed LMN is illustrated in Figure 1. In general, LMN is an end-to-end storage unit that compresses and memorizes historical behavior sequences and user information. 2.2.1 Overall Framework. The learning objective of LMN is to build a large-scale memory block and find the top\ud835\udc3e most relevant memory slots in the block for assisting user intention modeling. It is noted that all users share the same memory block, allowing user behavior information to generalize in a much larger parameter space (memory), which establishes perception among users spatially and memorizes long-term user interest temporally. As shown Sequence Item User Aware Block User Info Query MLP Memory search search search output Optional Mask Smooth L1 Loss headn head2 head1 \u2026\u2026 Optional Mask Stop Gradient score, index Query-Col input Query-Row Memory Row Keys top K & gather Memory Values weight pooling output Query MLP & Norm Memory Col Keys Memory Injection Memory Extraction Figure 1: Overall framework of the proposed LMN. There are two learnable components in the memory block including memory keys and memory values. The LMN uses a two-axis activation for locating the top \ud835\udc3e relevant values for a given specific query. Besides, the Smooth L1 loss is performed to inject historical interaction with corresponding user information into the memory value slots. The optional mask is used as some of the sequence items are padded to a fixed length with zero embeddings in practice. in Figure 1, the overall framework of the LMN has two stages, i.e., memory extraction and memory injection, which will be introduced in detail. 2.2.2 Memory Extraction. Generally, denote the memory key as \ud835\udc40 \ud835\udc3e \u2208 R \ud835\udc5b \u00d7 \ud835\udc51 , and the memory value as \ud835\udc40 \ud835\udc49 \u2208 R \ud835\udc5b \u00d7 \ud835\udc51 , where \ud835\udc5b and \ud835\udc51 represent the number and dimension of the memory slots. Given an interacted item \ud835\udc65 \u2208 R 1 \u00d7 \ud835\udc51 in user sequence as the memory query, the calculation process of memory activation can be written as:  where \ud835\udc40 \ud835\udc42 denotes the output of the memory block. As shown in Eq. 2, the computational complexity of the similarity score, i.e., \ud835\udc60 = \ud835\udc65 \u00b7 \ud835\udc40 T \ud835\udc3e , is O ( \ud835\udc5b\ud835\udc51 ) which is linearly proportional to the scale of the number of memory slots \ud835\udc5b . To further decrease the computational cost, the memory activation mechanism is designed based on product quantization [8, 9]. Specifically, we quantize the original \ud835\udc40 \ud835\udc3e \u2208 R \ud835\udc5b \u00d7 \ud835\udc51 as:  where we decompose the original \ud835\udc40 \ud835\udc3e with \ud835\udc5b memory slots into two subgroups, i.e., \ud835\udc40 \ud835\udc3e -\ud835\udc5f\ud835\udc5c\ud835\udc64 \u2208 R \u221a \ud835\udc5b \u00d7 \ud835\udc51 and \ud835\udc40 \ud835\udc3e -\ud835\udc50\ud835\udc5c\ud835\udc59 \u2208 R \u221a \ud835\udc5b \u00d7 \ud835\udc51 , corresponding to the row index keys and column index keys, respectively. \" \u00d7 \" here denotes a cross operation. With both the row and col indices, the activated memory values in \ud835\udc5b memory value slots can be Large Memory Network for Recommendation WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia located, as illustrated in Fig. 1. As each subgroup includes \u221a \ud835\udc5b memory slots, the computational cost of the attention score calculation in Eq. 2 can be decreased from O ( \ud835\udc5b\ud835\udc51 ) to O ( \u221a \ud835\udc5b\ud835\udc51 ) effectively. User-Aware Block. Correspondingly, to lookup a memory value for a given interacted-item \ud835\udc65 , we first obtain the corresponding queries \ud835\udc40 \ud835\udc44 -\ud835\udc5f\ud835\udc5c\ud835\udc64 and \ud835\udc40 \ud835\udc44 -\ud835\udc50\ud835\udc5c\ud835\udc59 for both keys in Eq. 3. However, due to the highly personalized characteristics of users in recommenders, the same representation of an item \ud835\udc65 exhibits varying semantics across various users. Thus, relying only on the similarity of item embedding as a metric is insufficient. To this end, we present useraware block, which introduces user-aware information into the queries for better generalization performance:  where \ud835\udc96 is the user-side features like the user id or demographic features, and the Merge operation means the add or concatenation operation. In this paper, concatenation is used. Then, the two queries can be generated corresponding to the row keys and column keys:   Based on the \ud835\udc40 \ud835\udc44 -\ud835\udc5f\ud835\udc5c\ud835\udc64 \u2208 R 1 \u00d7 \ud835\udc51 and \ud835\udc40 \ud835\udc44 -\ud835\udc50\ud835\udc5c\ud835\udc59 \u2208 R 1 \u00d7 \ud835\udc51 , we obtain the memory activation scores at the row axis, column axis, and the overall:    where both \ud835\udc46 \ud835\udc5f\ud835\udc5c\ud835\udc64 and \ud835\udc46 \ud835\udc50\ud835\udc5c\ud835\udc59 \u2208 R 1 \u00d7 \u221a \ud835\udc5b , and the broadcast operation means that \ud835\udc46 \ud835\udc5f\ud835\udc5c\ud835\udc64 and \ud835\udc46 \ud835\udc50\ud835\udc5c\ud835\udc59 are first reshaped as \ud835\udc46 \ud835\udc5f\ud835\udc5c\ud835\udc64 \u2208 R 1 \u00d7 \u221a \ud835\udc5b and \ud835\udc46 \ud835\udc50\ud835\udc5c\ud835\udc59 \u2208 R \u221a \ud835\udc5b \u00d7 1 , then the two scores are added with the broadcast mechanism 1 to obtain a final score at \ud835\udc46 \u2208 R \u221a \ud835\udc5b \u00d7 \u221a \ud835\udc5b , which is finally reshaped to R \ud835\udc5b . With the memory activation score \ud835\udc46 , we select the top \ud835\udc3e highest scores to lookup the values and gather them with a weighted-sum mechanism:  in which \ud835\udc46 \ud835\udc47\ud835\udc5c\ud835\udc5d\ud835\udc3e denotes the highest \ud835\udc3e activation scores in \ud835\udc46 and \ud835\udc40 \ud835\udc49 -\ud835\udc47\ud835\udc5c\ud835\udc5d\ud835\udc3e represents the top \ud835\udc3e score indexed-values. To further increase the model capacity and improve performance, both the query and keys can be obtained via a multi-head mechanism. 2.2.3 Memory Injection. The stage of injection aims to memorize compressed information into the value table. Traditional memory structures might use a write-back operation to inject information [12]. However, under the query-key activation framework, a more 1 https://www.tensorflow.org/guide/tensor#broadcasting reasonable solution is to use the gradient-descent approach to update both the key and value memory tables. Furthermore, to memorize the information of the user-item pairs in the designed memory slots, a Smooth-L1 [6] based memory loss is adopted to inject useritem pair information into the memory slots:  2.2.4 Model training and deployment. The LMN is a lightweight module while with the potential to significantly increase the model parameter capacity. It can be used as a plug-and-play module for current CTR prediction models, and the overall loss can be obtained as follows with a balanced parameter \ud835\udefc :  where \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc36\ud835\udc47\ud835\udc45 is the loss value of the original CTR task. Figure 2: Online deployment framework of memory parameter server (MPS) for the proposed LMN at ByteDance. All2All Update via Grads Lookup OP Indices Mem Search All2All Lookup via Indices Distributed HBM Storage on top of NVLink HBM Memory Matrix Shard GPU HBM Memory Matrix Shard GPU HBM Memory Matrix Shard GPU HBM Memory Matrix Shard GPU RecSys Model To deploy the LMN in online scenarios, it is noted that among the two modules including the memory keys and memory values, the latter one has a much larger capacity which should be carefully designed for deployment in industrial applications. Specifically, we introduce memory parameter server (MPS), which adopts a GPUsharding strategy for distributed storage of the memory values on high bandwidth memory (HBM) of multi-GPUs. The detailed framework of MPS is illustrated in Fig. 2. In the model training process, the recommendation model launches a request for lookup of corresponding values according to the memory activation score \ud835\udc46 . Then, the all2all lookup is performed for distributed HBM storage on top of GPU NVLink 2 . Then, the memory values are updated with the all2all gradient updates. For serving, only the All2All lookup is reserved for memory search. 2 https://www.nvidia.com/en-us/data-center/nvlink/ WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia Hui Lu et al.", "3 Experiment": "", "3.1 Evaluation Setting": "Dataset . To investigate the performance of LMN, we conduct experiment at Douyin E-Commerce Search (ECS), which serves as the most important scenario to meet the e-commerce demands of users in search scenarios at ByteDance and attracts millions of page views of online traffic each day. We collect and sample online traffic logs from Oct. 1st, 2024 to Oct. 28th, 2024, with 4 weeks and 3.6 billion samples, and the former 3 weeks are used for training and the last week is for evaluation. Comparison Methods and Metrics . We select the following SOTA methods for evaluating the effectiveness of the proposed LMN. The DNN baseline is a typical DNN-based structure for CTR prediction. The basic sequential modeling methods include Pooling and DIN [19] conducted on user sequence with click behaviors whose length and embedding size are 50 and 32, respectively. Based on DIN, we further introduce SIM [13], MIMN [12], and the proposed LMN . The number of memory keys is set to 300. Both the AUC and LogLoss metrics are used to evaluate the ranking model performance. Further, we introduce relative improvement (Imp.) to measure the relative AUC gain [14].", "3.2 Experimental Results": "Comparison Results . The performance of different methods is summarized in Table 1. There are the following observations. First, it is observed that sequence modeling is of great significance in recommendation, and the basic Pooling method achieves 1.32% AUC improvement compared with the DNN baseline. Then, the personalized weights in DIN are more effective than Pooling, and it is observed that modeling a much longer sequence using SIM achieves better performance than DIN (0.7348 vs. 0.7327). Finally, compared to MIMN, the proposed LMN shows prominent effectiveness (4.68% and 1.46% improvements in AUC and LogLoss). We believe that the reason is due to the effectiveness of the user-aware memory learning mechanism that enables both spatial perception and temporal memorization. Performance of Scaling Up Memories . Further, to validate the performance when scaling up the memory, we vary the number of memory keys from [50, 100, 200, 300, 500], and the results are illustrated in Table 2. From the table, it can be observed that both performance metrics consistently improve with the number of keys scales up.", "3.3 Online A/B Experiment": "We perform online A/B test from 30th Aug. to 5th Sep. 2024, with 20% online traffic on Douyin E-Commerce Search to validate the effectiveness, which hits more than 160 million users. The results are illustrated in Table 3. It can be observed that after deploying the proposed LMN, the most influential commercial metrics, i.e., order/user and order/search, are raised by 0.87% and 0.72%, respectively, with a substantial significance of \ud835\udc5d =0. Besides, after deploying the LMN, there is only a slight increment of online serving latency at 0.38%, showing the efficiency of the deployment framework. Table 1: Comparison results of different methods on Douyin E-Commerce Search Dataset. Boldface denotes the best results with a confidence level at \ud835\udc5d < 0 . 05 . Table 2: Model performance with the number of memory keys scaling up. Table 3: Online A/B test result of the proposed LMN on Douyin E-Commerce Search. The boldface denotes that the improvement is significant with \ud835\udc5d = 0.", "4 Conclusion": "In this paper, we present Large Memory Network (LMN), a novel memory-enhanced user sequential modeling and intention understanding framework for recommendation. It compresses and memorizes user interest through a large-scale memory block. With such design, the memory can 1) spatially perceive different users' sequences for better generalization, and 2) temporally memorize user long-term interest through a user-aware block. Besides, to further reduce computational cost and deployment overhead, both a product quantization-based memory decomposition and an industrial online memory parameter server framework are devised for industrial online deployment. Currently, LMN has been fully deployed at ByteDance, which serves the major online traffic of E-Commerce Search service at Douyin. It is noted that instead of memorizing the user's historical behavioral items in the current work, it is a promising direction to memorize any important information within the industrial recommender models, for example, memorizing the output of the feature-cross modules, or the output of the high-level compressor MLPs, to achieve the goal of perceiving similar user's instances and memorizing the same user's instance and further improve the effectiveness of the recommendation model. Large Memory Network for Recommendation", "References": "[1] Weijie Bian, Kailun Wu, Lejian Ren, Qi Pi, Yujing Zhang, Can Xiao, Xiang-Rong Sheng, Yong-Nan Zhu, Zhangming Chan, Na Mou, et al. 2022. CAN: feature co-action network for click-through rate prediction. In Proceedings of the fifteenth ACM international conference on web search and data mining . 57-65. [2] Yue Cao, Xiaojiang Zhou, Jiaqi Feng, Peihao Huang, Yao Xiao, Dayao Chen, and Sheng Chen. 2022. Sampling is all you need on modeling long-term user behaviors for CTR prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 2974-2983. [3] Zheng Chai, Zhihong Chen, Chenliang Li, Rong Xiao, Houyi Li, Jiawei Wu, Jingxu Chen, and Haihong Tang. 2022. User-aware multi-interest learning for candidate matching in recommenders. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1326-1335. [4] Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, et al. 2023. TWIN: TWo-stage interest network for lifelong user behavior modeling in CTR prediction at kuaishou. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 3785-3794. [5] Gabriel de Souza Pereira Moreira, Sara Rabhi, Jeong Min Lee, Ronay Ak, and Even Oldridge. 2021. Transformers4rec: Bridging the gap between nlp and sequential/session-based recommendation. In Proceedings of the 15th ACM conference on recommender systems . 143-153. [6] R Girshick. 2015. Fast R-CNN. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) . [7] Xingzhuo Guo, Junwei Pan, Ximei Wang, Baixu Chen, Jie Jiang, and Mingsheng Long. 2024. On the Embedding Collapse when Scaling up Recommendation Models. In Forty-first International Conference on Machine Learning . [8] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. IEEE Transactions on Pattern Analysis and Machine Intelligence 33, 1 (2010), 117-128. [9] Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. 2019. Large memory layers with product keys. Advances in Neural Information Processing Systems 32 (2019). [10] Qi Liu, Xuyang Hou, Haoran Jin, Zhe Wang, Defu Lian, Tan Qu, Jia Cheng, Jun Lei, et al. 2023. Deep Group Interest Modeling of Full Lifelong User Behaviors WWWCompanion '25, April 28-May 2, 2025, Sydney, NSW, Australia for CTR Prediction. arXiv preprint arXiv:2311.10764 (2023). [11] Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-Value Memory Networks for Directly Reading Documents. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing . [12] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on long sequential user behavior modeling for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2671-2679. [13] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2685-2692. [14] Ling Yan, Wu-Jun Li, Gui-Rong Xue, and Dingyi Han. 2014. Coupled group lasso for web-scale ctr prediction in display advertising. In International conference on machine learning . PMLR, 802-810. [15] Wenhui Yu, Chao Feng, Yanze Zhang, Lantao Hu, Peng Jiang, and Han Li. 2024. IFA: Interaction Fidelity Attention for Entire Lifelong Behaviour Sequence Modeling. arXiv preprint arXiv:2406.09742 (2024). [16] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Jiayuan He, et al. [n. d.]. Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations. In International conference on machine learning . [17] Buyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Shen Li, Yanli Zhao, Yuchen Hao, Yantao Yao, Ellie Dingqiao Wen, et al. [n. d.]. Wukong: Towards a Scaling Law for Large-Scale Recommendation. In Forty-first International Conference on Machine Learning . [18] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 5941-5948. [19] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068."}
