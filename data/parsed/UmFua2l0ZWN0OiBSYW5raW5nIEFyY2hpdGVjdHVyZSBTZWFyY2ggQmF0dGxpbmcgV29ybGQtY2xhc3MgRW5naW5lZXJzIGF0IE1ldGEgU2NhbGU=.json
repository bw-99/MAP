{
  "Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale": "Wei Wen ‚àó , Kuang-Hung Liu ‚àó , Igor Fedorov, Xin Zhang, Hang Yin, Weiwei Chu Kaveh Hassani, Mengying Sun, Jiang Liu, Xu Wang, Lin Jiang, Yuxin Chen Buyun Zhang, Xi Liu, Dehua Cheng, Zhengxing Chen, Guang Zhao Fangqiu Han, Jiyan Yang, Yuchen Hao, Liang Xiong, Wen-Yen Chen {wewen,khliu,ifedorov,xinzhang5,hyin5,wchu,kavehhassani,mengyingsun,jiangliu,wangxu85,amylinjiang,yuxinc}@meta.com {buyunz,xliu1,dehuacheng,czxttkl,gzhao27,fhan,chocjy,haoyc,lxiong,wychen}@meta.com Meta Platforms, Inc., USA",
  "ABSTRACT": "Neural Architecture Search (NAS) has demonstrated its efficacy in computer vision and potential for ranking systems. However, prior work focused on academic problems, which are evaluated at small scale under well-controlled fixed baselines. In industry system, such as ranking system in Meta, it is unclear whether NAS algorithms from the literature can outperform production baselines because of: (1) scale Meta's ranking systems serve billions of users, (2) strong baselines the baselines are production models optimized by hundreds to thousands of world-class engineers for years since the rise of deep learning, (3) dynamic baselines engineers may have established new and stronger baselines during NAS search, and (4) efficiency the search pipeline must yield results quickly in alignment with the productionization life cycle. In this paper, we present Rankitect , a NAS software framework for ranking systems at Meta. Rankitect seeks to build brand new architectures by composing low level building blocks from scratch. Rankitect implements and improves state-of-the-art (SOTA) NAS methods for comprehensive and fair comparison under the same search space, including sampling-based NAS, one-shot NAS, and Differentiable NAS (DNAS). We evaluate Rankitect by comparing to multiple production ranking models at Meta. We find that Rankitect can discover new models from scratch achieving competitive tradeoff between Normalized Entropy loss and FLOPs. When utilizing search space designed by engineers, Rankitect can generate better models than engineers, achieving positive offline evaluation and online A/B test at Meta scale.",
  "ACMReference Format:": "Wei Wen ‚àó , Kuang-Hung Liu ‚àó , Igor Fedorov, Xin Zhang, Hang Yin, Weiwei Chu, Kaveh Hassani, Mengying Sun, Jiang Liu, Xu Wang, Lin Jiang, Yuxin Chen, Buyun Zhang, Xi Liu, Dehua Cheng, Zhengxing Chen, Guang Zhao, and Fangqiu Han, Jiyan Yang, Yuchen *These authors contributed equally. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'24 Companion, May 13-17, 2024, Singapore ¬© 2023 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn Hao, Liang Xiong, Wen-Yen Chen. 2023. Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale. In Proceedings of ACM Web Conference 2024 (WWW '24 Companion). ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn",
  "1 INTRODUCTION": "Neural Architecture Search (NAS) seeks to automatically generate new architectures for deep learning models. A seminal work [45] proved the efficacy of NAS in the era of deep learning to design convolutional networks for computer vision and recurrent cells for natural language processing. It stimulated a line of academic research, typically in computer vision [34, 38, 46] and later in ranking systems [12, 18, 42]. One of the major research areas around NAS has been improving its computational efficiency, including more efficient sampling-based methods [10, 29, 35, 36], one-shot [6, 28] and DNAS [3, 11, 22, 39]. However, those methods have never been evaluated at web-scale applications with billions of users for generating brand new ranking models from scratch by composing low-level building blocks. In this paper, we develop Rankitect, which advances NAS to Meta scale with comprehensive evaluation: ¬∑ Search space : Our search space includes almost no inductive bias from the production model. The discovered architecture is a mapping between feature embeddings and final classification layer. Building blocks are low-level operations like linear, dot product, etc. Connectivity among blocks is arbitrary. Block dimensions are searchable; ¬∑ Improved three categories of search algorithms : samplingbased methods [35, 45] under different low cost proxies, oneshot method [6] with in-place co-trained baseline model and DNAS [39]; ¬∑ Baseline : the strongest production model at Meta, which is a Click Through Rate (CTR) model optimized by world-class engineers driven by years of business needs. This industry level NAS at Rankitect scale imposes significant challenges and we advance current SOTA methods to succeed: ¬∑ Low cost evaluation analysis : for sampling-based methods, training a sampled model for long term is unrealistic, e.g. > 50 billion examples at Meta scale. When a small amount of data is used, we compare the ranking quality between weight-sharing proxy [4] and early-stop proxy, and find weight-sharing proxy is superior when the data amount is small but will be surpassed by simple early-stop proxy when more data is used; based on WWW'24 Companion, May 13-17, 2024, Singapore Wen and Liu, et al. our decision framework, early stop proxy is preferred in practice when requiring a higher ranking quality and more efficient engineering and system optimization. ¬∑ Noise reduction by an in-place baseline in one-shot method : data distribution of real-world ranking systems shifts dramatically, which makes the absolute metric - normalized entropy (NE) loss [13] - noisy over the training course. To cancel the noise introduced by data shifting, we co-train a baseline model in parallel with the weight sharing supernet in one-shot method [6], and use the relative improvement of a model/subnet sampled from the supernet versus the baseline as the reward of the Reinforcement Learning (RL) agent; ¬∑ A joint on-policy/off-policy RL method to improve sample efficiency : RL based one-shot NAS suffers from costly reward evaluation and extreme sample inefficiency. To address both challenges, we propose a joint on-policy and off-policy policy gradient method that uses experience replay to amortize the computational cost of reward evaluation and improve sampling efficiency by decoupling RL policy updates from that of weight sharing supernet. ¬∑ DNAS : We implement differentiable NAS (DNAS) to optimize decision variables in conjunction with supernet weights. The DNAS approach exhibits increased efficiency compared to samplingbased methods and improves convergence characteristics compared to one-shot method using RL. ¬∑ System optimization : Optimizing NAS within expensive search spaces presents inherent computational challenges in production system. To overcome this challenge, we developed tailored techniques such as: (1) activation checkpointing; (2) employing system-wide vectorized operations; (3) utilizing transfer learning to facilitate supernet weight sharing; (4) Round-Robin process groups to mitigate AllReduce overhead; (5) dynamic execution of partial operators in PyTorch graphs. These strategies collectively enhance the efficiency and effectiveness of Rankitect search. With improved NAS methods, we find that ¬∑ Rankitect can generate brand new ranking models from scratch and outperform production models. Three methods in Rankitect all generated models along the NE-FLOPs Pareto front, but one-shot and DNAS are about 145 √ó efficient in terms of GPU hours. While one-shot and DNAS are on-par in terms of model quality and search efficiency, we find one-shot method can be implemented more easily with higher memory efficiency. ¬∑ During the dynamic iteration of the production model, in-house world-class engineers were also able to invent models beating the baseline. When battling human models, Rankitect models can cover different regions along the Pareto frontier of NE-FLOPs. Moreover, by utilizing the human designed model as a search space, Rankitect produces models with NE-FLOPs trade-off better than engineers achieving positive offline evaluation and online A/B test at Meta scale.",
  "2 RELATED WORK": "Neural Architecture Search (NAS) is a generic approach to design deep learning models in various applications, such as, computer vision [5, 6, 28, 29, 34, 35, 38, 45, 46], natural language processing [17, 32, 45] and speech [1, 24]. Its adoption to ranking systems is also ramping up [8, 12, 18, 33, 42, 43], however those work evaluated NAS with small public datasets using small search space. For example, the DNAS (for ads CTR prediction [18]) and PROFIT [12] only search a portion of ranking models with a limited number of choice options. NASRec [42] scales the search space to a larger extent but the number of choices to select building blocks is just 7. Unlike those work, Rankitect evaluates web-scale NAS at Meta scale with a much larger dataset ( > 50 billions of examples) and a much larger search space ( ‚â• 28 choices of building blocks). NAS has also been applied to industry level ranking systems [2, 19, 21, 44], however, the search space is relatively trivial: AutoFIS [21] uses DNAS like gates to select predefined feature interactions for Huawei App Store ranking systems; ByteDance proposed AutoEmb [44] to learn embedding dimensions, and Google's H2ONAS [19] is adopted to learn MLP sizes and embedding dimensions. None of them has studied the aggressive search space like Rankitect, which searches building block per layer, connections among blocks and dimensions per block, all at once.. Moreover, only one NAS algorithm was adopted in previous work, such as sampling-based methods in AutoCTR [33] and NASRec [42], DNAS in AutoFIS and one-shot method in H2O-NAS. This imposes challenges to understand the scalability and effectiveness of different NAS algorithms because the setups are different across different works/companies. Rankitect evaluates three categories of NAS algorithms (sampling-based methods, DNAS and one shot method) using the same dataset, search space, baseline and engineering constraint. This provides valuable insights to select appropriate NAS methods based on real-world conditions.",
  "3 METHOD": "In this section, we introduce the search space and three NAS algorithms we developed.",
  "3.1 Search Space and Supernet Design": "Search space is defined as the set of all possible models. In Rankitect, we bag all possible models into a single weight sharing supernet model [4] as detailed in Figure 1, and a model in the search space is produced by masking out connections & building blocks and selecting specific dimensions for building blocks. As a selected model is materialized as a sub-architecture of the supernet, we interchangeably use 'subnet' and 'model'. Rankitect searches the end to end architecture between raw inputs (a dense feature 2D vector and a 3D embeddings concatenated from sparse/category embeddings and content embeddings) and the final logit used for CTR prediction. For simplicity, a 3D or 2D tensor is denoted as ùëã 3 ùëë ‚àà R ùêµ √ó ùëÅ √ó ùê∑ or ùëã 2 ùëë ‚àà R ùêµ √ó ùëÜ , respectively. ùêµ is batch size, ùëÅ is the number of embeddings, ùê∑ is embedding dimension, and ùëÜ is the size of a 2D vector. In Rankitect, all 3D tensors have the same embedding dimension ùê∑ . The supernet is constructed by stacking a cascade of 'Choice' modules which can be fully densely connected to previous choices and raw inputs. All Choice modules have the same architecture which consists of three parts: (1) Input adaptors , which adapt inputs for valid computation of each building 'block'. We have three types of adaptors as shown in Figure 1: (1) reshape each input to a 2D tensor and Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale WWW'24 Companion, May 13-17, 2024, Singapore Figure 1: Search space in sampling-based methods and supernet in one-shot method and DNAS Calibration Loss Input Stem Linear with output size 1 Output Stem Choice Choice Choice Linear Compressed dot product Pairwise gating with sum EmbedFC (i.e. LCE) Input 1 Input 2 Input K Reshape each input to a 3D Tensor Concat with projection ‚Ä¶‚Ä¶ multiply Sum with zero padding (which is applied to 2D tensors or 3D tensors separately) ‚Ä¶ Choice Annotations by colors: Fixed structure, searched structure, masks for subnet selection Pairwise sum w/ projection Reshape to 2D tensors multiply A 2D tensor A 3D tensor 2D 2D 2D 2D 3D multiply Reshape each to a 2D Tensor 2D 3D 3D 2D & 3D 2D & 3D Mask Mask Mask K independent binary masks left binary masks binary output masks right binary masks multiply 2D & 3D Reshape each to a 2D Tensor Concat concatenate together before the linear block; (2) concatenate all inputs to a single 3D tensor along the middle axis (when an input is ùëã 2 ùëë ‚àà R ùêµ √ó ùëÜ , it is expanded to ùëã 3 ùëë ‚àà R ùêµ √ó 1 √ó ùëÜ and then linearly projected to ùëã 3 ùëë ‚àà R ùêµ √ó 1 √ó ùê∑ for concatenation with other 3D tensors). This adaptor is used before compressed dot product and EmbedFC blocks; (3) reshapes each input to a 2D tensor before pairwise gating and sum blocks. (2) Building blocks , which are layer types we want to search. Rankitect generates models from the elementary low level building blocks below: ¬∑ linear or fully connected (FC) layer ¬∑ EmbedFC is a linear projection applied to the middle axis of input ùëã 3 ùëë ‚àà R ùêµ √ó ùëÅ √ó ùê∑ and produces ùëã 3 ùëë ‚àà R ùêµ √ó ùëÄ √ó ùê∑ , therefore, it is a linear with weight matrix ùëä ‚àà R ùëÄ √ó ùëÅ ; ¬∑ compressed dot product is simply pairwise dot products among compressed embeddings and raw embeddings as  where ùëèùëöùëö (¬∑) is batch matrix-matrix product 1 ; ¬∑ pairwise gating block has ùêæ inputs ùëã ( ùëñ ) 2 ùëë ‚àà R ùêµ √ó ùëÜ ùëñ where ùëñ ‚àà 1 ...ùêæ , it computes as  where ùêπùê∂ ùëñ ùëó (¬∑) projects ùëã ( ùëñ ) 2 ùëë from dimension ùëÜ ùëñ to ùëÜ ùëó and ùëñùëõùë°ùëíùëüùëéùëêùë°ùëñùëúùëõ ( ùë•,ùë¶ ) = ùë†ùëñùëîùëöùëúùëñùëë ( ùë• ) ‚ó¶ ùë¶ known as Hadamard product. Àù ùêæ ùëñ = 1 Àù ùêæ ùëó = 1 (¬∑) will zero pad smaller tensors to maximum size of { ùëÜ ùëñ | ùëñ ‚àà 1 ...ùêæ } before sum; ¬∑ pairwise sum block is very similar to pairwise gating block and the only difference is that ùëñùëõùë°ùëíùëüùëéùëêùë°ùëñùëúùëõ ( ùë•,ùë¶ ) = ùë• + ùë¶ . 1 https://pytorch.org/docs/stable/generated/torch.bmm.html Note that firstly layer normalization and then activation functions are appended after each block. (3) Output aggregators , which reduce outputs from all building blocks. In Rankitect, all 2D block outputs are zero-padded to the same shape and then sum together to form a single 2D tensor. As only one block (EmbedFC) produces a 3D tensor, the 3D tensor is outputted alone from the choice module. In the supernet as colored by orange in Figure 1, Rankitect samples masks to sample a subnet/model from the search space. A subnet is sampled by selecting a building block within each choice module, choice connections, and block dimensions: ¬∑ building block sampling: Rankitect selects one block within each choice module, which is accomplished by sampling an onehot 'binary output masks'. ¬∑ choice connection sampling: when linear, compressed dot product or EmbedFC block is selected, 'K independent binary masks' (multi-hot masks) are sampled to select a portion as inputs; when pairwise gating or sum block is selected, an one-hot 'left binary masks' and one-hot 'right binary masks' are sampled to enable only one pair of ' ùëñùëõùë°ùëíùëüùëéùëêùë°ùëñùëúùëõ (¬∑) ' in Eq. (2). ¬∑ block dimension sampling: when a block is linear, compressed dot product or EmbedFC, output dimension of the linear projection inside the block will be sampled. This is achieved by multiplying linear outputs by a vector of binary masks. A mask vector is constrained to values with leading ones followed by zeros (e.g. [ 1 , 1 , 1 , 0 , 0 ] ). The length of leading ones equals an selected output dimension. Note that, Rankitect can enable the whole supernet by setting all masks values to ones. With our supernet design, Rankitect flexibly supports a variety of NAS algorithms: ¬∑ sampling-based algorithms: any black-box sampling algorithms (random, reinforcement learning [45], Bayesian optimizer [36] WWW'24 Companion, May 13-17, 2024, Singapore Wen and Liu, et al. and neural predictor [35]) can be used to sample masks (in orange) to sample a subnet/model; ¬∑ one-shot method: an in-place RL agent is co-trained with the supernet, with the agent sampling masks; ¬∑ DNAS: masks are replaced by Gumbel-Softmax [15] learned by architecture parameters.",
  "3.2 NAS Algorithms": "At Meta, we use Normalized Entropy (NE) [13] loss to evaluate the prediction performance of ranking systems. A smaller NE presents a better ranking model. We use 'NE gain' over a baseline to measure improvement - a negative 'NE gain' implies a improved prediction. In the following, we introduce how each NAS algorithm works in Rankitect and what improvement we have proposed.",
  "3.2.1 Sampling-based method.": "Based on study in computer vision on NASBench-101 [35, 40], Neural Predictor [35] is a more efficient sampling-based method than Regularized Evolution [29] and Reinforcement Learning, we select Neural Predictor given it also has traits of full parallelism and friendly hyper-parameter tuning. Unlike original method which used a random sampler to pick top models by the predictor, we use a RL sampler to do so for faster convergence. As discussed, it is unrealistic to do a full job training at Meta dataset scale after a subnet/model is sampled. We compared two types of low cost NE proxies: ¬∑ weight-sharing proxy which has two stages: supernet pretraining and subnet finetuning . In supernet pretraining , the whole supernet is first trained for 10% data (by setting all masks to ones), followed by enabling subnet sampling with a proability of 0 . 75 at each mini-batch. That is, in the later 90% supernet pretraining, at each mini-batch, the supernet has 0 . 25 probability to train all architectures and 0 . 75 probability to only train a sub-architecture (subnet) by uniform random sampling of masks (in orange in Figure 1). In subnet finetuning , the Neural Predictor will first sample and fix masks to select a subnet, and then the specific subnet (with weights shared/transferred from corresponding parameters from supernet) is fine-tuned for a small amount of data to get the final weight-sharing proxy. Note that fine-tuned weights in a sampled subnet will not be updated to the supernet. ¬∑ early-stop proxy , which simply trains a standalone subnet/model from scratch for little data. For this proxy, the supernet is never instantiated or trained; only the sub-architecture of a sampled model is realized as a PyTorch model for more efficient training. Note that for both NE proxies of a subnet, window NE averaged over the last 25% training/fine-tuning data is used because it better correlates with long term NE which is more important for CTR models in real-world ranking systems.",
  "3.2.2 One-shot NAS with Reinforcement Learning.": "Motivated by the TuNAS one-shot method [5], we co-train an in-place RL agent with the supernet simultaneously illustrated in Figure 2. At each mini-batch, the RL agent samples masks to select a subnet and the mini-batch NE is used as reward to update the RL agent. More specific, we treat all the decisions (i.e., mask selections Figure 2: In-place training a baseline model with RL sampling subnets from the supernet. in the supernet) as independent multinomial r andom v ariables (r.v.). Let ùúÉ be a vector (collection) of parameters that define all the decision's multinomial r.v.. We use REINFORCE [37] to update the sampling distribution ùëÉ ùúÉ with policy gradient:  where ùëé ùëñ is a sampled subnet and ùëÖ ùëñ is the reward (e.g. NE and FLOPs) of ùëé ùëñ . A baseline function ùëè is used to reduce the variance of policy gradient and in our work we define ùëè to be the average over all ùëÖ ùëñ . In the following, we discuss technical challenges of this TuNAS-like one-shot method in Rankitect and our proposals to overcome them. We use 'RL method' and 'one-shot method' interchangeably in this paper. Noisy reward and variance reduction . When applying RL to NAS for ranking problem, a main component of ùëÖ ùëñ is the minibatch NE, ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëé ùëñ ) . However, ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëé ùëñ ) in production environment is: (1) extremely noisy (i.e., high variance) and (2) nonstationary caused by data distribution shift. To overcome these challenges, we borrow ideas from the control variate method where we subtract a correlated baseline reward, denoted as ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëèùëéùë†ùëíùëôùëñùëõùëí ) , from ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëé ùëñ ) . More specifically, we propose the following: (1) Calculate ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëèùëéùë†ùëíùëôùëñùëõùëí ) from an in-place baseline model co-trained in parallel with the supernet. (2) Replace ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëé ùëñ ) used in ùëÖ ùëñ with ùëÅùê∏ % ( ùëé ùëñ ) :  Figure 2 illustrates how the proposed ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëèùëéùë†ùëíùëôùëñùëõùëí ) and ùëÅùê∏ % ( ùëé ùëñ ) integrate with the weight sharing supernet training. A detailed ablation study for this part is included in Appendix A (see Figure 8 and 9) Sample scarcity and on/off-policy RL . REINFORCE is an onpolicy method where it can only learn through sampling from the current ùëÉ ùúÉ . This limitation introduces inefficiency in NAS for industry-scale ranking problems, because of: (1) costly reward evaluation: evaluating ùëÖ ùëñ for an industry-scale model architecture is computationally expensive such that the number of samples is prohibited; and (2) extreme sampling inefficiency: RL policy's updating schedule is slower than that of the supernet update (i.e., Eq. (3) requires a batch of model evaluations where each needs a mini-batch of subnet/supernet training). Consider an example of using batch size 100 for Eq. (3), then a supernet trained over one million Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale WWW'24 Companion, May 13-17, 2024, Singapore steps (typical budget) only generates 10 , 000 RL policy updates which is a challenge for on-policy RL algorithms. To address these challenges we adopted off-policy RL methods reusing past samples ( ùëé ùëñ , ùëÖ ùëñ ) for policy updates with experience replay [26]. This allows us to amortize the computational cost of reward evaluation and improves sample efficiency by decoupling policy updates from supernet updates. We followed a standard formulation of the off-policy policy gradient (PG) based on importance sampling (IS) [23]. A main challenge of off-policy implementation is controlling for the possible high variance (infinite variance in some cases) of the IS weights (a ratio of two probability functions). In this work, we adopted two techniques to mitigate the potential problem of high IS weights variance: (1) IS weights clipping; (2) weighted importance sampling (WIS) [23, 25, 31]. In summary, we adopted the following policy gradient with off-policy IS:   where ùúñ is a small number to avoid divide-by-zero, ùë§ ùëñ is the WIS weights, ùëè ‚Ä≤ is the average over all ùëÖ ‚Ä≤ ùëñ , and ( ùúÉ ‚Ä≤ ùëñ , ùëé ‚Ä≤ ùëñ , ùëÖ ‚Ä≤ ùëñ ) is a random sample drawn from a replay buffer ùêµ that stores past samples. Note that we also store the multinomial r.v. parameter ùúÉ ‚Ä≤ that are used to sample ùëé ‚Ä≤ ùëñ . Since RL policy is trained concurrently with the supernet, recent ( ùëé ‚Ä≤ ùëñ , ùëÖ ‚Ä≤ ùëñ ) samples are more relevant/accurate than older samples. Based on this observation, we propose a joint on-policy and off-policy PG (denoted as on/off-policy PG) where we first perform on-policy update using Eq. (3) (and store new batch of ( ùúÉ ‚Ä≤ ùëñ , ùëé ‚Ä≤ ùëñ , ùëÖ ‚Ä≤ ùëñ ) to ùêµ ) followed by ùêæ off-policy updates using Eq. (5). We also experimented with prioritized experience replay [30] but found our proposal that guarantee on-policy update at each step to perform better in practice . With the proposed on/off-policy method, we can achieve a ùêæ √ó increase in the number of policy updates compared to on-policy REINFORCE. A detailed ablation study for this part is included in Appendix (see Figure 10). NE search with FLOPs constraint . To promote the discovering of models with good NE and FLOPs trade-off, we augment reward function with an additive cost term:  where ùõº is a regularization weight that balances the trade-off between NE and FLOPs. For ùê∂ùëúùë†ùë° ( ùëé ùëñ ) , we experimented with L1penalty ‚à• ùêπùêøùëÇùëÉùë† ( ùëé ùëñ ) -ùëê ‚à• and ReLU-penalty max ( ùêπùêøùëÇùëÉùë† ( ùëé ùëñ ) -ùëê, 0 ) , where ùëê is the target FLOPs. From our experiment, we observe that using ReLU-penalty often leads to a model with smaller FLOPs and worse NE compared to using using L1-penalty. Our hypothesis is that the L1-penalty can better guide RL search during initial exploration towards the targeted FLOPs (penalize both above and below target FLOPs) and later lead to the convergence of a better local optimum model.",
  "3.2.3 DNAS.": "We implemented DNAS [7, 9, 22] as an alternative to one-shot method for comprehensive study. DNAS directly optimizes the pair ( ùëä,ùúÉ ) :  where ùõæ ( ùúÉ ) denotes the most likely subnet given ùúÉ , ùëë is a batch of data, and ùê∂ùê∏ is the cross-entropy loss. The fundamental difference between RL and DNAS is in how we deal with the non-differentiable expectation over ùëé in (8). Each of the decisions of ùëé is a categorical r.v., such that differentiation of (8) is not possible. In DNAS, the categorical r.v. 's are replaced by Gumbel-Softmax r.v.'s, which lend themselves to the reparameterization trick for differentiation [10, 16, 18]. The expectation in (8) is approximated using Monte-Carlo sampling, with each trainer and each minibatch element using a separate realization of ùëé . In order to enforce the constraint in (8), we follow [11] and transform (8) into a regularized objective with a regularizer whose minimizer is guaranteed to satisfy the constraint:",
  "4 EXPERIMENTS": "In this section, we provide experimental results for end-to-end Rankitect search and compare the discovered model to strong production baselines created by ML experts. Three categories of algorithms are also compared.",
  "4.1 Experiment Setup": "We use industrial ranking systems' datasets at Meta for all experiments. Input features include floating-point dense features, content embeddings, and sparse embeddings from categorical lookup tables. All embeddings are concatenated to a single 3D tensor. Click Through Rate (CTR) and Conversion Rate (CVR) models are evaluated, which are binary classifiers. Binary cross entropy loss is used for optimization, and we report Normalized Entropy [13] (NE). NE is simply normalized cross entropy which is less noisy over data distributions. A typical ranking model is trained by more than 50 billion examples with Adam optimizer. All Rankitect searches are performed in GPU (up to 128) training cluster. During supernet search, PyTorch dynamic computation graphs are not well-suited for conventional optimizations. To streamline this process, we have explored and implemented several techniques, including activation check-pointing for reduced memory, employing vectorized operations to expedite computations, utilizing partial parameter transfer from the supernet to standalone subnets to efficiently obtain weight-sharing proxy, and implementing Round-Robin process groups to mitigate All-Reduce overhead [20].",
  "4.2 Full Architecture Search from Scratch Battling World-class Engineers": "4.2.1 Supernet Design. To study the scalability of Rankitect, this section performs NAS in a search space as large as possible, using the largest and strongest CTR model baseline (dubbed as 'CTR app 1') at Meta. If we naively use all blocks within each choice and fully connect them, the supernet size explodes easily. For example, a single full choice already has > 8 . 5 billion dense parameters, major contributors of which are pairwise gating and sum blocks. To reduce supernet size, we propose two optimizations: WWW'24 Companion, May 13-17, 2024, Singapore Wen and Liu, et al. ¬∑ Sharing pairwise linear projections ùêπùê∂ ùëñ ùëó (¬∑) in Eq. (2): when the same pairwise interaction (gating or sum) appears in different choices, the interaction is computed once and shared; ¬∑ bottleneck layers in pairwise linear projections: ùêπùê∂ ùëñ ùëó (¬∑) in Eq. (2) is decomposed to two linear layers with a bottleneck dimension 256 at the middle. This gives supernet better scalability, however the number of choices we can scale is still prohibited when the supernet has full connections among all choices with each choice having all blocks. We constrain the connections and possible blocks per choice as below and reach a supernet with around one billion dense parameters. We start from a supernet with an architecture matching our current production model (which is a model variant of DLRM [27]), where each choice only has one building block to match a layer. We then grow the supernet as following steps: ¬∑ randomly permute the list of choices while keeping its directed acyclic graph (DAG) order, i.e., predecessors of a node always have smaller indices in the permuted list. This removes the layer order bias encoded by engineers; ¬∑ copy any choice having EmbedFC or compressed dot product block, and then insert it right after; ¬∑ in any choice, if any of [compressed dot product, EmbedFC] block exists, add the other; if any of [linear, pairwise gating, pairwise sum] block exists, add the others; ¬∑ each choice is connected to previous choices with distances of 1, 2, 3, 6 and 9, where 'distance' is defined as the relative index of two choices in the choice list; ¬∑ enlarge linear dimensions in all blocks to 1 . 25 √ó . There are 7 dimension options uniformly distributed between 0 . 5 √ó and 1 . 25 √ó . The search space consists of 2 . 8 √ó 10 114 models, which is 4 . 3 √ó 10 95 times larger than the search space size in a typical computer vision problem [35] and 5 . 6 √ó 10 78 times larger than a ranking systems NAS work in academia [42]. The number of models is more than the number of atoms in the observable universe 2 . After finalizing the supernet, we tune the sampling probability of 'K independent binary masks' in Figure 1 such that the distribution of subnets are reasonable. With probability 0 . 8 to set a binary mask to one, Figure 3 plots the histogram of statistics of 10 , 000 random subnets, where we can see that distributions spread well around our production model. Note that the production model is 5 √ó in terms of FLOPs, which is indicated by vertical dashed lines in Figure 3.",
  "4.2.2 Weight-Sharing Proxy and Early-Stop Proxy.": "To evaluate which low cost NE proxy is better under specific conditions, we randomly sample 60 models and train each from scratch for 28B examples ('B' is for 'billion') to get long term NE. The window NE averaged on last 0 . 5B examples is used as ground truth long term NE. Figure 4 plots the Kendall Tau rank correlation between proxies and ground truth NE. We find that weight-sharing NE has better correlation quality initially to rank ground truth NE, however, its advantage disappears when the amount of data passes a threshold (200 million in our search space). This is reasonable because weights used in weight-sharing proxy have been pre-trained in the supernet, such that a subnet does not need to train from scratch and therefore outperforms when the training data is less; 2 https://www.universetoday.com/36302/atoms-in-the-universe/ Figure 3: The distribution of FLOPs, # dense parameters, # choices and # connections in subnets. Vertical dashed lines are for the production model. 2000 1000 X FLOPs 2000 1000 the number of dense parameters 1500 1000 500 The number ol choices 750 500 250 The number of connections Figure 4: Rank correlation between low cost proxies and long term ground truth NE, which is obtained by training for 28 B examples. Vertical intervals are confidence intervals from 25% to 75% quantile. Right figure just zooms into the beginning parts of curves on the left. 0.85 0.85 early-stop proxy 0.80 0.80 weight-sharing proxy 0.75 0.75 0.70 0.65 0,55 0,50 early proxy 0.50 weight-sharing proxy 0.45 0.45 Traininglfine-tuning examples Trainingifine-tuning examples stop however, as data amount increases, early-stop proxy approaches to ground truth NE but weight-sharing proxy may have inferior initialization for any subnet to achieve so. In conclusion, weight-sharing proxy should be used if speedy subnet evaluation is required and a moderate rank quality (e.g. Kendall Tau 0 . 6) satisfies; however, early-stop proxy should be used if a high rank correlation is demanded. Moreover, engineering and machine learning system optimization overheads should also be considered in practice, such as, in weight-sharing proxy, it is non-trivial to implement partial weight transfer from the supernet to any standalone subnet. In our experiments, we find high rank correlation is important to search models with long term NE, and decided to use early-stop proxy with 3B training data, which has around 0 . 8 Kendall Tau ranking quality.",
  "4.2.3 Results of Sampling-based Method.": "We applied sampling-based method to search models for low early-stop proxy (which is better if lower). Figure 5a is the minimal early-stop NE found as more and more models are sampled and evaluated. In experiments, Neural Predictor bent the curve after initial random sampling. In total, we sampled 5 , 000 models. As low early-stop proxy does not guarantee long term NE, we picked 100 models with lowest early-stop NE and trained for more data with Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale WWW'24 Companion, May 13-17, 2024, Singapore Figure 5: Results of Sampling-based Method: (a) NE gain of early-stop proxy. The vertical red lines divide different rounds of sampling-based search. The first round is random search (whose curve is smoothed by averaging over 500 random permutations), followed by six rounds of Neural Predictor based sampling. For each new round, a MLP is trained by using all previously obtained data points and then picks top models based on MLP prediction; (b) long term NE gain at 65 billion data examples versus production model. (a) (b) DHEN 0,000O Nawve scaled Rankilect models 8 8 70.12 Merged top Rankitect models 2 0.0004 0.0006 ~0.16 0.0008 70.18 2 0,0010 ~0.20 70.22 1000 3000 5000 20 30 50 60 70 sampled subnets x FLOPs Successive Halving [14], ending up with final 4 best models plotted as green cycles in Figure 5b. In Figure 5b, we can find that Rankitect can discover a new model with 0 . 10% absolute NE gain which is considered as significant in ranking systems. Moreover, discovered models cover a good range of model complexity (FLOPs) from 14 . 5 √ó to 24 √ó , which provides a good pool of models to select from for different serving cost constraints required by different products and model types. During Rankitect development, in-house engineers also invented a new model - DHEN [41] as plotted in red triangles. Battling with Rankitect, DHEN covers a new region - more NE gain with more FLOPs. The combination of Rankitect and DHEN covers a wider range of FLOPs for different products' serving cost requirements. To compare with DHEN in the high-FLOPs region, we propose two methods to scale Rankitect models: ¬∑ Naive scaling (blue diamonds): 3 √ó linear dimension in EmbedFC (first blue diamond from the left), OR, simultaneously 1 . 5 √ó linear dimensions in [linear, EmbedFC and compressed dot product] blocks and 2 √ó bottleneck dimensions in pairwise gating and sum blocks; ¬∑ Merging (purple squares): pick a subset of Rankitect models (green circles), for a connection or a block existing in any Rankitect model, add it to the final merged model. For a block having different linear dimensions in different Rankitect models, use the maximal dimension in the final merged model. In Figure 5b, we find the 'Merging' method has a better NE-FLOPs trade-off than 'Naive scaling'. Battling with DHEN, scaled Rankitect models have on-par NE-FLOPs trade-off trend, but fills the big blank FLOPs region which DHEN is unable to cover. Moreover, Rankitect automates the process of new architecture design, which can release human engineering resources.",
  "4.2.4 Results by One-shot Method and DNAS.": "In our experiments, we find that it is challenging for one-shot method and DNAS to learn connections in the supernet designed in Section 4.2.1, therefore, we simplified the supernet by disallowing adding new connections and only learn to select a building block per choice. The NE gain and FLOPs of the simplified supernet is plotted as the black star in Figure 5b. Our goal in this section is to answer the following questions: ¬∑ Does one-shot method or DNAS produce better NE models than sampling-based method? ¬∑ Howdo the search costs of three categories of methods compare? ¬∑ Is DNAS able to leverage the reduced variance of the GumbelSoftmax gradient estimator compared to RL's policy gradient (in one-shot method) to converge to solutions which satisfy a FLOPs constraint faster? Figure 6 shows that the DNAS is able to find models which outperform the production model and sampling-based model, albeit at higher FLOPs cost. When we looked at the efficiency/cost of each method, DNAS and RL yield results with roughly 145 √ó less compute resources (1 . 45 days vs. 209 . 5 days used by sampling-based method on 64 GPU system), which is a huge benefit at Meta-scale recommendation system training. Finally, we compare DNAS to RL and evaluate the effect of gradient variance on convergence performance since one of the benefits of DNAS is the reduced gradient variance compared to RL [15]. We setup an experiment where both DNAS and RL try to search for a model with a target FLOPs (without considering its NE) and report that DNAS requires ‚àº 26 √ó less SGD steps to converge to a given FLOPs target. Figure 6: NE and FLOPs comparison of three categories of methods. Merged top Rankitect models ~0.06 DNAS Sampling-based ~0.07 One-shot method (with ~0.09 ~0.10 20 25 30 35 40 45 50 55 x FLOPs RL) Although DNAS has overall the best performance, in the end we chose to focus our software system on RL instead of DNAS for the following reasons: ¬∑ The engineering cost of maintaining two search algorithms with relatively similar search performance is too high. ¬∑ Memory consumption of naive DNAS implementation is higher, demanding potentially more memory optimization overhead. ¬∑ The engineering cost of DNAS is higher than RL. DNAS requires careful tuning of hyperparameters like Gumbel-Softmax temperature, sparsity level of sampled r.v's, etc. [11]. Since the loss function is required to be differentiable for DNAS, optimization of FLOPs requires us to build a software library modeling FLOPs as a differentiable function of architecture. While this is possible, the engineering cost of building and maintaining such a library WWW'24 Companion, May 13-17, 2024, Singapore Wen and Liu, et al. Table 1: Rankitect search summary for DHEN-based supernet. 1. all searches ran with 64 GPUs and the search cost are reported in total GPU-hours, e.g., 6144 GPU-hour = 64 GPU * 96 wall-clock hour. are quite high. On the other hand, RL can natively optimize non-differentiable objectives. Table 2: Comparison of Rankitect and sampling-based production AutoML searcher.",
  "4.3 Reusing Search Space Invented by World-class Engineers": "As proved in Section 4.2, Rankitect automatically produces competitive models versus world-class engineers with more diverse FLOPs coverage. More importantly, the result is generated from an unspecified supernet (or search space) with minor human prior. To bring the advantages from both worlds (of Rankitect and in-house engineers) together, we use DHEN as supernet backbone and apply Rankitect. We hypothesize that: human crafted DHEN-based supernet has a higher density of good models than the unspecified supernet, which makes Rankitect easier to find better models and can outperform human experts. We prove so in this section. We setup a supernet to match the architecture of Deep and Hierarchical Ensemble Networks (DHEN) [41] and focus on searching for all building block dimensions. For example, a 4 layer DHENbased supernet has 93 building block dimensions to search; with each block dimension having 9 options, we have a search space with size 9 93 . The primary motivation for us to only search for block dimensions is to ensure an easy transfer of Rankitect search results to DHEN models in Meta's production stack. To test the transferability, Rankitect is only applied on the strongest baseline at Meta for the 'CTR app 1' product, and we simply apply the discovered models to other products. To meet different serving cost constraints of different products, we applied our Rankitect framework to various DHEN-based supernets, targeting different FLOPs goals. We summarize our search results in Table 1. Beating strongest human baseline . During model iteration of 'CTR app 1', our in-house engineers were targeting on an approximately 30 √ó model for product and established the largest and strongest CTR baseline at Meta. To battle human engineers, Rankitect generated 'model-1' in Table 1 which is a 31 . 2 √ó model with > 0 . 02% absolute NE gain at 90B (with gain still enlarging with more training data shown in Figure 7 in Appendix) compared with the human tuned 31 . 3 √ó baseline model. This proves that Rankitect can discover better models than Meta's world-class engineers. Beating AutoML in production . We applied 'model 2' to product 'CVR app 0' and conducted a side-by-side comparison with a model discovered by sampling-based AutoML in production shown in Table 2. It turns out that our Rankitect model was able to meet FLOPs constraint for this product with significant NE gain, while production AutoML was not able to satisfy the FLOPs constraint. Table 3: Transferability of Rankitect discovered models. Furthermore, Rankitect achieved 8 . 3 √ó search efficiency gain over production AutoML (measured by GPU hours). Due to those, 'model 2' was selected for online A/B test and show statistically significant gain over production model. Strong model transferability to other products . We applied 'Model 3' and 'Model 4' across different products and observe strong model transferability. The results are summarized in Table 3 where we can see that 'Model 3' is a smaller model but still achieve better NE performance in many of the products. On the other hand, 'Model 4' achieve significant NE-gain over product baselines at the cost of model size increase. These results demonstrate that Rankitect has the potential to search once and apply to many. WWW'24 Companion, May 13-17, 2024, Singapore Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale",
  "REFERENCES": "[1] Mohamed S Abdelfattah, Abhinav Mehrotra, ≈Åukasz Dudziak, and Nicholas D Lane. 2021. Zero-cost proxies for lightweight nas. arXiv preprint arXiv:2101.08134 (2021). [2] Rohan Anil, Sandra Gadanho, Da Huang, Nijith Jacob, Zhuoshu Li, Dong Lin, Todd Phillips, Cristina Pop, Kevin Regan, Gil I Shamir, et al. 2022. On the factory floor: ML engineering for industrial-scale ads recommendation models. arXiv preprint arXiv:2209.05310 (2022). [3] Colby Banbury, Chuteng Zhou, Igor Fedorov, Ramon Matas, Urmish Thakker, Dibakar Gope, Vijay Janapa Reddi, Matthew Mattina, and Paul Whatmough. 2021. MicroNets: Neural Network Architectures for Deploying TinyML Applications on Commodity Microcontrollers. In Proceedings of Machine Learning and Systems , A. Smola, A. Dimakis, and I. Stoica (Eds.), Vol. 3. 517-532. https://proceedings.mlsys.org/paper_files/paper/2021/file/ c4d41d9619462c534b7b61d1f772385e-Paper.pdf [4] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. 2018. Understanding and simplifying one-shot architecture search. In International conference on machine learning . PMLR, 550-559. [5] G. Bender, H. Liu, B. Chen, G. Chu, S. Cheng, P. Kindermans, and Q. V. Le. 2020. Can Weight Sharing Outperform Random Architecture Search? An Investigation With TuNAS. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE Computer Society, Los Alamitos, CA, USA, 14311-14320. https://doi.org/10.1109/CVPR42600.2020.01433 [6] Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans, and Quoc V Le. 2020. Can weight sharing outperform random architecture search? an investigation with tunas. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 14323-14332. [7] Han Cai, Ligeng Zhu, and Song Han. 2018. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. In International Conference on Learning Representations . [8] Yufan Cao, Tunhou Zhang, Wei Wen, Feng Yan, Hai Li, and Yiran Chen. 2023. Farthest Greedy Path Sampling for Two-shot Recommender Search. arXiv preprint arXiv:2310.20705 (2023). [9] Xuanyi Dong and Yi Yang. 2019. Searching for a Robust Neural Architecture in Four GPU Hours. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . [10] Igor Fedorov, Ryan P Adams, Matthew Mattina, and Paul Whatmough. 2019. SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers. In Advances in Neural Information Processing Systems , H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2019/ file/044a23cadb567653eb51d4eb40acaa88-Paper.pdf [11] Igor Fedorov, Ramon Matas, Hokchhay Tann, Chuteng Zhou, Matthew Mattina, and Paul Whatmough. 2022. UDC: Unified DNAS for Compressible TinyML Models for Neural Processing Units. In Advances in Neural Information Processing Systems , S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 18456-18471. https://proceedings.neurips.cc/paper_files/paper/2022/file/ 753d9584b57ba01a10482f1ea7734a89-Paper-Conference.pdf [12] Chen Gao, Yinfeng Li, Quanming Yao, Depeng Jin, and Yong Li. 2021. Progressive feature interaction search for deep sparse network. Advances in Neural Information Processing Systems 34 (2021), 392-403. [13] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising . 1-9. [14] Kevin Jamieson and Ameet Talwalkar. 2016. Non-stochastic best arm identification and hyperparameter optimization. In Artificial intelligence and statistics . PMLR, 240-248. [15] Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical Reparameterization with Gumbel-Softmax. In International Conference on Learning Representations . [16] Diederik P Kingma and Max Welling. 2022. Auto-Encoding Variational Bayes. arXiv:1312.6114 [stat.ML] [17] Nikita Klyuchnikov, Ilya Trofimov, Ekaterina Artemova, Mikhail Salnikov, Maxim Fedorov, Alexander Filippov, and Evgeny Burnaev. 2022. Nas-bench-nlp: neural architecture search benchmark for natural language processing. IEEE Access 10 (2022), 45736-45747. [18] Ravi Krishna, Aravind Kalaiah, Bichen Wu, Maxim Naumov, Dheevatsa Mudigere, Misha Smelyanskiy, and Kurt Keutzer. 2021. Differentiable NAS Framework and Application to Ads CTR Prediction. arXiv preprint arXiv:2110.14812 (2021). [19] Sheng Li, Garrett Andersen, Tao Chen, Liqun Cheng, Julian Grady, Da Huang, Quoc V Le, Andrew Li, Xin Li, Yang Li, et al. 2023. Hyperscale Hardware Optimized Neural Architecture Search. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 . 343-358. [20] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala. [42] Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Feng Yan, Hai Li, Yiran Chen, and Wei Wen. 2023. NASRec: weight sharing neural architecture search for recommender systems. In Proceedings of the ACM WWW'24 Companion, May 13-17, 2024, Singapore Wen and Liu, et al. Web Conference 2023 . 1199-1207. [43] Tunhou Zhang, Wei Wen, Igor Fedorov, Xi Liu, Buyun Zhang, Fangqiu Han, Wen-Yen Chen, Yiping Han, Feng Yan, Hai Li, et al. 2023. DistDNAS: Search Efficient Feature Interactions within 2 Hours. arXiv preprint arXiv:2311.00231 (2023). [44] Xiangyu Zhaok, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, Chong Wang, Ming Chen, Xudong Zheng, Xiaobing Liu, and Xiwang Yang. 2021. Autoemb: Automated embedding dimensionality search in streaming recommendations. In 2021 IEEE International Conference on Data Mining (ICDM) . IEEE, 896-905. [45] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578 (2016). [46] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. 2018. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition . 8697-8710.",
  "APPENDIX": "",
  "A. Ablation study of RL efficient search": "In Section 3.2.2 we proposed ùëÅùê∏ % ( ùëé ùëñ ) to reduce noise by an inplace baseline in one-shot method and proposed an on/off-policy PG method to improve RL one-shot NAS sample efficiency. To validate the effectiveness of our proposals we present a detailed ablation study. First, we plot the observed ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëé ùëñ ) and ùëÅùê∏ % ( ùëé ùëñ ) during a supernet training in Figure 8 where we can see that ùëÅùê∏ % ( ùëé ùëñ ) has significantly smaller variance compared to ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëé ùëñ ) . Next, we verify that using the less noisy ùëÅùê∏ % ( ùëé ùëñ ) as RL rewards also lead to improved Rankitect RL search. Figure 9 shows the NE gain of supernet when using ùëÅùê∏ % ( ùëé ùëñ ) as RL reward versus using ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëé ùëñ ) ; a negative value indicates that Rankitect with ùëÅùê∏ % ( ùëé ùëñ ) reward consistently samples models with better NE performance than when using ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëé ùëñ ) as reward (note that Figure 9 only optimizes for NE, i.e., ùõº = 0). Since computing ùëÅùê∏ % ( ùëé ùëñ ) (Eq. (4)) requires additional in-place training a baseline model, a natural question to ask is how does this affects Rankitect training/searching efficiency? We found that using ùëÅùê∏ % ( ùëé ùëñ ) as reward does not adversely affect search efficiency (compared to ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëé ùëñ ) ); our speculation is that the training cost of the baseline model is negligible compared to the much larger supernet training cost. To validate the effectiveness of the proposed on/off-policy PG method, we plot the RL reward convergence of REINFORCE and on/off-policy PG in Figure 10. From Figure 10 we can see that on/offpolicy PG achieves faster learning . Moreover, we found similar Figure 7: Offline training NE-gain of 'model 1' compared against a 31.3 √ó model tuned by strong human. 0.01 0.01 4 \"0.03 10B 30B 5OB 9OB Training examples search efficiency despite on/off-policy method receives 50 √ó more policy updates than on-policy REINFORCE. Our speculation is that the computation cost of RL policy update is negligible compared to the much higher supernet training cost. 4 2e-3 Training steps 1e-3 { Training steps Figure 8: Observed NE signal during supernet training (left: ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëé ùëñ ) , right: ùëÅùê∏ % ( ùëé ùëñ ) ) Figure 9: Supernet NE gain of using ùëÅùê∏ % ( ùëé ùëñ ) and ùëÅùê∏ ùëèùëéùë°ùëê‚Ñé ( ùëé ùëñ ) as reward 0.02 0.04 2 0.06 0.08 2B 4B 6B 8B Training examples Figure 10: RL training comparison (red: on/off-policy PG, blue: on-policy REINFORCE) 3e-3 REINFORCE onloff-policy PG 2e-3 1e-3 2 -1e-3 OB 10B 20B 40B Training examples",
  "keywords_parsed": [
    "None"
  ]
}