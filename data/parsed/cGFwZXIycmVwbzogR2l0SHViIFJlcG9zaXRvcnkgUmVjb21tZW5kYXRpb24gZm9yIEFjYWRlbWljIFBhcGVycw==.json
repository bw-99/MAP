{"title": "paper2repo: GitHub Repository Recommendation for Academic Papers", "authors": "Huajie Shao; Dachun Sun; Jiahao Wu; Zecheng Zhang; Aston Zhang; Shuochao Yao; Shengzhong Liu; Tianshi Wang; Chao Zhang; Tarek Abdelzaher", "pub_date": "2020-04-13", "abstract": "GitHub has become a popular social application platform, where a large number of users post their open source projects. In particular, an increasing number of researchers release repositories of source code related to their research papers in order to attract more people to follow their work. Motivated by this trend, we describe a novel item-item cross-platform recommender system, paper2repo, that recommends relevant repositories on GitHub that match a given paper in an academic search system such as Microsoft Academic. The key challenge is to identify the similarity between an input paper and its related repositories across the two platforms, without the benefit of human labeling. Towards that end, paper2repo integrates text encoding and constrained graph convolutional networks (GCN) to automatically learn and map the embeddings of papers and repositories into the same space, where proximity offers the basis for recommendation. To make our method more practical in real life systems, labels used for model training are computed automatically from features of user actions on GitHub. In machine learning, such automatic labeling is often called distant supervision. To the authors' knowledge, this is the first distant-supervised cross-platform (paper to repository) matching system. We evaluate the performance of paper2repo on real-world data sets collected from GitHub and Microsoft Academic. Results demonstrate that it outperforms other state of the art recommendation methods.", "sections": [{"heading": "INTRODUCTION", "text": "This paper proposes a novel item-item recommender system that matches papers with related code repositories (on GitHub [8]) based on a joint embedding of both into the same space, where shorter distances imply a higher degree of relevance. The joint embedding is novel in considering both text descriptions (of papers and repositories) as well as their relations (to other papers and repositories) expressed in appropriately defined graphs.\nThe work is motivated by the growing popularity of GitHub as a platform for collaboration on open source projects and ideas. In recent years, more researchers from academia and industry have shared the source code of their research with others on GitHub. A particularly clear example of that trend in computer science is research on machine learning and data mining, where links are often provided to open source repositories in published papers. This trend is expected to grow, as it is attributed to the increasingly computation-intensive nature of modern scientific discovery. Advances in science are increasingly assisted by complex computing systems and algorithms. Reproducibility, therefore, calls for availing the research community not only of the published scientific results but also of the software responsible for producing them. In the foreseeable future, readers will frequently want to find the source code needed to repeat the experiments when searching for papers of interest on academic search systems, such as Google Scholar and Microsoft Academicfoot_0 . Our new cross-platform recommender system, paper2repo, addresses this need by automatically finding repositories relevant to a paper, even if pointers to the code were not included in the manuscript.\nIn the past few years, cross-platform recommender systems [3,9,29] attracted a lot of attention. Cross-platform recommender systems refer to those that leverage information from multiple different platforms to recommend items for users. Existing work mainly adopts transfer learning to enrich users' preference models with auxiliary information from multiple platforms. For example, Elkahky et al. [3] developed a multi-view deep learning model to jointly learn users' features from their preferred items in different domains of Microsoft service. Such prior work typically uses supervised learning to infer user preferences from their past behavior. In contrast, this paper designs a new item-item cross-platform recommender system, paper2repo, that matches repositories on GitHub to papers in the academic search system, without the benefit of prior access history or explicit labeling. Instead, we use joint embedding to quickly find repositories related to papers of interest, thereby seamlessly connecting Microsoft Academic with GitHub (acquired by Microsoft in June, 2018 2 ).\nA simple candidate solution to our matching problem might be to search for keywords from paper titles in the database of repositories on GitHub. However, it may not recommend sufficiently diverse items to users. For instance, a user who is interested in the textCNN repository 3 may like the Transformer repository 4 as well, because both of them are related to natural language processing. Such a comprehensive set of relevant repositories is hard to infer from paper titles and, in fact, will often not be included in the text of the manuscript either.\nDeep neural networks have recently been used to implement recommender systems. Among the recent advances, a semi-supervised graph convolutional network (GCN) [6,26,27] has been widely used for recommendation because it can exploit both the graph structure and input features together. However, past work applied the approach to single platforms only. Since papers and repositories live on two different platforms, their embeddings generated by traditional GCN are not in the same space. Our contribution lies in extending this approach to joint embedding that bridges the two platforms. The embedding requires constructing appropriate graphs for papers and repositories as inputs to the GCN, as well as selecting the right text features for each. We address these challenges with the following core techniques:\nContext graphs: Relations between papers can be described by a citations graph. Relations between repositories are less obvious. In this paper, we leverage the tags and user starring to connect repositories together. There exists an edge between two repositories if there are overlapped keywords between them, or if they are starred by the same users. In addition, term frequency-inverse document frequency (TF-IDF) [16] is adopted to extract the important tags to construct the repository-repository context graph.\nConstrained GCN model with text encoding. To map the embeddings of papers and repositories to the same space, we develop a joint model that incorporates a text encoding technique into the constrained GCN model based on their contents and graph structures. Since some research papers explicitly name (in the manuscript) their corresponding repositories of source code on GitHub, these papers could be used as bridge papers to connect the two cross platforms. Accordingly, we propose a constrained GCN model to minimize the distance between the embeddings of bridge papers and their original repositories. Moreover, we leverage a text encoding technique with convolutional neural networks (CNN) to encode information from the abstracts of papers, and the descriptions and tags of repositories as input features of the constrained GCN model. As a result, the proposed constrained GCN with text encoding can We conduct experiments to evaluate the performance of the proposed paper2repo system on real-world data sets collected from GitHub and Microsoft Academic. The evaluation results demonstrate that the proposed paper2repo outperforms other compared recommendation methods. It can achieve about 10% higher recommendation accuracy than other methods for the top 10 recommended candidates. To sum up, we make the following contributions to applications, methodology, and experimentation, respectively:\n\u2022 Application: We develop a novel item-item cross-platform recommender system, paper2repo, that can automatically recommend repositories on GitHub to a query paper in the academic search system. \u2022 Methodology: We propose a joint model that incorporates the text encoding technique into the constrained GCN model to jointly learn and map the embeddings of papers and repositories from two different platforms into the same space. \u2022 Experimentation: Evaluation results demonstrate that the proposed paper2repo produces better recommendations than other state of the art methods. The rest of the paper is organized as follows. Section 2 presents the motivation, background, and formulation for our paper2repo problem. In Section 3, we introduce the overall architecture of the paper2repo recommender system. Section 4 evaluates its performance on real-world data sets. Section 5 summarizes related work. Finally, we conclude the paper in Section 6.", "publication_ref": ["b7", "b2", "b8", "b28", "b2", "b5", "b25", "b26", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "PRELIMINARIES", "text": "In this section, we first introduce relevant features of GitHub, and use a real-world example to motivate our work. We then review traditional graph convolutional networks (GCNs) that constitute the analytical basis for our solution approach. Finally, we formulate the problem of GitHub repository recommendation and offer the main insight behind the adopted solution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Background and Motivation", "text": "GitHub is a social application platform, where users can collaborate on open source projects and share code with others. Users can create different repositories (i.e., digital directories) to store files and source code for their research projects. Fig. 1 illustrates an example of the Tensorflow repository for deep learning. The repository contains basic information such as descriptions, tags (also called topics), and user starring. Descriptions refer to a couple of sentences that describe the repository. Tags are keywords used to classify a repository, including its purpose and subject area. Starring a repository represents an expression of interest from a user in the repository. Much like browser bookmarks, stars give users improved shortcut-like access to repository information.\nConsider a simple example to motivate features of our solution approach based on real-world data sets collected from GitHub and Microsoft Academic. Fig. 2 illustrates selected repositories and papers in the two different platforms. Repository R 1 is the original open source code for paper P 1 , ResNeXt [25], which presents a highly modularized network architecture for image classification developed by Facebook Research. The pair of R 1 and P 1 could be used as a bridge to connect the GitHub platform and Microsoft Academic platform. In addition, we find that users who star the repository R 1 also star certain other repositories related to image classification and deep neural networks. We collect 540 users who star repository R 1 and discover that 241 of them (such as user U 1 ) star R 4 as well. Thus, we infer that the two repositories starred by many of the same users are likely related to each other. We can also find repositories in the same subarea or topic that have similar tags. Take repository R 2 and R 3 , for example. These two repositories are related to image classification and they have the same tag \"ImageNet\". In this paper, we use both user starring and tags to construct a context graph for repositories.\nIn the papers domain, related papers are connected together via citation relationships. As illustrated in It has more than 100 citing/cited papers. Most of them are also involved in the same or similar areas as P 1 . For example, paper P 1 cites paper P 2 , P 3 , and P 4 that are related to image classification using deep neural networks. Paper P 3 cites P 2 and P 4 because they share the same research topics on deep residual networks. Hence, related papers can be identified from the paper citation graph. We further find that the more edges (connections) exist among papers, the more related they are. These observations form the basis of constructing the context graph for publications.\nFinally, we feed both the context graphs (describing relations) and content information (describing nodes), such as raw paper abstracts and repository descriptions, into the graph convolutional neural networks framework to perform joint embedding.", "publication_ref": ["b24"], "figure_ref": ["fig_0", "fig_2"], "table_ref": []}, {"heading": "Graph Convolutional Networks", "text": "Graph convolutional networks (GCNs) [2,12] have been widely used in classification and recommendation systems because they encode both the graph neighborhood and content information of nodes to be classified or recommended. The basic idea of GCNs is to learn the embeddings of nodes based on both their input features, X, and graph structures. The graph structure is often denoted by an adjacency matrix, A. A multi-layer GCN forward model could be expressed by:\nH (l +1) = \u03c3 D-1 2 \u00c3 D-1 2 H (l ) W (l ) ,(1)\nwhere \u00c3 = A + I N , A is the adjacency matrix for the input graph structure and I N is the identity matrix, Dii = j \u00c3ij , W (l ) is the weight matrix to be trained, \u03c3 (.) is the activation function such as ReLU, H (l ) is the hidden layer, l is the layer index and H (0) = X is the input features of nodes. Because the embeddings of papers and repositories generated by traditional GCNs for the two different platforms are not in the same space, it is necessary for us to jointly train the corresponding neural networks subject to a constraint that aligns the projections of related papers and repositories onto the same (or neighboring) points in the embedding space. The above challenge leads to the following research problem formulation.", "publication_ref": ["b1", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Statement and Solution Idea", "text": "Suppose there are N p papers in an academic search system, such as Microsoft Academic, and N r repositories on GitHub. Some papers explicitly name their corresponding original repositories of source code on GitHub. We call these papers bridge papers. They can be used to connect the two platforms. For instance, paper P 1 in Fig. 2 is a bridge paper that connects to its original repository R 1 on GitHub. Each paper has an abstract, while each repository has tags and descriptions. Our main task is to learn high-quality embeddings of papers and repositories that can be used for recommendation across the two different platforms based on their content information and graph structures.\nTo this end, we construct two undirected context graphs, one for the paper platform and one for the repository platform, respectively. These graphs are introduced below:\n\u2022 Paper-citation graph: We model the papers and their references as an undirected paper-citation graph, because only the connection between two papers is required in this work.\nWe adopt an adjacency matrix with binary measurements to denote the graph structure. In this graph, each paper, p i , is a node. Each node has an abstract as its input attributes. \u2022 Repository-repository context graph: Since there is no direct citation relationship between repositories, it is more difficult to construct the repository-repository context graph. Motivated by the example in Section 2.1, we leverage tags and user starring to construct this graph. Specifically, as previously motivated in Fig. 2, we add a binary edge between two repositories if they are starred together by at least one user or share at least one term whose TF-IDF score is over a threshold (e.g., 0.3) in their description or tags. While binary edges ignore information available on the degree of overlap (in tags or starring users), we find that the simplification is adequate, especially that (at the end of the day) our embedding also considers full text of paper abstracts, repository descriptions, and tags.\nAfter obtaining these two context graphs, we can combine the graph structures with the input content information of papers and repositories to generate high-quality embeddings. We can then compute similarity scores between neighboring paper and repository embeddings to recommend highly related repositories to a query paper. Namely, the top (i.e., nearest) repository candidates will be recommended to the query papers. Below, we elaborate this general idea in more detail.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "MODEL", "text": "In this section, we first introduce the overall architecture of the proposed paper2repo recommender system, then present techniques for model training.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "paper2repo Architecture", "text": "The key contribution of this paper lies in developing a joint embedding that incorporates both text encoding and constrained GCN to generate embeddings that take into account similarity (between repositories and papers) in both text features and context graph structures.\nFig. 3 shows the overall architecture of the proposed paper2repo system. There are two main modules: text encoding and constrained GCN. The first module tries to develop a text encoding technique to encode the content information of papers and repositories into vectors as inputs of the constrained GCN model. The second module implements the GCN model with an added constraint. The constraint specifies that repositories explicitly mentioned in paper descriptions must be mapped to the same place as the referring paper, thereby linking the mappings of repositories and papers into a single consistent space. We detail these two modules in the following.\nText encoding. Fig. 4 shows the detailed framework of using text encoding to learn the features from both the tags and descriptions of repositories on GitHub. In general, the descriptions of repositories are limited and simple, so it is difficult to learn the overall features of repositories from them. Hence, we add the tags from repositories to enrich the descriptions, because tags often extract some key information about the topics of the repositories on GitHub. Therefore, we design the text encoding module to encode the features of both tags and descriptions. As illustrated in Fig. 4, there are four steps for description encoding and tag encoding of a repository: We first use Step 1 and Step 2 to encode the descriptions of a repository as shown in Fig. 4. Let x i \u2208 R k denote the k-dimensional word vector of the i-th word in the description. The word vector comes from the pre-trained embeddings of Wiki words offered by the GloVe algorithm [15]. The length of each description is fixed to n (padded or cropped where necessary). So a description can be defined by\nx 1:n = x 1 \u2295 x 2 \u2295 . . . \u2295 x n ,(2)\nwhere \u2295 is the concatenation operator that concatenates all the embeddings of words for a sentence. Then we apply filters w \u2208 R hk for the convolution operation in order to learn new features, where h is the window size of words for filters. Each filter computes the convolution to generate a new feature, c i , from a possible window of words x i:i+h-1 , which can be expressed by\nc i = f (w.x i:i+h-1 + b),(3)\nwhere b is a bias parameter and f (.) is a non-linear activation function such as ReLU. As the window slides from x 1:h to x n-h+1:n , the filter yields a feature map\nc = [c 1 , c 2 , . . . , c n-h+1 ].(4)\nAfter obtaining the feature map, a max-over-time pooling operation is adopted to get the maximum value, max{c}, of each output feature map as illustrated in Step 2.\nNext, we implement Step 3 to encode tags. Since there is no sequence for tags, we leverage fully connected layers to learn their features. For each tag, we first use the fastText trick [11] to get its word representation via merging the embeddings of each word. Here, fastText is a simple text encoding technique where word embeddings are averaged into a single vector. As illustrated in Fig. 4, the embedding of \"machine learning\" would be denoted by a new word vector by adding the word vectors of \"machine\" and \"learning\". We then apply fully connected layers to produce the new features whose dimensions are aligned with the number of feature maps for description encoding. After that, feature fusion is adopted in Step 4 to add the new features of tags generated in Step 3 to the produced features in Step 2. Finally, the new features, v j , integrated with description encoding and tag encoding are input of the constrained GCN model. In order to improve the stability of model training, we adopt batch normalization to normalize the new features.\nSimilarly, for abstract encoding of a paper, we can propose the same methodology to learn the new features as the description encoding of a repository in Step 1 and Step 2. For brevity, we will not describe it in detail.\nConstrained GCN model. Since papers and repositories are in two different platforms, the generated embeddings by the traditional GCN are not in the same space. Thus, we propose a constrained GCN model to constrain the embeddings to the same space. Specifically, it leverages the general GCN as the forward propagation model in Equation (1) and minimizes the distance between the embeddings of some bridge papers and their original repositories as a constraint. We use the cosine similarity to measure the distance. In order to compute the cosine similarity distance for the embeddings, we normalize their embeddings as p i and r j , respectively. Let p \u2032 i and r \u2032 i denote the normalized embeddings of the i-th bridge paper and the corresponding original repository, respectively. Then, for each pair of bridge paper and repository, the corresponding constraint could be expressed as:\n1 -p \u2032 i \u22a4 r \u2032 i \u2264 \u03f5,(5)\nPaper abstract", "publication_ref": ["b14", "b10", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Repository tags & descriptions Text Encoding", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Text encoding", "text": "Constrained GCN", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hidden Layers GCN Input", "text": "Embedding Layer Score Ranking  where \u03f5 is a small error term, such as 0.001.\nIn this paper, we adopt weighted approximate-rank pairwise (WARP) loss [24] to train the paper2repo model in order to recommend the target repositories to a query paper at the top of the ranked candidates. Let L be the WARP loss for the labeled pairs of papers and repositories during training, and m be the number of pairs for bridge papers and their original repositories in the training data. Then, the constrained GCN model is defined as:\nmin L subject to m i=1 (1 -p \u2032 i \u22a4 r \u2032 i ) \u2264 \u03f5.(6)\nIn the above (6), the WARP loss function L is defined by:\nL = n k L rank \u2206 p \u22a4 r + |\u2206 -p \u22a4 r + + p \u22a4 r n k | + rank \u2206 p \u22a4 r + ,(7)\nwhere (0 < \u2206 < 1) is the margin hyper-parameter; n k denotes the number of negative examples in a batch during model training; r + and r n k denote the representations of positive and negative repositories, respectively; |t | + is the positive part of t. In addition, L(.) and rank \u2206 p \u22a4 r + will be introduced below. First, L(.) in ( 7) is a transformation function which transforms the rank into a loss, defined as\nL(K) = K j=1 1 j ,(8)\nwhere K denotes the position of a positive example in the ranked list.\nIn addition, rank \u2206 p \u22a4 r + is the margin-penalized rank of positive examples, defined as:\nrank \u2206 p \u22a4 r + = n k I \u2206 -p \u22a4 r + + p \u22a4 r n k > 0 ,(9)\nwhere I (.) is the indicator function that outputs 1 or 0. After defining the WARP loss, the next step is to solve the above non-linear optimization model in (6). One possible solution is to transform it into a dual problem. Based on the method of Lagrange multipliers, we can rewrite the above constrained model (6) as:\nmin L + \u03bb m i=1 (1 -p \u2032 i \u22a4 r \u2032 i ).(10)\nIt is very hard to obtain a closed-form solution that minimizes the objective function (10) due to the nonlinear nature of the optimization problem. Instead, we feed this objective function as a new loss function to the neural network. Convergence of training of this neural network to a solution that meets both the original loss function, L (left term in objective function 10), and the joint embedding constraint (right term in objective function 10), requires that the two terms have comparable weights. Otherwise, learning gradients will favor optimizing the larger term and ignore the smaller. Unfortunately, the loss function L dynamically drops during training, making it difficult to choose the hyper-parameter, \u03bb, to create gradients that properly trade off the loss function, L, and the constraint error. To address this issue, we replace \u03bb in the second term with the varying WARP loss, L, and normalize the constraint error instead of using the total error. Accordingly, our model can be formulated as:\nmin 1 + C e L,(11)\nwhere C e is the average constraint error, defined as:\nC e = 1 2m m i=1 (1 -p \u2032 i \u22a4 r \u2032 i ),(12)\nNote that, C e \u2208 [0, 1]. This is because the cosign similarity for any two vectors, p \u2032 i \u22a4 r \u2032 i , ranges between 1 and -1. Thus, each term in the summation above ranges between 0 and 2. The entire summation ranges between 0 and 2m, and the normalization leads to a value of C e between 0 and 1. Using the mean constraint error, C e , helps keep the scale of the two terms in the objective function (11) comparable. While hypothetically, we can even remove the constant, 1, from objective function (11), we find that keeping it improves numeric stability and convergence.\nIn the new formulation, we no longer need to dynamically adjust the hyper-parameter, \u03bb, for model training. The constrained optimization problem in ( 6) can be solved by minimizing the new loss function (11) by feeding it to the graph neural network.", "publication_ref": ["b23", "b5", "b9", "b10", "b10", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Model Training", "text": "We summarize the main notations used in our model in Table 1. The output of our trained network is the ranked inner products of the embeddings for pairs of papers and repositories. The closer the embeddings of a paper and a repository, the higher their inner product, and the higher the ranking. Given that output, one can identify the top ranked recommended repositories for a given paper (or the top ranked papers for a given repository). Training computes the weights and biases of each convolutional neural network layer, the weights and biases of each fully connected layer (for tag encoding), and the weights and biases of filters for the convolution operation. Besides, we need to tune the number of filters used for text encoding, the filter window h, the number of fully connected layers for tag encoding of repositories, the number of hidden layers of neural networks, and the dimension of output embeddings of each node as well as the margin hyper-parameter for WARP loss. We set the output dimension of representations (embeddings) of each paper and repository equal in order to compute their similarity scores. We then select positive and negative samples as follows:\n\u2022 Positive samples: We use the bridge papers and the corresponding repositories as labeled training data. Let p i be the i-th paper and r j be its highly related repositories. Such pairs, (p i , r j ), constitute positive samples. We further hypothesize that if users, who star repository A, also star repository B more often than C, then (on average) B is more related than C to repository A. Thus, besides bridge repositories, we collect the top T related repositories, ranked by the frequency they are starred by users who star the original bridge repository. In order to get more training data, we also sample some one-hop neighbors of bridge papers combined with the corresponding bridge repositories to be positive examples  (at most T ). Different values of T represent how liberally one interprets \"related\" (and will result in different recommender performance). We can think of this method as a form of distant supervision; an imperfect heuristic to label more samples than what can be inferred from explicit bridge references. It is important to note that we do not use the same heuristic for recommender evaluation. Rather, as we describe in a later section, we use Mechanical Turk workers to evaluate resulting recommendation accuracy, allowing us to understand the efficacy of our distant supervision framework. \u2022 Negative samples: We also introduce negative examples, referring to repositories that are not highly related to a query paper. In this work, we randomly sample n k negative examples of repositories across the entire graph to train the model. We expect the similarity scores of positive examples to be larger than those of negative examples.\n\u00c3 = A p + I N , D = j \u00c3i j , H (0) = X p ; 3 p i \u2190 ReLU D-1 2 \u00c3 D-1 2 H (l ) W (l ) ; 4 p i \u2190 p i / \u2225p i \u2225 2 ; 5 /* For repo embeddings */ 6 \u00c3 = A r + I N , D = j \u00c3i j , H (0) = X r ; 7 r j \u2190 ReLU D-1 2 \u00c3 D-1 2 H (l ) W (l ) ; 8 r j \u2190 r j /\nWe briefly summarize the proposed constrained GCN algorithm in Algorithm 1. In this algorithm, the first l layers learn the hidden vectors (embeddings) of each paper and repository using the GCN algorithm in (1). Then we use the normalized embeddings of paper p i , and repository r j to compute their similarity scores. Additionally, we try to minimize the distance between the embeddings of bridge papers and their original repositories as an added constraint.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "EXPERIMENTS", "text": "We carry out experiments to evaluate the performance of the proposed paper2repo on the real-world data sets collected from GitHub and Microsoft Academic. Further, we show that the proposed model performs well on larger data sets without any hyper-parameter tuning. In addition, we conduct ablation experiments to explore how design choices impact performance. Finally, we conduct a case study to evaluate the effectiveness of the proposed method.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "We collected a repository data set and a research paper data set from GitHub and Microsoft Academic, respectively. Microsoft Academic provides an API for users to query the detailed entities of academic papers, such as paper title, abstract, and venue names. For a proof of concept, we query the top 20 venue names and 8 journals of computer science, such as KDD, WWW, ACL, ICML, NIPS, CVPR and TKDE, to retrieve the entities of 59, 404 raw papers from year 2010 to 2018. After that, we query for the titles of these papers to collect some original open source repositories through the GitHub API. We obtain about 2, 427 original repositories corresponding to the named papers. We define bridge papers as those for which we found a matching repository (which we call the bridge repository). In addition, we collect about 8, 000 popular repositories from users who star the bridge repositories. After data cleaning and preprocessing, we have 32, 029 research papers and 7, 571 repositories, including 2, 107 bridge repositories. In our experiments, we evaluate the performance of the proposed model on both small and full data sets as illustrated in Table 2. Testing and ground truth estimation. To evaluate the accuracy of our recommender system, its results need to be compared to ground truth (that is not part of training data). For that purpose, we asked human graders on Amazon Mechanical Turk (AMT) to evaluate produced recommendations. Specifically, for each pair to be evaluated, we provided the graders with (i) the paper title, abstract, and (ii) the description, ReadMe (if available) and URL the corresponding repository on GitHub. The graders were asked to grade similarity on a three point scale: Score \u00e2\u0102\u01322\u00e2\u0102\u0130 meant that the paper and repository were highly related; score \u00e2\u0102\u01321\u00e2\u0102\u0130 meant that they were somewhat related; and score \u00e2\u0102\u01320\u00e2\u0102\u0130 meant that the pair was not related. Three graders were used per pair.\nAfter labelling, we considered those pairs that received a score of 1 or 2 from all graders to be correct matches. Pairs that received a score of 1 or 0 from all graders (and received at least one zero) were considered to be incorrect matches. Finally, pairs where graders disagreed, receiving scores 0 and 2 together (about 7% result), were graded by an additional grader. An average was then computed. If the average score was less than 1, the pair was considered unrelated (an incorrect match). Otherwise, it was considered a correct match.\nIt remains to describe how we partition data into the training set and test set. We already described how positive and negative training pairs were selected. It is tempting to try and choose test papers and repositories at random. However, due to the large number of papers and repositories available on the respective platforms, this random sampling leads to a predominance of unrelated pairs, unless the test set is very large (which would be very expensive to label). Clearly, restricting testing to predominantly unrelated papers and repositories would artificially reduce the recommender's ability to find matches. In other words, the test set has to include related pairs. Hence, we first selected 580 bridge papers and their one-hop neighbors not present in the training set. We then included their repositories as well as the two hop neighborhood of those repositories according to the context graph. Thus, for each paper in the test set, we included some repositories that are likely related to different degrees, and hundreds of repositories that are likely not related (i.e., repositories related to other papers). This allowed for a much richer diversity in the test set and Mechanical Turk outputs. It is important to underscore that the criteria above were used merely for paper and repository selection for inclusion in the test set. The labeling of the degree of match for pairs returned by the recommender was done strictly by human graders and not by machine heuristics.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Experimental Settings", "text": "Baseline methods. We compare the proposed paper2repo with the algorithms below:\n\u2022 NSCR [23]: This is a cross-domain recommendation framework that combines deep fully connected layers with graph Laplacian to recommend items from information domain to social domains. \u2022 KGCN [21]: This method leverages knowledge graph and graph convolutional neural networks to recommend interested items to users. \u2022 CDL [20]: This is a hybrid recommendation algorithm, which jointly performs deep representation learning for the content information and collaborative filtering for the ratings matrix. \u2022 NCF [7]: This is a neural collaborative filtering model for recommendation, which combines the matrix factorization (MF) and MLP to learn the user-item relationship.\n\u2022 LINE [18]: This is a graph embedding algorithm that uses BFS to learn the embedding of nodes in the graph with unsupervised learning. In order to better perform LINE, we construct the entire graph of papers and repositories via the bridge papers and their original repositories. \u2022 MF [19]: This method is widely used for traditional recommender systems due to its good performance for dense data.\nIt is a supervised algorithm, so we use 50% positive examples as training data and the remaining data as testing data.\n\u2022 BPR [17]: This is an optimized MF model with Bayesian analysis for implicit recommendation. It is a supervised model and we use the same method as MF to do experiments.\nEvaluation measures. In order to evaluate the performance of the proposed paper2repo recommender system, this paper adopts three commonly used information retrieval measures: HR@K (hit ratio), MAP@K (mean average precision), MRR@K (mean reciprocal rank).  In general, HR is used to measure the accuracy about the number of correct repositories recommended by the paper2repo system, while MAP and MRR measure the accuracy of the rank of recommended items.\nModel architecture and parameters. In order to compare the performance of our proposed paper2repo to the other recommender algorithms, we tune the hyper-parameters during model training. We use 90% of training data to train the model and the remaining training data as validation data. After extensive experiments (not shown), we obtain the best parameters for our model below: we set the number of layers to 2 for the graph convolutional neural networks and the size of each layer to 256. The number of fully connected layers is 2 for tags encoding of repositories. The length of each abstract from papers, and description of repositories is fixed to 200 and 50, respectively. For paper abstract encoding, the filter window (h) is 2, 3, 5, 7 with 64 feature maps each. For repository descriptions encoding, its filter windows (h) is 2, 4 with 64 and 32 feature maps, respectively. The pre-trained embeddings of words produced by GloVe are used with size of 200. We set the learning rate to 0.0005. For training, we set the number of positive repositories, T , to 6 given a paper. In addition, we randomly sample 44 negative samples to train the model. We set the margin hyper-parameter, \u2206, to 0.5 in this experiment. For testing, we sample 50 examples include T positive examples and the remaining 50 -T negative example to measure the three metrics above. Our evaluation results are obtained by averaging 3 runs per point.", "publication_ref": ["b22", "b20", "b19", "b6", "b17", "b18", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Performance Comparison", "text": "To understand dependence on scale, we start with a subset of 11, 272 papers and 7, 516 repositories, including 1, 386 pairs of bridge papers and original repositories, to conduct experiments (later we shall use the full data set). We compare the performance of the proposed paper2repo with the competing methods. Fig. 5 illustrates the comparison results of different methods as K increases with three measures: HR@K, MRR@K and MAP@K. We can observe that the proposed paper2repo outperforms the other recommendation algorithms for all the three metrics. This is because our paper2repo encodes both the contents and graph structures of papers and repositories to learn their embeddings. In addition, paper2repo performs better than the cross-domain NCSR algorithm for two reasons. First of all, NCSR leverages spectral embeddings method without using node features in social domain, while our model encodes both content information and graph structures. Secondly, the items in our data sets have more attributes (keywords and tags) than that (20 attributes) in the NCSR paper, making it hard to train the NCSR model. In Fig. 5(c), we also find that the MAP of paper2repo slightly drops. This reason is that more positive repositories are recommended but not at the top of ranking list, leading to lower MAP. Moreover, we can observe that CDL and NCF do not perform well in our case.\nThe main reason is that the number of positive examples for model training is very small such that they suffer from data sparsity as the traditional MF. Besides, KGCN does not work very well because it only adopts one hot embedding without taking node features into account in the paper domain.", "publication_ref": [], "figure_ref": ["fig_6", "fig_6"], "table_ref": []}, {"heading": "Performance on Larger Data Sets", "text": "We conduct another experiment to evaluate the performance of the paper2repo on larger data sets. We do not otherwise perform any data-set-specific tuning. We change the number of papers from 11, 272 to 32, 029, and the number of pairs of bridge papers and their original repositories from 1, 386 to 2, 107. Fig. 6 illustrates the comparison results for two different sizes of data sets. From this figure, it can be observed that the proposed paper2repo performs better on larger data sets than that on small data sets in terms of MAP and MRR, and their hit ratios are comparable. The is because our model can learn more information from different nodes in a large-scale graph.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Ablation Studies", "text": "Effect of Pre-trained Embeddings. We first explore how the settings of pre-trained embeddings impact the performance of the proposed paper2repo. We compare three different cases: (i) fixed pre-trained embeddings; (ii) pre-trained embeddings are used as initialization (not fixed); (iii) fixed and not fixed embeddings are concatenated together. Table 3 illustrates the comparison results for these three cases when K = 10. We can see that when the pretrained embeddings are fixed, it performs better than the other two cases. The main reason is that the pre-trained embeddings are produced from the large Wikipedia corpus by GloVe. The performance of concatenated embeddings seems not very good, because it is more difficult to train the model with more complicated networks. Effect of top T positive repositories . Next, we explore how the number of positive repositories in training, T , influences the performance of paper2repo. In our experiment, the number of positive repositories, T , varies from 3 to 7 with step 1. From Table 4, it can be seen that the performance of paper2repo gradually boosts as T increases from 3 to 6. When T = 6 and 7, their performance are close to each other. Thus, we can find that paper2repo has a better performance when the number of positive repositories in training is large in some degree. In this paper, we use T = 6 to do experiments because most of papers in the testing data labelled by graders have at most 6 positive examples.\nEffect of the Margin Hyper-parameter. We also study the impact of the margin hyper-parameter on the performance of the paper2repo. We change the margin parameter, \u2206, from 0.1 to 0.7 with step 0.1 while keeping the other hyper-parameters unchanged.  We further discuss the advantages and disadvantages of pape2repo.\nAccording to our experiments, we discover that the recommended repositories are relevant to a query paper when there exists substantial user co-starring between bridge repositories and other repositories or when there are multiple overlapped tags between them. The main reason is that two repositories starred by many of the same users or that have multiple overlapped keywords are very likely to involve similar research topics. However, when only few users star both repositories, the recommendation performance is not very good. For example, when two repositories are only starred by a couple of users together, it is hard to judge whether these two repositories are similar or not. As a result, the recommended repositories seem not to be very relevant to the query papers. The need to find a sufficient number of bridge repositories is another limitation. Extensions of this joint embedding framework to domains with no natural analogue to bridge papers/repositories can be a good topic for future investigation. Cold start is one of the most important research topics in recommender systems. As we know, when users release new source code, their repositories have very little user starring in the beginning. Thus, in practice, cold start is an issue similar to lack of sufficient co-starring, discussed above. Our pape2repo is able to partially deal with cold start because, as mentioned in Section 2.3, we construct the repository-repository graph using tags as well. Even if a repository has very few stars, we can still use its tags to construct the repository graph. Therefore, we can still recommend some repositories to the query papers, although the qualify of recommendation will be impacted.\nThe accuracy of our evaluation results is impacted by the accuracy of estimating ground truth. Due to budget constraints, we used three Mechanical Turk graders per item, when they coincided. Grading reliability can be improved by increasing the graders and performing grader training.\nOther limitations include the general applicability of repository recommendations. While software libraries are becoming an increasingly important research enabler, some fields (including theory) are less likely to benefit from this system. The system is also less relevant to research enabled by artifacts not in the public domain. One may also argue that authors will tend to cite relevant repositories as a best practice. This is true, but our recommender system can also uncover subsequently created repositories that impact a given paper, such as novel implementations of relevant libraries, or algorithm implementations on new hardware platforms and accelerators. This is the same reason why one might not exclusively rely on citations in a paper to find other relevant papers, especially if the paper in question is a few years old.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3", "tab_4"]}, {"heading": "RELATED WORK", "text": "This section reviews related work on recommender systems that use deep learning and graph neural networks, especially for crossdomain recommendations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cross-domain Recommendation", "text": "In order to deal with data sparsity and cold start issues in recommender systems, recent research proposed cross-domain recommendations [3,9,10,23] to enrich user preferences with auxiliary information from other platforms. Cross-domain recommendations get rich information on items that users prefer buying based on their history on multiple platforms. For example, some users like to purchase furniture on Walmart and buy clothes on Amazon. So we can recommend furniture to the users on Amazon next time, which can improve the diversity of recommendations. Motivated by this observation, a multi-view deep learning model was proposed by Elkahky et al. [3] to jointly learn the users' features from their past preferred items on different domains. Following this work, Lian et al. [13] further developed a CCCFNet model that integrates collaborative filtering and content-based filtering in a unified framework to address the data sparsity problem. However, these algorithms mainly adopt the idea of transfer learning to boost users' preferences via jointly learning users' items on multiple platforms or domains. In addition, Wang et al. [23] proposed a user-item crossdomain framework, NSCR, to recommend items from information domains to users on social media. However, a limitation is that NSCR is a supervised learning algorithm. In contrast, we propose a item-item cross-domain recommendation framework with a distantsupervised learning model (without human labelling for training).", "publication_ref": ["b2", "b8", "b9", "b22", "b2", "b12", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Recommendation with Graph Neural Networks", "text": "In recent years, graph neural networks have been widely proposed in recommender systems [1,5,14,22,27,28] because they can encode both node information and graph structure. For instance, Fan et al. [5] developed a new framework, called GraphRec, to jointly capture interactions and opinions in the user-item graph. In addition, Shaohua et al. [4] proposed a meta-path guided heterogeneous Graph Neural Network for intent recommendation on an e-commerce platform. However, these efforts apply graph neural networks to recommender sytems on a single platform. Different from prior work, we propose a novel item-item crossdomain recommendation framework that automatically recommends related repositories on GitHub to a query paper in an academic search system, helping users find their repositories of interest quickly by a joint embeddings across platforms.", "publication_ref": ["b0", "b4", "b13", "b21", "b26", "b27", "b4", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS AND FUTURE WORK", "text": "This paper developed an item-item cross-platform recommender system, paper2repo that can automatically recommend repositories on GitHub matching a specified paper in the academic search system. We proposed a joint model that incorporates a text encoding technique into a constrained GCN formulation to generate joint embeddings of papers and repositories from the two different platforms. Specifically, the text encoding technique was leveraged to learn sequence information from paper abstracts and descriptions/tags of repositories. In order to map the representations of papers and repositories onto the same space, we adopted a constrained GCN model that forces the embeddings of bridge papers and their corresponding repositories to be equal as a constraint. Finally, we conducted experiments to evaluate the performance of paper2repo on real-world data sets. Our evaluation results demonstrated that the proposed paper2repo systems can achieve a higher recommendation accuracy than prior methods. In the future, we can extend our method to consider other entities (such as venues and authors), in addition to papers, to construct the knowledge graph. We can improve text embedding (e.g., by use of phrase embedding instead of word embedding). We can also investigate generalizations of our joint embedding, especially to domains with no analogue to bridge papers to serve as mapping constraints.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "An E icient Adaptive Transfer Neural Network for Social-aware Recommendation", "journal": "", "year": "2019", "authors": "Chong Chen; Min Zhang; Chenyang Wang; Weizhi Ma; Minming Li; Yiqun Liu; Shaoping Ma"}, {"ref_id": "b1", "title": "Convolutional neural networks on graphs with fast localized spectral filtering", "journal": "", "year": "2016", "authors": "Micha\u00ebl Defferrard; Xavier Bresson; Pierre Vandergheynst"}, {"ref_id": "b2", "title": "A multi-view deep learning approach for cross domain user modeling in recommendation systems", "journal": "", "year": "2015", "authors": "Ali Mamdouh; Elkahky ; Yang Song; Xiaodong He"}, {"ref_id": "b3", "title": "Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation", "journal": "", "year": "2019", "authors": "Junxiong Shaohua Fan; Xiaotian Zhu; Chuan Han; Linmei Shi; Biyu Hu; Yongliang Ma;  Li"}, {"ref_id": "b4", "title": "Graph Neural Networks for Social Recommendation", "journal": "", "year": "2019", "authors": "Wenqi Fan; Yao Ma; Qing Li; Yuan He; Eric Zhao; Jiliang Tang; Dawei Yin"}, {"ref_id": "b5", "title": "Representation learning on graphs: Methods and applications", "journal": "", "year": "2017", "authors": "Rex William L Hamilton; Jure Ying;  Leskovec"}, {"ref_id": "b6", "title": "Neural collaborative filtering", "journal": "", "year": "2017", "authors": "Xiangnan He; Lizi Liao; Hanwang Zhang; Liqiang Nie; Xia Hu; Tat-Seng Chua"}, {"ref_id": "b7", "title": "Open Source Repository Recommendation in Social Coding", "journal": "", "year": "2017", "authors": "Jyun-Yu Jiang; Pu-Jen Cheng; Wei Wang"}, {"ref_id": "b8", "title": "Social recommendation with cross-domain transferable knowledge", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2015", "authors": "Meng Jiang; Peng Cui; Xumin Chen; Fei Wang; Wenwu Zhu; Shiqiang Yang"}, {"ref_id": "b9", "title": "Little is much: Bridging cross-platform behaviors through overlapped crowds", "journal": "", "year": "2016", "authors": "Meng Jiang; Peng Cui; Nicholas Jing Yuan; Xing Xie; Shiqiang Yang"}, {"ref_id": "b10", "title": "Bag of tricks for efficient text classification", "journal": "", "year": "2016", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Tomas Mikolov"}, {"ref_id": "b11", "title": "Semi-supervised classification with graph convolutional networks", "journal": "", "year": "2016", "authors": "N Thomas; Max Kipf;  Welling"}, {"ref_id": "b12", "title": "CCCFNet: a content-boosted collaborative filtering neural network for cross domain recommender systems", "journal": "", "year": "2017", "authors": "Jianxun Lian; Fuzheng Zhang; Xing Xie; Guangzhong Sun"}, {"ref_id": "b13", "title": "Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks", "journal": "", "year": "2019", "authors": "Namyong Park; Andrey Kan; Xin ; Luna Dong; Tong Zhao; Christos Faloutsos"}, {"ref_id": "b14", "title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"ref_id": "b15", "title": "Using tf-idf to determine word relevance in document queries", "journal": "", "year": "2003", "authors": "Juan Ramos"}, {"ref_id": "b16", "title": "BPR: Bayesian personalized ranking from implicit feedback", "journal": "", "year": "2009", "authors": "Steffen Rendle; Christoph Freudenthaler; Zeno Gantner; Lars Schmidt-Thieme"}, {"ref_id": "b17", "title": "Line: Large-scale information network embedding", "journal": "", "year": "2015", "authors": "Jian Tang; Meng Qu; Mingzhe Wang; Ming Zhang; Jun Yan; Qiaozhu Mei"}, {"ref_id": "b18", "title": "Deep content-based music recommendation", "journal": "", "year": "2013", "authors": "Aaron Van Den Oord; Sander Dieleman; Benjamin Schrauwen"}, {"ref_id": "b19", "title": "Collaborative deep learning for recommender systems", "journal": "", "year": "2015", "authors": "Hao Wang; Naiyan Wang; Dit-Yan Yeung"}, {"ref_id": "b20", "title": "Knowledge graph convolutional networks for recommender systems", "journal": "ACM", "year": "2019", "authors": "Hongwei Wang; Miao Zhao; Xing Xie; Wenjie Li; Minyi Guo"}, {"ref_id": "b21", "title": "KGAT: Knowledge Graph Attention Network for Recommendation", "journal": "", "year": "2019", "authors": "Xiang Wang; Xiangnan He; Yixin Cao; Meng Liu; Tat-Seng Chua"}, {"ref_id": "b22", "title": "Item silk road: Recommending items from information domains to social users", "journal": "", "year": "2017", "authors": "Xiang Wang; Xiangnan He; Liqiang Nie; Tat-Seng Chua"}, {"ref_id": "b23", "title": "Wsabie: Scaling up to large vocabulary image annotation", "journal": "", "year": "2011", "authors": "Jason Weston; Samy Bengio; Nicolas Usunier"}, {"ref_id": "b24", "title": "Aggregated residual transformations for deep neural networks", "journal": "", "year": "2017", "authors": "Saining Xie; Ross Girshick; Piotr Doll\u00e1r; Zhuowen Tu; Kaiming He"}, {"ref_id": "b25", "title": "How powerful are graph neural networks?", "journal": "", "year": "2018", "authors": "Keyulu Xu; Weihua Hu; Jure Leskovec; Stefanie Jegelka"}, {"ref_id": "b26", "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems", "journal": "", "year": "2018", "authors": "Rex Ying; Ruining He; Kaifeng Chen; Pong Eksombatchai; William L Hamilton; Jure Leskovec"}, {"ref_id": "b27", "title": "Hierarchical Temporal Convolutional Networks for Dynamic Recommender Systems", "journal": "ACM", "year": "2019", "authors": "Jiaxuan You; Yichen Wang; Aditya Pal; Pong Eksombatchai; Chuck Rosenburg; Jure Leskovec"}, {"ref_id": "b28", "title": "Cross-domain novelty seeking trait mining for sequential recommendation", "journal": "", "year": "2018", "authors": "Fuzhen Zhuang; Yingmin Zhou; Fuzheng Zhang; Xiang Ao; Xing Xie; Qing He"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An example of the tensorflow repository jointly learn and map the embeddings of papers and repositories from the two platforms into the same space.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig 2, paper P 1 , called ResNeXt, develops a deep residual networks for image classification.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: A real-world example of repository-repository context graph on GitHub and paper citation graph on Microsoft Academic. In this figure, each orange circle represents a repository while each green circle represent a paper. In addition, repository R 1 is the original open source code of paper P 1 , which could be used to bridge the two different platforms.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "\u2022Step 1: convolution with multiple filters; \u2022 Step 2: max-over-time pooling; \u2022 Step 3: transformation with fully connected layers; \u2022 Step 4: feature fusion.", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 :Figure 4 :34Figure 3: Overall architecture of the paper2repo system. There are two main modules: text encoding and constrained GCN model. The inputs of the paper2repo are the content information and graph structures of papers and repositories.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 :5Figure 5: Performance comparison of different methods as the number of candidates K increases.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 6 :6Figure 6: Performance comparison of different sizes of data sets using same parameters.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Main notations in our model. Features of papers and repos X p , X r , adjacency matrix A p , A r for paper-citation graph and repository-repository graph output : Embeddings of paper p i and repository r j", "figure_data": "Notations Descriptionsp ii-th paper among all the papersr j p \u2032 i r \u2032 i Lj-th repositories among all the repositories i-th bridge paper bridge (original) repository of i-th bridge paper WARP lossC eMean of the constraint error\u2206Margin hyper-parameter for WARP lossTNumber of top related repositories for a paperAlgorithm 1: Constrained GCN algorithm1 /* For paper embeddings*/2"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Information summary about paper and repositories data sets.", "figure_data": "Data # papers # repositories # bridge papersSmall11, 2727, 5161, 386Large32, 0297, 5712, 107"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Performance comparison for different settings of pre-trained embeddings when K = 10, T = 6.", "figure_data": "Pre-trained embeddings MAP MRRHRfixed0.399 0.460 0.466not fixed0.330 0.364 0.367fixed & not fixed0.332 0.374 0.374"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Performance comparison for different T when K = 10.", "figure_data": "Top T34567MAP 0.269 0.323 0.350 0.399 0.388MRR 0.291 0.36 0.390 0.460 0.438HR0.409 0.414 0.461 0.466 0.432"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Performance comparison for different margin parameters when K = 10.", "figure_data": "\u22060.10.20.30.40.50.60.7MAP 0.394 0.392 0.394 0.399 0.349 0.307 0.274MRR 0.436 0.456 0.440 0.460 0.400 0.345 0.312HR0.446 0.500 0.466 0.466 0.412 0.381 0.310"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "illustrates the evaluation results for different margin parameters when K = 10. We can observe that the performance of paper2repo gradually increases as \u2206 rises from 0.1 to 0.4, and then gradually drops as \u2206 increases from 0.4 to 0.7. This is to say, when \u2206 = 0.4, it performs best among them. The main reason is that as margin \u2206 is too small, it is hard to separate the positive and negative examples. At the same time, a larger \u2206 may result in higher loss during training. Effect of Number of Bridge Papers. Finally, we explore how the number of bridge papers and repositories affect the recommendation performance of paper2repo. Table6illustrates the comparison results under different ratios of 1, 386 bridge papers in the small data set. We can observe from it that the evaluation metrics, HR, MAP and HRR, gradually rise as the number of bridge papers increases. This is because more bridge papers and repositories can make the embeddings of the similar nodes closer to each other in the graph.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Performance comparison for different number of bridge papers when K = 10.", "figure_data": "Ratio0.40.60.81.0MAP 0.298 0.314 0.341 0.399MRR 0.325 0.364 0.391 0.460HR0.339 0.369 0.379 0.4664.6 Discussion of paper2repo"}], "formulas": [{"formula_id": "formula_0", "formula_text": "H (l +1) = \u03c3 D-1 2 \u00c3 D-1 2 H (l ) W (l ) ,(1)", "formula_coordinates": [3.0, 113.86, 631.02, 180.19, 12.92]}, {"formula_id": "formula_1", "formula_text": "x 1:n = x 1 \u2295 x 2 \u2295 . . . \u2295 x n ,(2)", "formula_coordinates": [4.0, 390.29, 120.67, 167.92, 9.89]}, {"formula_id": "formula_2", "formula_text": "c i = f (w.x i:i+h-1 + b),(3)", "formula_coordinates": [4.0, 395.55, 205.41, 162.65, 10.4]}, {"formula_id": "formula_3", "formula_text": "c = [c 1 , c 2 , . . . , c n-h+1 ].(4)", "formula_coordinates": [4.0, 394.91, 258.65, 163.29, 10.4]}, {"formula_id": "formula_4", "formula_text": "1 -p \u2032 i \u22a4 r \u2032 i \u2264 \u03f5,(5)", "formula_coordinates": [4.0, 412.09, 697.94, 146.12, 13.88]}, {"formula_id": "formula_5", "formula_text": "min L subject to m i=1 (1 -p \u2032 i \u22a4 r \u2032 i ) \u2264 \u03f5.(6)", "formula_coordinates": [5.0, 111.13, 621.88, 182.92, 41.54]}, {"formula_id": "formula_6", "formula_text": "L = n k L rank \u2206 p \u22a4 r + |\u2206 -p \u22a4 r + + p \u22a4 r n k | + rank \u2206 p \u22a4 r + ,(7)", "formula_coordinates": [5.0, 86.7, 683.94, 207.35, 27.7]}, {"formula_id": "formula_7", "formula_text": "L(K) = K j=1 1 j ,(8)", "formula_coordinates": [5.0, 413.09, 412.61, 145.12, 28.67]}, {"formula_id": "formula_8", "formula_text": "rank \u2206 p \u22a4 r + = n k I \u2206 -p \u22a4 r + + p \u22a4 r n k > 0 ,(9)", "formula_coordinates": [5.0, 353.68, 484.26, 204.53, 21.67]}, {"formula_id": "formula_9", "formula_text": "min L + \u03bb m i=1 (1 -p \u2032 i \u22a4 r \u2032 i ).(10)", "formula_coordinates": [5.0, 387.47, 568.61, 170.73, 28.67]}, {"formula_id": "formula_10", "formula_text": "min 1 + C e L,(11)", "formula_coordinates": [6.0, 142.15, 153.5, 151.89, 9.78]}, {"formula_id": "formula_11", "formula_text": "C e = 1 2m m i=1 (1 -p \u2032 i \u22a4 r \u2032 i ),(12)", "formula_coordinates": [6.0, 129.32, 180.32, 164.73, 28.67]}, {"formula_id": "formula_12", "formula_text": "\u00c3 = A p + I N , D = j \u00c3i j , H (0) = X p ; 3 p i \u2190 ReLU D-1 2 \u00c3 D-1 2 H (l ) W (l ) ; 4 p i \u2190 p i / \u2225p i \u2225 2 ; 5 /* For repo embeddings */ 6 \u00c3 = A r + I N , D = j \u00c3i j , H (0) = X r ; 7 r j \u2190 ReLU D-1 2 \u00c3 D-1 2 H (l ) W (l ) ; 8 r j \u2190 r j /", "formula_coordinates": [6.0, 320.59, 288.91, 233.42, 85.45]}], "doi": "10.1145/3366423.3380145"}
