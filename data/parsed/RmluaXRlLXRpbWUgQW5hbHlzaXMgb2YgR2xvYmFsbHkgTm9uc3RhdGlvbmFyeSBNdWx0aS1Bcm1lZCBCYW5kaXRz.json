{"Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits": "", "Junpei Komiyama": "junpei@komiyama.info Stern School of Business New York University 44 West 4th Street, New York, NY, USA", "Edouard Fouch\u00b4 e": "edouard.fouche@kit.edu Institute for Program Structures and Data Organization Karlsruhe Institute of Technology Am Fasanengarten 5, 76131 Karlsruhe, Germany", "Junya Honda": "honda@i.kyoto-u.ac.jp Department of Systems Science, Graduate School of Informatics Kyoto University Yoshida Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan", "Abstract": "We consider nonstationary multi-armed bandit problems where the model parameters of the arms change over time. We introduce the adaptive resetting bandit (ADR-bandit), a bandit algorithm class that leverages adaptive windowing techniques from literature on data streams. We first provide new guarantees on the quality of estimators resulting from adaptive windowing techniques, which are of independent interest. Furthermore, we conduct a finite-time analysis of ADR-bandit in two typical environments: an abrupt environment where changes occur instantaneously and a gradual environment where changes occur progressively. We demonstrate that ADR-bandit has nearly optimal performance when abrupt or gradual changes occur in a coordinated manner that we call global changes. We demonstrate that forced exploration is unnecessary when we assume such global changes. Unlike the existing nonstationary bandit algorithms, ADR-bandit has optimal performance in stationary environments as well as nonstationary environments with global changes. Our experiments show that the proposed algorithms outperform the existing approaches in synthetic and real-world environments. Keywords: multi-armed bandits, adaptive windows, nonstationary bandits, change-point detection, sequential learning.", "1. Introduction": "", "1.1 Motivations": "The multi-armed bandit (MAB; Thompson (1933); Robbins (1952)) is a fundamental model capturing the dilemma between exploration and exploitation in sequential decision making. This problem involves K arms (i.e., possible actions). The decision-maker selects a set of arms at each time step and observes a corresponding reward. The goal of the decision-maker is to maximize the cumulative reward over time. The performance of a bandit algorithm is usually measured via the metric of 'regret': the difference between the obtained rewards and the rewards one would have obtained by choosing the best arms. Minimizing the regret corresponds to maximizing the expected reward. MAB has been used to solve numerous problems, such as experimental clinical design (Thompson, 1933), online recommendations (Li et al., 2010), online advertising (Chapelle and Li, 2011; Komiyama et al., 2015), and stream monitoring (Fouch\u00b4 e et al., 2019). The most widely studied version of this model is the stochastic MAB, which assumes that the reward for each arm is drawn from an unknown but fixed distribution. In the stochastic MAB, several algorithms, such as the upper confidence bound (UCB; Lai and Robbins (1985); Auer et al. (2002)) and Thompson sampling (TS; Thompson (1933)) are known to have \u0398( K log T/ \u2206 min ) regret, 1 which is optimal (Lai and Robbins, 1985). While it is reasonable to assume in some cases that the reward-generating process does not change, as in these algorithms, the distribution of rewards may change over time in many applications. To observe this, we consider the following two examples: Example 1 (Online advertising) A website has several advertisement slots. Based on each user's query, the website decides which ads to display from a set of candidates (i.e., 'relevant advertisements'). Some advertisements are more appealing to a user than others. Each advertisement is associated with a click-through rate (CTR), the number of clicks per view. Websites receive revenue from clicks on advertisements; thus, maximizing the CTR maximizes the revenue. This problem is structured as the bandit problem, with advertisements and clicks as arms and rewards. However, it is well known that the CTR of some advertisements may change over time for several reasons, such as seasonality or changing user interests. In this case, naively applying a stochastic MAB algorithm leads to suboptimal rewards. Example 2 (Predictive maintenance) Correlation often results from physical relationships, for example, between the temperature and pressure of a fluid. A change in these correlations often signals a shift in the system's state (like a fluid solidifying) or a failure or degradation in equipment (such as a leak). In the context of large-scale factory monitoring, keeping track of these correlations can help anticipate issues, thus reducing maintenance costs. Nevertheless, constantly updating the full correlation matrix is computationally unfeasible due to the data's high-dimensionality and dynamic nature. A more efficient solution consists of updating only a few elements of the matrix based on a notion of utility (e.g., high correlation values). The system must minimize the monitoring cost while maximizing the total utility in a possibly nonstationary environment. In other words, correlation monitoring can be considered an instance of the bandit problem (Fouch\u00b4 e et al., 2019), in which pairs of sensors and correlation coefficients correspond to arms and rewards. In such settings, the reward may evolve over time (i.e., it is nonstationary 2 ). The nonstationary MAB (NS-MAB) describes a class of MAB algorithms addressing this particular setting. Most of the NS-MAB algorithms rely on passive forgetting methods based on a sliding window (Garivier and Moulines, 2011) or fixed-time resetting (Gur et al., 2014). Recent work has proposed more sophisticated change detection mechanisms based on adaptive windows (Srivastava et al., 2014) or sequential likelihood ratio tests (Besson et al., 2020). However, the existing methods have several drawbacks, as we describe later.", "1.2 Challenges in nonstationary bandits": "Although change detectors can help MAB algorithms adapt to changing rewards, they often come with costs. Let us consider the case of an abruptly changing environment, where the reward distributions change drastically at some time steps. Previous work by Garivier and Moulines (2011) indicated that the regret of any K -armed bandit algorithm for such a case is 3 \u2126( \u221a T ). This finding implies that the performance of NS-MAB algorithms in stationary (i.e., non-changing) environments is inferior to the performance of standard stochastic MAB algorithms, such as the UCB and TS, because O ( \u221a T ) is much larger than O ( K log T/ \u2206 min ) given a moderate value of \u2206 min . Virtually all NS-MAB algorithms conduct O ( \u221a T ) forced exploration for all the arms, which is the leading factor of regret-no matter what changepoint detection algorithm is used. Another drawback of most existing methods is that they require several parameters that are highly specific to the problem and require unreasonable environmental assumptions, such as the number of changes or an estimation of the amount of 'nonstationarity' in the stream. If such parameters are not set correctly, the actual performance may deviate widely from the given theoretical bounds. This paper solves these issues by introducing adaptive resetting bandit (ADR-bandit) algorithms, a new class of bandit algorithms. Several algorithms, such as UCB and TS, have been established in the stationary case; therefore, we aim to extend them without introducing any forced exploration. For this purpose, we combine them with an adaptive windowing technique. For example, ADR-TS, which is an instance of the ADR-bandit, combines the adaptive windowing with TS. Our method deals with a subclass of changes that we call global changes . Intuitively, if all arms change in a coordinated manner, we can avoid forced exploration to improve performance. This type of change is natural in the predictive maintenance of Example 2, as illustrated in Figure 1, where the nonstationarity often results from changes in the entire system. Our analysis shows that the proposed method has optimal performance for stationary streams and comparable bounds with existing NS-MAB algorithms for abruptly changing and gradually changing environments under the global change assumption. Table 1 compares the bounds from the proposed method with the existing types of NS-MAB algorithms.", "1.3 Contributions": "We articulate our contributions as follows. First, we provide an extensive analysis of adaptive windowing techniques. We focus, on the adaptive windowing (ADWIN) algorithm (Bifet and Gavald` a, 2007), which is still considered state-of-the-art in the data stream literature. The ADWIN algorithm performs well regarding various types of streams (Gama et al., 2014). Bifet and Gavald` a (2007) provided false positive and false negative rate Arms Pairs Sun,00:00 Sun,16:00 Mon,09:00 Tue,02:00 Tue,18:00 Wed,11:00 Thu,04:00 Thu,20:00 Fri,13:00 Sat,06:00 Sat,22:00 T 0 50 100 150 190 ( ) 0 1 Table 1: The table below compares the achievable performance of the existing NS-MAB algorithms, ADR-bandit, and stationary MAB algorithms (e.g., TS and UCB). The value d defines the speed of gradual changes. ADR-bandit has optimal performance in stationary environments as well as nonstationary environments with global changes. Most existing NS-MAB algorithms only handle abrupt changes and do not provide any performance guarantees under gradual changes. Performance bounds under the gradual changes are provided in Besbes et al. (2019); Allesiardo and F\u00b4 eraud (2015); Wei et al. (2016); Wei and Srivastava (2018); Trov` o et al. (2020); Krishnamurthy and Gopalan (2021). Here, \u02dc O is a Landau notation that ignores a polylogarithmic factor. Bioliq bandit algorithms using ADWIN, we need an estimate on the accuracy of the estimator \u02c6 \u00b5 t . Thus, we conduct a finite-time analysis on the estimation error | \u00b5 t -\u02c6 \u00b5 t | (Section 3). As a by-product, this analysis explains why adaptive windowing methodologies perform well in many stream learning problems by bounding the error for abrupt and gradual changes. After formalizing the MAB problem (Section 4), we introduce the ADR (ADaptive Resetting) bandit algorithm (Section 5). Our study builds on a recent paper by Fouch\u00b4 e et al. (2019) that applies the MAB to sensor streams. Although Fouch\u00b4 e et al. (2019) proposed a way to combine the stationary bandit algorithms (e.g., UCB and TS) with adaptive windows, they did not provide any theoretical guarantees for the nonstationary case. We slightly modify their framework to enable rigorous analysis. Our work bridges the divide between these methods' practical application and theoretical comprehension. We believe our analysis and characterization of global changes present novel contributions to the literature on nonstationary bandit algorithms. Finally, we demonstrate the performance of our proposed method concerning synthetic and real-world environments (Section 6). The proposed method outperforms the existing nonstationary bandit methods in stationary streams and abruptly and gradually changing environments.", "2. Related Work": "", "2.1 Change detection": "Change detection is an important problem in data mining. The goal is to detect when the statistical properties (e.g., the mean) of a stream of observations change. Such changes are commonly attributed to the phenomenon known as concept drift (Gama et al., 2014; Lu et al., 2019), which characterizes unforeseeable changes in the underlying data distribution. Detecting such changes is crucial to virtually any monitoring tasks in streams, such as controlling the performance of online machine learning algorithms (Bifet and Gavald` a, 2007) or detecting correlation changes (Seliniotaki et al., 2014). There are numerous methods to detect changes. The fundamental idea is to measure whether the estimated parameters of the current data distribution (e.g., its mean) have changed at any time. In other words, change detection is about separating the signal from the noise (Gama and Castillo, 2006). Change-point detection approaches can be classified into three major categories (cf. Table II in Gama et al. (2014)). These categories are sequential analysis approaches (e.g., CUSUM (Page, 1954) and its variant Page-Hinkley (PH) testing (Hinkley, 1971)), statistical process control (e.g., the Drift Detection Method (DDM) (Gama et al., 2004) and its variants), and monitoring two distributions (e.g., ADWIN (Bifet and Gavald` a, 2007)). In this work, we primarily consider ADWIN (Bifet and Gavald` a, 2007) because it works with any bounded distribution and has a good affinity with online learning analyses. Moreover, ADWIN monitors the mean from a sequence of observations over a sliding window of adaptive size. When ADWIN detects a change between two subwindows, the oldest observations are discarded. Otherwise, the window grows indefinitely. The success of this approach is due to the quasi-absence of parameters, making it highly adaptive. Gon\u00b8 calves et al. (2014) empirically compared ADWIN with other drift detectors. They found that ADWIN is one of the fastest detectors and is one of the only methods to provide false positive and false negative (fp/fn) rate guarantees. Many of the existing methods do not provide any performance guarantees, which is the case for DDM (Gama et al., 2004), EDDM(Baena-Garc\u0131a et al., 2006), and ECDD (Ross et al., 2012) (per Blanco et al. (2015)). Although we believe that one can provide fp/fn rate guarantees for the statistical process control and CUSUM (Page, 1954) approaches by choosing the appropriate parameters, limited discussions are available on the theoretical properties of these algorithms. For a thorough history and comparison of change-detection methods, we refer to the surveys by Gama et al. (2014); Krawczyk et al. (2017).", "2.2 Nonstationary bandits": "In comparison, theoretical performance guarantees (i.e., regret bounds) are much more emphasized in the MAB literature. Traditionally, NS-MAB algorithms are divided into two categories. Active methods actively seek to detect changes, and passive methods do not. Another type of algorithms, known as adversarial bandits (Auer et al., 1995), such as Exp3, can deal with changing environments. However, the guarantee of adversarial bandit algorithms is limited when no arm is consistently good. In the following paragraphs, we discuss the related work on active and passive methods. Active methods: Hartland et al. (2007) proposed Adapt-EvE, which combines the PH test with UCB, and Mellor and Shapiro (2013) suggested change-point TS. However, these two studies do not provide any regret bounds. Liu et al. (2018) proposed CUSUM-UCB and PH-UCB, which combine the UCB algorithm with a CUSUM-based (or PH-based) resetting and forced exploration. They derived a regret bound of \u02dc O ( \u221a MT ) in an abruptly changing environment with M known change points. Cao et al. (2019) suggested the MUCB algorithm, which combines UCB with an adaptive-window-based resetting method with a regret bound of \u02dc O ( \u221a MT ) for an abruptly changing environment. Allesiardo and F\u00b4 eraud (2015) proposed Exp3.R, which combines Exp3 (Auer et al., 1995) with changepoint detection and provides a \u02dc O ( M \u221a T ) regret bound. Although the assumptions are slightly different among algorithms, many of the regret bounds are of the order of \u02dc O ( \u221a MT ) concerning the number of change points M and the number of time steps T . Auer et al. (2019) provided an epoch-based bandit algorithm that does not require the knowledge of M . Moreover, Seznec et al. (2020) applied the adaptive window to the rotting bandit problem where the reward of the arms only decreases. Krishnamurthy and Gopalan (2021) proposed an elimination-based algorithm and analyzed its performance in a two-armed gradually changing environment. They derived a similar regret bound O ( T 1 -d/ 3 ) as this paper, where d is the change-speed parameter that is the same as ours. Besson et al. (2020) proposed GLR-klUCB, a combination of KL-UCB (Garivier and Capp\u00b4 e, 2011) with the Bernoulli Generalized Likelihood Ratio (GLR) test (Maillard, 2017) and forced exploration for an abruptly changing environment. They discussed global and local resets and provided a regret bound of \u02dc O ( \u221a MT ). 5 One of the closest works to ours is Mukherjee and Maillard (2019), where they proposed algorithms UCBL-CPD and ImpCPD by combining UCB with a change point detector. They considered the global changes in the abrupt environment with the same spirit as ours to avoid forced exploration. Still, their main goal is to characterize detailed regret bounds for this environment, while ours is to cover both the global abrupt and global gradual environments by a single algorithm. To be more specific, they obtain both distribution-dependent and independent bounds using a confidence interval established through the Laplace method. This approach circumvents the need for union bounds, leading to a reduced confidence level. 6 In terms of distribution-independent regret, ImpCPD has O ( \u221a MT ) distribution-independent regret bound, while UCBL-CPD and our algorithm have \u02dc O ( \u221a MT ) regret including the polylogarithmic factor in T . On the other hand, most of the effort of this paper is devoted to the analysis for the gradual changes. We think that the application of the Laplace method could potentially improve polylogarithmic factors of our bounds, but we opted to use confidence bounds based on union bounds for simplicity to focus on simultaneously handling two environments. Passive methods: Kocsis and Szepesv\u00b4 ari (2006) proposed Discounted UCB (D-UCB). Garivier and Moulines (2011) suggested Sliding Window UCB (SW-UCB) and analyzed D-UCB to demonstrate that the two algorithms have a \u02dc O ( \u221a MT ) regret bound for abruptly changing environments. Besbes et al. (2019) considered the case of limited variation V and proposed the Rexp3 algorithm with the regret bound \u0398( V 1 / 3 T 2 / 3 ). Wei et al. (2016) generalized the analysis of Besbes et al. (2019) to the case of intervals where each interval has limited variation. Wei and Srivastava (2018) proposed the LM-DSEE and SW-UCB# algorithms with a \u02dc O ( \u221a MT ) regret bound for an abruptly changing environment. The latter algorithm adopts an adaptive window with a limited length. They also analyzed the case of a slowly changing environment and derived a regret bound of SW-UCB#. Chen et al. (2019) proposed a very involved algorithm and stated that it can adapt to abrupt changes as well as gradual changes with a \u0398(min( \u221a MT,V 1 / 3 T 2 / 3 )) regret bound. Trov` o et al. (2020) proposed the Sliding Window TS (SW-TS) algorithm and provided a \u02dc O ( \u221a MT ) regret bound for an abruptly changing environment. They also provided a distribution-dependent regret bound for a gradually changing environment. Our approach is an active one; however, it does strikingly differ from the existing methods. Our algorithm does not sacrifice performance in the stationary case, whereas almost all existing NS-MAB algorithms are exclusively designed for nonstationary environments. In real-world settings, users may not know whether a given stream of data must be considered stationary or not. Similarly, the nature of the stream may also change over time. In such settings, our approach is advantageous, as our experiments show. Although the analysis that follows is quite involved, our algorithms are conceptually simple and aimed at practical use. In comparison, our competitors tend to require more computation and rely on parameters that are difficult to set.", "3. Analysis of ADWIN": "This section analyzes ADWIN (Bifet and Gavald` a, 2007). This algorithm monitors at any time t an estimate of the mean \u02c6 \u00b5 t from a single stream of univariate observations. x t T T T T x t x t x t Abrupt drift Recurring drift Incremental drift Gradual drift Abrupt changes Gradual changes", "3.1 Data streams": "We assume that each observation x t \u2208 [0 , 1] at any time step t is drawn from some distribution with the mean \u00b5 t . The value \u00b5 t is not known to the algorithm, and ADWIN estimates it from a sequence of (possibly noisy) observations S : ( x 1 , x 2 , . . . , x T ). The goal of ADWIN is to maintain an estimator of \u00b5 t at any t based on past observations. Due to concept drift (Gama et al., 2014), the mean \u00b5 t may change over time, so the task is not trivial. The data stream literature (Gama et al., 2014) identifies the four main categories of concept drifts: abrupt, recurring, gradual, and incremental drifts. Figure 2 indicates that the mean \u00b5 t (the dashed line in the figure) changes abruptly and stays the same for some time with abrupt and recurring drifts, whereas \u00b5 t changes gradually over time with gradual and incremental drifts. The actual observations x t can be arbitrarily noisy. We are only interested in the change in the mean \u00b5 t ; thus, we simplify our analysis to the two types of changes. 7 In addition, we also consider stationary streams where the mean \u00b5 t never changes over time. In summary, we consider three types of streams: Definition 1 (Stationary, abruptly changing , and gradually changing streams) 1. A stream is static if \u00b5 t = \u00b5 for all t \u2208 [ T ] and some \u00b5 \u2208 [0 , 1] . 2. A stream is abruptly-changing if \u00b5 t = \u00b5 t +1 except for changepoints T C \u2282 [ T ] . 3. A stream is gradually-changing if | \u00b5 t +1 -\u00b5 t | \u2264 b for all t \u2208 [ T ] and some constant b \u2208 (0 , 1) . These definitions refer to the mean \u00b5 t . The observations { x 1 , x 2 , . . . } can be noisy.", "3.2 The adaptive window algorithm": "Algorithm 1 is the pseudo-code for ADWIN (Bifet and Gavald` a, 2007). The law of large numbers implies that using more observations helps estimate the parameter. However, the", "Algorithm 1 ADWIN": "Require: A univariate stream of values S : ( x 1 , x 2 , . . . ) \u2208 [0 , 1], confidence level \u03b4 \u2208 (0 , 1). 1: W (1) = {} 2: for t = 1 , 2 , . . . do 3: W ( t +1) = W ( t ) \u222a { t } 4: while | \u02c6 \u00b5 W 1 -\u02c6 \u00b5 W 2 | \u2265 \u03f5 \u03b4 cut holds for some split W ( t +1) = W 1 \u222a W 2 do 5: W ( t +1) = W 2 . 6: end while 7: end for \u00b5 t T changepoint (a) changepoint x t T detection time W 1 W 2 (b) detection time \u0338 nature of older observations might be different from more recent observations. To determine a good trade-off between these two effects, ADWIN maintains a window W ( t ) of past time and discards data points outside the window. We omit the index ( t ) of W ( t ) when the time step of interest is clear. At each time step t , ADWINreceives a new observation x t and extends the window W to include the observation. For the current window W , we let \u02c6 \u00b5 W = 1 | W | \u2211 t \u2208 W x t be the corresponding empirical mean. For each new observation, ADWIN tests whether the mean of the underlying distribution has changed. If we can split W into two consecutive disjoint subwindows W 1 \u222a W 2 = W whose empirical means are significantly different (i.e., by some threshold \u03f5 cut ), then ADWIN discards W 1 (i.e., ADWIN 'shrinks' the window): where | W | is the cardinality of a set W . The threshold \u03f5 cut is based on Hoeffding's inequality 8 . At each time step, ADWIN check every possible split of W . The following definition formalizes the notion of a window and the shrinking of the current window. See also Figure 3 for illustration. Definition 2 (Detection times) The time step t is a detection time of ADWIN if | W ( t + 1) | \u2264 | W ( t ) | . In addition, ADWIN 'shrinks' the window at time t if time step t is a detection time. We define the breakpoint as the last round of W 2 from the split W = W 1 \u222a W 2 (Figure 3). We let T d be the set of all detection times. Remark 3 Unlike the set of changepoints T c (Definition 1) that is only defined for an abruptly changing stream and is independent of any underlying algorithm, T d is defined for both abrupt and gradual stream. The detection time is a random variable that is defined via ADWIN.", "3.2.1 Bound for the estimator of the mean \u00b5 t": "Bifet and Gavald` a (2007) derived a bound on the false positive and false negative rates by replacing \u03b4 with \u03b4 \u2032 = \u03b4/ | W ( t ) | , to account for multiple tests 9 . However, the existing analysis is incomplete because it implicitly assumes that the window size | W ( t ) | is deterministic. In this paper, we use \u03b4 instead of \u03b4 \u2032 because it clarifies the analysis. The value of \u03b4 is in accordance with the possible multiplicity. With this aim, we introduce the notion of the window set and split and then introduce a confidence bound that holds with high probability. Definition 4 (Window set) The window set W is the set of all the segments, which are the candidates of the current window W . Definition 5 (Split and estimator in a window) Letting W \u2032 = { t \u2032 , t \u2032 + 1 , . . . , t \u2032\u2032 } , a split W 1 , W 2 of a window W \u2032 \u2208 W is defined as two disjoint subsets of W \u2032 such that W 1 = { t \u2032 , t \u2032 +1 , . . . , m } and W 2 = { m +1 , m +2 , . . . , t \u2032\u2032 } for some m \u2208 W \u2032 \\ { t \u2032\u2032 } . For a window W \u2032 \u2208 W , and its empirical estimator is Proposition 1 (Uniform Hoeffding bound for the window set) Let p > 0 be arbitrary. With probability 1 -2 /T p , we have the following: Proposition 1 is derived using the Hoeffding inequality (Lemma 25, in Appendix C) over all possible |W| \u2264 T 2 windows. The bound above holds regardless of the randomness of the current window W = W ( t ). Moreover, it holds for any W and any split W 1 \u222a W 2 = W . We typically set p = 1 because this ensures a uniform bound with probability 1 -O (1 /T ), and an event with probability O (1 /T ) is usually negligible. Our results are based on this bound.", "3.2.2 Bound for the total error of the mean estimator": "In the following, we provide a bound on the following total error of the estimator \u02c6 \u00b5 W : Definition 6 (Total error) The total error of estimator \u02c6 \u00b5 W is defined as follows: We first derive the error bound of ADWIN for a stationary stream, which directly follows from the false positive rate (Eq. (5)). Theorem 7 (ADWIN in stationary environments) Let the stream be stationary. Then, for ADWIN with \u03b4 = 1 /T 3 , we have the following: thm:adwinstationary Equation (5) with p = 1, together with Definition 2 implies that no shrink occurs with probability 1 -2 /T . Therefore, case where no shrink occurs which completes the proof. The \u02dc O ( \u221a T ) error is the optimal rate because we can only identify the true value of \u00b5 W up to O ( \u221a 1 / | W | ). The error per time step | \u00b5 t -\u00b5 W | is at least \u2126(1 / \u221a | W | ) = \u2126(1 / \u221a t ), and the total error is \u2126( \u2211 t 1 / \u221a t ) = \u2126( \u221a T ). Theorem 7 states that ADWIN can learn from a stationary environment without any unnecessary shrinking. This statement contrasts with such methods as periodic resetting or fixed-size windowing algorithms which discard their entire memory after a fixed period. Having derived the learnability of ADWIN for a stationary stream, we are now interested in the property of ADWIN in the face of a nonstationary stream.", "3.3 Analysis of ADWIN for abrupt changes": "This section derives an error bound of ADWIN in the face of an abrupt change. As informally discussed in Bifet and Gavald` a (2007), ADWIN is able to detect abrupt changes if the changes are infrequent and gaps are detectable. Our results here are even more robust. Somewhat surprisingly, the bound in Theorem 8 does not depend on the detectability of the change. No matter how large or small the changes are and in which interval the changes occur, the algorithm's performance is bounded in terms of the number of changes. Theorem 8 (Error bound of ADWIN under abrupt changes) Let the environment be abrupt with M changepoints. The total error of ADWIN with \u03b4 = 1 /T 3 is bounded as Proof Sketch Let c ( t ) be the number of time steps after the last changepoint. First, we demonstrate that | \u00b5 W -\u00b5 t | = \u02dc O (1 / \u221a c ( t )) because ADWIN otherwise would shrink the window further (i.e., the event B in the proof). Second, the window size | W | is also bounded below. Given a sufficiently high confidence parameter, | W | = O ( c ( t )) and thus | \u00b5 W -\u02c6 \u00b5 W | = \u02dc O (1 / \u221a c ( t )). Combining these two yields the bound of Using the Cauchy-Schwarz inequality yields the bound of Eq. (11). The formal proof is found in Appendix D.1. Remark 9 Theorem 8 implies the optimality of ADWIN under abrupt drift. To observe this, assume that a changepoint exists every T/M time steps. As discussed in Theorem 7, the optimal rate of error for each interval between changepoints is \u02dc \u0398( \u221a T/M ), and the total error should be \u02dc \u0398( M \u00d7 \u221a T/M ) = \u02dc \u0398( \u221a MT ), which matches Theorem 8.", "3.4 Analysis of ADWIN for gradual changes": "In this section, we analyze ADWIN for a gradually changing stream, where the mean \u00b5 t changes slowly with constant b (Definition 1). We consider the error for b = T -d by following the framework of Wei and Srivastava (2018). Theorem 10 (Error bound of ADWIN under gradual changes) Assume that there exists d \u2208 (0 , 3 / 2) such that, the stream is gradually changing with its constant satisfies b \u2264 T -d . Then, the performance of ADWIN with \u03b4 \u2264 1 /T 3 is bounded as follow: Proof Sketch We establish two lemmas in the appendix. Lemma 26 states that the drift | \u00b5 s -\u00b5 t | for any two time steps s, t in the current window W ( t ) is bounded by \u02dc O ( bN + \u221a 1 /N ) for any N \u2265 | W ( t ) | . Moreover, Lemma 27 states that the window is likely to grow until | W | = O ( b -2 / 3 ). Combining these two lemmas yields | \u00b5 s -\u00b5 t | = O ( b 1 / 3 ) = O ( T -d/ 3 ). The formal proof is found in Appendix D.4. Theorem 10 states that, if the change is slow compared with the current scale of interest T then the error per time step Err( T ) /T approaches zero. In this section, we have bounded the total error for streams with abrupt changes (Section 3.3) as well as for streams with gradual changes (Section 3.4). This concludes our discussion on ADWIN. In subsequent sections, we consider the idea of combining ADWIN with the MAB setting where multiple streams are involved, and only a selected subset of streams are observable.", "4. Multi-armed Bandits": "Up to now, we have studied the single-stream learning problem, in which every values of the stream are observed. From this section, we study the Multi-Armed Bandit (MAB) problem (Thompson, 1933; Robbins, 1952). This problem involves K streams, and a forecaster can only observe one of these streams at each time step. We first formalize the problem setting we consider. Let there be K arms. At each time step t = 1 , . . . , T , the forecaster selects an arm I ( t ) \u2208 [ K ], and then receives a reward x I ( t ) ,t from the selected arm. We also use the term 'environment' to describe K streams that generates rewards x i,t . Let ( \u00b5 i,t ) t =[ T ] be the i -th stream. The stationary (resp. abruptly-changing and gradualy-changing) environment is defined as a set of K stationary (resp. abruptly-changing and gradualy-changing) streams, respectively, where these streams are defined in Definition 1. The goal of the forecaster is to maximize the sum of the rewards of the selected arms by using a good algorithm, and the performance of a bandit algorithm is usually measured by the regret, which is defined as the difference between the expected reward of the best arm and the expected reward of the arms selected by the algorithm. That is, and In the K -armed bandit problem, Thompson sampling (Thompson, 1933, TS) and the upper confidence bound (Lai and Robbins, 1985; Auer et al., 2002, UCB) are widely known algorithms that balance exploration and exploitation. Among several variants of UCB, the one that utilizes KL divergence, which is called KL-UCB (Garivier and Capp\u00b4 e, 2011), is known to have a state-of-the-art performance in stationary environments. TS and KL-UCB in our notation are described in Algorithms 2 and 3, respectively. Here, 1 [ A ] = 1 if A or 0 otherwise. Under a changing environment, the performance of these algorithms is no longer guaranteed. The next section extends the MAB framework for the case of abruptly-changing and gradually-changing streams, by combining adaptive window technology with bandit algorithms. These algorithms are designed to deal with stationary environments: The regret of KL-UCB and TS is bounded as follows: Definition 11 (Stationary regret) For a stationary environment (i.e., \u00b5 i,t = \u00b5 i ), for ease of discussion, we assume \u00b5 1 > \u00b5 2 > \u00b7 \u00b7 \u00b7 > \u00b5 K . Let the suboptimality gap be \u2206 i = \u00b5 1 -\u00b5 i . A bandit algorithm has a logarithmic stationary regret if a universal constant C st exists such that \u0338 where Reg( T ) denotes the regret when we run the bandit algorithm. In the literature of multi-armed bandit problem, this bound is often referred to as the distribution-dependent bound in the sense that it depends on \u2206 i . The inverse of \u2206 i defines the hardness of the instance. Note that the logarithmic bound and its first-order dependence on \u2206 -1 i is optimal (Lai and Robbins, 1985; Auer et al., 2002; Kaufmann et al., 2012; Agrawal and Goyal, 2013).", "4.1 Drift-tolerant regret": "While the stationary regret is the most widely studied measure of the performance of stationary bandits, it cannot characterize the performance of a bandit algorithm under a changing environment. The following extends this to the environments with drifts. Definition 12 (Drift-tolerant regret) Assume a nonstationary environment that is abruptly or gradually changing. Let be the gap at t = 1 . Let which is the maximum drift of the arms by time step t . For c > 0 , let where ( x ) + = max( x, 0) . Namely, Reg tr ( T, c ) is the regret where the regret proportional to drift is tolerated. A bandit algorithm has logarithmic drift-tolerant regret if a factor C dt = O (1) exists such that \u0338 Remark 13 Definition 12 is a generalization of the distribution-dependent regret in the literature of stationary bandits that allows a drift proportional to \u03f5 ( t ). Letting \u03f5 ( t ) = 0 immediately derives the stationary regret of Definition 11. As the following lemma characterizes, Thompson sampling (TS, Algorithm 2) is drifttolerant. Lemma 14 Assume that rewards x 1 ,t , x 2 ,t , . . . are binary (i.e., 0 or 1 ). Then, TS has logarithmic drift-tolerant regret. The formal proof is in Appendix E.2. Note that it is not very difficult to derive the drifttolerant regret of KL-UCB by following similar steps as the proof of Lemma 14. Although the drift-tolerant regret characterizes the performance of TS and KL-UCB under nonstationary environments, these algorithms are not good enough to deal with nonstationarity. This is because \u03f5 ( t ) increases over time and can lead to a meaningless bound of \u0398( T ) when \u03f5 ( t ) reaches \u0398(1). A capable nonstationary bandit algorithm should be able to forget past data when it identifies a substantial drift. In the following section, we will merge these base-bandit algorithms with ADWIN.", "5. Analysis of MAB for Globally Nonstationary Environments": "This section proposes the combination of adaptive windowing and bandit algorithms.", "5.1 Adaptive resetting bandit algorithm": "ADS-bandit (adaptive shrinking bandit; Algorithm 4) combines adaptive windows with a base-bandit algorithm such as TS or KL-UCB. In this algorithm, each arm has an associated adaptive window change detector. When such a detector identifies a split in the window W into W 1 and W 2 , it follows the ADWIN procedure (as explained in Section 3) of reducing the window size to retain W 2 .", "Algorithm 4 ADS-bandit": "", "Algorithm 5 ADR-bandit": "We explain the steps of Algorithm 4 with TS as a base-bandit algorithm. Before the first time step, the base-bandit algorithm (Line 1 of Algorithm 2) is initialized. At each time step t = 1 , 2 , . . . , Line 3 of Algorithm 4 runs an iteration of the base-bandit (Line 2 of Algorithm 2) to obtain I ( t ) and corresponding reward x ( t ) = x I ( t ) ,t . Afterwards, Line 4 of Algorithm 4 checks whether a change occurred for each arm. If at least one change is detected, it shrinks window W ( t +1) to W 2 and drops the memory of W 1 . Although we will demonstrate the superior empirical performance of Algorithm 4 in Section 6, it is highly nontrivial to derive a regret bound of Algorithm 4. The ADR-bandit (adaptive resetting bandit; Algorithm 5) addresses the two primary issues for the analysis as follows: First, the updated window W 2 after a shrink can include some amount of observations before the changepoint 10 . To address this issue, ADR-bandit resets the entire window for each detection of a changepoint. Second, it is hard to guarantee a consistent estimation of a single arm. For example, in a data stream with an abrupt change, if only arm 1 is selected prior to the change and only arm 2 is selected after the change, the algorithm is incapable of identifying this change. Although this particular scenario is unlikely to happen, it poses a challenge to the analysis of the algorithm. To address this issue, ADR-bandit introduces the monitoring arm that we continue to draw with some frequency. ADR-bandit (Algorithm 5) works as follows. After each observation, if one of the change point detectors identifies a split in the window W into W 1 and W 2 , it resets the entire algorithm (Line 12) to start with W ( t +1) = \u2205 . Moreover, Algorithm 5 divides the rounds into blocks l = 1 , 2 , 3 , . . . (Line 2). Each block consists of O (2 l -1 ) subblocks, and each subblock consists of KN rounds. Before the beginning of the last subblock of each l , it determines the monitoring arm i ( l ) (Line 17), and then i ( l ) is drawn once in each K rounds during the next block. Here, monitoring arm is chosen to be arg max i N ( l ) i , where be the number of draws of arm i in the current block l . Note that there require some special treatment for l = 1 (Line 15).", "5.2 Properties of Algorithm 5": "As shown in Figure 4, the duration of two monitoring arms i ( l -1) , i ( l ) are overlapped so that the following properties are satisfied: Definition 15 (Monitoring consistency) ADR-bandit with any base-bandit algorithm has the following two properties: \u00b7 Let t \u2265 KN +1 (i.e., any round in l \u2265 2 ). Then, at least one of the arms { i ( l -1) , i ( l ) } satisfies the following: This arm was drawn at least N times before round t and will be drawn at least N times within the next KN rounds. \u00b7 For any block l = 1 , 2 , . . . , there exists at least one arm that is drawn at least N times for each subblock in l . The first property in Definition 15 is used for the abrupt case, and the second property in Definition 15 is used for the gradual case. Moreover, the following theorem states that ADR-bandit is designed so that it does not compromise the performance of the base-bandit algorithm when no reset takes place. Lemma 16 (Regret due to monitoring) Assume that we run Algorithm 5 with a base-bandit algorithm with K \u2265 3 . Assume that Algorithm 5 has not reset itself until round S . Let be the set of rounds where the base-bandit algorithm is evoked (i.e., when Line 9 in Algorithm 5 is executed). Then, the following holds: The proof of Lemma 16 is found in Appendix E.3. The first term is a constant multiplication of the base-bandit regret. As a result, it can fully utilize the logarithmic drift-tolerant regret of the base-bandit algorithm, which we formalized in the following Lemma. Lemma 17 (Constant multiplication) Assume that we run ADR-bandit algorithm with a base-bandit algorithm that has logarithmic drift-tolerant regret (Definition 12). Assume that no reset has occurred 11 until round S . Then, there exists a constant C dt = O (1) that is independent of S , and the regret up to S is bounded as \u0338 Proof of Lemma 17 Definition 12 and Lemma 16 imply \u0338 which implies Eq. (22).", "5.3 Regret bounds of ADR-bandit algorithms": "Having defined the ADR-bandit algorithm and the related properties, we state the main results regarding the performance of these algorithms in stationary abruptly or gradually changing environments. Theorem 18 (Regret bound of ADR-bandit, stationary case) Let the environment be stationary. Let the base-bandit algorithm has a logarithmic stationary regret. Then, the regret of Algorithm 5 with \u03b4 = 1 /T 3 is bounded as follows: Theorem 18 states that change-point detectors do not deteriorate the performance of the base-bandit algorithm. The proof directly follows from the following facts. First, adaptive windows do not make a false reset with at least with probability of 1 -O ( K/T ). Second, the regret of the ADR-bandit algorithm is equal to the base-bandit algorithm when no reset occurs (Lemma 17). For completeness, we provide the proof of Theorem 18 in Appendix E.1. This result has a striking difference from most existing nonstationary bandit algorithms, which have the cost of a forced exploration of \u2126( \u221a T ). In the following sections, we derive the regret bound in abruptly and gradually changing environments. We next derive a regret bound for an abruptly changing environment. We first define the global changes that we assume on the nature of the streams (Definition 19) and then state the regret bound. \u0338 Definition 19 (Globally abrupt changes) Let M be the number of change points and T c = { T 1 , T 2 , . . . , T M } = { t : \u2203 i \u2208 [ K ] \u00b5 i,t = \u00b5 i,t +1 } be the set of changepoints. Moreover, ( T 0 , T M +1 ) = (0 , T ) . The m -th changepoint is a global change with C ab if is finite. Intuitively, C ab is the maximum ratio of changes among arms. Assuming it as a constant indicates that any modification in one arm is proportionally mirrored in all the other arms. Definition 20 (Detectability) For the m -th changepoint, let \u03f5 m = min i | \u00b5 i,m -\u00b5 i,m +1 | . We let U ( \u03f5 ) = (log( T 3 )) / (2 \u03f5 2 ) . The m -th changepoint is detectable if T m -T m -1 , T m +1 -T m \u2265 48 KU ( \u03f5 m ) . The detectability assumption is essentially the same as those in the literature of active nonstationary bandit algorithms, such as Cao et al. (2019) and Besson et al. (2020). Theorem 21 (Regret bound of ADR-bandit under abrupt changes) Let the base-bandit algorithm have a logarithmic drift-tolerant regret bound (Definition 12). Let the environment have M detectable globally abrupt changes (Definitions 19, 20) with C ab > 0 . Then, the regret of the Algorithm 5 is bounded as follows: if \u03b4 = 1 /T 3 , N \u2265 16 U ( \u03f5 m ) , KN \u2264 ( T m +1 -T m ) / 3 holds for all m . Proof Sketch The proof sketch is as follows. \u00b7 With a high probability, the algorithm resets itself within 16 KU (\u2206 m ) time steps after m -th changepoint there exists a corresponding reset. \u00b7 By the sublinear drift-tolerant regret of the algorithm the expected regret between m -th and ( m + 1)-th changepoint is \u02dc O ( \u221a K ( T m +1 -T m )), where we have used the standard discussion of distribution-independent regret (c.f., p32 in Bubeck (2010)) that utilizes the Cauchy-Schwarz inequality. \u00b7 The desired bound is yielded by summing the regret over all change points and another application of the Cauchy-Schwarz inequality on \u2211 m ( T m -T m -1 ) \u2264 T . The formal proof is found in Appendix E.4. Theorem 21 implies that the ADR-bandit learns the environment when the changes are infrequent (i.e., M = o ( T )) and the changes are detectable. This bound matches many existing active algorithms, such as those in Cao et al. (2019); Besson et al. (2020). We next analyze the performance of ADR-bandit with a gradually changing environment. We employ a technique similar to that in Section 3.4 for single-stream ADWIN. We first describe the global changes in a gradually changing environment and then state a regret bound. Assumption 22 (Globally gradual changes) A set of K streams has globally gradual changes if a constant C gr \u2208 (0 , 1] exists such that holds for any two arms i, j \u2208 [ K ] and any two time steps t, s \u2208 [ T ] . Intuitively, Assumption 22 states that all arms are drifting in a coordinated manner. The drift on the mean of an arm is proportional to the drift on the means of other arms. Theorem 23 (Regret bound of the ADR-bandit under gradual changes) Let the base-bandit algorithm have a logarithmic drift-tolerant regret (Definition 12) and have monitoring consistency (Definition 15). Let the environment be gradual with change speed b and d be such that b = T -d . Let the changes be global (Assumption 22). Then the regret of Algorithm 5 with \u03b4 = 1 /T 3 , N = \u02dc \u0398(( bK ) -2 / 3 ) is bounded as: Proof Sketch The proof sketch is as follows. \u00b7 Using Lemma 34, the current window grows until | W | = O ( b -2 / 3 ) with a high probability, which implies that the number of resets (= detection times) M d is at most \u02dc O ( T/b 2 / 3 ) = \u02dc O ( T 1 -2 d/ 3 ). \u00b7 With a high probability, | \u00b5 t -\u00b5 W | = \u02dc O ( bKN )+ \u02dc O ( \u221a log T/N ) always holds (Lemma 33). This term is minimized when N = \u02dc \u0398(( bK ) -2 / 3 ) and | \u00b5 t -\u00b5 W | = \u02dc O (( bK ) 1 / 3 ). \u00b7 Similarly to abrupt case, the regret between the m -th and ( m +1)-th resets is bounded in expectation as \u02dc O ( \u221a K ( T d,m +1 -T d,m )) plus the drift term of \u2211 t \u02dc O (( bK ) 1 / 3 ). \u00b7 By using the Cauchy-Schwarz inequality on \u2211 M d m =1 T d,m +1 -T d,m = T and M d = \u02dc O ( T 1 -2 d/ 3 ), we have \u2211 M d m =1 \u02dc O ( \u221a K ( T d,m +1 -T d,m )) = \u02dc O ( \u221a M d KT ) = \u02dc O ( \u221a KT 1 -d/ 3 ). Moreover, \u2211 t \u02dc O (( bK ) 1 / 3 ) = T \u02dc O (( bK ) 1 / 3 ) = \u02dc O ( K 1 / 3 T 1 -d/ 3 ). The formal proof is in Appendix E.5. Theorem 23 states that ADR-bandit learns the environment when the change is sufficiently slow (i.e., b = o (1)). Unlike most nonstationary bandit algorithms, our algorithm design does not require prior knowledge of b . Remark 24 (Optimality of the regret bound) The rate matches the \u02dc \u0398( V 1 / 3 T 2 / 3 ) lower bound of Besbes et al. (2019) up to a logarithmic order. Under a gradually changing environment, the total variation is V := \u2211 t | \u00b5 i,t -\u00b5 i,t +1 | = bT = O ( T 1 -d ); thus, V 1 / 3 T 2 / 3 = O ( T 1 -d/ 3 ). A recent work by Krishnamurthy and Gopalan (2021) shows the optimality of this rate for gradually changing streams. 12 Note that, unlike Besbes et al. (2019); Krishnamurthy and Gopalan (2021), our guarantee in the nonstationary setting is limited to the case of global changes. In summary, ADR-bandit adapts to both the abruptly and gradually changing environments under mild knowledge on the speed of changes if the base-bandit satisfies has a logarithmic drift-tolerant regret bound. We demonstrate that TS has this bound and consider that it is not very difficult to derive the same bound for KL-UCB.", "6. Experiments": "This section reports the empirical performance of our proposed algorithms in simulations. We release the source code to reproduce our experiments on GitHub 13 .", "6.1 Environments": "The rewards from arm i are drawn from a Bernouilli distribution with parameter \u00b5 i that we determine for each arm. We define the following synthetic environments: \u00b7 Stationary : We define a stationary environment (i.e., with no change) with 100 arms where the mean \u00b5 i = (101 -i ) / 100 never changes over time, i.e., the rewards of arms are evenly distributed between 0 and 1. \u00b7 Gradual : We define a gradually changing environment with 100 arms with \u00b5 i,t = ( T -t +1) T \u00b5 i, 1 + t -1 T (1 -\u00b5 i, 1 ), where \u00b5 i, 1 = (101 -i ) / 100. In this environment, the mean of each arm is the same as the stationary environment at time t = 1, but evolves gradually toward 1 -\u00b5 i, 1 as t increases up to T . \u00b7 Abrupt : We define an abruptly changing environment with 100 arms and \u00b5 i, 1 = (101 -i ) / 100 at t = 1. Unlike the stationary environment, \u00b5 i,t = 1 -\u00b5 i, 1 between round t \u2032 = T/ 3 and round t \u2032\u2032 = 2 T/ 3. In other words, the rewards of every arms change abruptly two times at t = T/ 3 , 2 T/ 3. \u00b7 Abrupt local : We define an environment with 100 arms in which only a portion of the arms change abruptly, i.e., the global change assumption does not hold. As in the other environment, \u00b5 i, 1 = (101 -i ) / 100 at t = 1, but for i \u2208 [1 , 10], \u00b5 i,t = 0 . 5 between round t \u2032 = T/ 3 and round t \u2032\u2032 = 2 T/ 3. In other words, only the top-10 arms change abruptly two times, and the other arms are stationary. The stationary, abrupt, and gradual settings are in line with the assumptions from our algorithms (ADS-TS, ADR-TS, ADR-KL-UCB) as the changes are global. The abrupt local setting violates these assumptions because only a subset of the arms change. In addition, we consider the following real-world scenarios: \u00b7 Bioliq : This dataset was released by Fouch\u00b4 e et al. (2019). The rewards are generated from a stream of measurements between 20 sensors in power plant for a duration of 1 week. The authors considered the task of monitoring high-correlation between those sensors and wanted to use bandit algorithms for efficient monitoring. Each pair of sensors is seen as an arm (i.e., there are 20 \u2217 19 / 2 = 190 arms) and the reward is 1 in case the correlation is greater than some threshold over the last 1000 measurements, otherwise 0. Figure 1 is the reward distribution for this environment, as we can see, arms tend to change periodically together. \u00b7 Zozo (Saito et al., 2021) is a real-world environment where the rewards are generated from an ad recommender on an e-commerce website. We use the data generated by the uniform recommender from the duration of a week and adopted an unbiased offline evaluator (Li et al., 2011) for our experiment 14 . There are 80 different ads. Due to the sparseness of the rewards, we picked 10 arms. Since only a few of such ads are presented at each round to each user, we concatenated together all the ads presented within a second and assigned a reward of 1 to ads on which at least one user clicked. We assign a reward of 0 to ads that were presented, but no user clicked on them. We skip the round whenever the selected was not presented at all. Figure 5 shows the reward distribution of the dataset. All the results are averaged over 100 runs. Note that these datasets match with our motivational examples (Example 1 and 2). Arms Ads Sun,00:00 Sun,06:00 Sun,12:00 Sun,18:00 Mon,00:00 Mon,06:00 Mon,12:00 Mon,18:00 T 0 1 2 3 4 5 6 7 8 9 ( ) 0 1", "6.2 Bandit algorithms": "We compared the following bandit algorithms: \u00b7 ADS-TS is the ADS-bandit algorithm (Algorithm 4) with TS (Algorithm 2) as a base-bandit algorithm. \u00b7 ADR-TS and ADR-KL-UCB are the ADR-bandit algorithm (Algorithm 5) with TS (Algorithm 2) and KL-UCB (Algorithm 3) as a base-bandit algorithm, respectively. \u00b7 Passive algorithms: RExp3 (Besbes et al., 2019) is an adversarial bandit algorithm. Discounted UCB ( D-UCB ) (Garivier and Moulines, 2011) and Sliding Window TS ( SW-TS ) (Trov` o et al., 2020), SW-UCB# (Wei and Srivastava, 2018) (comes in two variants: for Abrupt (A) and Gradual (G) changes) are bandit algorithms with finite memory. \u00b7 Active algorithms: GLR-klUCB (Besson et al., 2020) is an likelihood-ratio based change detector. It comes in two variants: for local and global changes, and we show the results of the latter. M-UCB (Cao et al., 2019) is a bandit algorithm using another change detector as ours. These algorithms involve forced exploration (uniform exploration of size O ( \u221a T )) unlike our algorithms. UCBL-CPD and ImpCPD", "Open Bandit Dataset (Zozo, Inc.)": "(Mukherjee and Maillard, 2019) are algorithms for global abrupt changes that are also free from forced exploration. The original implementation of ADWIN requires O ( | W | K ) times at each round, which can be large in the case of stationary environments. Bifet and Gavald` a (2007) introduced a more computationally efficient version of ADWIN, named ADWIN2, which only checks a logarithmic number of such splits. We adopt ADWIN2 in our simulations.", "6.3 Experimental results": "First, Figure 6 compares ADR-bandit and existing algorithms in the stationary, abrupt, and gradual environments. In the stationary environment, ADR-TS has very low regret, which is consistent with the fact that its regret is logarithmic. In the two nonstationary environments with global changes, ADR-TS outperforms the other algorithms. Second, Figure 7 compares these algorithms in the abrupt local environment. Although the ADR-TS has linear regret as it sometimes do not detect these two local changes, it still outperforms the other algorithms. Finally, Figure 8 compares these algorithms against the Bioliq and Zozo datasets. The proposed algorithm consistently outperforms existing nonstationary bandits algorithms. We provide in Section B additional simulations on the effect of resetting (compared with the shrinking of the original ADWIN), the sensitivity ADR-TS to the value of parameter \u03b4 , and results with L > 1. We find that ADR-TS is not very sensitive to the choice of parameter \u03b4 , as long as it is moderately small.", "7. Conclusions": "We have expanded stationary bandit algorithms to nonstationary settings without requiring forced exploration. To this aim, we first analyzed the theoretical property of adaptive windows in a single-stream setting (Section 3). After that, we combined bandit algorithms (Section 4) with adaptive windowing by introducing ADR-bandit. Unlike existing algorithms, ADR-bandit does not act on the base-bandit algorithm unless change points are detected, and thus, it does not compromise the performance in a stationary environment. We have demonstrated its ability to detect global changes in Section 5 and tested it in simulated and real-world settings through experiments in Section 6. Cumulative regret over time (stationary) 0 2000 4000 6000 8000 10000 /u1D447 0 1000 2000 3000 4000 5000 /u1D445 /u1D447 Ours vs. passive approaches ADS-TS ADR-TS ADR-KL-UCB D-UCB RExp3 SW-TS SW-UCB#-A SW-UCB#-G 0 2000 4000 6000 8000 10000 /u1D447 Ours vs. active approaches ADS-TS ADR-TS ADR-KL-UCB GLR-KL-UCB M-UCB ImpCPD UCBL-CPD 0 2000 4000 6000 8000 10000 /u1D447 0 1000 2000 3000 4000 5000 /u1D445 /u1D447 Ours vs. passive approaches ADS-TS ADR-TS ADR-KL-UCB D-UCB RExp3 SW-TS SW-UCB#-A SW-UCB#-G 0 2000 4000 6000 8000 10000 /u1D447 Ours vs. active approaches ADS-TS ADR-TS ADR-KL-UCB GLR-KL-UCB M-UCB ImpCPD UCBL-CPD Cumulative regret over time (abrupt) 0 2000 4000 6000 8000 10000 /u1D447 0 500 1000 1500 2000 2500 /u1D445 /u1D447 Ours vs. passive approaches ADS-TS ADR-TS ADR-KL-UCB D-UCB RExp3 SW-TS SW-UCB#-A SW-UCB#-G 0 2000 4000 6000 8000 10000 /u1D447 Ours vs. active approaches ADS-TS ADR-TS ADR-KL-UCB GLR-KL-UCB M-UCB ImpCPD UCBL-CPD Cumulative regret over time (gradual) Cumulative regret over time (abrupt local) 0 2000 4000 6000 8000 10000 /u1D447 0 1000 2000 3000 4000 5000 /u1D445 /u1D447 Ours vs. passive approaches ADS-TS ADR-TS ADR-KL-UCB D-UCB RExp3 SW-TS SW-UCB#-A SW-UCB#-G 0 2000 4000 6000 8000 10000 /u1D447 Ours vs. active approaches ADS-TS ADR-TS ADR-KL-UCB GLR-KL-UCB M-UCB ImpCPD UCBL-CPD \u2211 \u2211 0 1000 2000 3000 4000 5000 6000 /u1D447 0 1000 2000 3000 4000 5000 /u1D43E /u1D456 /u1D44B /u1D447 /u1D456 Ours vs. passive approaches ADS-TS ADR-TS ADR-KL-UCB D-UCB RExp3 SW-TS SW-UCB#-A SW-UCB#-G 0 1000 2000 3000 4000 5000 6000 /u1D447 Ours vs. active approaches ADS-TS ADR-TS ADR-KL-UCB GLR-KL-UCB M-UCB ImpCPD UCBL-CPD 0 2000 4000 6000 8000 10000 /u1D447 0 25 50 75 100 125 150 175 200 /u1D43E /u1D456 /u1D44B /u1D447 /u1D456 Ours vs. passive approaches ADS-TS ADR-TS ADR-KL-UCB D-UCB RExp3 SW-TS SW-UCB#-A SW-UCB#-G 0 2000 4000 6000 8000 10000 /u1D447 Ours vs. active approaches ADS-TS ADR-TS ADR-KL-UCB GLR-KL-UCB M-UCB ImpCPD UCBL-CPD Cumulative gain over time (Zozo) Cumulative gain over time (Bioliq)", "References": "", "Globally Nonstationary Bandits": "", "Appendix A. Hyperparameters in Experiments": "The hyperparameters of the algorithms are shown in Table 2.", "Appendix B. Additional Experiments": "This section reports the results of additional experiments. 0 2000 4000 6000 8000 10000 /u1D447 0 100 200 300 400 500 /u1D445 /u1D447 ADS-TS; /u1D6FF =0.1 ADS-TS; /u1D6FF =0.01 ADS-TS; /u1D6FF =0.001 ADS-TS; /u1D6FF =1.0E-4 ADS-TS; /u1D6FF =1.0E-8 ADS-TS; /u1D6FF =1.0E-12 ADR-TS; /u1D6FF =0.1 ADR-TS; /u1D6FF =0.01 ADR-TS; /u1D6FF =0.001 ADR-TS; /u1D6FF =1.0E-4 ADR-TS; /u1D6FF =1.0E-8 ADR-TS; /u1D6FF =1.0E-12 0 2000 4000 6000 8000 10000 /u1D447 0 1000 2000 3000 4000 5000 Window size ADS-TS; /u1D6FF =0.1 ADS-TS; /u1D6FF =0.01 ADS-TS; /u1D6FF =0.001 ADS-TS; /u1D6FF =1.0E-4 ADS-TS; /u1D6FF =1.0E-8 ADS-TS; /u1D6FF =1.0E-12 ADR-TS; /u1D6FF =0.1 ADR-TS; /u1D6FF =0.01 ADR-TS; /u1D6FF =0.001 ADR-TS; /u1D6FF =1.0E-4 ADR-TS; /u1D6FF =1.0E-8 ADR-TS; /u1D6FF =1.0E-12 0 2000 4000 6000 8000 10000 /u1D447 0 200 400 600 800 1000 1200 1400 /u1D445 /u1D447 ADS-TS; /u1D6FF =0.1 ADS-TS; /u1D6FF =0.01 ADS-TS; /u1D6FF =0.001 ADS-TS; /u1D6FF =1.0E-4 ADS-TS; /u1D6FF =1.0E-8 ADS-TS; /u1D6FF =1.0E-12 ADR-TS; /u1D6FF =0.1 ADR-TS; /u1D6FF =0.01 ADR-TS; /u1D6FF =0.001 ADR-TS; /u1D6FF =1.0E-4 ADR-TS; /u1D6FF =1.0E-8 ADR-TS; /u1D6FF =1.0E-12 0 2000 4000 6000 8000 10000 /u1D447 0 2000 4000 6000 8000 Window size ADS-TS; /u1D6FF =0.1 ADS-TS; /u1D6FF =0.01 ADS-TS; /u1D6FF =0.001 ADS-TS; /u1D6FF =1.0E-4 ADS-TS; /u1D6FF =1.0E-8 ADS-TS; /u1D6FF =1.0E-12 ADR-TS; /u1D6FF =0.1 ADR-TS; /u1D6FF =0.01 ADR-TS; /u1D6FF =0.001 ADR-TS; /u1D6FF =1.0E-4 ADR-TS; /u1D6FF =1.0E-8 ADR-TS; /u1D6FF =1.0E-12", "Appendix C. Lemmas": "This section describes the lemmas that are used in the proofs of this paper. The Hoeffding inequality, which is one of the most well-known concentration inequality, provides a high-probability bound of the sum of bounded independent random variables. Lemma 25 (Azuma-Hoeffding inequality) Let x 1 , x 2 , . . . , x n be martingale random variables in [0 , 1] with their conditional mean \u00b5 m = E [ x m | x 1 , x 2 , . . . , x m -1 ] . Let \u00af x = (1 /n ) \u2211 n t =1 x t and \u00af \u00b5 = (1 /n ) \u2211 n t =1 \u00b5 t . Then, Moreover, taking a union bound over the two inequalities yields", "Appendix D. Proofs of ADWIN": "We denote A , B = A\u2229B for two events A , B .", "D.1 Proof of Theorem 8": "The overall idea here is as follows. With sufficiently long time after a changepoint, we can expect that ADWIN shrinks the window. However, there might be some time steps left in the current window W ( t ) even if a shrink occurs. Still, we can show that where c ( t ) be the number of time steps after the changepoint. 15 Events C and D in the following corresponds to the bounds of | \u00b5 W ( t ) -\u00b5 t | and | \u02c6 \u00b5 W ( t ) -\u00b5 W ( t ) | , respectively.", "Proof of Theorem 8": "By Eq. (5), event holds with probability at least 1 -2 /T . For each time step t \u2208 [ T ], let Namely, c ( t ) is the number of the time steps since the latest changepoint. Moreover, for c ( t ) > 2, let and \u2206( c ) = 1 for c \u2264 2. Let In the following we show C holds under B . If | W ( t ) | +1 \u2264 c ( t ), then all the time steps in W ( t ) are after the last change point, which implies \u00b5 W ( t ) = \u00b5 t and thus C . Otherwise, let Clearly | W 1 | , | W 2 | \u2265 c ( t ) / 2 -1 and \u00b5 W 2 = \u00b5 t . ADWIN at time step t -1 shrinks the window until holds. Let n 1 , n 2 = | W 1 | , | W 2 | . (42) In summary, B implies C . Note that under event B , ADWIN never makes a false shrink (i.e., a shrink when \u00b5 W 1 = \u00b5 W 2 ). This implies that between two changepoints a shrink that makes | W ( t ) | +1 < c ( t ) occurs at most once, which leads to the fact that event and thus By using this, the total error is bounded as Here Another application of the Cauchy-Schwarz inequality, combined with D , yields which completes the proof. We next discuss the error bound under gradual drift. The following Lemmas 26 and 27 characterize the accuracy of estimator \u02c6 \u00b5 W under gradual drift. These lemmas are used in the proof of Lemma Theorem 10.", "D.2 Lemma 26": "Lemma 26 Let the stream be gradual with its speed of the change b . Let the parameter of ADWIN be \u03b4 = 1 /T 3 . Then, with probability at least 1 -2 /T , for any two time steps s, s \u2032 in the current window W = W ( t ) , from which it easily follows that Lemma 26 is a strong characterization because \u00b5 s -\u00b5 s \u2032 does not depend on the window size | W | : No matter how long the current window is, \u00b5 s -\u00b5 s \u2032 and thus \u00b5 t -\u00b5 W is bounded in terms of change speed b : In other words, if \u00b5 t -\u00b5 W is larger, ADWIN shrinks the window. Note that the argument here is more general than the (informal) discussion in Bifet and Gavald` a (2007) for gradual drift. The discussion in Bifet and Gavald` a (2007) is limited to the case of linear drift, whereas our result holds for arbitrary drift as long as it moves at most b per time step. Proof of Lemma 26 We first consider the case | W | = CN for some integer C \u2208 N + . We decompose W into C subwindows of equal size N and let W c be the c -th subwindow for c \u2208 [ C ]. For c \u2208 [ C ] \\ { 1 } , let W : c be the joint subwindow of W before W c . Namely, W : c = W 1 \u222a W 2 \u222a\u00b7\u00b7\u00b7\u222a W c -1 . The fact that the window grows to size W without a detection implies that each split W : c \u222a W c satisfies Let c \u2208 [ C ] be arbitrary. By recursively applying Eq.(61) we have . . . (65) which implies for any c, c \u2032 \u2208 [ C ] we have By Eq. (5) we have By the fact that \u00b5 t moves bN within a subwindow of size w , for any s \u2208 W c , s \u2032 \u2208 W c \u2032 , we have By using these, we have The general case of | W | = CN + n for n \u2208 { 0 , 1 , . . . , N -1 } is easily proven by replacing 2 bN with 3 bN since \u00b5 t drift at most bN in n time steps.", "D.3 Lemma 27": "Lemma 26 characterizes the accuracy of the estimator. However, Lemma 26 only holds when window size | W | \u2265 N . When ADWIN shrinks the window very frequently, we cannot guarantee the quality of the estimator \u02c6 \u00b5 W . The following Lemma 27 states that this is not the case: With high probability, the current window grows until | W | = O ( b -2 / 3 ). Lemma 27 (Bound on erroneous shrinking) Let C 1 = \u02dc O (1) be a sufficiently large value that is later defined in Eq. (86) . Let the parameter of ADWIN be \u03b4 = 1 /T 3 . Let the drift speed b be such that C 1 b -2 / 3 \u2264 T . Let Let Then, Proof of Lemma 27 Let C 1 = \u02dc O (1) that we define later in Eq. (86). Let be the subset of the window set W with their sizes at most C 1 b -2 / 3 . It is easy to show that |W C 1 | \u2264 TC 1 b -2 / 3 . Let d be such that T -d = b . Similarly to Eq. (5), by the union bound of the Hoeffding bound over all windows of W C 1 , with probability at least we have holds for all t and any split W 1 \u222a W 2 = W ( t ) : | W 1 | , | W 2 | \u2264 C 1 b -2 / 3 . Let N = C 1 b -2 / 3 . By definition of gradually changing stream, In the following, we show that | W 1 | \u2264 C 1 b -2 / 3 , | W 2 | \u2264 C 1 b -2 / 3 , | \u02c6 \u00b5 W 1 -\u02c6 \u00b5 W 2 | \u2265 \u03f5 \u03b4 cut never occur under Eq. (79) by using a proof-by-contradiction argument. Namely, (by triangular inequality) \u2264 | \u00b5 W 1 -\u00b5 W 2 | + | \u02c6 \u00b5 W 1 -\u00b5 W 1 | + | \u02c6 \u00b5 W 2 -\u00b5 W 2 | (82) which implies which does not hold for In summary, with probability 1 -2 C T , we have P 1 -1 c .", "D.4 Proof of Theorem 10": "Let d be such that b = T -d . Let C 1 be the value defined in Eq. (86) and N 1 = C 1 b -2 / 3 . By using Lemma 27, we bound the number of shrinks. Lemma 28 (Number of shrinks) Let Then, under P c , |T sml d | \u2264 T/N 1 holds. Proof of Lemma 28 P c implies that for any shrink | W 1 | \u2265 N 1 or | W 2 | \u2265 N 1 holds. A shrink of the latter case is not included in T sml d . A shrink of the former case reduces the size of window at least N 1 , and cannot occur more than T/N 1 times. Proof of Theorem 10 Let c br ( t ) = t -max s<t,s \u2208T d s be the number of time steps after the last detection time. We have By Lemma 27, the expectation of the first term of Eq. (88) is bounded as We next bound the second term of Eq. (88). By Eq. (5), holds with probability 1 -2 /T . The expectation of the second term is bounded as: (by Eq. (90) and the fact that c br ( t ) = n for each n occurs at most once between two detection times) where we used b, N 1 = T -d , \u02dc O ( T (2 d ) / 3 ) in the last transformation. Moreover, by using Lemma 26, the expectation of the third term is bounded as which completes the proof.", "Appendix E. Proofs of MAB": "We first clarify the notation: In bandit streams, let Let and its empirical estimate be In the bandit setting, only one arm I ( t ) \u2208 [ K ] is observable at round t , and thus W i \u2282 W . Still, the following inequality holds with probability 1 -2 /T p : Eq. (100), which is the same form as Eq. (5), holds because it corrects T 2 multiplications. The union bound of Eq. (100) over all arms holds with 1 -K/T p .", "E.1 Proof of Theorem 18": "Proof of Theorem 18 Under stationary environment, Eq. (100) with p = 1 implies that ADR-bandit never reset itself with high probability. The regret is bounded by regret of ADR-bandit, Lemma 17", "E.2 Analysis of Thompson sampling under drift": "This section derives a proof of Lemma 14. Without loss of generality, we define \u00b5 1 , 1 \u2265 \u00b5 2 , 1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u00b5 K, 1 at round 1. For ease of notation, let \u00b5 i := \u00b5 i, 1 and \u03f5 = \u03f5 ( T ). In the following, we derive the drift-tolerant regret of TS. Lemma 29 (Number of suboptimal draws) Let c > 0 be arbitrary. Assume that we run TS. The following inequality holds", "Proof of Lemma 29": "We have, Here, if \u0338 then arm 1 is drawn. This fact implies that event { \u2211 T t =1 1 [ \u03b8 I ( t ) ,t \u2264 \u00b5 1 , 1 -\u03f5 ( t ) -c, N 1 ( t ) = n ] \u2265 m } requires that \u0338 holds in the first m rounds of the subsequence \u03c4 1 , \u03c4 2 , \u03c4 3 , . . . , \u03c4 m = { t : max i =1 \u03b8 i,t \u2264 \u00b5 1 , 1 -\u03f5 ( t ) -c, N 1 ( t ) = n } . Letting S \u03b2 ( x ; \u00b5, n ) be the survival function 16 of Beta(1 + \u00b5n, 1 + (1 -\u00b5 ) n ), we have where, to apply Lemma 31, we used the fact that \u02c6 \u00b5 ( n ) 1 is the mean of n independent samples from Bernoulli distributions with means \u00b5 1 , 1 -\u03f5 ( \u03c4 1 ) or more.", "E.2.1 Proof of Lemma 14": "Let ( x ) + = max( x, 0). We have, (116) The first term is bounded in expectation as where we used Lemma 29 in the first transformation with c = \u2206 i / 3. We next bound the second term. Here, By using the Hoeffding inequality, the first term of Eq. (121) is bounded as where we have used the Hoeffding inequality and the fact that \u02c6 \u00b5 i,t is the mean of N i ( t ) samples with each mean no more than \u00b5 i + \u03f5 ( t ). By using Lemma 32, the second term of Eq. (121) is bounded as", "E.2.2 Lemmas for Thompson sampling": "The following lemma is a version of Lemma 4 in Agrawal and Goyal (2013). Lemma 30 Let \u02c6 \u00b5 ( n ) 1 is the mean of n independent samples that are drawn from Bernoulli distributions with their means no less than \u00b5 1 -\u03f5 ( t ) . Then, where D i = 2 c 2 . Lemma 31 Let \u02c6 \u00b5 ( n ) 1 is the mean of n independent samples that are drawn from Bernoulli distributions with their means no less than \u00b5 1 -\u03f5 ( t ) . Then, the following inequality holds: Proof of Lemma 31 The proof of Lemma 31 is straightforward from Lemma 30 by following the similar steps to the problem-independent bound of Theorem 2 in Agrawal and Goyal (2013). The following lemma is well-known in the Beta posterior.", "Lemma 32": "for any c > 0 .", "Proof of Lemma 32": "", "E.3 Regret due to Monitoring": "Proof of Lemma 16 We call each consecutive KN rounds as 'subblock'. For each block l = 1 , 2 , . . . , the number of subblocks is at most 2 l -1 . The l -th monitoring arm i ( l ) is drawn during 2 l +1 subblocks, and at each subblock it is drawn for N rounds (Figure 4). The goal of this theorem is bound the ratio between the regret during monitoring rounds divided by the regret during base-bandit rounds. Let as illustrated in Figure 4. By definition, where T l is the set of rounds in block l . First, for each monitoring arm i ( l ) , we bound the ratio between the number of draws of the monitoring arms divided by the number of draws of base-bandit arms. Namely, for each l . First, we consider the ratio of the regret due to the first monitoring arm i (1) . The arm i (1) is the most drawn arm in the first subblock, which is drawn at least KN/K = N rounds at l = 1 by the base-bandit algorithm. We draw i (1) for 2 N times during block l = 2 as a monitoring arm. Therefore, Eq. (134) for l = 1 is at most two. In the following, we consider the Eq. (134) for l \u2265 2. Since the arm most frequently pulled in the first (2 l -2) N subblocks is chosen as i ( l ) , it has been drawn at least (2 l -2) N times by the base-bandit algorithm before the block l . We draw this arm for (2 l + 1) N times as a monitoring arm. Therefore, the ratio is at most In summary, Eq. (134) is at most 3. In the following, we bound the total regret due to the regret during rounds T base . Namely, Let N base l be the number of draws of arm i ( l ) at block l . By the fact that the regret due to draws of arm i ( l ) at round t is at most \u2206 i ( l ) + \u03f5 ( t ) and one arm is drawn for each round, the regret due to monitoring rounds is at most by using the fact that the regret during the base-bandit rounds is at least N base l \u2206 i ( l ) -\u2211 t \u03f5 ( t ). The total regret, which is the sum of that of base-bandit rounds and monitoring rounds, is bounded as which concludes the proof.", "E.4 Regret in abrupt environment: proof of Theorem 21": "Proof of Theorem 21 Similar to the case of single stream, we define detection times, which is the subset of rounds where the reset occurs. Let T d = { t \u2208 [ T ] : | W ( t +1) | = 0 } be the set of detection times, and M d = |T d | be the number of detection times 17 . Let T d,m be the m -th element of T d . For convenience, let ( T d, 0 , T d,M d +1 ) = (0 , T ). We denote S m = { T d,m +1 , T d,m +2 , . . . , T d,m +1 } for m \u2208 { 0 , . . . , M d } , which is the interval between m -th and ( m +1)-th detection times. We write S m = |S m | . We define an event Event V states that for each changepoint T m \u2208 T c (indexed by m = 1 , 2 , . . . , M = |T c | ), there exists a corresponding detection time T d,m within 16 KU ( \u03f5 m ) time steps. We first show that V holds with high probability by induction. Assume that for every changepoint m \u2032 up to 1 , 2 , . . . , m -1, there exists unique detection time s m \u2032 such that 0 \u2264 s m \u2032 -T m \u2032 \u2264 16 KU ( \u03f5 m ). We show that 0 \u2264 s m -T m \u2264 16 KU ( \u03f5 m ). During t \u2264 T m (these time steps are between m -th and ( m -1)-th changepoint), \u00b5 i,t stays the same. We denote \u00b5 i = \u00b5 i,t during these time steps. Eq. (100) implies that, with probability 1 -2 K/T , for any split W ( t ) = W 1 \u222a W 2 , where W i, 1 = { t \u2208 W 1 : i \u2208 I ( t ) } and W i, 2 = { t \u2208 W 2 : i \u2208 I ( t ) } . This implies ADR-bandit with \u03b4 = T 3 never makes a split before the m -th changepoint (i.e., 0 \u2264 s m -T m ). Let s = T m +16 KU ( \u03f5 m ). Assume that there is no detection between time step T m -1 +16 KU ( \u03f5 m ) and T m +16 KU ( \u03f5 m ). Then for a split W ( s ) = W 1 \u222a W 2 , W 1 = W \u2229 [ T m ], W 2 = W \\ W 1 , we have where we have used s m -1 -T m -1 \u2264 16 KU ( \u03f5 m ) in the first inequality. By assumption of Theorem 21, the first inequality implies By the first property the monitoring consistency (Definition 15), there exists an arm i \u2208 [ K ] (i.e., monitoring arm i ( l ) ) such that Again by Eq. (100) we have By definition of the global changepoint, Combining these yields (by Eq. (149)) (155) which implies that ADR-bandit resets the window at round s . In summary, under Eq. (100) with p = 1, V holds. Therefore, In the following, we bound the regret. Let Namely, Reg m corresponding to the regret between m -th and ( m +1)-th detection times. The regret is decomposed as The regret in the case of V c is bounded as Under V , Let \u00b5 i,m be the mean of i -th arm between m -th changepoint and ( m +1)-th changepoint. Let \u2206 i,m = max j \u00b5 j,m -\u00b5 i \u2265 0 be the corresponding gap. By the definition of abrupt changes, \u03f5 ( t ) \u2264 C ab \u03f5 m after the m -th changepoint and 0 before the changepoint. By Lemma 17 and Eq. (161), we have Moreover, letting N i,m = \u2211 T d,m +1 t = T d,m +1 1 [ I ( t ) = i ], we also have and thus Taking the minimum of Eq. (162) and Eq. (163), it holds that where R i,m = min(\u2206 i,m E [ N i,m ] , log T \u2206 i,m ). In the following, we bound the two terms of Eq. (164) separately. The first term of Eq. (164) is bounded by using standard discussion of distributionindependent regret as follows. We have M \u2264 \u221a log T \u221a K ( T m +1 - T m ) m =0 (by the Cauchy-Schwarz inequality and \u2211 i N i,m = ( T d,m +1 -T d,m )) (168) \u2211 The second term of Eq. (164) is bounded by a similar technique as follows.", "E.5 Regret in gradual environment: proof of Theorem 23": "We first state Lemmas 33 and 34, then go to the proof of Theorem 23. The high-level implications of these lemmas are as follows. Lemma 33 limits the possible amount of drift such that no reset occurs. Lemma 34 bounds the number of resets as M d := |T d | = \u02dc O ( T 1 -2 d/ 3 ). Lemma 33 With probability at least 1 -2 K/T , for any N < | W ( t ) | , t \u2208 [ T ] , i \u2208 [ K ] and s, s \u2032 \u2208 W ( t ) holds. Lemma 33 is a version of Lemma 26 for the bandit setting. This case is much more challenging mainly because the monitoring arm may can change among blocks. Proof of Lemma 33 We use a tuple ( l, c ) to represent the c -th subblock of the l -th block for l = 1 , 2 , . . . , and c = 1 , 2 , . . . , 2 l -1 , that is, the window consisting of ( KN (2 l -1 + c -2) + 1 , . . . , KN (2 l -1 + c -1))-th rounds after the last reset. We write t l (resp. t l ) for the first (resp. last) round of the l -th block, that is, t l = KN (2 l -1 -1) + 1 and t l = KN (2 l -1). The window W :( l,c ) consists of all subblocks before W ( l,c ) (not including W ( l,c ) ). Similarly, subwindow W ( l,c ):( l,c \u2032 ) for c < c \u2032 denotes the joint window consisting of W ( l,c ) , W ( l,c +1) , . . . , W ( l,c \u2032 -1) . Fix an arbitrary l \u2208 N . The second property of the monitoring consistency (Definition 15) implies the following: Assume that no reset occurred up to the l -th block. There exists an arm that is drawn at least N times for each subblock c = 1 , 2 , . . . , 2 l -1 in the l -th block. Moreover, this arm is drawn at least N times in the final subblock of the ( l -1)-th block. By the property above and the fact that no reset occurs up to subblock ( l, c ), there exists i l such that for any l \u2208 N and c \u2208 [2 l -1 ] because otherwise a reset should occur. Then, for any l \u2265 2 and 2 \u2264 c \u2264 2 l -1 we have | Also, for c = 1, (179) is trivial. For l = 1, it is also trivial since c = 1 must hold from c \u2264 2 l -1 . By following the same discussion, we also have By Eq. (100) with p = 1 we have for all l \u2208 N , c \u2208 2 n -1 with probability at least 1 -2 K/T . By the fact that \u00b5 t moves at most bKN within a subblock of size KN , Now, let W l,c (resp. W l \u2032 ,c \u2032 ) be the subwindow that s -th (resp., s \u2032 -th) round belongs to. Here, we assume without loss of generality that s < s \u2032 . Then we have and recursively applying this transformation for l, l +1 , l +2 , . . . , l \u2032 -1, we have where we obtain the last inequality by applying the same transformation as that from (183) to (184). We obtain the desired result since l \u2032 \u2264 log 2 T = O (log T ) = \u02dc O (1). Lemma 34 There exists an event X that holds with probability at least such that, under X c , holds. Lemma 34 is a version of Lemma 27 for the bandit setting. In the following, we derive Lemma 34 for completeness. The steps are very similar to the proof of Lemma 27.", "Proof of Lemma 34 Let": "where C 1 = \u02dc O (1) is a factor that is specified in Eq. (200). Let X = \u22c3 t \u2208 [ T ] ,j \u2208 [ K ] X j ( t ) . Let be the set of windows of size at most C 1 b -2 / 3 . It is easy to show that |W C 1 | \u2264 TC 1 b -2 / 3 . For a fixed window W and i \u2208 [ K ], Hoeffding inequality states that occurs with probability at most 2 \u03b7 . By the union bound of Eq. (189) with \u03b7 -1 = T 2+ d over all i and all windows of W C 1 and all K arms, with probability at least In summary, holds with probability at least 1 -2 C 1 KT -1 . In the following, we show that X never occurs under the event of Eq. (191). In particular we use the proof-by-contradiction argument on the split event | \u02c6 \u00b5 j,W 1 -\u02c6 \u00b5 j,W 2 | \u2265 \u03f5 \u03b4 cut . Eq. (191) implies for all j, t and any split W 1 \u222a W 2 = W ( t ) : | W 1 | , | W 2 | \u2264 C 1 b -2 / 3 . Let N = C 1 b -2 / 3 . By the definition of the gradually changing stream, Assuming that | \u02c6 \u00b5 j,W 1 -\u02c6 \u00b5 j,W 2 | \u2265 \u03f5 \u03b4 cut , it holds that (by triangular inequality) Eq. (194) \u2264 Eq. (198) is equivalent to which cannot hold for Therefore, X j ( t ) never occurs under the event of Eq. (191). Proof of Theorem 23 Let R ( t ) be the most recent reset before t , or R ( t ) = 0 if no reset has occurred yet. Let which is the amount of drift in view of the current ADR-bandit. By Eq. (18) and Lemma 33, the event holds with probability at least 1 -2 K/T . We assume X c \u2229 Y , because it holds with probability 1 -O ( K/T ) and thus the regret under X \u222a Y c is negligible. Eq. (186) states that event X c implies that M d = O ( T 1 -2 d/ 3 ). Event Y with N = \u02dc \u0398(( bK ) -2 / 3 ) implies that for t \u2208 [ T ]. By Eq. (186) we see that M d \u2264 T/ ( C 1 b -2 / 3 ) under X c . Letting T d, 0 = 1 and T d,m = T for m>M d , we first have Let N i,m = \u2211 T d,m +1 t = T d,m +1 1 [ I ( t ) = i ] be the number of draw on arm i between the m -th and ( m +1)-th reset, and \u2206 i,m = max j \u00b5 j,T d,m +1 -\u00b5 i,T d,m +1 be the gap at the first round after the m -th reset. Let H m be the history until the m -th reset. Then the regret between the m -th and ( m +1)-th reset is bounded by Lemma 17 and we have Here, the summation over m for the first term is transformed as follows. where, in the second and third inequalities, we have applied Jensen's inequality on T/ ( C 1 b -2 / 3 ) \u00d7 K elements such that \u2211 T/ ( C 1 b -2 / 3 ) m =1 \u2211 i E [ N i,m ] = T holds. For the summation over the second term we have In summary,"}
