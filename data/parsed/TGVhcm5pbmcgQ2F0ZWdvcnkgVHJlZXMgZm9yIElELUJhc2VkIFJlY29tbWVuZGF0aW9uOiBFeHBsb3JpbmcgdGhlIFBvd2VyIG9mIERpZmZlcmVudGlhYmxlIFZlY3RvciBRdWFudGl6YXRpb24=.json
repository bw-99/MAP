{
  "Learning Category Trees for ID-Based Recommendation: Exploring the Power of Differentiable Vector Quantization": "Qijiong Liu The HK PolyU Hong Kong SAR, China liu@qijiong.work Lu Fan ∗ The HK PolyU Hong Kong SAR, China cslfan@comp.polyu.edu.hk Jiaren Xiao ∗ The HK PolyU Hong Kong SAR, China jiaren.xiao@polyu.edu.hk Jieming Zhu Huawei Noah's Ark Lab Shenzhen, China jiemingzhu@ieee.org Xiao-Ming Wu † The HK PolyU Hong Kong SAR, China xiao-ming.wu@polyu.edu.hk",
  "ABSTRACT": "Category information plays a crucial role in enhancing the quality and personalization of recommender systems. Nevertheless, the availability of item category information is not consistently present, particularly in the context of ID-based recommendations. In this work, we propose a novel approach to automatically learn and generate entity (i.e., user or item) category trees for ID-based recommendation. Specifically, we devise a differentiable vector quantization framework for automatic category tree generation, namely CAGE, which enables the simultaneous learning and refinement of categorical code representations and entity embeddings in an end-to-end manner, starting from the randomly initialized states. With its high adaptability, CAGE can be easily integrated into both sequential and non-sequential recommender systems. We validate the effectiveness of CAGE on various recommendation tasks including list completion, collaborative filtering, and click-through rate prediction, across different recommendation models. We release the code and data 1 for others to reproduce the reported results.",
  "CCS CONCEPTS": "· Information systems → Recommender systems ; Clustering and classification ; Data mining .",
  "KEYWORDS": "recommender system, differentiable vector quantization",
  "ACMReference Format:": "Qijiong Liu, Lu Fan, Jiaren Xiao, Jieming Zhu, and Xiao-Ming Wu. 2024. Learning Category Trees for ID-Based Recommendation: Exploring the Power of Differentiable Vector Quantization. In Proceedings of the ACM Web ∗ Both authors contributed equally to this research (co-second authors). † Corresponding author. 1 https://github.com/Jyonn/Cage/ Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'24, May 13-17, 2024, Singapore, Singapore © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0171-9/24/05...$15.00 https://doi.org/10.1145/3589334.3645484 Fruit Food Item Category Tree Generator Items Users Good Staple Wearing Gaming Recommender System Young Food Girls Click Probability Fruit I-123 User Category Tree Generator Girls Boys Pregnant Business Young Middled-Aged Traditional Method Our Proposed Method U-456 Figure 1: Illustration of our approach in learning category trees for ID-based recommendation. In contrast to traditional methods that solely offer item or user IDs to the recommender system, our approach involves implicit learning of user/item category trees. The category information, encoded as vectors, is subsequently integrated with the user/item ID and provided as input to the recommender system. Conference 2024 (WWW '24), May 13-17, 2024, Singapore, Singapore. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3589334.3645484",
  "1 INTRODUCTION": "Recommendersystems[15, 42, 50] aim to ease the burden of decisionmaking by automatically suggesting personalized item recommendations tailored to a user's preferences and historical behavior. They cater to diverse objectives such as list completion, collaborative filtering, and click-through rate prediction. The varied objectives underscore the importance of devising methodologies that can adapt to different recommendation scenarios and deliver improved recommendations. When crafting recommendation models and algorithms, the integration of categorical information is of paramount importance. Categorical attributes, such as product types [6] and user locations [29, 34], find widespread use due to their ability to capture crucial attributes and establish meaningful connections for users and items. Furthermore, these category features serve to mitigate the WWW'24, May 13-17, 2024, Singapore, Singapore Qijiong Liu, Jiaren Xiao, Lu Fan, Jieming Zhu, & Xiao-Ming Wu Figure 2: Comparison between (a) the traditional three-stage vector quantization pipeline for content-based recommendation and (b) our proposed end-to-end differential vector quantization framework for ID-based recommendation. Vector Quantization Vector Quantization (a) Traditional 3-stage vector quantization pipeline for content-based RS. (b) Our proposed end-to-end di ff erential vector quantization framework for ID-based RS. stop gradient gradient propagation Representation Learning Recommender System Recommender System cold-start problem, providing an additional layer of information for less active (sparsely interacting) entities (i.e., users or items) [2, 11]. This supplementary information is progressively refined by interactions from active users or items during training, thereby aiding less active entities in obtaining more robust representations. However, category features are not always available, since many recommendation datasets only have ID information. To address the absence of category attributes in ID-based recommendation contexts, we propose a novel automatic ca tegory tree ge neration framework, namely CAGE, under the assumption that hierarchical categorical structures commonly arise in recommendation scenarios of varying scales. As illustrated in Figure 1, CAGE serves as the precursor to the recommender system, dynamically constructing an item/user category tree, which incorporates hierarchical categorical knowledge (e.g., 'Good' and 'Gaming') relevant to the current entity (e.g., 'I-123'). The categorical information, encoded as vectors, is provided to the recommender system as auxiliary information alongside the user/item ID. The implementation of CAGE is based on differentiable and cascaded vector quantization (VQ). Previous vector quantization methods for recommendation [37, 55] often rely on meaningful and fixed entity (e.g., user or item) embeddings, derived from side information like content-aware item embeddings using pretrained models. They commonly adopt a three-stage design, as displayed in Figure 2, where representation learning, vector quantization, and recommendation training are carried out separately. However, the lack of side information in ID-based recommendation hinders the generation of meaningful entity embeddings during the initial training phase, making this approach impractical. We tackle this challenge by employing differentiable VQ [46]. It enables dynamic adjustments of both entity embeddings and categorical code vectors from the quantization codebooks through recommendation tasks and quantization constraints, starting from their initial random states and resulting in a robust and stable form. Moreover, it is crucial to select the appropriate level of detail for categories. Employing finely detailed categories could potentially result in data sparsity issues within the recommendation system, while adopting broader, coarse-grained categories may obscure significant differentiations among entities. In light of this challenge, we propose cascaded VQ to construct a category tree with varying levels of granularity. Unlike the single-layer category system commonly used in datasets, our cascaded approach creates a hierarchical taxonomy of categories, offering a more comprehensive representation of entities. To summarize, we introduce CAGE, an automatic category tree generation framework for ID-based recommendation, which offers several notable advantages and capabilities as outlined below. i. End-to-end framework. Differing from the common multistage application of VQ in recommender systems [37, 55], we are the first to explore differentiable VQ as an end-to-end solution for category generation in a scenario without side information, i.e., ID-based recommendation. Such end-to-end training allows for refining and optimizing the categorization for both items and users to align with specific recommendation objectives. ii. Easy adoption and high adaptability. CAGE is a pluggable module that can be conveniently integrated into both sequential and non-sequential recommendation models for accommodating different recommendation scenarios, including list completion, collaborative filtering, and click-through rate prediction. iii. Effectiveness. We conduct a comprehensive evaluation of CAGE on multiple recommendation tasks, including list completion, collaborative filtering, and click-through rate prediction. The evaluation involves tem datasets and a comparison with 14 baseline methods. The results demonstrate the effectiveness of CAGE, showcasing significant improvements across most scenarios. For example, CAGE demonstrates a relative improvement of up to 21.41% over state-of-the-art baselines in list completion tasks.",
  "2 RELATED WORK": "",
  "2.1 Vector Quantization": "Vector quantization (VQ) techniques [12] map a large set of input vectors into a small set of vectors (i.e., a codebook), which have been widely studied in computer vision [1, 38, 54] and speech coding [5, 22] domains. To date, only a few studies explore the potential of vector quantization in recommendation systems. One line of research aims to improve recommendation efficiency [24, 28, 45]. The other line of research focuses on improving recommendation quality [35, 37], as shown by the growing interest from researchers in recent years. The studies on enhancing recommendation quality can be categorized into two paradigms: the commonly used multi-stage approach [19, 37, 55] and an end-to-end training [35] strategy. To our knowledge, AQCL [35] is the only work using end-to-end training for quality improvement in ID-based recommendation, which leverages VQ to assist contrastive learning in the CTR prediction scenario. Differ from AQCL, our proposed CAGE, is the first to introduce VQ for learning categorical knowledge in ID-based recommendation.",
  "2.2 Recommender Systems": "Recommender systems have been extensively studied in various application scenarios including (1) list completion, which aims to continue the user-curated list by sequence generation, (2) collaborative filtering (CF) that makes recommendation based on user-item interactions, and (3) click-through rate (CTR) prediction, which is a crucial task in the ranking phase of the recommendation pipeline. CAGE WWW'24, May 13-17, 2024, Singapore, Singapore Figure 3: Overview of our proposed category tree generation framework (CAGE). Entity Embeddings Quantizer (8-entry Codebook) Quantizer (4-entry Codebook) Quantizer (2-entry codebook) Category Tree Generation (CAGE) update Code Fusion Layer Recommender System argmin selection forward propagation backward propagation categories of current entity Current Entity List completion. Pioneer works based on Markov chain [8, 32, 33] or neural networks [7, 10, 44, 47] are mostly proposed for automatic playlist continuation. In recent years, sequential recommenders [17, 18, 42, 43] have been proposed to generation items autoregressive for list completion task, while FANS [30] uses nonautoregressive generation to improve both quality and efficiency. Collaborative filtering. Collaborative filtering (CF) is widely adopted in the matching phase of the recommendation pipeline. Traditional CF methods [4, 25, 27, 40] employ neighborhood-based approaches and use similarity metrics to identify users or items with similar preferences, which face scalability and sparsity issues in large-scale systems. To overcome these limitations, matrix factorization [26] techniques have been widely adopted to capture underlying preferences and characteristics for personalized recommendation. More recently, deep learning-based methods [15, 16, 39, 56] have emerged to learn complex user-item interactions and capture nonlinear relationships. Click-through rate prediction. In recent years, deep learningbased CTR prediction models [9, 13, 20, 31, 50] have gained popularity. These models have demonstrated improved performance by leveraging the expressive power of neural networks to capture intricate patterns in user-item interactions.",
  "3 PROPOSED FRAMEWORK: CAGE": "Overview. Figure 3 illustrates our automatic category tree generation (CAGE) framework, which is designed to enhance id-based representations of both items and users. It involves a series of cascaded vector quantizers for extracting category-aware information at multiple levels of granularity, thereby forming a tree-like architecture. Specifically, during training, each node (entity or category node) at the lower level searches for the closest category node at the higher level to establish a connection of belonging. Since the node embeddings are dynamically changed during the training process, the connections may be constantly updated. The searching process constructs a path from the bottom layer to the top layer for each entity node, which includes the multi-level category embeddings associated with the entity. Subsequently, the quantized multi-level code vectors are then fused and fed to the recommender system to facilitate downstream recommendation tasks. CAGE and the recommender system are trained together in an end-to-end manner. Once the training process is complete, all the connections are finalized, and the category tree is generated.",
  "3.1 Category Tree Construction": "Before training, both the embedding vectors and code vectors in codebooks are randomly initialized. There are no connections between each entity and the nodes in the bottom-level codebook (the leftmost layer in Figure 3), as well as between adjacent codebooks. 3.1.1 Searching with Vector Quantization. Specifically, we establish links between adjacent layers using the vector quantization technique. Vector quantization [53] targets at grouping similar vectors into clusters by representing them with a small set of prototype vectors. We use a vector quantizer to locate the code vector within a codebook that closely matches the input embedding. The code vector is anticipated to capture and represent the categorical information associated with the input embedding. The vector quantizer includes a 𝑘 -entry codebook C ∈ R 𝑘 × 𝑑 , where 𝑘 is the number of the code vectors and 𝑑 is the dimension of each code vector. Given an input embedding e ∈ R 𝑑 , nearest neighbour search is performed to find the most similar code to e within C :  where c 𝑖 ( 1 ≤ 𝑖 ≤ 𝑘 ) is any code vector in the codebook C , and 𝑗 is the index of the matched code vector c 𝑗 . Note that the current matching pair ( e , c 𝑗 ) is a temporary result that evolves as the training process continually adjusts entity embeddings and code vectors. 3.1.2 Cascaded Linking Flow. CAGE employs a series of cascaded vector quantizers to capture categorical information at multiple levels of granularity. Figure 3 shows an example with three quantizers. Let 𝐻 be the number of quantizers (or levels of granularity). Each quantizer 𝑄 ( 𝑖 ) has a 𝑣 𝑖 -entry codebook C ( 𝑖 ) , where 𝑖 = 1 , 2 , . . . , 𝐻 . The quantizers are interconnected in a cascaded fashion, generating fine-to-coarse code vectors, i.e., 𝑣 𝑖 > 𝑣 𝑗 for 𝑖 < 𝑗 . Each quantizer 𝑄 ( 𝑖 ) takes the output of the previous quantizer (i.e., 𝑄 ( 𝑖 -1 ) ) as WWW'24, May 13-17, 2024, Singapore, Singapore Qijiong Liu, Jiaren Xiao, Lu Fan, Jieming Zhu, & Xiao-Ming Wu input, creating a quantization flow defined as follows.   where c ( 𝑖 ) is the output of quantizer 𝑄 ( 𝑖 ) .",
  "3.2 Code Fusion Layer": "After obtaining multi-level codes (or categories) c ( 𝑖 ) ( 𝑖 = 1 , 2 , · · · , 𝐻 ) , we employ an average pooling operation to combine them into a single vector:  In addition, we use a weighted residual connection to add the original vector e to obtain the final category-aware representation z :  where 𝛼 is a hyperparameter that balances the two terms. We use ' 𝑓 ' to denote the aforementioned operations, i.e., z = 𝑓 ( e ) .",
  "3.3 Tree Back Propagation": "Since the nearest neighbour search algorithm is not differentiable, we utilize the straight-through estimator (STE) [3] to approximate the gradient of each quantizer. Specifically, the gradient of the quantizer is approximated by the gradient of the identity function, which is defined as:  where I is the identity matrix. Therefore, the quantization loss (encouraging the quantizer to select the closest vector in the codebook) can be defined as:  where 𝑠𝑔 is the stop gradient operation. Furthermore, we introduce a commitment loss that encourages the input embedding c ( 𝑖 -1 ) to approach the currently matched code vector c ( 𝑖 ) , which reduces the frequency of link changes, resulting in a smoother training process:  Finally, the overall tree generation loss can be defined by:  where 𝛽 is a hyper-parameter that controls the trade-off between the two losses.",
  "3.4 End-to-end Training": "As mentioned in Section 3.1, the cascaded code vectors and entity embeddings are both initialized randomly prior to training. Initially, entity embeddings lack meaningful information, leading to insignificant quantization outcomes. As training progresses, the CAGE module and the recommender model are jointly optimized through an external recommendation task (i.e., recommendation loss), gradually imbuing entity embeddings with semantic context. Furthermore, internal tree generation loss, 𝐿 cage , is introduced to enhance the clustering effectiveness of the codebook. The enriched category information (code representation) subsequently contributes to improved recommendation performance for entity embeddings in subsequent training batches. This cyclic iteration results in a double-helix refinement process, where the codebook and entity embeddings continuously enhance their representation learning throughout the training process.",
  "4 APPLICATIONS": "CAGE can be effortlessly and seamlessly integrated into a variety of recommendation models to enhance recommendation performance, making it highly adaptable and suitable for a wide range of recommendation scenarios. In the following, we demonstrate how CAGE can be applied to non-sequential (e.g., collaborative filtering and CTR prediction) and sequential recommenders (e.g., list completion).",
  "4.1 CAGE for Non-sequential Recommenders: Incorporating both Item and User CAGE": "4.1.1 Scenario 1: Collaborative Filtering. Given a set of users U and a set of items V , the collaborative filtering agent [15, 16, 39, 56] aims to estimate the user-item interaction matrix R , where each weight 𝑟 𝑢𝑖 represents the preference or rating of user 𝑢 for item 𝑖 . The matrix R is typically sparse, as not all users rate or interact with all items. Therefore, the task is to fill in these missing weights by existing ratings. 4.1.2 Scenario 2: Click-Through Rate Prediction. In contrast to collaborative filtering, which is typically employed in the matching phase of the recommender pipeline, CTR prediction is a rankingbased task that aims to predict the probability of a user clicking on a particular item. The input to the CTR model is a user-item pair, and the output is a probability score indicating the likelihood of the user clicking on the item. Existing deep CTR prediction models [13, 20, 36, 50] are typically designed to learn feature interactions from the raw input features such as user ID, item ID, and other statistical features if exists. 4.1.3 Integration. In the non-sequential scenario, both user and item representations are obtained as embedding matrices and play integral roles in the training process. Therefore, we can incorporate two CAGE modules (i.e., Item CAGE 𝑓 ( 𝑖 ) and User CAGE 𝑓 ( 𝑢 ) ) on dual sides to extract hierarchical category knowledge, denoted as:  Finally, the loss function can be calculated by:   where Φ is the non-sequential recommender, 𝑙 is the label, and 𝜔 𝑞 is a hyperparameter balancing the internal tree generation loss and external recommendation loss. CAGE WWW'24, May 13-17, 2024, Singapore, Singapore",
  "4.2 CAGE for Sequential Recommenders: Incorporating Item CAGE with an Additional Category Tree Classification Loss": "4.2.1 Scenario: List Completion. Given a set of item vocabulary V( 𝑣 0 = |V|) and a user curated list x = [ 𝑥 1 , 𝑥 2 , · · · , 𝑥 | x | ] ( 𝑥 𝑖 ∈ V ), the list completion agent [17, 23, 30, 43] is required to predict an item sequence y = [ 𝑦 1 , 𝑦 2 , · · · , 𝑦 | y | ] ( 𝑦 𝑖 ∈ V ) that is a subsequent of x , which can be formulated as maximizing the probability  where y ′ represents any possible list of length | y | . Different from non-sequential recommenders which generate a scalar score, the output of the list completer is a prediction item vector ¯ z . More precisely, the item completer undergoes training using the item prediction task 2 . Therefore, a classification module is designed to infer the probability distribution over the item vocabulary by the softmax function for each prediction item vector:  4.2.2 Integration. In the sequential scenario, since the user representation is derived by fusing the historical item list, we only need to insert a single CAGE module for item categorization. Additionally, the category tree generated by our CAGE naturally serves as a valuable aid for the item prediction task. Assuming that the ground truth label for the item vector to be predicted, ¯ z , is the 𝑦 ( 0 ) -th item, and its current embedding is z . We can start by using Item CAGE to obtain pseudo-labels (i.e., code indices in the multi-level codebooks, denoted as 𝑦 ( 𝑖 ) , 𝑖 = 1 , 2 , . . . , 𝐻 ) for the current embedding z . Subsequently, we design an auxiliary tree classification task that encourages the current prediction vector ¯ z to predict the category it corresponds to in each layer of the category tree which multiple node classification module:  Such auxiliary task further strengthens the connection between items and categories, leading to more precise predictions. Then, we proceed with the multi-level classification training and the loss function can be defined as:   Finally, the overall recommendation loss function is:   where 𝜔 c is a hyperparameter that controls the importance of the tree classification loss, and please refer to Equation 12 for 𝜔 q . 2 This task can take the form of predicting the next item for autoregressive methods [17, 43] or a masked item prediction task [30] for non-autoregressive methods.",
  "5 EXPERIMENT": "",
  "5.1 Experimental Setup": "5.1.1 Datasets. We conducted offline experiments on three recommendation tasks, namely list completion, collaborative filtering (CF), and click-through rate (CTR) prediction. For the list completion task, we use three real-world datasets: Zhihu, Spotify, and Goodreads, which were crawled and compiled by [17]. For the collaborative filtering task, we utilize five public datasets: Amazon Toys, Amazon Kindle Store, Amazon Phones, Amazon Grocery, and MovieLens [14] (1M version). Regarding the CTR prediction task, we employ two public datasets: MIND [52] (small version) and MovieLens [14] (100K version). Please refer to Appendix A for the dataset statistics. 5.1.2 Preprocessing. For the list completion task, we adopt the data preprocessing steps proposed by [30]. We iteratively perform the following two operations until the data no longer changes: 1) remove items with a frequency less than 10 from all lists; 2) truncate or filter the item list according to the maximum and minimum lengths specific to each dataset. Furthermore, we uniformly divide a qualifying list into two segments, namely the input and target lists. The lists are then partitioned into training, validation, and testing sets using an 8:1:1 ratio. For the CF and CTR prediction datasets, we only use the useritem interaction data without any additional information. To be specific, for the MIND dataset, user historical behaviors are transformed into a list of user-item pairs, which are subsequently included in the training set. More details about the dataset preprocessing will be provided in the public code repository upon accepted. 5.1.3 Baselines and Variants of Our Method. List completion. We take the state-of-the-art sequential recommendation methods and the item list completion models as baselines, including Caser [43], GRU4Rec[18], SASRec [23], BERT4Rec [42], CAR [17] and FANS [30]. Weintegrate CAGE into BERT4Rec and FANS to obtain CAGE BERT4Rec and CAGE FANS models, respectively. It is worth noting that FANS [30] pre-extracts categorical item features based on the curated item lists among training, validation, and testing sets. These categorical knowledge is also added into baseline models for a fair comparison in the FANS paper. Since we learn the cascaded categorical features in an end-to-end manner, we do not use the pre-extracted categorical entity features in our experiments for both our method variants and baselines. Collaborative filtering. Wecompareourmethodwithrepresentative CF models as baselines, including BPRMF [39], NeuMF [16], CFKG[56] and LGCN [15] We integrate our proposed CAGE module into these baselines and denote them as CAGE BPRMF , CAGE NeuMF , CAGECFKG , and CAGE LGCN , respectively. Click-through rate prediction. We compare our method with the widely used and state-of-the-art deep CTR models, including DeepFM [13], DCN [50], FiBiNET [20], and FinalMLP [31]. We integrate our proposed CAGE module into these baselines and denote the integrated models as CAGE DeepFM , CAGE DCN , CAGE FinalMLP , and CAGE FiBiNET , respectively. 5.1.4 Evaluation Protocols. We follow the common practice [41] to evaluate the effectiveness of recommendation models with the WWW'24, May 13-17, 2024, Singapore, Singapore Qijiong Liu, Jiaren Xiao, Lu Fan, Jieming Zhu, & Xiao-Ming Wu Table 1: Effectiveness of CAGE in list completion. We bold the best results. Asterisk symbol * indicates that the method uses pre-extracted categorical features which are learnt from the overall dataset including the test set. Table 2: Effectiveness of CAGE in collaborative filtering. We bold the best results. widely used metrics, i.e., Normalized Discounted Cumulative Gain [21] (NDCG@k) and Hit Ratio (HR@k). In this work, we set 𝑘 = { 5 , 10 } .",
  "5.2 Main Results": "List Completion. Table 1 presents a comparison of the state-of-theart sequential recommenders with our proposed CAGE variants on the list completion task. Based on the results, we can make the following observations. Firstly , for both autoregressive and non-autoregressive models, our proposed CAGE module can significantly improve the performance of the baseline models. For example, CAGE BERT4Rec can achieve an average improvement of 38% and 31% in terms of NDCG@5 and HR@5 among all datasets, compared with BERT4Rec. Secondly , since the FANS models leverage item category information in their design, they outperform other autoregressive baselines. However, our CAGE-integrated variant CAGE FANS can still achieve better performance than FANS, which implies that the end-to-end training models utilizing differentiable vector quantization can effectively learn improved clustering features compared to the word2vec+kmeans [30] approach that relies on pre-extracted features. Thirdly , in the Spotify dataset, the performance of the CNN-based Caser model is better than Transformer-based BERT4Rec model, which is aligned with the observation in [30]. One possible reason is that the local knowledge of the Spotify dataset is more important than the global information. CAGE WWW'24, May 13-17, 2024, Singapore, Singapore Table 3: Effectiveness of CAGE in click-through rate prediction. We bold the best results. Table 4: Impact of the number of CAGE layers (H) and the number of entries of each layer ( 𝑣 𝑖 ). The best results are indicated in bold, while the second-best results are underlined. A hyphen (-) indicates the absence of a layer. For example, '100( 𝑣 1 ) 10( 𝑣 2 ) -( 𝑉 3 )' means that CAGE only has two layers, and the first and second layers correspond to the 100-entry and 10-entry codebooks, respectively. We fix 𝛼, 𝛽, 𝜔 c , 𝜔 q to be 1 . 0 in this experiment. Collaborative Filtering. Table 2 displays the results of the popular CF models, along with our proposed CAGE variants on the collaborative filtering task. From the results, we can observe that our proposed CAGE consistently enhances the performance on the five datasets, resulting in significant improvements compared to the baseline models. Click-Through Rate Prediction. Table 3 shows the results of the widely-used CTR prediction models and our proposed CAGE variants on the CTR prediction task. Based on the results, we can find that among all CTR prediction models, our CAGE variants outperform the baseline models.",
  "5.3 Ablation Study": "Structure of the Category Tree. We study the effects of the number of layers and the number of entries (i.e., codebook size) in CAGE. We vary the number of layers from 1 to 3 and the number of entries within a range from 10 to 8,000. We fix other hyper-parameters and report the results of CAGE BERT4Rec and CAGE FANS . As illustrated in Table 4, we conduct experiments on the Zhihu and Goodreads datasets. From the results, we can make the following Figure 4: Influence of the use of user and item CAGE in the non-sequential recommenders. Backbone User CAGE Item CAGE Dual CAGE DeepFM FinalMLP 60 65 70 75 N@5 (a) CTR BPRMF LGCN 30 31 32 33 N@10 (b) CF observations. Firstly , the best results of two-layer CAGE variants are better than those of one-layer CAGE variants on both datasets, indicating that CAGE can effectively capture the hierarchical category information to further improve the entity representations. Secondly , different variants prefer different numbers of entries. For example, on the Zhihu dataset, CAGE BERT4Rec prefers a small number of entries in the first layer (i.e., 200), while CAGE FANS prefers a large number of entries in the same layer (i.e., 500). Thirdly , WWW'24, May 13-17, 2024, Singapore, Singapore Qijiong Liu, Jiaren Xiao, Lu Fan, Jieming Zhu, & Xiao-Ming Wu Figure 5: Impact of the residual connection weight 𝛼 , the quantization commitment cost 𝛽 , the codebook classification loss weight 𝜔 c , and the quantization loss weight 𝜔 q. We use the model with 𝛼 = 0 as the reference baseline for (a), and measure the relative improvement of each metric compared to the baseline for various values of 𝛼 , defined as ( 𝑚 𝛼 -𝑚 0 )/ 𝑚 0 ∗ 100% , where 𝑚 is one of the metrics in {N@5, N@10, HR@5, HR@10}. Therefore, the relative improvement of 𝛼 = 0 is constant at 0%. Similarly, we use the model with 𝛽 = 0 as the reference baseline for (b), 𝜔 q = 0 for (c), and 𝜔 q = 0 for (d). N@5 N@10 HR@5 HR@10 0 0.2 0.4 0.6 0.8 1.0 1.5 2.0 0 5 10 𝛼 Relative Improvement % (a) Zhihu 0 0.25 0.50 0.75 1.00 0 100 200 𝛽 Relative Improvement % (b) Zhihu 0 0.2 0.4 0.6 0.8 1.0 0 5 10 𝜔 c Relative Improvement % (c) Zhihu 0 0.2 0.4 0.6 0.8 1.0 0 100 200 300 𝜔 q Relative Improvement % (d) Zhihu 0 0.2 0.4 0.6 0.8 1.0 1.5 2.0 0 100 200 𝛼 Relative Improvement % (e) Goodreads 0 0.25 0.50 0.75 1.00 0 500 1 , 000 𝛽 Relative Improvement % (f) Goodreads 0 0.2 0.4 0.6 0.8 1.0 0 5 10 15 𝜔 c Relative Improvement % (g) Goodreads 0 0.2 0.4 0.6 0.8 1.0 0 500 1 , 000 𝜔 q Relative Improvement % (h) Goodreads different datasets prefer different numbers of entries. For example, for the CAGE BERT4Rec variant, the best number of entries is 20 on the Zhihu dataset and 50 on the Goodreads dataset. Fourthly , as the number of entries increases, the performance of CAGE variants first increases and then decreases. One possible reason is that a small number of entries may exhibit boundary effects, and as the entry size increases, the boundaries of the clusters gradually become blurred. However, when the number of entries is too large, the number of entities in each entry is too small, which may lead to insufficient learning of categorical feature. Moreover, the layer and entry numbers need to be carefully adjusted, otherwise it may lead to negative effects. Effectiveness of Item/User CAGE. We also test the effectiveness of the dual CAGE (i.e., using both user and item CAGE) in both CF and CTR prediction scenario. As shown in Figure 4, the results prove that both user and item CAGE could boost the performance of baselines. 𝛼 = 0 . 6, while for the Goodreads dataset, 𝛼 = 1 . 0. Thirdly , unlike the computer vision domain where the quantization commitment cost 𝛽 is usually set to 0 . 25 [46], in the recommendation domain, a higher 𝛽 (i.e., 1 . 0 for the Zhihu dataset or 0 . 50 for the Goodreads dataset) gets a higher performance. Fourthly , due to the equivalent performance shown by the hyperparameters when set to 1 in the Figure 5 (a-d) (e.g., the performance of 𝛼 = 1 in Figure 5 (a) is equivalent to that of 𝛽 = 1 in Figure 5 (b)), we can assess the performance when the hyperparameters are set to 0 by examining the range on the vertical axis (e.g., comparing the performance of 𝛼 = 0 in Figure 5 (a) with that of 𝛽 = 0 in Figure 5 (b)). A wider range signifies a larger disparity between the performance at 0 and 1 for the hyperparameters. This indicates that when this particular hyper-parameter is set to 0, the resulting effect is poorer, highlighting its greater significance. Therefore, we can observe that the ranking of importance for these four hyperparameters is: 𝜔 q > 𝛽 > 𝛼 ≈ 𝜔 c . Similarly, according to Figure 5 (e-h) for the Goodreads dataset, the importance ranking is: 𝜔 q ≈ 𝛽 > 𝛼 > 𝜔 c .",
  "5.4 Impact of Hyper-parameters": "We explore the impacts of the residual connection weight 𝛼 , the quantization commitment cost 𝛽 , the quantization loss weight 𝜔 q , and the codebook classification loss weight 𝜔 c . The experiments are conducted on two list completion datasets, i.e., Zhihu and Goodreads. Based on the results from Section 5.3, we take the best CAGE configuration of the CAGE FANS model, i.e., (500, 10) for the Zhihu dataset and (500, 50) for the Goodreads dataset. Based on the results from Figure 5, we can make the following observations. Firstly , the performance of baselines (i.e., when hyper-parameters are set to 0) is inferior to the most of the cases, indicating the effectiveness of these hyper-parameters. Secondly , different datasets achieve the best performance at different hyper-parameter settings. For example, the Zhihu dataset reaches the best performance at",
  "6 CONCLUSION": "We have proposed CAGE, a novel framework for leaning item/user category trees for ID-based recommendation, by employing differentiable vector quantization techniques. The flexibility of CAGE allows for its seamless integration into a variety of existing recommender systems. Through comprehensive experiments conducted across diverse recommendation scenarios, we have demonstrated the effectiveness of CAGE in enhancing the performance of various recommendation models. In future work, we plan to explore extending the applicability of CAGE beyond ID-based recommendation.",
  "ACKNOWLEDGMENTS": "We thank the anonymous reviewers for their valuable feedback. CAGE WWW'24, May 13-17, 2024, Singapore, Singapore",
  "REFERENCES": "[1] Artem Babenko and Victor Lempitsky. 2014. Additive quantization for extreme vector compression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 931-938. [2] Surajit Das Barman, Mahamudul Hasan, and Falguni Roy. 2019. A genre-based item-item collaborative filtering: facing the cold-start problem. In Proceedings of the 2019 8th international conference on software and computer applications . 258-262. [3] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 (2013). [4] John S Breese, David Heckerman, and Carl Kadie. 2013. Empirical analysis of predictive algorithms for collaborative filtering. arXiv preprint arXiv:1301.7363 (2013). [5] Andrés Buzo, A Gray, RM Gray, and John Markel. 1980. Speech coding based upon vector quantization. IEEE Transactions on Acoustics, Speech, and Signal Processing 28, 5 (1980), 562-574. [6] Renqin Cai, Jibang Wu, Aidan San, Chong Wang, and Hongning Wang. 2021. Category-aware collaborative sequential recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval . 388-397. [7] Ching-Wei Chen, Paul Lamere, Markus Schedl, and Hamed Zamani. 2018. Recsys Challenge 2018: Automatic Music Playlist Continuation. In Proceedings of the 12th ACM Conference on Recommender Systems (Vancouver, British Columbia, Canada) (RecSys '18) . Association for Computing Machinery, New York, NY, USA, 527-528. https://doi.org/10.1145/3240323.3240342 [8] Shuo Chen, Josh L. Moore, Douglas Turnbull, and Thorsten Joachims. 2012. Playlist Prediction via Metric Embedding. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (Beijing, China) . Association for Computing Machinery, New York, NY, USA, 714-722. (KDD '12) https://doi.org/10.1145/2339530.2339643 [9] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [10] Anna Gatzioura, João Vinagre, Alípio Mário Jorge, and Miquel Sanchez-Marre. 2019. A Hybrid Recommender System for Improving Automatic Playlist Continuation. IEEE Transactions on Knowledge and Data Engineering 33, 5 (2019), 1819-1830. [11] Anupriya Gogna and Angshul Majumdar. 2015. A comprehensive recommender system model: Improving accuracy for both warm and cold start users. IEEE Access 3 (2015), 2803-2813. [12] Robert Gray. 1984. Vector quantization. IEEE Assp Magazine 1, 2 (1984), 4-29. [13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He, and Zhenhua Dong. 2018. Deepfm: An end-to-end wide & deep learning framework for CTR prediction. arXiv preprint arXiv:1804.04950 (2018). [14] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015), 1-19. [15] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval . 639-648. [16] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173-182. [17] Yun He, Yin Zhang, Weiwen Liu, and James Caverlee. 2020. Consistency-Aware Recommendation for User-Generated Item List Continuation. In Proceedings of the 13th International Conference on Web Search and Data Mining (Houston, TX, USA) (WSDM '20) . Association for Computing Machinery, New York, NY, USA, 250-258. https://doi.org/10.1145/3336191.3371776 [18] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.). [19] Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023. Learning vector-quantized item representation for transferable sequential recommenders. In Proceedings of the ACM Web Conference 2023 . 1162-1171. [20] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . 169-177. [21] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422-446. [22] Biing-Hwang Juang and A Gray. 1982. Multiple stage vector quantization for speech coding. In ICASSP'82. IEEE International Conference on Acoustics, Speech, and Signal Processing , Vol. 7. IEEE, 597-600. [23] Wang-Cheng Kang and Julian McAuley. 2018. Self-Attentive Sequential Recommendation. In 2018 IEEE International Conference on Data Mining (ICDM) . IEEE, 197-206. https://doi.org/10.1109/ICDM.2018.00035 [24] Yunyong Ko, Jae-Seo Yu, Hong-Kyun Bae, Yongjun Park, Dongwon Lee, and Sang-Wook Kim. 2021. MASCOT: A Quantization Framework for Efficient Matrix Factorization in Recommender Systems. In 2021 IEEE International Conference on Data Mining (ICDM) . IEEE, 290-299. [25] Yehuda Koren. 2009. The bellkor solution to the netflix grand prize. Netflix prize documentation 81, 2009 (2009), 1-10. [26] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30-37. [27] Daniel Lemire and Anna Maclachlan. 2005. Slope one predictors for online rating-based collaborative filtering. In Proceedings of the 2005 SIAM International Conference on Data Mining . SIAM, 471-475. [28] Defu Lian, Xing Xie, Enhong Chen, and Hui Xiong. 2020. Product quantized collaborative filtering. IEEE Transactions on Knowledge and Data Engineering 33, 9 (2020), 3284-3296. [29] Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiaoming Wu. 2022. Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation. In Proceedings of the 29th International Conference on Computational Linguistics . 2823-2833. [30] Qijiong Liu, Jieming Zhu, Jiahao Wu, Tiandeng Wu, Zhenhua Dong, and XiaoMing Wu. 2023. FANS: Fast Non-Autoregressive Sequence Generation for Item List Continuation. In Proceedings of the ACM Web Conference 2023 . 3309-3318. [31] Kelong Mao, Jieming Zhu, Liangcai Su, Guohao Cai, Yuru Li, and Zhenhua Dong. 2023. FinalMLP: an enhanced two-stream MLP model for CTR prediction. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 37. 4552-4560. [32] Brian McFee and Gert RG Lanckriet. 2011. The Natural Language of Playlists.. In The International Society for Music Information Retrieval (ISMIR) , Vol. 11. 537-541. [33] Brian McFee and Gert RG Lanckriet. 2012. Hypergraph Models of Playlist Dialects.. In The International Society for Music Information Retrieval (ISMIR) , Vol. 12. 343-348. [34] Gabriel de Souza P Moreira, Dietmar Jannach, and Adilson Marques da Cunha. 2019. On the importance of news content representation in hybrid neural sessionbased recommender systems. arXiv preprint arXiv:1907.07629 (2019). [35] Yujie Pan, Jiangchao Yao, Bo Han, Kunyang Jia, Ya Zhang, and Hongxia Yang. 2021. Click-through rate prediction with auto-quantized contrastive learning. arXiv preprint arXiv:2109.13921 (2021). [36] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In 2016 IEEE 16th international conference on data mining (ICDM) . IEEE, 1149-1154. [37] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Tran, Jonah Samost, et al. 2024. Recommender systems with generative retrieval. Advances in Neural Information Processing Systems 36 (2024). [38] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. 2019. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems 32 (2019). [39] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [40] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web . 285-295. [41] Chuan Shi, Xiaotian Han, Li Song, Xiao Wang, Senzhang Wang, Junping Du, and S Yu Philip. 2019. Deep collaborative filtering with multi-aspect information in heterogeneous networks. IEEE transactions on knowledge and data engineering 33, 4 (2019), 1413-1425. [42] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (Beijing, China) (CIKM '19) . Association for Computing Machinery, New York, NY, USA, 1441-1450. https://doi.org/10.1145/3357384.3357895 [43] Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding (WSDM '18) . Association for Computing Machinery, New York, NY, USA, 565-573. https://doi.org/10.1145/3159652. 3159656 [44] Thanh Tran, Renee Sweeney, and Kyumin Lee. 2019. Adversarial Mahalanobis Distance-Based Attentive Song Recommender for Automatic Playlist Continuation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (Paris, France) (SIGIR'19) . Association for Computing Machinery, New York, NY, USA, 245-254. https: //doi.org/10.1145/3331184.3331234 [45] Jan Van Balen and Mark Levy. 2019. PQ-VAE: Efficient Recommendation Using Quantized Embeddings.. In RecSys (Late-Breaking Results) . 46-50. WWW'24, May 13-17, 2024, Singapore, Singapore Qijiong Liu, Jiaren Xiao, Lu Fan, Jieming Zhu, & Xiao-Ming Wu [46] Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. Advances in neural information processing systems 30 (2017). [47] Maksims Volkovs, Himanshu Rai, Zhaoyue Cheng, Ga Wu, Yichao Lu, and Scott Sanner. 2018. Two-Stage Model for Automatic Playlist Continuation at Scale. In Proceedings of the ACM Recommender Systems Challenge 2018 (Vancouver, BC, Canada) (RecSys Challenge '18) . Association for Computing Machinery, New York, NY, USA, Article 9, 6 pages. https://doi.org/10.1145/3267471.3267480 [48] Chenyang Wang, Min Zhang, Weizhi Ma, Yiqun Liu, and Shaoping Ma. 2020. Make it a chorus: knowledge-and time-aware item modeling for sequential recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 109-118. [49] Fangye Wang, Hansu Gu, Dongsheng Li, Tun Lu, Peng Zhang, and Ning Gu. 2023. Towards Deeper, Lighter and Interpretable Cross Network for CTR Prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 2523-2533. [50] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In Proceedings of the ADKDD'17 (Halifax, NS, Canada) (ADKDD'17) . Association for Computing Machinery, New York, NY, USA, Article 12, 7 pages. [51] Zhiqiang Wang, Qingyun She, and Junlin Zhang. 2021. Masknet: Introducing feature-wise multiplication to CTR ranking models by instance-guided mask. arXiv preprint arXiv:2102.07619 (2021). [52] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. 2020. Mind: A large-scale dataset for news recommendation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . 3597-3606. [53] Ze-bin Wu and Jun-qing Yu. 2019. Vector quantization: a review. Frontiers of Information Technology & Electronic Engineering 20, 4 (2019), 507-524. [54] Yan Xia, Kaiming He, Fang Wen, and Jian Sun. 2013. Joint inverted indexing. In Proceedings of the IEEE International Conference on Computer Vision . 3416-3423. [55] Jin Zhang, Defu Lian, Haodi Zhang, Baoyun Wang, and Enhong Chen. 2023. Query-Aware Quantization for Maximum Inner Product Search. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 37. 4875-4883. [56] Yongfeng Zhang, Qingyao Ai, Xu Chen, and Pengfei Wang. 2018. Learning over knowledge-base embeddings for recommendation. arXiv preprint arXiv:1803.06540 (2018).",
  "A DATA STATISTICS": "The dataset statistics are summarized in Table 5.",
  "B IMPLEMENTATION DETAILS": "In our training process, we employ the Adam optimizer as the gradient descent algorithm. Across all models, the embedding dimension is consistently set to 64.",
  "B.1 List Completion": "We adhere to the experimental setup established by FANS [30] and maintain the following parameters: For all attention-based models (SASRec, BERT4Rec, and FANS), weset the number of hidden layers as 3, and the number of attention heads as 8. For the Caser model, we specify the number of horizontal and vertical channels as 16 and 4, respectively, and set the max sequence length to 5. For the CAR model, we define the number of CAR layers as 3. As for the GRU4Rec model, we use 3 GRU layers. For the BERT4Rec model, each item in the sequence has a 𝑆 probability to be selected. For each selected item, it has 1) an 80% probability to be replaced with a < 𝑀𝐴𝑆𝐾 > token; 2) a 10% probability for substitution with a random item, and 3) the remaining 10% probability to remain unchanged. Furthermore, there is an additional probability of 𝐴 , resulting in a combined probability of 𝑆 + 𝐴 for the last token to be masked. This strategy is proposed by BERT4Rec for the sequential recommendation scenario that requires next item prediction during inference. In our experiments, we vary the value of S from the set { 15 , 30 , 45 , 60 } while keeping 𝐴 fixed at 10. Conversely, we also vary the value of 𝐴 from the set { 10 , 20 , 30 , 40 } while keeping 𝑆 fixed at 15. Finally, we select the configuration of 𝑆 = 15 and 𝐴 = 10 for both the original and CAGE-integrated models. Moreover, for all the aforementioned baseline models, the batch size is set to 256, and we explore a range of learning rates from { 0 . 001 , 0 . 005 , 0 . 01 , 0 . 05 , 0 . 1 } .",
  "B.2 Collaborative Filtering": "In this scenario, we adhere to the default setting of ReChorus [48], which serves as the foundation for our implementation. Subsequently, based on the default setting, we execute a comprehensive grid search of significant hyperparameters. These include the learning rate, which is selected from { 0 . 05 , 0 . 001 , 0 . 005 , 1 𝑒 -4 , 1 𝑒 -5 , 1 𝑒 -6 } , and L2 that ranges from 1 𝑒 -8 to 1 𝑒 -5 using a logarithmic scale. Specifically, for the LightGCN model, we set the number of layers as 3. For the NeuMF model, we tune the number of layers from { 1 , 2 , 3 } . In the case of CFKG, we incorporate four types of relations: user-item : buy , item-category : has_cat , item-item : also_bought (complement) , item-item : also_view (substitute) . Notably, the useritem matrix is incorporated into the knowledge graph as a unique relation, denoted as 'buy' .",
  "B.3 Click-through Rate Prediction": "For the CTR scenario, we set the batch size to 5000 and tune the learning rate from the set { 0 . 0001 , 0 . 0005 , 0 . 001 , 0 . 005 , 0 . 01 } . In addition, we maintain a uniform configuration across all base models, including setting the number of DNN layers to 3 and a dropout ratio of 0.1. For models like DCN, DeepFM, and FiBiNET, we vary the DNN hidden size from the set { 500 , 1000 , 2000 } . For the FinalMLP model, we tune the MLP hidden size from { 64 , 128 , 256 } . ReLU serves as the activation function for the DNN. As for the DCN model, we vary the number of cross layers from { 3 , 4 , 5 } .",
  "B.4 Integration of CAGE": "When integrating CAGE with base models, we tune the following parameters: 1) the number of CAGE layers, 2) the number of codes per layer, 3) the weight factor 𝛼 for residual connections, 4) the quantization commitment cost 𝛽 , 5) the codebook classification loss weight 𝜔 𝑐 for the list completion task, and 6) the quantization loss weight 𝜔 𝑞 . For a comprehensive analysis of the impact of these parameters, please refer to the Abaltion Study section.",
  "C INTEGRATING CAGE WITH OTHER CTR MODELS": "We report additional experiments integrating our GAGE approach with other recent CTR models, i.e., MaskNet [51] and GDCN [49], on the MIND dataset, as illustrated in Table 6, which have also showcased notable improvements.",
  "D DEEPER ANALYSIS OF THE CATEGORIES LEARNED BY CAGE": "We carry out further experiments on the MIND dataset, which comprises 270 real subcategories. We set the codebook size for the CAGE WWW'24, May 13-17, 2024, Singapore, Singapore Table 5: Dataset statistics. Density is defined as the ratio of the number of samples to the number of all possible interactions. Table 6: Integrating CAGE with other CTR models. Table 7: Influence of the real categorical knowledge. 'Ori.' represents the original base model. '+' represents the model that integrates category features. first and second layers as 1000 and 10, respectively. After training the CAGE DeepFM model, we examine the correlation between the learned codes of the first layer and the actual subcategories, specifically, how each category is distributed among the codes. Out of the 270 real categories, 55 categories (over 20%) are exclusive to one code. There are 116 categories (over 40%) that are spread across fewer than 10 codes (1%). Additionally, 148 categories (approximately 55%) are distributed among fewer than 20 codes (2%). Furthermore, 208 categories (over 75%) are distributed among fewer than 100 codes (10%). The results show a degree of correlation between the learned and actual categories, validating the effectiveness of CAGE in automatic categorization.",
  "E INTEGRATION OF REAL CATEGORY DATA": "Here, we study the integration of the original baseline and CAGEenhanced variants with the real category data on the MIND dataset. The category information is integrated as follows:  where e is the original item embedding, e cat is the corresponding category embedding, and e ′ is the category-informed item embedding. Such integration is denoted by '+' in Table 7. The results indicate that real category information can enhance both the baselines and our CAGE variants. Furthermore, it is surprising to note that our CAGE method outperforms 'Ori. +' that utilizes real category information. A possible reason could be that CAGE acquires categorical information from interaction data, which might be more beneficial for the recommendation task. This could imply that the category knowledge learned by CAGE effectively complements the provided category information.",
  "F AGGREGATION OF MULTI-LEVEL CATEGORY EMBEDDINGS": "Here, we explored different ways of aggregating the generated multi-level category embeddings, including average pooling, denoted as 'avg', and concat-and-project (i.e., concatenate the multilevel category vectors into a long vector and then project it to the dimension of a single vector using a fully connected layer), denoted as 'concat'. Based on our experiments, we have found that both approaches yield effective results, with no significant difference in performance between them. This suggests that average pooling can retain the multi-level categorical information, similar to concat-and-project. Below we provide the experimental results on the MIND dataset for CTR prediction. Qijiong Liu, Jiaren Xiao, Lu Fan, Jieming Zhu, & Xiao-Ming Wu WWW'24, May 13-17, 2024, Singapore, Singapore Table 8: Training and inference time comparisons between the original base models and CAGE-enhanced ones. 'Ori.' represents the original model.",
  "G TIME EFFICIENCY": "Here, we study the impact of integrating our CAGE module on model efficiency, especially with large-scale datasets, we report both the training time (per epoch) and inference time in various recommendation scenarios. The results indicate that our CAGE module introduces only a marginal increase in computational cost, which is generally negligible in the majority of cases.",
  "keywords_parsed": [
    "recommender system",
    "differentiable vector quantization"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Additive quantization for extreme vector compression"
    },
    {
      "ref_id": "b2",
      "title": "A genre-based item-item collaborative filtering: facing the cold-start problem"
    },
    {
      "ref_id": "b3",
      "title": "Estimating or propagating gradients through stochastic neurons for conditional computation"
    },
    {
      "ref_id": "b4",
      "title": "Empirical analysis of predictive algorithms for collaborative filtering"
    },
    {
      "ref_id": "b5",
      "title": "Speech coding based upon vector quantization"
    },
    {
      "ref_id": "b6",
      "title": "Category-aware collaborative sequential recommendation"
    },
    {
      "ref_id": "b7",
      "title": "Recsys Challenge 2018: Automatic Music Playlist Continuation"
    },
    {
      "ref_id": "b8",
      "title": "Playlist Prediction via Metric Embedding"
    },
    {
      "ref_id": "b9",
      "title": "Wide & deep learning for recommender systems"
    },
    {
      "ref_id": "b10",
      "title": "A Hybrid Recommender System for Improving Automatic Playlist Continuation"
    },
    {
      "ref_id": "b11",
      "title": "A comprehensive recommender system model: Improving accuracy for both warm and cold start users"
    },
    {
      "ref_id": "b12",
      "title": "Vector quantization"
    },
    {
      "ref_id": "b13",
      "title": "Deepfm: An end-to-end wide & deep learning framework for CTR prediction"
    },
    {
      "ref_id": "b14",
      "title": "The movielens datasets: History and context"
    },
    {
      "ref_id": "b15",
      "title": "Lightgcn: Simplifying and powering graph convolution network for recommendation"
    },
    {
      "ref_id": "b16",
      "title": "Neural collaborative filtering"
    },
    {
      "ref_id": "b17",
      "title": "Consistency-Aware Recommendation for User-Generated Item List Continuation"
    },
    {
      "ref_id": "b18",
      "title": "Session-based Recommendations with Recurrent Neural Networks"
    },
    {
      "ref_id": "b19",
      "title": "Learning vector-quantized item representation for transferable sequential recommenders"
    },
    {
      "ref_id": "b20",
      "title": "FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction"
    },
    {
      "ref_id": "b21",
      "title": "Cumulated gain-based evaluation of IR techniques"
    },
    {
      "ref_id": "b22",
      "title": "Multiple stage vector quantization for speech coding"
    },
    {
      "ref_id": "b23",
      "title": "Self-Attentive Sequential Recommendation"
    },
    {
      "ref_id": "b24",
      "title": "MASCOT: A Quantization Framework for Efficient Matrix Factorization in Recommender Systems"
    },
    {
      "ref_id": "b25",
      "title": "The bellkor solution to the netflix grand prize"
    },
    {
      "ref_id": "b26",
      "title": "Matrix factorization techniques for recommender systems"
    },
    {
      "ref_id": "b27",
      "title": "Slope one predictors for online rating-based collaborative filtering"
    },
    {
      "ref_id": "b28",
      "title": "Product quantized collaborative filtering"
    },
    {
      "ref_id": "b29",
      "title": "Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation"
    },
    {
      "ref_id": "b30",
      "title": "FANS: Fast Non-Autoregressive Sequence Generation for Item List Continuation"
    },
    {
      "ref_id": "b31",
      "title": "FinalMLP: an enhanced two-stream MLP model for CTR prediction"
    },
    {
      "ref_id": "b32",
      "title": "The Natural Language of Playlists"
    },
    {
      "ref_id": "b33",
      "title": "Hypergraph Models of Playlist Dialects"
    },
    {
      "ref_id": "b34",
      "title": "On the importance of news content representation in hybrid neural sessionbased recommender systems"
    },
    {
      "ref_id": "b35",
      "title": "Click-through rate prediction with auto-quantized contrastive learning"
    },
    {
      "ref_id": "b36",
      "title": "Product-based neural networks for user response prediction"
    },
    {
      "ref_id": "b37",
      "title": "Recommender systems with generative retrieval"
    },
    {
      "ref_id": "b38",
      "title": "Generating diverse high-fidelity images with vq-vae-2"
    },
    {
      "ref_id": "b39",
      "title": "BPR: Bayesian personalized ranking from implicit feedback"
    },
    {
      "ref_id": "b40",
      "title": "Item-based collaborative filtering recommendation algorithms"
    },
    {
      "ref_id": "b41",
      "title": "Deep collaborative filtering with multi-aspect information in heterogeneous networks"
    },
    {
      "ref_id": "b42",
      "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer"
    },
    {
      "ref_id": "b43",
      "title": "Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding"
    },
    {
      "ref_id": "b44",
      "title": "Adversarial Mahalanobis Distance-Based Attentive Song Recommender for Automatic Playlist Continuation"
    },
    {
      "ref_id": "b45",
      "title": "PQ-VAE: Efficient Recommendation Using Quantized Embeddings"
    },
    {
      "ref_id": "b46",
      "title": "Neural discrete representation learning"
    },
    {
      "ref_id": "b47",
      "title": "Two-Stage Model for Automatic Playlist Continuation at Scale"
    },
    {
      "ref_id": "b48",
      "title": "Make it a chorus: knowledge-and time-aware item modeling for sequential recommendation"
    },
    {
      "ref_id": "b49",
      "title": "Towards Deeper, Lighter and Interpretable Cross Network for CTR Prediction"
    },
    {
      "ref_id": "b50",
      "title": "Deep & Cross Network for Ad Click Predictions"
    },
    {
      "ref_id": "b51",
      "title": "Masknet: Introducing feature-wise multiplication to CTR ranking models by instance-guided mask"
    },
    {
      "ref_id": "b52",
      "title": "Mind: A large-scale dataset for news recommendation"
    },
    {
      "ref_id": "b53",
      "title": "Vector quantization: a review"
    },
    {
      "ref_id": "b54",
      "title": "Joint inverted indexing"
    },
    {
      "ref_id": "b55",
      "title": "Query-Aware Quantization for Maximum Inner Product Search"
    },
    {
      "ref_id": "b56",
      "title": "Learning over knowledge-base embeddings for recommendation"
    }
  ]
}