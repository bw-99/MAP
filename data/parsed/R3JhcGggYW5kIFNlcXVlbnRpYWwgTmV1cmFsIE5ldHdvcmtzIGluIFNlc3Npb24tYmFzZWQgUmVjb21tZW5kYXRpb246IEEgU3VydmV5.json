{
  "Graph and Sequential Neural Networks in Session-based Recommendation: A Survey": "ZIHAO LI, University of Technology Sydney, Australia CHAO YANG, University of Technology Sydney, Australia YAKUN CHEN, University of Technology Sydney, Australia XIANZHI WANG, University of Technology Sydney, Australia HONGXU CHEN, University of Technology Sydney, Australia GUANDONG XU, and University of Technology Sydney, Australia The Education University of Hong Kong, Hong Kong Special Administrative Region of China LINA YAO, University of New South Wales, Australia QUAN Z. SHENG, Macquarie University, Australia Recent years have witnessed the remarkable success of recommendation systems (RSs) in alleviating the information overload problem. As a new paradigm of RSs, session-based recommendation (SR) specializes in users' short-term preference capture and aims to provide a more dynamic and timely recommendation based on the ongoing interacted actions. In this survey, we will give a comprehensive overview of the recent works on SR. First, we clarify the definitions of various SR tasks and introduce the characteristics of session-based recommendation against other recommendation tasks. Then, we summarize the existing methods in two categories: sequential neural network based methods and graph neural network (GNN) based methods. The standard frameworks and technical details are also introduced. Finally, we discuss the challenges of SR and new research directions in this area. CCS Concepts: Â· Information systems â†’ Recommender systems. Additional Key Words and Phrases: recommendation survey, session-based recommendation, graph neural networks, sequential neural networks",
  "1 INTRODUCTION": "With the prosperity and prevalence of the Internet, a surge of companies inclined to provide their products or services in an E-commerce modal, e.g. , Amazon, YouTube, Yelp, LinkedIn. Although the abundance of products and information provides more probability for users to satisfy their various personality requirements, the information overload problem becomes more serious. As an effective and efficient information filtering technology, Recommendation Systems (RSs) are capable of mining user's Point-of-Interest ( POI ) based on her historical interaction records ( e.g. , click, watch, read, add cart, and purchase) and recommend interested items in an automatic fashion. Recently, RSs have evolved into a prominent solution and attracted widespread attention from academia and industry [48, 175, 194]. Overall, in the past decades, the conventional mainstream attempts for RSs can be divided into content-based models [2] , collaborative filtering-based (CF-based) models [143, 169] , and hybrid models ( i.e. , the combination of the content-based and CF-based two models) [108]. As for CF-based methods, they can be further subdivided into Authors' addresses: Zihao Li, zihao.li@student.uts.edu.au, University of Technology Sydney, Australia; Chao Yang, chao.yang@student.uts.edu.au, University of Technology Sydney, Australia; Yakun Chen, yakun.chen@student.uts.edu.au, University of Technology Sydney, Australia; Xianzhi Wang, xianzhi.wang@uts.edu.au, University of Technology Sydney, Australia; Hongxu Chen, hongxu.chen@uts.edu.au, University of Technology Sydney, Australia; Guandong Xu, gdxu@eduhk.hk, The Education University of Hong Kong, Hong Kong Special Administrative Region of China and University of Technology Sydney, Australia; Lina Yao, University of New South Wales, Australia, lina.yao@unsw.edu.au; Quan Z. Sheng, Macquarie University, Australia, michael.sheng@mq.edu.au. 1 2 Li et al. Fig. 1. The statistics of publications with regard to SR. \"<16\" means 2016 and before. The bar chart (left) displays the number of published papers each year, and the pie chart (right) illustrates the percentage of papers published in each top venue. 140 IPSM ICDE KDD 120 5% IJCAI TKDE 100 AAAI Others ICDM 80 29 WSDM 53.1% 5% TOIS WWW 3.2% CIKM 4.2% SIGIR 40 22.4% RecSys 20 <16 17 18 19 20 21 22 23 24 CoRR Year (20--) the memory-based models ( e.g. , item-based and user-based) [143, 169] and model-based attempts ( e.g. , Naive Bayes, matrix factorization, latent factor model, and neural networks) [80, 81, 111, 114, 142, 151, 175, 194, 229]. However, most of above methods are committed to utilizing users' whole historical information and capture the long-term, statistic preference for recommendation [45, 140]. It may not be applicable for practical scenarios since (1) the user's preference will shift and evolve over time, and (2) those methods are difficult to capture the user's ongoing behaviors and realize a real-time recommendation. We believe modeling users' short-term preferences and intentions based on the current interaction behaviors are also critically important for dynamic and accurate recommendation. To bridge this gap, Session-based Recommendation (SR) [175] or Session-aware Recommendation 1 have emerged with increasing attention in recent years. Reviewing the development of SR, Modani et al. [116] first proposed the concept of session for a dynamic recommendation in 2002. On top of that, in 2005, Modani et al. [117] further devised a basic framework based on a bipartite graph, which is the pioneering work for the SR task. Since then, SR gradually become a hot topic in recommendation research avenues. Figure 1 shows the number of papers (593 in total retrieved from DBLP Database 2 by June 2024) published in top venues. We can find that the relevant work has increased significantly since the year 2019. In 2020 and 2021 the number of papers (85 and 87) is almost twice of 2019 (46). In 2022 and 2023, the number rises to 126 and 142 respectively. Besides, RecSys, SIGIR, CIKM, WWW, WSDM, TOIS and TKDE are the most popular target venues for SR. In general, the early works for SR focus more on popularity-based solutions [1], and machine learning-based methods, including KNN [38, 109, 209], Markov Chain [44, 137, 137, 235] and matrix factorization [28, 96, 136, 146]. Benefit from the powerful ability of feature extraction and representation, deep learning solutions, such as sequential neural networks [57, 65, 75, 87, 135, 160] and graph neural networks (GNNs) [12, 15, 195, 198, 225, 241], become ubiquitous and achieve promising results for SR in recent years. More concretely, the sequential neural networks, e.g. , recurrent neural network (RNN), long short-term memory neural network (LSTM), and gated recurrent unit neural network (GRU), model the input session as a sequence and capture the order dependency among items for recommendation. In contrast, GNNs are required to predefine a graph structure based on sessions first, then items' correlations will be modeled via the information propagation and aggregation for recommendation. 1 Quadrana et al. [133] point out for session-aware recommendation , the user's all historical sessions are known, but in terms of session-based recommendation , the users are anonymous and we only focus on the current session. However, in most of the works, we do not distinguish these two terminologies. 2 https://dblp.org/ Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 3 Although there are some surveys related to our work on recommendation systems [14, 48, 132, 175, 194, 228], they either put the lens on sequential recommendation [48, 132] or discuss the application of GNNs in recommendation systems [16, 194]. Ludewig and Jannach [109] compare the performance of various models for SR and provide an indepth analysis. However, these methods were proposed before 2019, leaving GNNs to be explored. Wang et al. [175, 178] clarify the definitions of SR first, then a categorization of existing SR arts is proposed. Besides, the challenges and new opportunities in SR are illustrated for future development. Although this work provides an overall picture of SR, the detailed technical discussion and comparison between sequential neural networks and GNNs are insufficient. Given the impressive pace at which SR research with graph and sequential neural networks is growing, we believe it is necessary and valuable to analyze the characteristics, categorize the existing works in a unified framework, formalize their focused problems, and summarize the general ideas and solutions, the major challenges, and the future directions thoroughly. Consequently, in this paper, we are devoted to proposing a systematic classification, detailing a comparison of GNNs and sequential neural networks, and providing a comprehensive overview and survey of their application in SR. The key contributions of this survey are summarized as follows: Â· We standardize the concepts and definitions with regard to SR and introduce various graph structures that are commonly used in SR. Besides, we summarize the features of SR and compare the similarities and distinctions between SR and sequential recommendation. Moreover, we propose unified frameworks to regulate sequential neural networks and GNNs for SR, respectively. Â· We propose a systematic category to organize the existing works regarding sequential neural networks and GNNs for SR. Additionally, a comprehensive analysis and comparisons of the properties of these two mainstream methods are presented. We further introduce the overall pipeline and key modules in these methods in detail. Â· Finally, we identify open challenges and discuss future directions to inspire more research on this topic. The rest of the survey is organized as follows. Section 2 introduces the preliminaries of SR and different graph structures in this paper for easy reading. Section 3 introduces the typical features of SR and illustrates the difference between sequential neural networks and GNNs for SR. Moreover, the categorization scheme of relevant works and their characteristics are also introduced. In section 4, we first formalize unified frameworks of sequential neural networks and GNNs for SR, respectively. Subsequently, a detailed exposition with respect to efficacious modules in those frameworks is also provided. In Section 5, we analyze the statistical characteristics of real-world datasets, introduce the commonly used performance evaluation metrics with regard to accuracy and diversity Section 6 outlines this field's challenges and future directions. Finally, we conclude the survey in Section 7.",
  "2 PRELIMINARIES": "In this section, we will first clarify some basic definitions in SR, e.g. , intra-sessions, inter-sessions, and so forth. Additionally, various typical graph structures and the corresponding concepts are also introduced briefly.",
  "2.1 Session Definitions": "According to whether a user's information is anonymous or not, SR can be further divided into personalized sessionbased recommendation (PSR) and session-based social recommendation (SSR), which are illustrated in Figure 2. In addition, the definitions and the key concepts, i.e. , sessions, items, attributes, and social relations are listed below. Definition 2.1.1. Item is the object that users interact with. It is usually formalized as an item ID in recommendation system. 4 Li et al. Fig. 2. A toy example of different SR tasks. (a) Session-based Recommendation â€¦ (b) Personalized Session-based Recommendation user â€¦ â€¦ user user friends (c) Session-based Social Recommendation session Author ï¼š Leo Tolstoy Country: Russia Published: 1869 Pages: 1,225 Genre: Novel item and attributes user MARTIN EDEN MBEN Definition 2.1.2. Attributes are the associated external information (or side information in [203]) of items or interactions. Specifically, the item-oriented attributes contain brand, category, text descriptions, and images. As for interaction-oriented attributes, geographic information, interaction time and order, and behavior types ( e.g. , search, click, add cart, buy, share, comment, etc) are most commonly used. These attributes could serve as auxiliary information for performance improvement. Definition 2.1.3. Session refers to a user-interacted list consisting of items or services. The items in sessions are usually organized chronologically 3 . Definition 2.1.4. Session-based Recommendation (SR, depicted in Figure 2, circled by the black dotted box). Let I = { ğ‘– 1 , ğ‘– 2 , ğ‘– 3 , ...ğ‘– ğ‘› } as the set of items, where ğ‘› is the number of items. Each session ğ‘  = [ ğ‘– 1 , ğ‘– 2 , ğ‘– 3 , ..., ğ‘– ğ‘š ] consists of a sequence of interactive items ğ‘– ğ‘˜ âˆˆ ğ¼ from a user. Thus, given a session ğ‘  , the task of SR aims to generate probabilities Ë† y for all possible items. Each element's value in distribution Ë† y indicates the recommendation score of the corresponding item. The items with a topğ¾ recommendation score will be recommended. Although collaborative filtering solutions, including item-based, user-based and content-based methods, aim to predict user's next POI, these methods elaborate to capture the co-occurrence patterns based on the user-item interaction matrix, ignoring the sequential information of each click [158]. In contrast, session-based recommendation put efforts on current session modeling, i.e., instead of all the historical records, only the ongoing session and the sequential patterns encapsulated in the sessions required to be considered, thus, it will be more suitable for new users and timely and dynamic recommendation (more detail discussions please ref. Section 3.1). Definition 2.1.5. Personalized Session-based Recommendation (PSR, illustrated in Figure 2, circled by the blue dotted box). Let U be a set of users, for each user ğ‘¢ âˆˆ U , denote S ğ‘¢ = { ğ‘† ğ‘¢ ğ‘– } ğ‘› ğ‘¢ ğ‘– = 1 as all the historical sessions of ğ‘¢ , and ğ‘› ğ‘¢ stands for the total number of sessions. Let ğ‘† ğ‘¢ ğ‘– = [ ğ‘– ğ‘–,ğ‘— ] ğ‘š ğ‘– ğ‘— = 1 âˆˆ S ğ‘¢ as the ğ‘– -th session of user ğ‘¢ , and ğ‘š ğ‘– stands for the total number of items in session ğ‘† ğ‘¢ ğ‘– . We define ğ‘† ğ‘¢ ğ‘ as the current session of user ğ‘¢ , the previous sessions in the timeline are historical sessions denoted as S ğ‘¢ â„ . Thus, given all the historical sessions S ğ‘¢ â„ of user ğ‘¢ , the task of PSR is to predict the next interactive item of the current session ğ‘† ğ‘¢ ğ‘ . In [60], the PSR also named as streaming session-based recommendation. 3 In most cases this is true, albeit few papers ignore the sequence information in sessions [69, 176] Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 5 Fig. 3. The diagram of the intra-session graph and the inter-session graph. We represent the items or nodes as solid circles and the edges as arrow lines. i 1 i 3 i 4 i 2 , , , ( ) i 2 i 6 i 5 i 5 ( , , , ) i 6 i 3 i 7 i 7 , , , ( ) Heterogeneous Graph Sessions i 1 i 2 i 3 i 4 i 6 i 3 i 7 i 2 i 5 i 6 Intra-Session Graph i 5 i 6 i 7 i 1 i 2 i 3 i 4 Definition 2.1.6. Session-based Social Recommendation (SSR, illustrated in Figure 2, circled by the orange dotted box). Based on PSR, we denote { ğ‘¢ ğ‘˜ } ğ‘ ( ğ‘¢ ) ğ‘˜ = 1 âŠ† U as the neighbors or friends of user ğ‘¢ , ğ‘ ( ğ‘¢ ) is the number of neighbors. Let the sessions of user ğ‘¢ ğ‘˜ as S ğ‘¢ ğ‘˜ . Thus, given the current session S ğ‘¢ of user ğ‘¢ and all the sessions âˆª ğ‘ ( ğ‘¢ ) ğ‘˜ = 1 S ğ‘¢ ğ‘˜ from user ğ‘¢ and his/her neighbors ğ‘¢ ğ‘˜ , the task of SSR aims to predict the next interactive item of the current session S ğ‘¢ .",
  "2.2 Graphs Definitions": "Graph-based SR requires us to predefine a graph first, then the GNNs are adopted for information propagation and aggregation. To model more fertile and valuable information from neighbors, researchers endeavor to design dedicated graph structures. We, thereby, introduce five typical graph structures in SR, respectively. Definition 2.2.1 Digraph and Undigraph. Given a session ğ‘  , each item in the session can be presented as a node. Besides, if a user clicks item ğ‘– ğ‘— after item ğ‘– ğ‘– in the session, we add a directed edge ğ‘’ ğ‘– ğ‘— from node ğ‘– point to node ğ‘— . Hence, we could organize those nodes and the directed edges between all adjacent items via a digraph. In contrast, if we add an undirected edge ğ‘’ ğ‘– ğ‘— between node ğ‘– and node ğ‘— , we could construct an undigraph. Definition 2.2.2 Intra-session graph. As shown in Figure 3 (left), given a series of sessions S , we could construct intra-session graphs G = (V , E) for each session where the node set are all unique items in S . And ğ‘’ ğ‘– ğ‘— âˆˆ E represents an edge where a user clicks item ğ‘– ğ‘— after ğ‘– ğ‘– in sessions. Definition 2.2.3 Inter-session graph. As shown in Figure 3 (right), given a series of sessions S , we could construct a unified inter-session graph G = (V , E) between all the adjacent items for all the sessions, where V and E indicate the set of nodes and edges respectively. For an edge ğ‘’ ğ‘– ğ‘— âˆˆ E , it indicates the edge points from node ğ‘£ ğ‘– to node ğ‘£ ğ‘— . Definition 2.2.4 Hypergraph. Given a series of sessions S , we could construct a hypergraph G = (V , E) , where V is a node set, which consists of all the unique items I . We further define two adjacent metrics A ğ‘› 2 ğ‘’ âˆˆ R ğ‘€ Ã— ğ‘ and A ğ‘’ 2 ğ‘’ âˆˆ R ğ‘€ Ã— ğ‘€ , where ğ‘€ and ğ‘ are the number of hyperedges and items, respectively. Thus, if item ğ‘– belongs to the hyperedge ğ‘— , we set the element ğ‘ ğ‘› 2 ğ‘’ ğ‘– ğ‘— = 1, otherwise ğ‘ ğ‘› 2 ğ‘’ ğ‘– ğ‘— = 0. In addition, if hyperedge ğ‘— and hyperedge ğ‘˜ share the same items, we set the element ğ‘ ğ‘’ 2 ğ‘’ ğ‘—ğ‘˜ = 1, otherwise ğ‘ ğ‘’ 2 ğ‘’ ğ‘—ğ‘˜ =0. Based on the hypergraph, the high-order information in sessions can be captured efficiently. As shown in Figure 4, the hyperedge ğ‘’ âˆˆ E can be defined by: (1) the nodes sharing the same values of attributes ( i.e. , hypergraph with attributes) [82]; (2) the items from the same session [199] ( i.e. , hypergraph with session); (3) the item and its incoming items [92] ( i.e. , hypergraph with incoming items); (4) belonging to a specific contextual window [170] ( i.e. , hypergraph with slide windows); (5) a specific consecutive intent unit [58]. For instance, in [230] the hyperedge is defined based on the item side information, like item prices. ( i.e. , hypergraph with side information). 6 Li et al. Fig. 4. The diagram of hypergraphs. The shades of different colors represent different hyperedges. i 1 i 3 i 4 i 2 , , , ( ) i 2 i 6 i 5 i 5 ( , , , ) i 6 i 3 i 7 i 7 , , , ( ) Hypergraph with side-information Sessions Hypergraph with session i 3 i 6 i 5 i 2 i 7 i 4 i 1 i 5 i 1 i 4 i 2 i 6 i 7 i 3 Hypergraph with slide window i 2 i 1 i 3 i 4 i 6 i 5 i 7 Definition 2.2.5 Heterogeneous graph. Given a graph G = (V , E) , it consist of a node set V and an edge set E . We define a node type mapping function: ğœ™ : V â†’ M ğ‘›ğ‘œğ‘‘ğ‘’ and an edge type mapping function ğœ“ : E â†’ M ğ‘’ğ‘‘ğ‘”ğ‘’ . Denote M ğ‘›ğ‘œğ‘‘ğ‘’ and M ğ‘’ğ‘‘ğ‘”ğ‘’ are the sets of node types and edge types. Thus, if |M ğ‘›ğ‘œğ‘‘ğ‘’ | + |M ğ‘’ğ‘‘ğ‘”ğ‘’ | > 2, we define the graph G as a heterogeneous graph. Attribute to the various types of nodes and edges, the heterogeneous graph is capable of modeling more complicated structures against the homogeneous graph [184]. In SR, the user-item session graph [22, 125] and the item-attribute knowledge graph [112, 227] are the most commonly heterogeneous graph structures, which can be elucidated below: Â· User-Item Social Graph. Given a user set U and the historical interacted sessions of each user, we could construct a user-item session graph G = (I , U , E) based on user-item interacted records and the user's social network, where I , U are item nodes and user nodes, respectively. In [125], it contains two types of edges, i.e. , item-to-item and user-to-item . Specifically, if item ğ‘– ğ‘– and item ğ‘– ğ‘— are neighbors in sessions, we add an edge between them. Thus, the item transaction relations can be captured via item-to-item edge. Besides, if there are interactive records between user ğ‘¢ and item ğ‘– , we add an edge between them. Consequently, the user's historical interests can be modeled via user-item edges. Except for the above two types of edges in a user-item session graph, the social relations are also considered in [22, 172]. Specifically, if user ğ‘¢ ğ‘– and ğ‘¢ ğ‘— are friends or there exists some kind of interactive behavior ( i.e. , follow, shared, etc) between them, a user-to-user edge can also be added in the graph G . Moreover, if each session is regarded as a node, the user-to-session edge and item-to-session edge can also be created as an extension of the general user-item social graph [15],. Â· Item-Attributes Knowledge Graph. Given a session set S , an item set I and its relevant attributes set A , we could construct an item-attributed knowledge graph G = (I , A , E) , which contains two types of nodes (item node and attribute node) and two types of edges (item-to-item and item-to-attribute). If there is an ordered tuple ( ğ‘– ğ‘– , ğ‘– ğ‘— ) , we add an edge from item ğ‘– ğ‘– to ğ‘– ğ‘— . Furthermore, if item ğ‘– ğ‘˜ contains the attribute ğ‘ ğ‘– , we add an edge from ğ‘– ğ‘˜ to ğ‘ ğ‘– with the correspondent relation. As shown in Figure 5, we could construct a knowledge graph for a book recommendation scenario. The entities/nodes contain the book name and the corresponding attributes, e.g. , author, country, and genre. Apart from that, we could also define two relations, i.e. , published in and written by , and add edges from items to attributes. Â· User-Behavior Session Graph . Compared with the inter-session graph or the intra-session graph, the user-behavior session graph further defines the behavior relations, e.g, buy, click, between two adjacent items [149, 183]. Â· Spatialtemporal Session Graph . For some online service platforms, e.g. , Meituan and Yelp, both the user's location as well as time information are significant for the precise recommendation. Consequently, the authors [91] propose a spatiotemporal graph, which contains item, session, location, and time, four types of nodes, for session-based recommendation. Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 7 Fig. 5. The item-attributes knowledge graph. Session Author ï¼š Leo Tolstoy Country: Russia Published: 1869 Pages: 1225 Item and attributes Author ï¼š Xueqin Cao Country: China Published: 1792 Genre: Novel : item : author (entity) : county (entity) : written by (relation) : connection : published in (relation) Leo Tolstoy Marcel Proust Xueqin Cao J.D. Salinger MAN Overall, the connectivity and structure among the above graphs can be formalized as an adjacency matrix A âˆˆ R ğ‘ Ã— ğ‘ with A ğ‘– ğ‘— â‰  0 iff ( ğ‘£ ğ‘– , ğ‘£ ğ‘— ) âˆˆ E and A ğ‘– ğ‘— = 0 iff ( ğ‘£ ğ‘– , ğ‘£ ğ‘— ) âˆ‰ E , where ğ‘ is the total number of nodes. N ğ‘– indicates the neighbors of node ğ‘£ ğ‘– . In many works, the adjacency matrix A will be normalized as the formula: Ëœ A = Ai , j / Ë ğ‘— Aij .",
  "3 FEATURES AND CATEGORIZATION OF SR APPROACHES": "In this section, we will first summarize the features of SR. Then, a hierarchical categorization will be provided for different research lines organization.",
  "3.1 The Features of SR": "According to the aforementioned definitions, we summarize the features of SR as below. Â· Session Length. Compared with the sequential recommendation which organizes the user's whole historical interaction records in chronological order, SR only concentrates on the current ongoing sessions, thus, the length of sessions is quite limited, i.e. , the median length of sessions is less than six for most popular public datasets. Â· Dynamic and Timely Recommendation. In general, different from sequential recommendation, SR specializes in the current session. Therefore, it endeavors to model a user's short-term interests instead of the evolutionary process of the user's interests modeling in sequential recommendation. The dynamic and timely recommendation for the ongoing session is the aim of SR. Â· Adjacent Dependency. Although the items in sessions are organized chronologically, there are no obvious order patterns [34, 69, 177]. Consequently, many works [58, 70, 199, 205, 239] apply GNNs to model the co-occurrence between two items. Â· Beyond Item Information. Early attempts only adopt the ID as the identification of the item for recommendation. Currently, it becomes ubiquitous to fuse other external information, e.g. , item attributes, and interaction behaviors, to improve the explainability and performance of recommendations [36, 107, 112, 131, 191] (ref. Section 4.4 for detail). Â· Anonymous and Non-anonymous. As we discussed above, SR pays more attention to the current session modeling. Nevertheless, some papers [22, 125, 131, 155] propose personalized session-based recommendation (PSR) or sessionbased social recommendation (SSR), which consider the user historical records are non-anonymous. Hence, they utilize a user's whole historical sessions as auxiliary information to improve the performance of current session recommendation. In summary, we could find that the length of sessions is rather limited and there is no obvious dependency between two items in a session, in spite of the items organized in chronological order. Hence, most existing works focus on item correlation modeling with GNNs for a dynamic and timely recommendation. Additionally, although some works apply 8 Li et al. Fig. 6. The categorization of SR approaches. The gray boxes are representative models for each class. Popularity-based POP S-POP Conventional Model KNN Item-KNN, SKNN, VSKNN Matrix Factorization Decision Fusion FMC, FPMC Markov Chain RNNILSTMIGRU GRU4Rec; HRNN Sequence-based CNNICasual CNN HMN, GRec Model Variationa based VRM, CVRM, VASER Sequential Neural Network Attention Mechanism NARM, STAMP Session-based Transformer Transformer4Rec Bert4Rec Recommendation Memory-based RUM-I, CMN, CSRM Random Walk Intra-session Graph SR-GNN, GGNN, LESSR Sequence-based User-oriented A-PGNN, SERec Model Inter-session Graph Item-oriented GCR-GNN, MTD; DGTN Sequential Neural Network Hypergraph DHCN, SHARE Heterogeneous Graph SERec HGNN, KSTT Hybrid CoHHN, CA-TCN users' social networks or historical records as the auxiliary information for SR, modeling the current session is still the mainstream of this research venue. Moreover, more and more works emphasize the significance and efficiency of external information for recommendation. Hence, a surge of sophisticated model architectures emerges for side-information fusion.",
  "3.2 Classification of SR Methods": "The taxonomy of existing SR methods is presented in Figure 6. In this paper, we summarized the representative solutions as three main categories, i.e. , conventional methods , sequence-based methods , and graph-based methods . The conventional methods can be further divided into popularity-based methods , KNN, and matrix factorize. To be specific, the idea of popularity-based methods , e.g. , POP, S-POP, is to recommend the popular items to users while ignoring the cold start items. As the extension of popularity-based methods, frequent pattern or association rule mining approaches, e.g. , FP-tree, are also applied, which mine the frequent items and frequent patterns from raw data for recommendation [50, 115, 118, 120, 147, 214]. As for KNN-based methods [38, 54, 101, 109, 109], they rely on similarity calculation for recommendation, which can also be divided into item-oriented KNN and session-oriented KNN. The item-oriented KNN measures the similarity between a target item and candidate items and then recommends the closest ( i.e. , similar) items to users. In contrast, the session-oriented KNN first selects the most similar sessions via similarity calculation, thus, the candidate items from those sessions are collected for further recommendation. Matrix factorization is also a mature method for SR, in which a user's latent feature representation and an item's latent feature representation are learned for the user-item interaction matrix prediction [96, 97, 136]. Apart from that, Markov Chain is also a prominent recipe, which recognizes the next interaction prediction as a Markov Decision Process (MDPs) and learns a state transfer matrix for recommendation [19, 44, 49, 83, 137, 146, 153, 192, 196, 235, 244]. For instance, FMC [146] is a pioneering work that extracts sequential patterns to predict the next item based on Markov models. FPMC [137] models sequential behavior between every two adjacent items via the Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 9 personalized probability transition matrix factorization for recommendation. REKS [192] combines the MDP with a knowledge graph to generate a precise recommendation result and also provide an explanation simultaneously. Although the Markov chain achieved remarkable success for SR in the early years, the strong assumption that the next clicked item only depends on the previous one confines the development of this method. Attribute to the powerful feature representation ability of deep learning, sequential neural networks, e.g. , RNN, LSTM, and GRU, are carried out consecutively for SR [65, 133, 161, 173]. For instance, GRU4REC [65] is the first that applied RNNs to model the session information for the next item recommendation. To be specific, it stacks multiple GRU layers and applies a session-parallel mini-batch training strategy for performance improvements. HRNN [133] develops hierarchical RNNs with inter-session information to realize personalizing session-based recommendation. As a follow-up study [161], Tan et al. enhance RNNs by leveraging data augmentation. Hidasi and Karatzoglou [64] propose a new class of loss functions combined with modified sampling strategies to improve the performance of SR. Guo et al. [57] propose Hierarchical Leaping Network (HLN) with Leap Recurrent Unit (LRU) to decide whether the current item should be skipped or not. Being endowed with the property of global information modeling, the attention mechanism is also applied for SR. NARM [87] applies a hybrid encoder with an attention mechanism to model the user's sequential behavior and capture the main intentions in the current session. Liu et al. [105] take a similar idea but replace the recurrent neural network with multi-layer perception (MLP) and propose STAMP to enhance the influence of the latest interests in sessions for both long-term and short-term interests capture. Furthermore, CNN and Causal CNN are also proposed to capture n-gram multi-scale features for item representation and recommendation [154, 220]. Attribute to the remarkable performance achieved by Transformer [166] and BERT [76] in many NLP tasks, some researchers also elaborate on Transformer for SR, e.g. , Transformer4Rec [39] and BERT4Rec [160]. Other works, like memory neural networks [25, 43, 173] and Variational Encoder (VAE) are also explored. For example, in [186, 240], the latent variable module is introduced into sequential neural networks to improve the variation of recommendation. To sum up, we categorize the existing sequential neural networks with six aspects, as shown in Table 1. Although sequential methods achieve satisfactory performance in SR, they endeavor to the session's sequential information learning while ignoring the implicit dependency between items. Consequently, graph-based methods, a more flexible solution for transition pattern modeling, are accommodated for SR. Owing to the difference in item representation learning, graph-based methods can further be divided into random walk and GNNs. For random walk [56, 127, 210], in general, given a series of sessions, an item-item adjacent graph is constructed first. Then, different random walk strategies are adopted with unsupervised learning to generate item representations. For instance, DeepWalk [127] empirically learns a low-rank transformation of a normalized Laplacian matrix for recommendation. Node2vec [56] learns node representations based on the word2vec model for recommendation. To balance both the accuracy and scalability of SR, S-Walk [29] proposes a random walk with a restart strategy to capture inter-session and intra-session relations. In contrast, attributed to the process of information propagation and aggregation, GNNs are able to capture multi-hop contextual information between items for representation learning. Given that, GNNs demonstrate great superiority against sequential neural networks. As introduced in Section 2.2, based on the variation of graph structures, they can be divided into intra-session graphs, inter-session graphs, hypergraphs, heterogeneous graphs, and hybrids. A carefully selected representative and state-of-the-art GNN-based method for SR is presented in Tabel 2. In the following sections, we will present a comprehensive framework and summarize the technical details of GNN-based methods and sequential neural networks in SR. 10 Li et al. Table 1. Comparison of representative sequential neural networks with respect to six aspects including motivation, session, sequential modeling, prediction, loss function, and datasets. Illustration of Abbreviations:",
  "(1) Motivation:": "a) GNN for SR: A groundbreaking work that applies GNN for SR. b) Historical Info, User and Social Info: Introducing neighbor sessions via different strategies e.g. , social network, for SR. c) External Info: Introducing external information or side-information, e.g. , interaction behavior types, item's attributes, time and location, knowledge graph, other domain data, and order information for SR. d) Multi Interests: Construct multi-interest representations for each user and SR. e) High-order Connection: One-hop connection is insufficient for SR. Consequently, sophisticated graph structures ( e.g. , self-loop, short-cuts) are designed for global information or multi-hops information modeling. f) Efficiency: Focusing on low computational complexity and timely recommendation.",
  "(2) Neighbor Session:": "a) Sharing: the sessions sharing the same items with the current session are considered as neighbor sessions. b) User: a user's all historical sessions are selected as neighbor sessions. c) Current: only consider the current session for SR. d) Sim: calculate the similarity between sessions for neighbor session selection. e) Time Close: slice the timeline with the close principle for neighbor session selection. (3) Sequential Modeling: a) Att: variant attention mechanisms. b) Gate: Gate mechanism. c) MF: Matrix factorization.",
  "(4)Datasets :": "a) Dig: Diginetica. b) Yoo: Yoochoose (Yoochoose 1/4, Yoochoose 1/64 are the most common for SR). c) Gow: Gowalla. d) FM: Last.FM. e) Retail: Retailrocket. f) Now: Nowplaying. g) Amz: Amazon. h) Mov: MovieLens. i) Oth: some other less common datasets in SR, e.g. , Tianchi, G1 news, CLASS, Delicious, OTTO.",
  "4 SEQUENTIAL AND GRAPH NEURAL NETWORKS FOR SR": "As aforementioned in 3.2, the solutions of SR main include GNN-based methods and sequential neural network-based methods. In this section, we will first give an overview of the pipelines with regard to those two mainstream attempts. Then, the key modules and the technical details are introduced respectively. Specifically, subsection 4.3, 4.4 and 4.9 are the key modules contained by both two frameworks, the key modules introduced in subsection 4.5 only belong to the sequential neural networks and the remain subsections are typical modules in GNN. Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 11 Table 2. Summarization of representative GNN-based works with motivation, session, graph construction, item representation, session representation, loss function, and datasets. Illustration of Abbreviations:",
  "(2) Session Selection:": "a) Sharing: the neighbor sessions which contain the same item as the current session. b) User: a user's all historical sessions are selected as neighbor sessions. c) Current: only consider the current session for SR. d) Sim: calculate the similarity between two sessions or observe the last item in sessions for neighbor session selection. e) All: defines all the training sessions as neighbors.",
  "(3) Graph Construction:": "a) Intra: Intra-session graph. b) Inter: Inter-session graph. c) Het-Social: Heterogeneous graph with social relations. d) Het-Behavior: Heterogeneous graph with different interaction behavior types. e) Het-KG: Heterogeneous graph with knowledge graph. f) Het-Attribute: Heterogeneous graph with item correspondent attributes. g) Het-Spatialtemporal: Heterogeneous graph with time and location. h) Hyper: Hypergraph. i) Session: Introduce the session as a node in graph construction. j) ğœ– -Neighbor: construct a graph based on the ğœ– -Neighbor strategy. k) Variant Intra: intra-session graph with a virtual star or self-loop edge and short-cut edge. (4) Information Propagation and Session Representation: a) Att: various attention mechanisms. b) AvgP: Average pooling. c) Gate: Gate mechanism. d) RW: Random walk.",
  "(5) Loss Function:": "a) CE: Cross-entropy loss. b) Oth: auxiliary loss functions for multi-task learning, e.g, link prediction, matrix learning, ELBO loss for item representation distribution reconstruction, loss function with regularization term, etc.",
  "(6)Datasets :": "a) Dig: Diginetica. b) Yoo: Yoochoose (Yoochoose 1/4, Yoochoose 1/64 are the most common for SR). c) Gow: Gowalla. d) FM: Last.FM. e) Retail: Retailrocket. f) Now: Nowplaying. j) Amz: Amazon. h) Oth: other datasets in SR, e.g. , Wechat, Cosmetics, Aotm, 30music, JD, Trivago, Delicious, Foursquare. 12 Li et al. Fig. 7. The framework of sequential neural networks for SR. Time i 1 i 2 i 3 i 4 i 5 ... h1 h2 h3 h4 h5 0.02 0.05 0.08 0.22 0.63 Softmax Item Embedding Session Representation Session Probability Distribution Item Embedding 0.30 0.08 0.16 0.05 0.27 i 1 i 2 i 3 i 4 i 5 Sequence Modeling Layer Session Representation Layer Prediction Layer Hidden State ...",
  "4.1 The Overall Framework of SR with Sequential Neural Networks": "Aim to model the sequential information, the sequential neural networks, e.g. , RNN, LSTM, GRU, Memory Neural networks, Transformer, and BERT, are proposed successively for SR. The completed pipeline of these works can be formalized in Figure 7, which mainly includes the sequence modeling layer, the session representation layer, and the prediction layer. More concretely, given a sequence ğ‘  = [ ğ‘– 1 , ğ‘– 2 , .., ğ‘– ğ‘š ] as the input of a sequential neural network, we first generate the embedding X of each item in ğ‘  which will undergo a sequential modeling layer SM (Â·) , a session representation layer SR (Â·) and a prediction layer P (Â·) in order for the next item prediction. The whole process can be formalized as follows:   where SM (Â·) can be RNN, LSTM, GRU [23, 173], attention [23, 39] and memory neural network [173] for sequential information modeling. SR (Â·) is the session representation layer, where soft-attention mechanism [23, 39, 173] are commonly used. H and S are the items' representation and session representation respectively. As for the prediction layer P (Â·) , the inner product between session representation and item embedding is most used for prediction score generation. After that, the loss function e.g. , cross-entropy loss, TOP1-max loss, BPR-max loss, etc, is designed for model training and optimization.",
  "4.2 The Overall Framework of SR with GNN": "Encouraged by the powerful expressive ability of complicated graph structure data modeling in many scenarios ( e.g. , social network, traffic prediction [74], and molecular representation for drug discovery [86]), in 2019, Wu et al. [195] proposed SR-GNN which is a pioneering work to apply GNN for SR. Since then, a plethora of GNN-based works emerged. In general, the whole process of GNN for SR includes five key modules: session selection, graph construction, information propagation and aggregation, session representation, and target item prediction, which are shown in Figure 8. Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 13 Fig. 8. The framework of GNNs for SR. Session Set Graph Neural Network Item Representation Session Representation Probability Distribution Input Information Propagation Layer Session Representation Layer Prediction Layer Item Embedding Specifically, given a session set S or a session ğ‘  , a graph G should be constructed first. Then, variants GNNs are designed to obtain the item representation via information propagation and aggregation. The item representation H will be fed into the session representation layer SR (Â·) for session representation generation. Finally, the predicted item probability Ë† ğ‘¦ can be generated via inner production. This process can be formalized as below.",
  "4.3 Neighbor Sessions Selection": "Considering the ongoing session for SR is insufficient, as it can only reveal users' current intentions. Therefore, many works tend to collect neighbor sessions as auxiliary information to facilitate the performance of recommendation. Summarizing the relevant works with sequential neural networks and GNNs, neighbor session selection strategies can be divided into four categories (as shown in Figure 9): adjacent-based, similarity-based, user-based and all together. For adjacent-based methods, they can be further split into space adjacent ( i.e. , different sessions contain the same items) and time adjacent ( i.e. , sessions are neighbors on the timeline). In terms of similarity-based methods, similarity calculation based on item ID and session representations are two main strategies. (1) Adjacent-based. Given a current session ğ‘  , the adjacent-based strategy considers the sessions that contain the same items with ğ‘  or adjacent to ğ‘  in the timeline should be neighbors. Specifically, it contains: Â· Item Sharing. Given a current session ğ‘  , we retrieve the sessions that contain the same item with ğ‘  as the neighbor sessions [198, 199, 230]. Â· Share Last Item. In [16], the authors suppose the last clicked item ğ‘– ğ‘™ reveals users' short-term or current interests. Thus, the sessions that contain ğ‘– ğ‘™ as the current session are selected as neighbor sessions. Â· Time Closest Principle. Given a current session, [173] defines the closest ğ‘š sessions in the timeline before the current session as neighborhood sessions. (2) Similarity-based. Similarity-based strategies believe the sessions that contain similar patterns as the current session are neighbor sessions. Thus, it is of critical importance to define a proper similarity measurement. 14 Li et al. Â· Number of Duplicates. Given the current session ğ‘  , [239] counts the number of the same items shared by both the current session and other sessions. Then, top-k sessions with the most same items are collected as the neighbor sessions of ğ‘  . Â· Binary Cosine Similarity. Given a current session ğ‘  ğ‘– , we denote the interacted item in the current session at time ğ‘¡ as ğ‘¥ ğ‘¡ . First, the sessions that contain the item ğ‘¥ ğ‘¡ are searched as candidates S ğ‘€ . Then, based on the binary cosine similarity, topğ¾ most similar sessions are obtained as the final neighbor sessions [6, 110]. The binary cosine similarity is defined as follows.  where ğ‘  ğ‘— âˆˆ S ğ‘€ . | ğ‘  ğ‘– âˆ© ğ‘  ğ‘— | count the same items between ğ‘  ğ‘– and ğ‘  ğ‘— . | ğ‘  ğ‘– | and | ğ‘  ğ‘— | are the total numbers of items in ğ‘  ğ‘– and ğ‘  ğ‘— respectively. Â· Cosine Similarity. Instead of item ID for similarity calculation as above two strategies, in [124, 215], the similarity between sessions is calculated based on the session representations generated by a neural network. Denote si as the current session representation and sj âˆˆ S ğ‘€ as the representation of session ğ‘  ğ‘— , where ğ‘€ is a memory block for session reserve. Thus, we could first obtain ğ¾ sessions in memory ğ‘€ based on the FIFO (first-in-first-out) strategy as the candidate, then calculate their cosine similarities to ğ‘  ğ‘– . Finally, top-k most similar sessions are selected as neighbor sessions. The cosine similarity is formalized as below:  Â· Inner Production with Projection Layer . Given any of two sessions ğ‘  ğ‘– and ğ‘  ğ‘— , denote si , sj are the representation of them. In [188], the authors calculate the similarity between s ğ‘– and s ğ‘— as follows.  Â· Euclidean Distance with Gaussian Kernel. Given any of two sessions ğ‘  ğ‘– and ğ‘  ğ‘— , denote si , sj are the representation of them. We calculate the Euclidean distance with the Gaussian kernel to measure the similarity between ğ‘  ğ‘– and ğ‘  ğ‘— .  where ğ‘‘ ğ‘– ğ‘— is the Euclidean distance between s ğ‘– and s ğ‘— , i.e. , ğ‘‘ ğ‘– ğ‘— = | | s ğ‘– -s ğ‘– | | 2 . ğ‘‘ âˆ— is the minimal Euclidean distance between ğ‘  ğ‘– and its neighbors. (3) User-based. User-based methods can also be classified into two categories: Â· Self-based. It considers all the historical sessions of this user as neighbor sessions, thus, the user's long-term or global preference [134] can be captured. Â· Self and social connections. Not only the user's whole historical sessions will be attained, but the sessions from his or her friends are also selected as neighbor sessions [172]. (4) All. It can also be divided as below: Â· All sessions. In [91], all the training sessions are selected as neighbors to construct a global information graph for recommendation. Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 15 Fig. 9. The categorization of neighbor session selection strategy. Same User User Time Time Closest Principle (m=3 Number of Duplicates (m=2) Share Last Item User 2 Item Sharing Sessions Neighbor Session Selection Strategies Â· Sessions from the same batch. Considering leverage all the sessions as neighbors for graph construction will be intractable in the practical scenario, to balance the efficiency and the precision, in [94], the authors consider the sessions from the same batch for explicit and implicit graph construction.",
  "4.4 Item and Side Information Embedding": "Summarizing existing works, existing embeddings can be divided into three categories: item-oriented ( e.g. , item embedding, attributes embedding), interaction-oriented ( e.g. , user embedding, interaction behaviors embedding), and position-oriented ( e.g. , positional embedding, session segment embedding), which are shown in Figure 10. Â· Item Embedding. Inspired by word embedding in NLP [4, 31, 71, 113], we could also represent the item ID via an item embedding. Hence, each discrete item ID can be projected into a continuous high-dimensional latent space as the input of neural networks. Â· Attribute Embedding. As the side information could also reveal the characteristics of items that will benefit to SR. Consequently, some works also consider the items' corresponding attributes, e.g. , category, brand, and the title of keywords, for embedding [36, 107, 112, 145]. Â· Item Description Embedding. The text information, e.g. , item description, title, and user's comments, contains fertile information about items' features and users' preferences. Thus, we could apply language models like BERT to generate an item description embedding [128] or leverage a pre-trained Word2Vec word embedding for title and meta attributes encoding [55] for a better recommendation. Â· Interacted Behavior Embedding. Users' micro-behaviors, e.g. , reading comments, adding to a cart, etc, which reveal a fine-grained and deep understanding to the user's preference. Therefore, a user's interacted behavior embedding can also be introduced for SR [112, 191, 221]. Â· User Embedding. To inject the user's identification information into the item embedding, in [131, 191], the item embedding concatenates the interacted user embedding for item representation initialization. Â· Time Interval Embedding. In [227], the authors believe that the time intervals between two adjacent behaviors could reveal the process of user's interest shift. Thus, the time interval embedding is created. Â· Positional Embedding. To supervise the model to capture the order information and the temporal effect of the input, positional embedding is proposed. It mainly includes (1) absolute positional encoding (modeling the item's absolute 16 Li et al. Fig. 10. A categorization of various embeddings. Item Embedding Item-oriented Attribute Embedding Behavior Embedding Embedding Interaction-oriented User Embedding Time Interval Embedding Absolute Position Embedding Position-oriented Position Embedding Relative Position Embedding Segmentation Embedding Learnable Position Embedding order in a session) which can be generated via sine and cosine functions [166]; (2) relative positional encoding, which encodes the distance information between two items in an order. Different from the absolute encoding, the relative positional encoding can be translation-invariant and free of the length of sessions [148, 156]; (3) learnable positional encoding, which is the same as item embedding, i.e. , based on the position index for embedding generation [160, 222]. Â· Session Segment Embedding. Similar to the segment embedding in NLP to identify two adjacent sentences [32], session segment embedding can be used to indicate the ordinal position of the current session [144] to the whole historical sequence. To be specific, given an item ğ‘– and its correspondent auxiliary information, we could obtain the item initial representation x via the concatenation, addition operation or gate mechanism [102, 144, 160, 222].  where x IE ğ‘– , x IOE ğ‘– , x NOE ğ‘– , x POE ğ‘– are item embedding, item-oriented embedding, interaction-oriented embedding, and position-oriented embedding, respectively. ğœ is an activation function, W is a learnable parameter matrix.",
  "4.5 Sequence Modeling Layer on Sequential Neural Networks": "Given a sequence ğ‘  = [ ğ‘– 1 , ğ‘– 2 , .., ğ‘– ğ‘š ] and the item embedding X , where X = [ x1 , x2 , ..., xm ] . We denote the output of the sequence modeling layer as H = [ h1 , h2 , ..., hm ] , which are the hidden state of LSTM/GRU or the updated item representation generated via Self-attention/MLP or CNN. Hence, the sequence modeling solution can be summarized below. Â· GRU/LSTM. Analyzing the existing sequential neural networks in SR, GRU, and LSTM are the most common backbone for sequential information modeling [23, 87, 173]. Â· Self-attention/MLP. In early works [23, 69, 191], MLP is elaborated for a user's all historical sessions modeling. With the prosperity of Transformer [166], self-attention also proposes to capture the correlation between items [39]. Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 17 Fig. 11. Various graph structures for SR. Inter-session graph with â‚¬-Neighbor (e=2) Sessions NXT PRE Teleportation Graph S2SG NPL SELF Intra-session graph with virtual satellite node (b) Inter-session graph with various relations Â· CNN/Causal CNN with Dilated . In [155], a densely connected 1D CNN with multiple convolution blocks is designed to capture n-gram features in the current session. As a variation of 1D CNN, the 1D causal CNN with dilated convolution possesses a wider receptive field, thus, it is applied for long-term dependency modeling in SR [220]. Â· Others. In [122, 222], the authors argue that the self-attention mechanism will introduce irrelevant information. Thus, a self-attention module with ğ›¼ -entmax is proposed as an alternative of softmax function for item and session representation. In [57], an extended GRU module with a leap gate, i.e. , Leap Recurrent Unit (LPU), is designed to capture users' various preferences in the current session. In [207], the authors fuse Fast Fourier Transforms (FFT) with Transformer to enhance the session representation for recommendation.",
  "4.6 Graph Construction for GNN": "Given neighbor sessions introduced in Section 4.3, we could construct different graphs for information propagation and aggregation. Specifically, apart from the intra-session graph, inter-session graph, hypergraph, and heterogeneous graph, the four standard graph structures introduced in Section 2.2, some other dedicated graphs are proposed to capture more complicated transaction patterns and user preferences for recommendation, as shown in Figure 11. Â· Inter-session Graph with ğœ– -Neighbor. As shown in Figure 11 (a), aside from the connection between adjacent items, the ğœ– -neighbor items are also considered for global graph construction, where ğœ– is the window size [187, 236]. Â· Intra-session Graph with Various Relations. In [82, 242], the authors define four types of relations or edges, i.e. , PRE, NET, SELF, NPL (as shown in Figure 11), for graph construction. Additionally, in [62, 221], the authors further consider the user's micro behaviors and item's multi-faceted global relations ( i.e. , base, sequential, co-occurrence, and incompatible) as external information for graph construction. Â· Intra-session Graph with Virtual Satellite Node. In [123], a virtual node is created for each session to aggregate the global information, as shown in Figure 11 (c). Similar to [123], in [150], the multi-virtual satellite nodes are created as the user's interests might be various and uncertain. Â· Teleportation Graph . As shown in Figure 11 (d), compared with the inter-session graph, the teleportation graph [29] adds an edge between any of the two items in each session. Therefore, a fully connected graph is created for global session information extraction. Â· S2SG . As shown in Figure 11 (e), based on the intra-session graph, in [21], the authors add a self-loop and a shortcut from item ğ‘ to the item ğ‘ , where ğ‘ is the precursor node of ğ‘ . 18 Li et al. Overall, the sophisticated graph structures aim to capture global information, fuse more external information, and learn more complicated transaction patterns for SR.",
  "4.7 Information Propagation and Aggregation Layer for GNN": "Through information propagation and aggregation under different graph structures, GNNs can update the node representation with an iterative approach. Comparing recent works, we summarize the following typical architectures for information propagation and aggregation. Â· Average Pooling . Average Pooling is an efficient yet effective method, which is widely used for information propagation and aggregation. Concretely, it can be formulated as:  where h ( ğ‘™ + 1 ) ğ‘– are the item ğ‘– representation derived from ğ‘™ multi-hops aggregation. N ğ‘– is the first-order neighbors of item ğ‘– . W is the learnable parameters, which can be removed in some works [22, 58]. Â· GCN. Compared with average pooling, GCN [77] considers the effect of node degree on information propagation and aggregation, which can be formalized below:  where ğœ (Â·) is an activation function, e.g. , ReLU or Sigmoid. ğ‘‘ ğ‘– and ğ‘‘ ğ‘— are the degrees of node ğ‘– and node ğ‘— , respectively. In many papers, Equation (9) can also be formalized as H ( ğ‘™ + 1 ) = ğœ ( Ëœ D -1 / 2 Ëœ A Ëœ D -1 / 2 H ( ğ‘™ ) W ( ğ‘™ ) ) , where Ëœ A = A + I , is an adjacency matrix defined by the graph structure. Ëœ D ğ‘–ğ‘– = Ë ğ‘— Ëœ A ğ‘– ğ‘— . Additionally, the adjacent matrix A can further be split into A ğ‘–ğ‘› and A ğ‘œğ‘¢ğ‘¡ for node representation respectively, where A ğ‘–ğ‘› and A ğ‘œğ‘¢ğ‘¡ are the point in and point out, respectively [131, 205]. Then, we fuse the representation from these two adjacent matrices via MLP layer or others. Moreover, motivated by the memory decay in reinforcement learning, [241] uses an exponential denominator to re-scale the edge weights on the adjacent matrix. Therefore, the historical edge weight impact will become smaller with the increase of time discrepancy between previous interactions and the current interaction. Similar to [241], TMI-GNN [150] defines an item-level transition interval as the edge weight to model the temporal information in sessions. Â· GAT. GAT [167] proposes an attention mechanism to learn the importance of each neighbor node for information propagation.  where a and W are learnable parameter matrices, LeakReLU (Â·) is a nonlinearity function (with negative input slope ğ›¼ = 0 . 2). Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 19 Â· Soft-attention . Different from the average pooling, or GAT, the soft-attention mechanism learns the importance of each first-order neighbor with various functions.    where W is a learnable matrix. ğœ (Â·) is an activation function, e.g. , Sigmoid, ReLU. If we implement the LeakyReLU as the activation function of the concat operation, it converts to GAT module. Â· Routing. Compared with the attention mechanism, the routing algorithm uses a Squash function for information aggregation without any trainable parameters for a more efficient model training.   where Squash( Â· ), which can be regarded as an activation function, Squash ( x ) = x | | x | | Â· | | x | | 2 1 +| | x | | 2 . In [237], the authors replace the Squash function as a unit normalization function i.e. , ( h ğ‘– ) = h ğ‘– | | h ğ‘– | | for information propagation and aggregation. Â· Hybrid. GCN/GAT/Soft-attention with GRU unit is the most common hybrid method for information propagation and aggregation. In [12, 84, 130, 195], the node representation will first be modeled via a GCN, GAT, or Soft-attention for neighbor information aggregation. Afterwards, they will be fed into a GRU unit and obtain the hidden states as the updated item representation. Formally, the update functions are given as follows:  where h ( ğ‘™ ) ğ‘– are the representations of items in session ğ‘  after ğ‘™ -th updated. a ( ğ‘™ ) ğ‘ ,ğ‘– is the item ğ‘– representation after ğ‘™ -th graph convolution. ğœ (Â·) is the Sigomid activation function. W ğ‘§ , U ğ‘§ , W ğ‘Ÿ , U ğ‘Ÿ , W ğ‘œ , U ğ‘œ are learnable parameters of GRU module. In [149], the authors apply GAT with GRU to generate the item representation, then a soft-attention is used for representation fusion. Â· Others. In [29, 236], a random walk with matrix learning is proposed for item representation generation. In [237], the authors first propose item entropy to select topğ‘€ similar items as neighbors, then use GAT for item representation update. In terms of GAT, [58] applies an element-wise max operation to replace concatenation or mean operations 20 Li et al. Fig. 12. Different stages of external information fusion. Session Fusion Graph Attribute- Attribute-2 Fusion Intra-Session Graph Autribule Gruph Information Proragntion Information Propapation Item Attribute - Attribute-2 Item nbure - Fmbaldine Emhedding Emhedding Embedding Embedding Embedding Embedding 1) Fusion-First 2) Fusion-in-Process 3) Fusion-Last for the multi-heads representation fusion. In [225], the GAT is first applied for item representation in the current session, then the max pooling and self-attention are applied for the user's historical preference modeling. Besides, an addition operation is further utilized to fuse the above two representations. In [123], a virtual node is created on the current intra-session graph to aggregate the information from all the nodes in the session. Thus, based on the gate mechanism, the neighbor nodes and virtual start representations can be fused. In [59], the authors combine GGNN with an ordinary differential equation (ODE) to capture continuous-time temporal information for session recommendation. It is noteworthy that introducing external information via GNNs is also a desideratum for SR. Reviewing the existing works, the external information fusion methods mainly include (1) max pooling or average pooling; (2) concatenation operation; (3) gate mechanism; (4) convolution operation. Additionally, as shown in Figure 12, the fusion stages mainly also include (1) Fusion-First; (2) Fusion-in-Process; and (3) Fusion-Last. Fusion-First means the external information will be fused at the embedding stage, then update the item embedding via information propagation on a graph. Fusionin-Process requires us first construct a heterogeneous graph that contains both the item node and the node with regard to external information, then inject the external information into the item representation by the process of information propagation and aggregation [36, 227, 230]. Fusion-Last requires us to construct various graphs for items and the correspondent external information respectively, then update the representation based on GCN, GRU, or other methods. Afterward, the external information and item representation can be fused [22, 112, 122, 221, 239].",
  "4.8 Session Representation Layer": "The item representation generated by the sequential neural networks or GNN will be further utilized for session representation. The typical methods include: Â· Last Item Representation. For a current session ğ‘  , we suppose the last clicked item could reveal the user's current preference. Hence, the last item representations are obtained for session representation generation, e.g. , s = Wh ğ‘š , where W is a linear projection operation, in some works it is omitted [57, 75, 144, 160, 203, 220]. Â· Mean Pooling/Max Pooling . Given a session and its item representations, the mean pooling or max pooling can be applied for session representation s [16, 70]. Â· Concate. In [16], the item representations after mean-pooling and max-pooling are concate.  Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 21 Â· Concatenation with MLP. To fuse the information of each item, the MLP layer with residual connection is used to generate session representation [110].  where W 1 , W 2 , b 1 , b 2 are learnable parameters. Â· GRU. In [236, 237], the GRU module is applied to learn session representation.  where h ğ‘– is item ğ‘– representation and h â€² ğ‘– is the correspondent hidden state of GRU. The last hidden state h ğ‘š is attained as the session representation. Â· Soft-attention. There are numerous methods to generate the session representation via the soft-attention mechanism. For instance, [131] supposes the last item in a session could reveal the user's current or local preference. Hence, we could model the impact of each previous item on the last item via MLP or softmax to capture the user's global preference for session representation generation.  where h ğ‘– âˆˆ H and h ğ‘™ are the item ğ‘– representation and last item representation. q , W 1 , W 2 , W 3, b 1 , b 2 are learnable matrices. ğœ is the activation function. Â· Self-attention with MLP. In [205, 227], the authors use Transformer for session representation.  where W ğ‘„ , W ğ¾ , W ğ‘‰ , W 1 , W 2 , b 1 , b 2 are learnable matrices. The dropout regularization and a residual connection are also applied during training. For a current session ğ‘  , the last dimension of E is obtained as the global embedding of session ğ‘  . Â· Memory Neural Network (MNN). In general, a memory neural network is composed of two components: (1) a memory block to store the historical information and (2) a read/write operator to update the information representation. Based on MNN, neighbor sessions (selected by the method introduced in Section 4.3) in memory blocks can be introduced for representation argumentation. Specifically, let s ğ‘ as the current session representation and s ğ‘– as the neighbor session ğ‘  ğ‘– 's representation. Therefore, the current session representation s ğ‘ can be updated via 22 Li et al. soft-attention mechanism [173], which is formalized as below.  where s ğ‘– is the session representation, ğ‘  ğ‘– âˆˆ ğ‘€ . ğœ is a temperature coefficient. Â· Others. In some works, the user information representation, position information representation, or the session representation from different modules are also fused via concatenation operation or linear projection for session representation generation [16, 42, 131, 187]. As the last clicked item could represent the user's current preference or short-term interests, many works will fuse the last item representation h ğ‘™ and the global session representation s ğ‘” generated by above methods for final session representation. The fusion methods mainly include concatenation, addition, weight, and so on.  where s ğ‘” and s ğ‘™ are the global session representation and local session representation respectively. ğ‘¤ can be defined as a hyperparameter or learnable parameter, i.e. , w = ğœ ( w [ s ğ‘” | | s ğ‘™ ]) , to balance the information from local and global.",
  "4.9 Prediction Layer and Loss Functions": "In most works, the inner production between item embeddings x ğ‘– and session representation s is calculated to yield the prediction score, which will be further fed into a softmax function for probability distribution generation. Apart from that, the MLP layer with an activation function ( e.g. , tanh, sigmoid) is also a popular strategy to generate the scores of predicted items and this method is more efficient against the inner product.  As to the loss function, summarizing the existing works it can be divided into three categories: (1) standard loss function and its improvement ( e.g. , cross-entropy Loss, BPR Loss, BPR-max Loss); (2) auxiliary loss function, which usually integrates a recommendation loss with an auxiliary loss or regularization terms; (3) multi-task loss function ( e.g. , link prediction and entity representation with knowledge graph).",
  "(1) Standard Loss Function and Its Improvements.": "Â· Cross-entropy Loss. Cross-entropy loss is the most commonly used loss function for model training in SR.  where ğ‘› is the total number of training samples. Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 23 Â· BPR Loss. Bayesian Personalized Ranking [136] is a standard pairwise ranking loss, which optimizes the difference between the scores of the positive item and sampled negative items.  where ğ‘ ğ‘† is sampled negative items, ğœ is the sigmoid activation function. Â· TOP1 Loss. Compared with the BPR loss function, TOP1 loss adds a regularization term to force the scores of the negative items to be around zero for a stable training process.  Â· BPR-max. Bayesian Personalized Ranking (BPR) aims to maximize the probability that the target score should be larger than negative sample scores. However, intuitively, if the score of the negative sample is already well below that of the target, there is nothing to learn from that negative sample anymore. Meanwhile, the gradient is always discounted by the total number of negative samples, which will also lead to the vanishing of gradients. To relieve this issue, Hidasi and Karatzoglou [64] propose a BPR-max loss function, which uses softmax scores on the negative examples. Thus, more relevant negative samples will produce high gradients and obtain more focus on the training process.  where ğ‘ ğ‘† is sampled negative samples. Besides, a softmax weighted ğ‘™ 2 regularization over the scores of the negative samples is also added. ğœ† is a regularization hyperparameter. Â· List-wise Ranking Loss. Compared with cross-entropy loss, list-wise loss only considers the top-k scores instead of all the items' in cross-entropy loss for model training [191]. Given a session ğ‘  , denote Ë† ğ‘¦ ğ‘– as the target item predicted probability generated by softmax function, and Ë† ğ‘¦ ğ‘— , ğ‘— âˆˆ G ğ‘˜ as the top-k predict scores. The list-wise ranking loss can be formalized as:  Â· Robust Distance Measuring (RDM). Inspired by contrastive learning, in [66], the authors propose RDM for better alignment and uniformity properties of item embedding.  where x + is the embedding of ground-truth of session ğ‘  . xi âˆˆ X is the item ğ‘– embedding. s is the session representation. ğœ is a hyper-parameter to adjust the distribution yield by the softmax function for different scenarios adaptation. Â· Adaptive Weight Loss. Motivated by Focal loss [99], adaptive weight loss is adopted to arrange different weights for each sample and hinder the gradient of negative samples from dominating the whole training process.   24 Li et al. where ğ›¾ is a temperature coefficient to control the effect of modulating factor to the loss, i.e. , expand the contribution of hard samples to the loss while diminishing that of simple samples. (2) Auxiliary Loss Function. Contrastive Learning with InfoNCE is also a commonly used loss as a regularization term to alleviate the data sparsity issue and improve recommendation performance [198, 199].  where ğœ™ (Â·) is the discriminator function that requires two vectors as the input and then scores the agreement between them with inner production or other methods. ( ğœƒ, ğœƒ + ğ‘– ) is the positive pairs, e.g. , the last item representation in the current session and the embedding of top-K highest confidence items predicted by the model, or session representations generated by two GNNs. As a regularization term, contrastive learning tends to optimize uniformity and alignment of representations [179]. Therefore, the item representations will be uniformly distributed on the unit hypersphere, and the item representations of positive pairs will also close together. Apart from that, other auxiliary loss functions like distillation loss [218] in causal learning SR are also adopted to block the influence of the shortcut path and accentuate the significance of causal dependencies within the session graph. In [150], the authors propose TMI-GNN to extract multiple interest representations for users and design an interest-independent loss that hires distance correlation measures as a regularizer to encourage the multi-interests representations to be diverse. (3) Multi-task based Loss Functions. Multi-task learning (MTL) is a satisfied recipe that exploits useful information from other related learning tasks to supervise the model to perform better on the original task [139, 145, 234]. There are also some works that apply MTL for SR. For instance, in [149], the authors consider multiple interaction types, such as click and purchase, in sessions. Consequently, an interaction session can also be constructed. Afterward, the authors apply two cross-entropy losses for the next behavior type prediction and item recommendation. Furthermore, some works [112, 227] incorporate item-relevant knowledge and construct knowledge graphs for SR. Therefore, the link prediction or entity representation tasks are also introduced for knowledge embedding learning.",
  "5 DATASETS, EVALUATION METRICS AND COMPLEXITY": "In this section, we will first analyze the statistical characteristics of publicly real-world datasets and evaluation metrics s with regard to the accuracy and diversity that are commonly used in SR.",
  "5.1 Public Datasets": "According to the existing works in SR, we summarize eight popular public datasets that cover E-commerce, Music and Video, Job Position, and Chick-in scenarios. The statistical characteristics after preprocessing 4 are presented in Table 3. 4 Following existing works, we filter out all sessions whose length is 1 and items appearing less than 5 times. 5 https://www.kaggle.com/chadgostopp/recsys-challenge-2015 6 https://tianchi.aliyun.com/dataset/dataDetail?dataId=42 7 https://competitions.codalab.org/competitions/11161 8 https://www.kaggle.com/retailrocket/ecommerce-dataset 9 http://millionsongdataset.com/lastfm/ 10 https://www.kaggle.com/chelseapower/nowplayingrs 11 http://2016.recsyschallenge.com/ 12 http://snap.stanford.edu/data/loc-gowalla.html Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 25 Table 3. The statistical characteristics of commonly used public datasets after preprocessing for SR. # means the total numbers, Avg. calculates the mean value. Â· Yoochoose is the dataset of RecSys Challenge 2015, which contains a stream of user clicks and buy events on an online webshop within six months. Since the set of Yoochoose is extremely large, the most recent portions 1/64 and 1/4 subsample of all sessions are usually used as the training set, denoted as \"Yoochoose1/64\" and \"Yoochoose1/4\", respectively [12, 21, 105, 135, 183, 195]. Â· Tmall comes from the IJCAI-15 competition, which contains users' shopping logs on the Tmall online shopping platform. Â· Digineitca is a personalized e-commerce research challenge dataset released in CIKM CUP 2016. The dataset contains transition histories, which are suitable for session-based recommendation. Â· RetailRocket contains user behavior data and item properties that collected from a real-world e-commerce website. Â· Last.FM is a music artist recommendation dataset published by Celma et al [11]. Â· NowPlaying dataset comes from [223] and is created from music-related tweets, which illustrate the music-listening behavior of users. Â· Xing Recsys Challenge 2016 Dataset 13 contains user interactions (click, bookmark, reply, and delete) on a job posting platform for 770k users over an 80-day period. Â· Gowalla contains the check-in data and social network on a location-based social networking website. It is widely used for point-of-interest recommendation. Observing Table 3, we could find that the length of sessions in existing public SR datasets is rather limited. As exemplified by the average/median lengths of sessions are 5 . 12/4 . 0 and 3 . 95/3 . 0 for Diginetica and Yoochoose, the most two popular session-based datasets. Therefore, we believe there is no obvious order dependency between any of the two items, indicating that modeling the items' correlation with GNNs rather than sequential information with sequential neural networks is more suitable for SR [20].",
  "5.2 Evaluation Metrics": "Evaluation Metrics for Accuracy. Accuracy aims to measure the alignment of recommendation results and user interests. In general, HR@K (Hit Rate calculated over top-K items), MRR@N (Mean Reciprocal Rank calculated over top-K items), and NDCG@K (Normalized Discounted Cumulative Gain calculated over top-K items) are widely used evaluation metrics for SR performance comparison, where ğ¾ = 5 , 10 , 20 are the most common settings for evaluation. 13 http://2016.recsyschallenge.com/ 26 Li et al. Â· HR@K . The HR@K score measures whether the target item is included in the top-K recommendations of the recommended list.  where ğ‘ is the number of test sessions in the dataset and ğ‘› â„ğ‘–ğ‘¡ counts the number that target items that appear in the top K position of the ranking list. Â· MRR@K . The MRR@K is a ranking evaluation matrix. When the target item Ë† ğ‘– is not in the top K position, the ğ‘…ğ‘ğ‘›ğ‘˜ ( Ë† ğ‘– ) is set to 0. It calculates as follows,  where S ğ‘¡ğ‘’ğ‘ ğ‘¡ are the set of test sessions. Rank ( Ë† ğ‘– ) is the position of item ğ‘– in the recommendation list. The MRR is a normalized ranking of hits. The higher the score, the better the quality of the recommendation, i.e. , a higher score indicates a higher ranking position of the target item. Â· NDCG@K . The NDCG estimates the ranking order of the recommendation list. The same as MRR, if the target item Ë† ğ‘– is not in the top K position, the ğ‘…ğ‘ğ‘›ğ‘˜ ( Ë† ğ‘– ) is set to 0.  Evaluation Metrics for Diversity. Diversification is first concerned in information retrieval (IR) community where researchers endeavor to disambiguate the input query to cover the user's real intent via diversification optimization [30]. In recommendation system, diversity is related to how different the recommended items are with respect to each other [10], which aims to alleviate the filter bubble problem. Consequently, the recommendation diversity can be identified as the average pairwise dissimilarity between items in list. In SR, intra-list diversity, coverage, and their variants are the most common metrics for diversity measurement [106, 165, 216]. Â· ILD . Intra-List Distance (ILD) measures the average distance between every pair of items in recommendation list (RL), which can be formalized as,  where ğ‘‘ ğ‘– ğ‘— is the Euclidean distance of category embeddings with respect to item ğ‘– and item ğ‘— . | RL | is the number of recommendation items in RL. Â· Coverage@K. Coverage@K measures how many different categories appear in the top-K recommendation items.  where ğ¶ ğ‘– is the category of item ğ‘– . Apart from ILD and coverage, some other metrics, such as long-tail coverage, which measure the diversity performance in long-tail items, and relevance sensitive expected intra-list diversity (RR-ILD) which considers the ranks and relevance of top ğ¾ recommendation simultaneously in ILD calculation. Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 27",
  "6 CHALLENGES AND DIRECTIONS": "GNNs and sequential neural networks have greatly prompted SR research but also are accompanied by some challenging issues. In this section, we outline the following prospective research directions, which we believe are critical to the further development of SR.",
  "6.1 More External Information": "As we discussed in Section 4.4, a plethora of works incline to fuse more external information to explore the user's multi interests or item's complicated transaction patterns for SR. However, comparing the existing efforts, we find that the external information should be selected carefully and not all of them are desiderata for performance improvement. Besides, the fusion method is also significant for SR. To sum up, we consider there are two open issues with regard to external information that deserve to be discussed. Â· What kind of external information is necessary for SR? We divide the external information into three categories: itembased, interaction-based, and position-based. Therefore, some of them are costly to model ( e.g. , item descriptions and user comments), some are not appropriate for SR ( e.g. , user personal information, since the sessions are expected to be anonymous in SR), some may not be necessary ( e.g. , position or order information [144], since the length of sessions is limited and there is no obvious order dependency between two items). In addition, for different recommendation scenarios, the required information may also be different. For instance, for news, book, or music recommendation, users will be interested in the categories of the items, while for product recommendations, the item's brand and price probability are more effective for SR. Therefore, what kind of external information should be considered for the practical scenarios is really important. Â· How to balance the trade-off between effectiveness and efficiency of fusion methods? The fusion methods can be various in SR, e.g. , embedding addition or concatenation, self-attention, and gate mechanism. Note that as the simplest function for information fusion, the addition operation will not change the model structure or increase numerous external parameters or computations. But at the same time, the external information will not be exploited thoroughly and the representation ability of models will not be enhanced either. Although concatenation could facilitate the representation ability of models, it will also increase the computational complexity and the number of parameters dramatically. In addition, concatenation can not model the implicit correlation and effects between any of the two external information. Recently, self-attention has been proven as an effective method for external information fusion [203], but it will also increase the number of parameters and computation cost. Therefore, more elegant fusion methods are expected to balance the trade-off between the representation ability and the number of parameters for an effective and efficient recommendation. Â· Couple v.s Decouple, which is better for SR? As shown in Figure 12, there are three phases for external information fusion. (1) Fusion first. Most of the works consider fusing the external information in the embedding stage. Thus, the external information and item embedding will couple together for SR. (2) Fusion in process. GNN-based methods regard the external information as nodes. Then, a heterogeneous graph is constructed, based on information propagation and aggregation the external information can be fused. (3) Fusion last. The external information and item embedding are decoupled and modeled respectively, then, fuse them in the end. For instance, we could construct two graphs for both item and external information, capitalizing on two GNNs for item and external information representation learning, which will be fused together in the end. In general, by decoupling external information representation with item representation, we could fully exploit the features from different priorities and avoid mixed correlation effects. 28 Li et al. Conversely, modeling the external information with a unified framework will limit the capacity of models to extract implicit features encapsulated in the heterogeneous side information, while it has a greater advantage in efficiency.",
  "6.2 Session Selection and Graph Construction": "As the length of sessions is quite limited, i.e. , less than six for most public datasets, many works introduce neighbor sessions for graph construction. Thus, sophisticated transaction patterns can be captured for SR. For neighbor session selection and graph construction, there are also some issues that need to be discussed. Â· Scalability Graphs in SR. Graph structure is pivotal for GNN-based SR. To harness the fertile information from neighbor sessions, large-scale and complicated graphs are constructed, which will include billions of nodes and edges in practical scenarios. Besides, each node contains a variety of features. Hence, it is nontrivial to update and apply such a huge graph for information propagation and prediction straightforwardly. To this end, sampling is a widely adopted solution to reduce the graph size and alleviate the above issue. For instance, we could randomly sample a fixed number of neighbors from original graphs ( e.g. , Graphsage [61]) or employ the random walk strategy for sampling ( e.g. , PinSage [217]). However, these methods are accompanied by a high degree of randomness, which may incur an unstable model training. Consequently, how to design a scalable graph structure is vital for SR. Â· Dynamic Graphs in SR. Furthermore, in the real world, the items and their relations are changing over time. To maintain up-to-date recommendation, graph structures should be also adjusted and updated dynamically. Most of the existing studies are based on a static graph structure, few works pay attention to dynamic graphs. Thus, it is a largely under-explored realm and deserves further study. Â· Self-learning Graph Structure. Obtaining a proper graph structure requires considerable effort and this process is also heuristic and problem-specific. Moreover, although recent research [23, 94] has revealed the necessity of modeling the implicit connections between items for SR, most of the existing graph-based methods can only capture the item relations with a few hops, which can not explore the implication relations between items thoughtfully. We believe the self-learning strategy for graph construction is a reasonable method to alleviate the above problems, and it is quite common in many other tasks [41, 85, 197]. Therefore, self-learning graphs can also be look-forwarded for SR.",
  "6.3 Diverse and Uncertain Representation of User Interests": "Despite the superior performance of existing methods for SR, most of the works concentrate on a single and fixed user interest representation for recommendation and fail to disentangle multiple interests of users. Considering the diversity of user interests, a fixed representation is insufficient and leads to sub-optimal results. In addition, users' sequential behaviors are uncertain. Therefore, compared with one single fixed representation, it is worthwhile to capture interest diversity, inject uncertainties, and provide more flexibility for SR. Â· Diverse Representation of User Interests. Rather than learning a single and fixed user interests representation, some works endeavor to extend such one-fold vector to multiple vectors with capsule networks or attention mechanism [141] for multi-interest, short-term and long-term interest, and interest diversity representation. For instance, Tian et al. [162] apply GCN and capsule network to capture user's multi-level and multi-interests representation for recommendation. Guo et al. [57] propose a Hierarchical Leaping Network (HLN), which extracts various subsequences from the current session. Thus, by learning the representation of each subsequence, the user's multi-interest representation can be captured. Li et al. [84] split the item embedding into several chunks and apply GGNN for each chunk to learn user interests representation with multiple factors for SR. [164] uses clustering technology for similar Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 29 products to improve the diversity of recommendations. Aside from multi-interest representation, Li et al. [95] believe the user interest can be split into interest trend and interest diversity. The former one is determined by her education level, income and occupation, while the latter is easier impacted on the advertisement or marketing. Therefore, the authors tailored two modules for user interest trend and diversity modeling. Although there are some works that focus on multiple interests representation, it is still in a preliminary stage and many issues ( e.g. , how to determine the number of different interests for each user in an adaptive fashion; how to fuse those interests representation for SR) need to be further explored. Â· Uncertain Representation of User Interests. Compared with vector representation, distribution representation could inject uncertainties and provide more flexibility, it has been attracting interest from the research community [5, 63, 159, 168]. For the recommendation system, the item embedding can also be initialized as a Multi Gaussian Distribution governed by a mean vector and a covariance vector. The mean vector could reveal the user's various basic preferences, while the covariance vector injects a potential uncertainty. Fan et al. elaborate to apply distribution representation and Wasserstein Distance for recommendation [46, 47]. However, for SR, there are very few works focused on this issue. It will be interesting and valuable to explore uncertain representation in SR.",
  "6.4 Explainability and Privacy Production for SR": "Apart from the accuracy, the explainability, security, and privacy production of outputs are also significant and expected for a good recommendation system. In general, the explanation of recommendation aims to answer 'why', that is, why the items are recommended, an explainable recommendation can improve the transparency and persuasiveness of systems, boosting the satisfaction and stickiness of users [231]. With the prosperity of deep learning, sequential and graph-based neural networks behave as black boxes, leaving this research area more challenging. Investigating the recent efforts, these methods can be divided into two categories: (1) the explanation generation based on language models [13, 53, 88, 89, 104, 208] knowledge graphs [51, 202] or image visualizations [24], and (2) the explainability of deep learning model structures [24, 233]. However, very few works [119, 193] focus on the explainability in session-based recommendation. As for privacy production in recommendation, the user's historical interaction records encompass privacy information such as gender, age, and even political orientation, which can be inferred by the recommender system [243]. Thereby, it is inappropriate to request such private specific historical interactions for recommendation. Recently attempts resort to unlearning strategies that eradicate the sensitive data in model training to tackle this problem [8, 204]. Overall, we believe it is an interesting and promising topic for future research in session-based recommendation.",
  "6.5 Streaming or Online SR": "As discussed in 6.2, in a real-life situation, sessions are dynamically produced as a stream, while most of the relevant work elaborates on training the recommendation system with the historical sessions which preserve the users' long-term static interests. Hence, it might be inappropriate to apply a static model for new coming sessions as users' preferences are changing over time. To alleviate this issue, the model should be online updated with the latest sessions. Therefore, it is a challenge to effectively learn users' dynamic and real-time preferences for better SR. Existing research [60, 131] maintains a reservoir to update the model for an online recommendation. To be specific, the reservoir can be the incoming sessions that contain new items or new users. Then, for each session, a sample probability is generated via Wasserstein distance (also known as Earth Mover's Distance [138]) or ranking-based distance [182]. After that, the sampling distribution is applied to update the reservoir. Thus, the model can be updated timely based on the updated 30 Li et al. reservoir. However, the above methods heavily rely on the sampling strategy and distance computation, which is also a cost-expensive process. Rendering it difficult to balance the trade-off between effectiveness and efficiency. To overcome this issue, some works [90, 200, 201] propose lightweight model architectures based on model compression techniques, such as low-rank decomposition, hash coding, and quantization. However, these solutions require a pre-defined fixed compressed ratio to retrain, leading to sub-optimal results when the ratio is inappropriate. How to effectively capture users' dynamic preference change and recommendation in real-time is a promising direction.",
  "6.6 Causal Debias and Denoise in SR": "Due to the exposure mechanism, popularity effects, and the feedback loop in recommendation system, bias and noise problems become serious, and heavily deteriorate the recommendation effectiveness [14]. To explain the relations between a cause and effect, causality-based methods, e.g. , causal-inference, and causal graph, are a major solution to debias and provide explanations of the recommendation system [103, 180, 189, 206, 212, 218, 232, 238]. However, there are very few works specializing in this issue on SR. We believe the causal model will bring SR research into a new frontier.",
  "6.7 Reinforcement Learning for SR": "Different from supervised and unsupervised learning, reinforcement learning [78] focuses on goal-directed learning that maximizes the total reward achieved by an agent when interacting with its environment. Hence, it is a potential and prominent solution to model interactions between the user and agent, capture rapid changes in users' preferences, and realize dynamic recommendation. Recent years have witnessed significant progress of reinforcement learning in recommendation systems [26, 100]. Specifically, modeling the user's interaction behaviors as a decision-making process, we could maintain and update a corresponding recommendation policy in real time. For instance, in [152], the authors treat music playlist generation as a language modeling process, Thus, an attention-based language model with the policy gradient is applied for recommendation. Wang et al. [174] propose a method named knowledge-guided reinforcement learning (KERL), which integrates knowledge graphs into reinforcement learning. To be specific, KERL adopts TransE [7] with the MLP layer to predict future knowledge of user preferences and recommendation. Overall, the above works with reinforcement learning could be summarized as (1) simulating users' interaction in SR for dynamic and timely recommendation; (2) capturing the interest shift of users; (3) filtering the noise in sessions; and (4) selecting the valuable items for SR. However, model-free deep reinforcement learning requires a significant number of samples as there is no guarantee that the received state is useful. In SR, the length of sessions is very short, accompanied by an extremely large action space ( i.e. , number of items), thus it will require more high-quality samples to cover the exploration space, which hinders the further development of reinforcement learning in SR.",
  "6.8 SR with Language Model and Diffusion Model": "Encouraged by the remarkable success of large language models (LLM) in NLP, utilizing language models for recommendation has become a cutting-edge very recently. Reviewing existing research, prompt or in-context learning, and parameter-efficient fine-tuning (PEFT) are prominent solutions in this venue. For instance, Wang and Lim [171] design different prompting strategies to investigate the performance of GPT-3 [79] for next-item prediction. Along this line of research, Hou et al. [67] devise various prompting templates and formalize the sequential recommendation as a conditional ranking task. Analyzing the zero-shot learning capability of GPT-3.5. Apart from that, TALLRec [3], applies LoRA [68] to effectively fine-tune LLaMA [163] on recommendation datasets. M6-Rec [37] obtains M6 [98], a Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 31 visual-linguistic pre-trained model, as the backbone and proposes an improved prompt tuning, named option tuning, for task-specific parameter fine-tuning. To alleviate the hallucination and improve the quality of outputs, retrievalaugmented generation (RAG) technology has become ubiquitous in LLM [52], where the external knowledge can be retrieved as auxiliary information to guide the LLM for generation. In recommendation with LLM, based on the user's historical interacted items, RAG is employed as a retriever for candidate selection or reranking [40, 224]. [9, 33] show that RAG is facilitated to address the cold-start problem and enhance the diversity of recommendation. However, there are few efforts that explore the application of LLM and RAG in SR, leaving this research direction to be cultivated. Unlike the auto-regressive generation methods used in LLMs, diffusion models have established a new paradigm for generative tasks and have achieved remarkable success across a broad spectrum of applications [27, 35, 93, 211]. Overall, diffusion model can be split into two stages, the diffusion stage aiming to corrupt the original input as a Gaussian distribution, and the reverse stage where to recover the data from a Gaussian noise iteratively conditioned on the input. Diffurec [93] is the first work that applies diffusion for sequential recommendation. Besides, some works explore the diffusion model in CTR prediction [181], multi-scenario recommendation [185]. However, no works investigate the performance of diffusion model in SR. The discrete embedding space and the time cost in the reverse stage also impede the widespread application in on-line recommendation. The diffusion model is an uphill research area and I believe it has a promising prospect in SR.",
  "7 CONCLUSION": "This paper systematically investigated session-based recommendation with GNNs and sequential neural networks. Specifically, we first clarify the corresponding definitions, and concepts, analyzing the features of SR. Then, we review more than 150 papers and provide a systematic taxonomy to organize the existing works with regard to their key motivations and typical models. Besides, we conduct a unified framework for SR with GNNs and sequential neural networks respectively. After that, a detailed introduction to the key modules including neighbor session selection, graph construction, embedding, information propagation and aggregation, sequence modeling, session representation, prediction, and loss function, are also provided. Finally, we introduce the challenges and point out new potential directions for the research on SR. It is our hope that this survey can provide readers with a comprehensive understanding of the key aspects, main challenges, and notable progress in this area, and shed some light on future research.",
  "REFERENCES": "[1] Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. IEEE transactions on knowledge and data engineering (2005). [2] Marko BalabanoviÄ‡ and Yoav Shoham. 1997. Fab: content-based, collaborative recommendation. Commun. ACM (1997). [3] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems . 1007-1014. [4] Yoshua Bengio, RÃ©jean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. Proc. of NeurIPS (2000). [5] Aleksandar Bojchevski and Stephan GÃ¼nnemann. 2018. Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking. In Proc. of ICLR . [6] Geoffray Bonnin and Dietmar Jannach. 2014. Automated generation of music playlists: Survey and experiments. ACM Computing Surveys (CSUR) (2014). [7] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. Proc. of NeurIPS (2013). [8] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021. Machine unlearning. In Proc. of SP . 141-159. [9] Diego Carraro and Derek Bridge. 2024. Enhancing Recommendation Diversity by Re-ranking with Large Language Models. arXiv preprint arXiv:2401.11506 (2024). 32 Li et al. [40] Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, RenÃ© Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, and Silvia Milano. 2024. A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys). arXiv preprint Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 33 arXiv:2404.00579 (2024). [41] Ailin Deng and Bryan Hooi. 2021. Graph neural network-based anomaly detection in multivariate time series. In Proc. of AAAI . [42] Zhi-Hong Deng, Chang-Dong Wang, Ling Huang, Jian-Huang Lai, and S Yu Philip. 2022. GË† 3SR: Global Graph Guided Session-Based Recommendation. IEEE Transactions on Neural Networks and Learning Systems (2022). [43] Travis Ebesu, Bin Shen, and Yi Fang. 2018. Collaborative memory network for recommendation systems. In Proc. of SIGIR . [44] Magdalini Eirinaki, Michalis Vazirgiannis, and Dimitris Kapogiannis. 2005. Web path recommendations based on page ranking and markov models. In Proceedings of the 7th annual ACM international workshop on Web information and data management . [45] Ehsan Elahi, Sajid Anwar, Babar Shah, Zahid Halim, Abrar Ullah, Imad Rida, and Muhammad Waqas. 2024. Knowledge Graph Enhanced Contextualized Attention-Based Network for Responsible User-Specific Recommendation. ACM Transactions on Intelligent Systems and Technology (2024). [46] Ziwei Fan, Zhiwei Liu, Shen Wang, Lei Zheng, and Philip S Yu. 2021. Modeling Sequences as Distributions with Uncertainty for Sequential Recommendation. In Proc. of CIKM . [47] Ziwei Fan, Zhiwei Liu, Yu Wang, Alice Wang, Zahra Nazari, Lei Zheng, Hao Peng, and Philip S Yu. 2022. Sequential recommendation via stochastic self-attention. In Proc. of WWW . 2036-2047. [48] Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020. Deep learning for sequential recommendation: Algorithms, influential factors, and evaluations. ACM Transactions on Information Systems (2020). [49] Shanshan Feng, Xutao Li, Yifeng Zeng, Gao Cong, Yeow Meng Chee, and Quan Yuan. 2015. Personalized ranking metric embedding for next new poi recommendation. In Proc. of IJCAI . [50] Rana Forsati, Mohammad Reza Meybodi, and A Ghari Neiat. 2009. Web page personalization based on weighted association rules. In 2009 International Conference on Electronic Computer Technology . [51] Zuohui Fu, Yikun Xian, Ruoyuan Gao, Jieyu Zhao, Qiaoying Huang, Yingqiang Ge, Shuyuan Xu, Shijie Geng, Chirag Shah, Yongfeng Zhang, and Gerard de Melo. 2020. Fairness-Aware Explainable Recommendation over Knowledge Graphs. In Proc. of SIGIR . 69-78. [52] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023). [53] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems . 299-315. [54] Alireza Gharahighehi and Celine Vens. 2020. Making session-based news recommenders diversity-aware. In Proceedings of the Workshop on Online Misinformation-and Harm-Aware Recommender Systems . [55] Shansan Gong and Kenny Q Zhu. 2022. Positive, Negative and Neutral: Modeling Implicit Feedback in Session-based News Recommendation. In Proc. of SIGIR . [56] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proc. of KDD . [57] Cheng Guo, Mengfei Zhang, Jinyun Fang, Jiaqi Jin, and Mao Pan. 2020. Session-based recommendation with hierarchical leaping networks. In Proc. of SIGIR . [58] Jiayan Guo, Yaming Yang, Xiangchen Song, Yuan Zhang, Yujing Wang, Jing Bai, and Yan Zhang. 2022. Learning Multi-granularity Consecutive User Intent Unit for Session-based Recommendation. In Proc. of WSDM . [59] Jiayan Guo, Peiyan Zhang, Chaozhuo Li, Xing Xie, Yan Zhang, and Sunghun Kim. 2022. Evolutionary Preference Learning via Graph Nested GRU ODE for Session-based Recommendation. In Proc. of CIKM . [60] Lei Guo, Hongzhi Yin, Qinyong Wang, Tong Chen, Alexander Zhou, and Nguyen Quoc Viet Hung. 2019. Streaming session-based recommendation. In Proc. of KDD . [61] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Proc. of NeurIPS (2017). [62] Qilong Han, Chi Zhang, Rui Chen, Riwei Lai, Hongtao Song, and Li Li. 2022. Multi-Faceted Global Item Relation Learning for Session-Based Recommendation. In Proc. of SIGIR . [63] Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. 2015. Learning to represent knowledge graphs with gaussian embedding. In Proceedings of the 24th ACM international on conference on information and knowledge management . [64] BalÃ¡zs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks with top-k gains for session-based recommendations. In Proc. of CIKM . [65] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [66] Yupeng Hou, Binbin Hu, Zhiqiang Zhang, and Wayne Xin Zhao. 2022. Core: simple and effective session-based recommendation within consistent representation space. In Proc. of SIGIR . 1796-1801. [67] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2024. Large language models are zero-shot rankers for recommender systems. In European Conference on Information Retrieval . 364-381. [68] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [69] Liang Hu, Longbing Cao, Shoujin Wang, Guandong Xu, Jian Cao, and Zhiping Gu. 2017. Diversifying Personalized Recommendation with User-session Context.. In Proc. of IJCAI . 34 Li et al. Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 35 [103] Dugang Liu, Pengxiang Cheng, Hong Zhu, Zhenhua Dong, Xiuqiang He, Weike Pan, and Zhong Ming. 2021. Mitigating Confounding Bias in Recommendation via Information Bottleneck. In Proc. of RecSys . [104] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is chatgpt a good recommender? a preliminary study. ArXiv preprint (2023). [105] Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. 2018. STAMP: short-term attention/memory priority model for session-based recommendation. In Proc. of KDD . [106] Siyi Liu and Yujia Zheng. 2020. Long-tail session-based recommendation. In Proceedings of the 14th ACM conference on recommender systems . 509-514. [107] Yuanxing Liu, Zhaochun Ren, Wei-Nan Zhang, Wanxiang Che, Ting Liu, and Dawei Yin. 2020. Keywords generation improves e-commerce session-based recommendation. In Proc. of WWW . [108] Zhongqi Lu, Zhicheng Dou, Jianxun Lian, Xing Xie, and Qiang Yang. 2015. Content-based collaborative filtering for news topic recommendation. In Proc. of AAAI . [109] Malte Ludewig and Dietmar Jannach. 2018. Evaluation of session-based recommendation algorithms. User Modeling and User-Adapted Interaction (2018). [110] Anjing Luo, Pengpeng Zhao, Yanchi Liu, Fuzhen Zhuang, Deqing Wang, Jiajie Xu, Junhua Fang, and Victor S Sheng. 2020. Collaborative Self-Attention Network for Session-based Recommendation.. In Proc. of IJCAI . [111] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. 2008. Sorec: social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information and knowledge management . [112] Wenjing Meng, Deqing Yang, and Yanghua Xiao. 2020. Incorporating user micro-behaviors and item knowledge into multi-task learning for session-based recommendation. In Proc. of SIGIR . [113] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013). [114] Koji Miyahara and Michael J Pazzani. 2000. Collaborative filtering with the simple bayesian classifier. In Proc. of PRICAI . [115] Bamshad Mobasher, Honghua Dai, Tao Luo, and Miki Nakagawa. 2001. Effective personalization based on association rule discovery from web usage data. In Proceedings of the 3rd international workshop on Web information and data management . [116] Natwar Modani, Parul A Mittal, Amit A Nanavati, and Biplav Srivastava. 2002. Series of dynamic targeted recommendations. In International Conference on Electronic Commerce and Web Technologies . [117] Natwar Modani, Yogish Sabharwal, and S Karthik. 2005. A framework for session based recommendations. In International Conference on Electronic Commerce and Web Technologies . [118] MarÃ­a N Moreno, Francisco J GarcÃ­a, M JosÃ© Polo, and Vivian F LÃ³pez. 2004. Using association analysis of web data in recommender systems. In International Conference on Electronic Commerce and Web Technologies . [119] Jyoti Narwariya, Priyanka Gupta, Garima Gupta, Lovekesh Vig, and Gautam Shroff. 2023. X4SR: Post-Hoc Explanations for Session-based Recommendations.. In Proc. of SIGIR . [120] Utpala Niranjan, RBV Subramanyam, and V Khanaa. 2010. Developing a web recommendation system based on closed sequential patterns. In International Conference on Advances in Information and Communication Technologies . [121] Kai Ouyang, Xianghong Xu, Miaoxin Chen, Zuotong Xie, Hai-Tao Zheng, Shuangyong Song, and Yu Zhao. 2023. Mining interest trends and adaptively assigning sample weight for session-based recommendation. In Proc. of SIGIR . 2174-2178. [122] Kai Ouyang, Xianghong Xu, Chen Tang, Wang Chen, and Haitao Zheng. 2022. Social-aware Sparse Attention Network for Session-based Social Recommendation. In Proc. of EMNLP Findings . [123] Zhiqiang Pan, Fei Cai, Wanyu Chen, Honghui Chen, and Maarten de Rijke. 2020. Star graph neural networks for session-based recommendation. In Proc. of CIKM . [124] Zhiqiang Pan, Fei Cai, Yanxiang Ling, and Maarten de Rijke. 2020. An intent-guided collaborative machine for session-based recommendation. In Proc. of SIGIR . [125] Yitong Pang, Lingfei Wu, Qi Shen, Yiming Zhang, Zhihua Wei, Fangli Xu, Ethan Chang, Bo Long, and Jian Pei. 2022. Heterogeneous global graph neural networks for personalized session-based recommendation. In Proc. of WSDM . [126] Andreas Peintner, Amir Reza Mohammadi, and Eva Zangerle. 2023. SPARE: Shortest Path Global Item Relations for Efficient Session-based Recommendation. In Proc. of RecSys . [127] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proc. of KDD . [128] Michael Potter, Hamlin Liu, Yash Lala, Christian Loanzon, and Yizhou Sun. 2022. GRU4RecBE: A Hybrid Session-Based Movie Recommendation System (Student Abstract). In Proc. of AAAI . [129] Shutong Qiao, Wei Zhou, Junhao Wen, Hongyu Zhang, and Min Gao. 2023. Bi-channel Multiple Sparse Graph Attention Networks for Session-based Recommendation. In Proc. of CIKM . 2075-2084. [130] Ruihong Qiu, Jingjing Li, Zi Huang, and Hongzhi Yin. 2019. Rethinking the item order in session-based recommendation with graph neural networks. In Proc. of CIKM . [131] Ruihong Qiu, Hongzhi Yin, Zi Huang, and Tong Chen. 2020. Gag: Global attributed graph neural network for streaming session-based recommendation. In Proc. of SIGIR . 36 Li et al. Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 37 38 Li et al. Graph and Sequential Neural Networks in Session-based Recommendation: A Survey 39",
  "keywords_parsed": [
    "recommendation survey",
    "session-based recommendation",
    "graph neural networks",
    "sequential neural networks"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions"
    },
    {
      "ref_id": "b2",
      "title": "Fab: content-based, collaborative recommendation"
    },
    {
      "ref_id": "b3",
      "title": "Tallrec: An effective and efficient tuning framework to align large language model with recommendation"
    },
    {
      "ref_id": "b4",
      "title": "A neural probabilistic language model"
    },
    {
      "ref_id": "b5",
      "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking"
    },
    {
      "ref_id": "b6",
      "title": "Automated generation of music playlists: Survey and experiments"
    },
    {
      "ref_id": "b7",
      "title": "Translating embeddings for modeling multi-relational data"
    },
    {
      "ref_id": "b8",
      "title": "Machine unlearning"
    },
    {
      "ref_id": "b9",
      "title": "Enhancing Recommendation Diversity by Re-ranking with Large Language Models"
    },
    {
      "ref_id": "b40",
      "title": "A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)"
    },
    {
      "ref_id": "b41",
      "title": "Graph neural network-based anomaly detection in multivariate time series"
    },
    {
      "ref_id": "b42",
      "title": "GË† 3SR: Global Graph Guided Session-Based Recommendation"
    },
    {
      "ref_id": "b43",
      "title": "Collaborative memory network for recommendation systems"
    },
    {
      "ref_id": "b44",
      "title": "Web path recommendations based on page ranking and markov models"
    },
    {
      "ref_id": "b45",
      "title": "Knowledge Graph Enhanced Contextualized Attention-Based Network for Responsible User-Specific Recommendation"
    },
    {
      "ref_id": "b46",
      "title": "Modeling Sequences as Distributions with Uncertainty for Sequential Recommendation"
    },
    {
      "ref_id": "b47",
      "title": "Sequential recommendation via stochastic self-attention"
    },
    {
      "ref_id": "b48",
      "title": "Deep learning for sequential recommendation: Algorithms, influential factors, and evaluations"
    },
    {
      "ref_id": "b49",
      "title": "Personalized ranking metric embedding for next new poi recommendation"
    },
    {
      "ref_id": "b50",
      "title": "Web page personalization based on weighted association rules"
    },
    {
      "ref_id": "b51",
      "title": "Fairness-Aware Explainable Recommendation over Knowledge Graphs"
    },
    {
      "ref_id": "b52",
      "title": "Retrieval-augmented generation for large language models: A survey"
    },
    {
      "ref_id": "b53",
      "title": "Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5)"
    },
    {
      "ref_id": "b54",
      "title": "Making session-based news recommenders diversity-aware"
    },
    {
      "ref_id": "b55",
      "title": "Positive, Negative and Neutral: Modeling Implicit Feedback in Session-based News Recommendation"
    },
    {
      "ref_id": "b56",
      "title": "node2vec: Scalable feature learning for networks"
    },
    {
      "ref_id": "b57",
      "title": "Session-based recommendation with hierarchical leaping networks"
    },
    {
      "ref_id": "b58",
      "title": "Learning Multi-granularity Consecutive User Intent Unit for Session-based Recommendation"
    },
    {
      "ref_id": "b59",
      "title": "Evolutionary Preference Learning via Graph Nested GRU ODE for Session-based Recommendation"
    },
    {
      "ref_id": "b60",
      "title": "Streaming session-based recommendation"
    },
    {
      "ref_id": "b61",
      "title": "Inductive representation learning on large graphs"
    },
    {
      "ref_id": "b62",
      "title": "Multi-Faceted Global Item Relation Learning for Session-Based Recommendation"
    },
    {
      "ref_id": "b63",
      "title": "Learning to represent knowledge graphs with gaussian embedding"
    },
    {
      "ref_id": "b64",
      "title": "Recurrent neural networks with top-k gains for session-based recommendations"
    },
    {
      "ref_id": "b65",
      "title": "Session-based recommendations with recurrent neural networks"
    },
    {
      "ref_id": "b66",
      "title": "Core: simple and effective session-based recommendation within consistent representation space"
    },
    {
      "ref_id": "b67",
      "title": "Large language models are zero-shot rankers for recommender systems"
    },
    {
      "ref_id": "b68",
      "title": "Lora: Low-rank adaptation of large language models"
    },
    {
      "ref_id": "b69",
      "title": "Diversifying Personalized Recommendation with User-session Context"
    },
    {
      "ref_id": "b103",
      "title": "Mitigating Confounding Bias in Recommendation via Information Bottleneck"
    },
    {
      "ref_id": "b104",
      "title": "Is chatgpt a good recommender? a preliminary study"
    },
    {
      "ref_id": "b105",
      "title": "STAMP: short-term attention/memory priority model for session-based recommendation"
    },
    {
      "ref_id": "b106",
      "title": "Long-tail session-based recommendation"
    },
    {
      "ref_id": "b107",
      "title": "Keywords generation improves e-commerce session-based recommendation"
    },
    {
      "ref_id": "b108",
      "title": "Content-based collaborative filtering for news topic recommendation"
    },
    {
      "ref_id": "b109",
      "title": "Evaluation of session-based recommendation algorithms"
    },
    {
      "ref_id": "b110",
      "title": "Collaborative Self-Attention Network for Session-based Recommendation"
    },
    {
      "ref_id": "b111",
      "title": "Sorec: social recommendation using probabilistic matrix factorization"
    },
    {
      "ref_id": "b112",
      "title": "Incorporating user micro-behaviors and item knowledge into multi-task learning for session-based recommendation"
    },
    {
      "ref_id": "b113",
      "title": "Efficient estimation of word representations in vector space"
    },
    {
      "ref_id": "b114",
      "title": "Collaborative filtering with the simple bayesian classifier"
    },
    {
      "ref_id": "b115",
      "title": "Effective personalization based on association rule discovery from web usage data"
    },
    {
      "ref_id": "b116",
      "title": "Series of dynamic targeted recommendations"
    },
    {
      "ref_id": "b117",
      "title": "A framework for session based recommendations"
    },
    {
      "ref_id": "b118",
      "title": "Using association analysis of web data in recommender systems"
    },
    {
      "ref_id": "b119",
      "title": "X4SR: Post-Hoc Explanations for Session-based Recommendations"
    },
    {
      "ref_id": "b120",
      "title": "Developing a web recommendation system based on closed sequential patterns"
    },
    {
      "ref_id": "b121",
      "title": "Mining interest trends and adaptively assigning sample weight for session-based recommendation"
    },
    {
      "ref_id": "b122",
      "title": "Social-aware Sparse Attention Network for Session-based Social Recommendation"
    },
    {
      "ref_id": "b123",
      "title": "Star graph neural networks for session-based recommendation"
    },
    {
      "ref_id": "b124",
      "title": "An intent-guided collaborative machine for session-based recommendation"
    },
    {
      "ref_id": "b125",
      "title": "Heterogeneous global graph neural networks for personalized session-based recommendation"
    },
    {
      "ref_id": "b126",
      "title": "SPARE: Shortest Path Global Item Relations for Efficient Session-based Recommendation"
    },
    {
      "ref_id": "b127",
      "title": "Deepwalk: Online learning of social representations"
    },
    {
      "ref_id": "b128",
      "title": "GRU4RecBE: A Hybrid Session-Based Movie Recommendation System (Student Abstract)"
    },
    {
      "ref_id": "b129",
      "title": "Bi-channel Multiple Sparse Graph Attention Networks for Session-based Recommendation"
    },
    {
      "ref_id": "b130",
      "title": "Rethinking the item order in session-based recommendation with graph neural networks"
    },
    {
      "ref_id": "b131",
      "title": "Gag: Global attributed graph neural network for streaming session-based recommendation"
    }
  ]
}