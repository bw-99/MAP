{"title": "\ud835\udc39 \ud835\udc40 2 : Field-matrixed Factorization Machines for Recommender Systems", "authors": "Yang Sun; Junwei Pan; Alex Zhang; Aaron Flores", "pub_date": "2021-03-19", "abstract": "Click-through rate (CTR) prediction plays a critical role in recommender systems and online advertising. The data used in these applications are multi-field categorical data, where each feature belongs to one field. Field information is proved to be important and there are several works considering fields in their models. In this paper, we proposed a novel approach to model the field information effectively and efficiently. The proposed approach is a direct improvement of FwFM, and is named as Field-matrixed Factorization Machines (FmFM, or \ud835\udc39 \ud835\udc40 2 ). We also proposed a new explanation of FM and FwFM within the FmFM framework, and compared it with the FFM. Besides pruning the cross terms, our model supports field-specific variable dimensions of embedding vectors, which acts as a soft pruning. We also proposed an efficient way to minimize the dimension while keeping the model performance. The FmFM model can also be optimized further by caching the intermediate vectors, and it only takes thousands floating-point operations (FLOPs) to make a prediction. Our experiment results show that it can out-perform the FFM, which is more complex. The FmFM model's performance is also comparable to DNN models which require much more FLOPs in runtime.", "sections": [{"heading": "INTRODUCTION", "text": "Click-through rate (CTR) prediction plays a key role in recommender systems and online advertising, and it has attracted much research attention in the past decade [3,6,14,20,23,25]. The data involved in CTR prediction are typically multi-field categorical data [16,26]. Such data possess the following properties. First, all the features are categorical and are very sparse since many of them are identifiers. Therefore, the total number of features can easily reach millions to billions. Second, every feature belongs to one and only one field and there can be tens to hundreds of fields.\nA prominent model for these prediction problems is logistic regression with cross-features [3]. When all cross-features are considered, the resulting model is equivalent to a polynomial kernel of degree 2 [2]. However, it takes too many parameters to consider all possible cross-features. To resolve this issue, matrix factorization [1,11] and factorization machines (FM) [18,19] was proposed to learn the effects of cross features by dot products of two feature embedding vectors. Based on FM, Field-aware Factorization Machines (FFM) [9,10] was proposed to consider the field information to model the different interaction effects of features from different field pairs. Recently, a Field-weighted Factorization Machine (FwFM) [15,16] model was proposed to consider the field information in a more parameter-efficient way.\nExisting models that consider the field information either has too many parameters, such as FFM [9,10], or is not very effective, such as [16]. We propose to use a field matrix between two feature vectors to model their interactions, where the matrix is learned separately for each field pair. We will show that our field-pair matrix approach achieves good accuracy performance while maintaining computational space and time efficiency.", "publication_ref": ["b2", "b5", "b13", "b19", "b22", "b24", "b15", "b25", "b2", "b1", "b0", "b10", "b17", "b18", "b8", "b9", "b14", "b15", "b8", "b9", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORKS OVERVIEW", "text": "Logistic Regression (LR) is the most widely used model on multifield categorical data for CTR prediction [3,20]. Suppose there are \ud835\udc5a unique features {\ud835\udc53 1 , \u2022 \u2022 \u2022 , \ud835\udc53 \ud835\udc5a } and \ud835\udc5b different fields {\ud835\udc39 1 , \u2022 \u2022 \u2022 , \ud835\udc39 \ud835\udc5b }. Each field may contain multiple features, while each feature belongs to only one field. To simplify the notation, we use index \ud835\udc56 to represent feature \ud835\udc53 \ud835\udc56 , and \ud835\udc39 (\ud835\udc56) to represent the field which \ud835\udc53 \ud835\udc56 belongs to. Given a data set \ud835\udc7a = {\ud835\udc66 (\ud835\udc60) , \ud835\udc99 (\ud835\udc60) }, where \ud835\udc66 (\ud835\udc60) \u2208 {1, -1} is the label (clicked or not) and \ud835\udc99 (\ud835\udc60) \u2208 {0, 1} \ud835\udc5a is the feature vector in which \ud835\udc65 log(1 + exp(-\ud835\udc66 (\ud835\udc60) \u03a6 \ud835\udc3f\ud835\udc45 (\ud835\udc98, \ud835\udc99 (\ud835\udc60) ))) + \ud835\udf06\u2225\ud835\udc98 \u2225 2  2 ]\n(1)\nThe first term is the log loss, and the second term is the L2 regularization term where \ud835\udf06 is the regularization weight, and\n\u03a6 \ud835\udc3f\ud835\udc45 (\ud835\udc98, \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56(2)\nis a linear combination of individual features. However, linear models lack the capability to represent the feature interactions [3]. As cross features may have more important factors than those single features, many improvements have been proposed in the past decades.\nDegree-2 Polynomial (Poly2) models as a general way to address this problem is to add feature conjunctions. It has been shown that Poly2 models can effectively capture the effect of feature interactions [2]. Mathematically, in the loss function of equation ( 1), Poly2 models consider replacing \u03a6 \ud835\udc3f\ud835\udc45 with\n\u03a6 \ud835\udc43\ud835\udc5c\ud835\udc59 \ud835\udc662 (\ud835\udc98, \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \ud835\udc64 \u210e (\ud835\udc56,\ud835\udc57)(3)\nwhere \u210e(\ud835\udc56, \ud835\udc57) is a function which hashes feature conjunction (\ud835\udc56, \ud835\udc57) into a natural number in the hashing space \ud835\udc3b to reduce the number of parameters. Otherwise the number of parameters in the model would be in the order of \ud835\udc42 (\ud835\udc5a 2 ), which is too many to be learned. Factorization Machines(FM) learn an embedding vector \ud835\udc97 \ud835\udc56 \u2208 R \ud835\udc3e for each feature, where \ud835\udc3e is a hyper-parameter and is usually a small integer, e.g., 10. FM model the interaction between two features \ud835\udc56 and \ud835\udc57 as the dot product of their corresponding embedding vectors \ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 :\n\u03a6 \ud835\udc39 \ud835\udc40 ((\ud835\udc98, \ud835\udc97), \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 \u27e9 (4)\nFM usually outperform Poly2 models in applications involving sparse data such as CTR prediction. This is because it models the interaction between two features by a dot product between their corresponding embedding vectors. These embedding vector of a feature is meaningful as long as the this feature appears enough times during model training. However, FM neglect the fact that a feature might behave differently when it interacts with features from different other fields.\nField-aware Factorization Machines (FFM) model such difference explicitly by learning \ud835\udc5b -1 embedding vectors for each feature, say \ud835\udc56, and only using the corresponding one \ud835\udc97 \ud835\udc56,\ud835\udc39 ( \ud835\udc57) to interact with another feature \ud835\udc57 from field \ud835\udc39 ( \ud835\udc57):\n\u03a6 \ud835\udc39 \ud835\udc39 \ud835\udc40 ((\ud835\udc98, \ud835\udc97), \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56,\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc57,\ud835\udc39 (\ud835\udc56) \u27e9 (5)\nAlthough FFM have gotten significant performance improvements over FM, their number of parameters is in the order of \ud835\udc42 (\ud835\udc5a\ud835\udc5b\ud835\udc3e). The huge number of parameters in FFM is undesirable in the real-world production systems [9]. Therefore, it is appealing to design alternative approaches that are competitive and more memory-efficient.\nField-weighted Factorization Machines (FwFM) was proposed in [16], which models the different field interaction strength explicitly. More specifically, the interaction of a feature pair \ud835\udc56 and \ud835\udc57 in our proposed approach is modeled as\n\ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 \u27e9\ud835\udc5f \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57)\nwhere \ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 are the embedding vectors of \ud835\udc56 and \ud835\udc57, \ud835\udc39 (\ud835\udc56), \ud835\udc39 ( \ud835\udc57) are the fields of features \ud835\udc56 and \ud835\udc57, respectively, and \ud835\udc5f \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) \u2208 R is a weight to model the interaction strength between fields \ud835\udc39 (\ud835\udc56) and \ud835\udc39 ( \ud835\udc57). The formulation of FwFM is:\n\u03a6 \ud835\udc39 \ud835\udc64\ud835\udc39 \ud835\udc40 ((\ud835\udc98, \ud835\udc97), \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 \u27e9\ud835\udc5f \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) (6)\nFwFM are extensions of FM in the sense that it uses additional weight \ud835\udc5f \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) to explicitly capture different interaction strengths of different field pairs. FFM can model this implicitly since they learn several embedding vectors for each feature \ud835\udc56, each one \ud835\udc97 \ud835\udc56,\ud835\udc39 \ud835\udc58 corresponds to one of other fields \ud835\udc39 \ud835\udc58 \u2260 \ud835\udc39 (\ud835\udc56), to model its different interactions with features from different fields. However, the model complexity of FFM is significantly higher than that of FM and FwFM.\nRecently, there are also lots of work on deep learning based click prediction models [4,7,8,13,17,21,22,24,26,28]. These models capture both low order and high order interactions and achieve significant performance improvement. However, the online inference complexity of these models is much higher than the shallow models [5]. Model compression techniques such as pruning [5], distillation [27] or quantization are usually needed to accelerate these models in the online inference. In this paper, we focus on improving the low order interactions, and the proposed model can be easily used as a shallow component in these deep learning models.", "publication_ref": ["b2", "b19", "b2", "b1", "b9", "b8", "b15", "b3", "b6", "b7", "b12", "b16", "b20", "b21", "b23", "b25", "b27", "b4", "b4", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "OUR MODEL", "text": "We propose a new model to represent the interaction of field pairs as a matrix. Similar to FM and FwFM, we learn an embedding vector for each feature. We define a matrix \ud835\udc40 \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) to represent the interaction between field \ud835\udc39 (\ud835\udc56) and field \ud835\udc39 ( \ud835\udc57)\n\ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56 \ud835\udc40 \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc57 \u27e9\nwhere \ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 are the embedding vectors of feature \ud835\udc56 and \ud835\udc57, \ud835\udc39 (\ud835\udc56), \ud835\udc39 ( \ud835\udc57) are the fields of feature \ud835\udc56 and \ud835\udc57, respectively, and \ud835\udc40 \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) \u2208 R \ud835\udc3e\u00d7\ud835\udc3e is a matrix to model the interaction between field \ud835\udc39 (\ud835\udc56) and field \ud835\udc39 ( \ud835\udc57). We name this model Field-matrixed Factorization Machines (FmFM):\n\u03a6 \ud835\udc39\ud835\udc5a\ud835\udc39 \ud835\udc40 ((\ud835\udc98, \ud835\udc97), \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56 \ud835\udc40 \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc57 \u27e9 (7)\nFmFM are extensions of FwFM in that it uses a 2-dimensional matrix \ud835\udc40 \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) to interact different field pairs, instead of a scalar weight \ud835\udc5f in FwFM. With those matrices, features from the embedding space can be transferred to \ud835\udc5b -1 spaces; we name those matrices Field-matrices. Figure 1 demonstrates the calculation of the interaction pairs (\ud835\udc63 \ud835\udc56 , \ud835\udc63 \ud835\udc57 ) and (\ud835\udc63 \ud835\udc56 , \ud835\udc63 \ud835\udc58 ), while features \ud835\udc56, \ud835\udc57 and \ud835\udc58 are from 3 different fields.\nv i,F(j) = v i \u00d7M F(i)F(j) v i,F(k) =v i \u00d7M F(i)F(k) Embedding v k Embedding v j Matrix M F(i)F(k) Matrix M F(i)F(j)\nEmbedding v i (3) Dot Product: The final interaction terms will be a simple dot product between \ud835\udc63 \ud835\udc57 and \ud835\udc63 \ud835\udc56,\ud835\udc39 ( \ud835\udc57) , as well as \ud835\udc63 \ud835\udc58 and \ud835\udc63 \ud835\udc56,\ud835\udc39 (\ud835\udc58) , which are the black dots showed in Fig. 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The United Framework of Factorization Machines' Family", "text": "FmFM have a similar design with, while extending, FM and FwFM; in this section, we deep dive into their design, explain their structure with the 3-step FmFM framework above, and figure out the fundamental relationships among these factorization machine models.\n3.1.1 FM. Figure 2 shows the calculation of feature interactions in FM, the difference to FmFM is that FM skip the step 2, and use the shared embedding \ud835\udc63 \ud835\udc56 to do the final dot product with \ud835\udc63 \ud835\udc57 and \ud835\udc63 \ud835\udc58 respectively. Since we know\n\ud835\udc63 \ud835\udc56 = \ud835\udc63 \ud835\udc56 \ud835\udc3c \ud835\udc3e ,\nwe can construct an identity matrix \ud835\udc3c \ud835\udc3e and let all field matrices equal to \ud835\udc3c \ud835\udc3e . As the identity matrix shows in Fig. 2, the FM actually is a special case of FmFM when all field matrices are \ud835\udc3c \ud835\udc3e . Since those matrices \ud835\udc3c \ud835\udc3e are fixed and non-trainable, we define its degree of freedom to be 0.\nEmbedding v k Embedding v j \u2022 \u2022 Embedding v i \u2026\u2026 \u2026\u2026 Embedding v i Embedding v i 1 1 1 1 1\nFigure 2: An explanation of FM with FmFM framework 3.1.2 FwFM. Fig. 3 shows the calculation of feature interactions in FwFM. There is a change from the original definition 2, while, it is easy to know that:\n\u27e8\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 \u27e9\ud835\udc5f \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) = \u27e8\ud835\udc97 \ud835\udc56 \ud835\udc5f \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc57 \u27e9\nThus, we calculate the term \ud835\udc97 \ud835\udc56 \ud835\udc5f \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) firstly in figure 3, instead of \u27e8\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 \u27e9 in the original definition in Eq.2. It is clear now that the intermediate vector in step 2 is actually a scaled embedding vector:\n\ud835\udc63 \ud835\udc56,\ud835\udc39 ( \ud835\udc57) = \ud835\udc63 \ud835\udc56 \ud835\udc5f \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) = \ud835\udc63 \ud835\udc56 (\ud835\udc5f \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) \ud835\udc3c \ud835\udc3e )\nThus, we construct the field matrix in FwFM as a scalar matrix \ud835\udc5f \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) \ud835\udc3c \ud835\udc3e , which is a diagonal matrix with all its main diagonal entries equal \ud835\udc5f . Its effect on the embedding vector \ud835\udc63 \ud835\udc56 is a scalar multiplication by \ud835\udc5f . We show this matrix at the corner of Fig. 3 (left one). Since the scalar \ud835\udc5f is trainable, it has one more degree of freedom than FM, we define its degree of freedom as 1. 3 FvFM. We follow the clue above, and give one more freedom to the field matrix in FwFM. Let the field matrix become a diagonal matrix in which the main diagonal entries are trainable variables, instead of one single variable in FwFM, we can rewrite the intermediate vector:\nv i r F(i)F(j) v i r F(i)F(k) Embedding v k Embedding v j \u2022 \u2022 Embedding v i,F(j) \u2026\u2026 \u2026\u2026 r F(i)F(j) r F(i)F(k)\n\ud835\udc63 \ud835\udc56,\ud835\udc39 ( \ud835\udc57) = \ud835\udc63 \ud835\udc56 \ud835\udc37 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) = \ud835\udc63 \ud835\udc56 \u2299 \ud835\udc51 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) ,\nwhere \ud835\udc37 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) = diag(\ud835\udc51 1 , \ud835\udc51 2 , . . . , \ud835\udc51 \ud835\udc3e ), this can be expressed more compactly by using a vector \ud835\udc51 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) \u2208 R \ud835\udc3e instead of the diagonal matrix, and taking the Hadamard product (\u2299) of the vectors \ud835\udc63 \ud835\udc56 . Figure 3 demonstrates this case in the right matrix at the corner.\nWe name this method Field-vectorized Factorization Machines (FvFM). The FvFM have one more freedom than FwFM: the trainable parameters become a vector instead of a scalar; thus, we define its degree of freedom to be 2.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "FmFM.", "text": "Let's revisit FmFM in figure 1. It has all the degrees of freedom of a matrix, which is 3. All the variables in those matrices are trainable, and we expect the FmFM to have a greater predictive capacity than other factorization machine models. We will evaluate this hypothesis in the next section.\nOverall, we have found that FM, FwFM, FvFM are all special cases of FmFM, the only differences are their field matrices' restrictions. According to their flexibility, we summarize them in the Table1  1: Degrees of freedom in different FM models 3.1.5 Connections to OPNN. FmFM can also be viewed as modeling the interaction of two feature embedding vectors by a weighted outer product:\n\u03a6 \ud835\udc39\ud835\udc5a\ud835\udc39 \ud835\udc40 ((\ud835\udc98, \ud835\udc97), \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \ud835\udc5d (\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 , \ud835\udc7e \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) ) (8)\nwhere \ud835\udc7e \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) \u2208 R \ud835\udc3e\u00d7\ud835\udc3e , and\n\ud835\udc5d (\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 , \ud835\udc7e \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) ) = \ud835\udc3e \u2211\ufe01 \ud835\udc58=1 \ud835\udc3e \u2211\ufe01 \ud835\udc58 \u2032 =1 \ud835\udc63 \ud835\udc58 \ud835\udc56 \ud835\udc63 \ud835\udc58 \u2032 \ud835\udc57 \ud835\udc64 \ud835\udc58,\ud835\udc58 \u2032 \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57)(9)\nOPNN also proposed to model the feature interactions via outer product. However, FmFM is different from OPNN in the following two aspects. First, FmFM is a simple shallow model without the fully connected layers as in [17]. We can use FmFM as a shallow component or a building block in any deep CTR models, like DeepFM [7]. Second, we support field-specific variable embedding dimensions for features from different fields, which will be discussed in Section 4.1.", "publication_ref": ["b16", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "FFM and FmFM, Memorization vs Inference", "text": "Unlike other factorization machines above, FFM cannot be reformed into the FmFM framework since it has a different way to look up their feature embeddings, we demonstrate its interaction terms' calculation in Figure 4. FFM never share the feature embedding; it always looks up the field specific embeddings from the embedding tables. Thus, there are \ud835\udc5b -1 embeddings for one single feature, which are prepared to cross \ud835\udc5b -1 other fields respectively. Those field-specific embeddings will be learned independently during the training process, and there are no restrictions among those embeddings even belonging to the same feature. This FFM mechanism gives the model maximal flexibility to fit the data, and the huge number of embedding parameters also has incredible memorization capacity. Meanwhile, there is always a risk of over-fitting with it, even when there are billions of instances to be trained. The nature of the features' distribution is a long tail distribution, instead of a uniform distribution, that makes the feature pairs' distribution even more imbalanced.\nEmbedding v k,F(i) Embedding v j,F(i) \u2022 \u2022 \u2026\u2026 \u2026\u2026 Embedding v i,F(j) Embedding v i,F(k) \u2026\u2026 \u2026\u2026 Embedding v i,F(j) Embedding v i,F(k)\nGiven an example in Fig. 4, assume that feature pair (\ud835\udc63 \ud835\udc56 , \ud835\udc63 \ud835\udc57 ) is a high frequency combination, while (\ud835\udc63 \ud835\udc56 , \ud835\udc63 \ud835\udc58 ) is a low frequency (possibly 0 frequency, or never appeared), since \ud835\udc63 \ud835\udc56,\ud835\udc39 ( \ud835\udc57) and \ud835\udc63 \ud835\udc56,\ud835\udc39 (\ud835\udc58) are 2 independent embeddings in the setting of FFM, thus embedding \ud835\udc63 \ud835\udc56,\ud835\udc39 ( \ud835\udc57) may be trained well but \ud835\udc63 \ud835\udc56,\ud835\udc39 (\ud835\udc58) may not. Due to the long tail distribution of features, those high frequent features pairs may dominate the number of training data, while other low frequency features which dominate the number of features, cannot be trained well.\nFmFM use shared embedding vectors, as there is only one copy for each single feature. It utilizes a transformation process to project this single embedding vector into other \ud835\udc5b -1 fields. This is basically an inference process, and those \ud835\udc5b -1 vectors \ud835\udc63 \ud835\udc56,\ud835\udc39 ( * ) are actually tied with the original embedding vector \ud835\udc63 \ud835\udc56 . With those field matrices, the vectors are transformable forward and backward. That is the fundamental difference between FFM and FmFM; those transformable intermediate vectors within the same feature help the model learn those low frequency feature pairs well.\nBack to the example in Fig. 1, even the feature pair (\ud835\udc63 \ud835\udc56 , \ud835\udc63 \ud835\udc58 ) is of low frequency, the feature embedding \ud835\udc63 \ud835\udc56 can still be trained well through the other high frequency feature pairs like (\ud835\udc63 \ud835\udc56 , \ud835\udc63 \ud835\udc57 ), and the field matrix \ud835\udc40 \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) can be trained well through other feature interactions between field \ud835\udc39 (\ud835\udc56) and field \ud835\udc39 (\ud835\udc58). Thus, if the low frequency feature pair (\ud835\udc63 \ud835\udc56 , \ud835\udc63 \ud835\udc58 ) occurs during evaluation or test, the intermediate vector \ud835\udc63 \ud835\udc56,\ud835\udc39 (\ud835\udc58) can be inferred through \ud835\udc63 \ud835\udc56 \ud835\udc40 \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) .\nDespite this difference between FFM and FmFM, they have more in common. An interesting observation between figure 4 and figure 1 is that, when all transformations are done, the FmFM model becomes a FFM model. We can cache those intermediate vectors and avoid matrix operations during runtime; the details will be discussed in the next section.\nIn contrast, FFM model cannot be reformed to a FmFM model, as we have mentioned above. Those \ud835\udc5b -1 field features embedding tables are independent, thus it is hard to compress them into one single feature embedding table and restore them when needed.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Model Complexity", "text": "The number of parameters in FM is \ud835\udc5a + \ud835\udc5a\ud835\udc3e, where \ud835\udc5a accounts for the weights for each feature in the linear part {\ud835\udc64 \ud835\udc56 |\ud835\udc56 = 1, ..., \ud835\udc5a} and \ud835\udc5a\ud835\udc3e accounts for the embedding vectors for all the features {\ud835\udc97 \ud835\udc56 |\ud835\udc56 = 1, ..., \ud835\udc5a}. FwFM use \ud835\udc5b(\ud835\udc5b -1)/2 additional parameters {\ud835\udc5f \ud835\udc39 \ud835\udc58 ,\ud835\udc39 \ud835\udc59 |\ud835\udc58, \ud835\udc59 = 1, ..., \ud835\udc5b} for each field pair so that the total number of parameters of FwFM is \ud835\udc5a + \ud835\udc5a\ud835\udc3e + \ud835\udc5b(\ud835\udc5b -1)/2. The additional matrices in FmFM is \ud835\udc5b(\ud835\udc5b-1)/2 as compared to FM, and it has extra\n\ud835\udc5b (\ud835\udc5b-1)\n2 \ud835\udc3e 2 parameters. For FFM, the number of parameters is \ud835\udc5a + \ud835\udc5a(\ud835\udc5b -1)\ud835\udc3e since each feature has \ud835\udc5b -1 embedding vectors. Given that usually \ud835\udc5b \u226a \ud835\udc5a, the number of parameters of other Factorization Machines are comparable with that of FM and significantly less than that of FFM. In Table 2 we compare the model complexity of all models mentioned so far. We also list the estimated number of parameters in the setting of section 5 for each model, which use the public data set Criteo. Those numbers can give us an intuitive impression about the size of each model. FM, FwFM, and FmFM have similar sizes while FFM have more than dozen times than others. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Model", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MODEL OPTIMIZATION", "text": "In this section we present our methodologies to optimize the FmFM model. There are 3 tactics which we can devise to reduce the complexity of FmFM further. In section 4.1 we introduce the fieldspecific embedding dimensions, which is a unique property in FmFM; it allows us to use field specific dimensions in the embedding table, instead of a fixed length \ud835\udc3e globally. In Section 4.2 we introduce the method to cache the intermediate vectors to avoid the matrix operations, which can reduce the FmFM model's computational complexity in runtime. In Section 4.5 we introduce the method to reduce the linear terms and replace them with field specific weights.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Field-specific Embedding Dimensions", "text": "The main improvement of FM over LR model is that, FM use the embedding vector to represent each feature. In order to make the dot product, it requires the vector dimension \ud835\udc3e of all feature embeddings to be the same, even though features come from different fields. The improved models like FwFM, FvFM also adopt this property. The vector dimension matters both in model complexity and model performance, the work [16] discussed this trade-off between performance and time cost, but the vector dimension can only be optimized globally.\nWhen we utilize the matrix multiplication in FmFM, it actually does not require the field matrices to be square matrices; we can adjust the output vector length by changing the shape of the field matrix. This property gives us an another flexibility to set the fieldspecific lengths on-demand in the embedding table, as we show in figure 5.\nEmbedding v j Matrix M F(i)F(j) Embedding v i Embedding v i \u2026\u2026 \u2026\u2026 v i \u00d7 M F(i)F(k) v i \u00d7 M F(i)F(j) \u2022 \u2022 Matrix M F(i)F(k) Embedding v i Embedding v k", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Figure 5: An Example of Variable Vector Length of Embeddings", "text": "The dimension of an embedding vector determines the amount of information it can carry; this property allows us to accommodate the need for each field. For the example (\ud835\udc56, \ud835\udc57) in Fig. 5, the field \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f _\ud835\udc54\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc5f may contain only 3 values (male, female, other), while another field \ud835\udc61\ud835\udc5c\ud835\udc5d_\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc59_\ud835\udc51\ud835\udc5c\ud835\udc5a\ud835\udc4e\ud835\udc56\ud835\udc5b may contain more than 1 million features. Thus, the embedding table of field \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f _\ud835\udc54\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc5f may only need 5-dimension (5D), while the field \ud835\udc61\ud835\udc5c\ud835\udc5d_\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc59_\ud835\udc51\ud835\udc5c\ud835\udc5a\ud835\udc4e\ud835\udc56\ud835\udc5b may need 7D. The field matrix \ud835\udc40 should be set up with a shape in (7, 5). When we cross the feature between \ud835\udc61\ud835\udc5c\ud835\udc5d_\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc59_\ud835\udc51\ud835\udc5c\ud835\udc5a\ud835\udc4e\ud835\udc56\ud835\udc5b and \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f _\ud835\udc54\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc5f , the matrix can transfer the 5D feature vector to a 7D vector, making it ready to do a dot product with the feature vector from field \ud835\udc61\ud835\udc5c\ud835\udc5d_\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc59_\ud835\udc51\ud835\udc5c\ud835\udc5a\ud835\udc4e\ud835\udc56\ud835\udc5b.\nTo optimize the field-specific embedding vector dimension without model performance loss, we propose a 2-pass method. In the first pass, we use a larger fixed embedding vector dimension for all fields, e.g. \ud835\udc3e = 16, and train the FmFM as a full model. From the full model, we learn how much information (variance) in each field, then we utilize a standard PCA dimensionality reduction to the embedding table in each field. From the experiment in Section 5.4 we found that the new dimension which contains 95% original variance is a good trade-off. With this new field specific dimension setting, we train the model in a second pass from scratch, and should get the second model without any significant performance loss, compared to the first full model.   3 shows the optimized dimension for each field in the Criteo dataset, with the PCA method. This list shows that the range of those dimensions are huge which from 2 to 14, and most of the dimensions are less than 10. The average \ud835\udc3e is only 7.72, which is less than the optimal setting in the FwFM. With keeping most variance from the dataset, the lower average dimension means the model has fewer parameters, requires less memory.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Feature", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Intermediate Vectors Cache", "text": "FmFM is a lower complexity model than FFM, in the number of parameters; however it requires expensive matrix operations in the transformation step. In table 4, we list the number of Floating Point Operations (FLOPs) for each model 1  Among those Factorization Machine models, the FmFM needs the most operations to accomplish its calculation, which is about \ud835\udc58 times as FwFM and FFM, but still faster than most DNN models. In section 3.2, we have shown that a FmFM model can be transformed into a FFM model, by caching all intermediate vectors. In this sense, we can reduce its number of operations to the same magnitude as FM and FFM, which is almost 20 times faster.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Embedding Dimension and Cache Optimization Combined", "text": "When we combine the field-specific embedding dimensional optimization and the cache optimization, the inference speed can be much faster, and the required memory can be reduced significantly. This benefits from another property of FmFM -the interaction matrices are symmetrical, which means\n\u27e8\ud835\udc97 \ud835\udc56 \ud835\udc40 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc57 \u27e9 = \u27e8\ud835\udc97 \ud835\udc57 \ud835\udc40 \ud835\udc47 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc56 \u27e9(10)\nWe have a proof for this lemma in the Appendix. Thus, we can choose to cache those intermediate vectors which have lower field dimensions, and dot-product with the other feature vectors during inference. For example, in the setting of table 3, two features \ud835\udc63 \ud835\udc56 and \ud835\udc63 \ud835\udc57 are from field #16 and #28 respectively. With this property, when we calculate the interaction between \ud835\udc63 \ud835\udc56 and \ud835\udc63 \ud835\udc57 , we can cache either \ud835\udc97 \ud835\udc56 \ud835\udc40 16,28 or \ud835\udc97 \ud835\udc57 \ud835\udc40 \ud835\udc47  16,28 . Since the field matrix \ud835\udc40 16,28 has a shape of [2,14], the former one \ud835\udc97 \ud835\udc56 \ud835\udc40 16,28 increased the dimension from 2 (field #16) to 14 (intermediate vector), then dot-product with \ud835\udc97 \ud835\udc57 whose dimension is also 14. It costs 14 units of memory for the intermediate vectors cache, and takes 2 \u00d7 14 FLOPs during inference. By contrast, the latter one \ud835\udc97 \ud835\udc57 \ud835\udc40 \ud835\udc47  16,28 reduces the dimension from 14 (field #28) to 2 (intermediate vector), then dot-product with \ud835\udc97 \ud835\udc56 whose dimension is also 2. It costs 2 units of memory for the intermediate vectors cache, and takes 2 \u00d7 2 FLOPs during inference. In this single field pair, the optimized cache with field-specific embedding dimensions can save 7 times memory and time without any precision loss.\nWith those two optimization techniques combined, the FmFM model's time complexity is reduced drastically; in table 4, we estimate that the optimized model only takes 8,960 FLOPs, which is only about 1/3 of FFM. In the section5.4, we will show that this optimized model can achieve the same performance as the full model.", "publication_ref": ["b1", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Soft Pruning", "text": "The field-specific embedding dimensions also act in a role similar to pruning actually; while traditional pruning such as DeepLight [5] gives a binary decision to keep or drop a field or a field pair, the fieldspecific embedding dimensions give us a new way to determine the importance of each field and field pair on-demand, and assign each field a factor to represent its importance. For example, in the FmFM model of Table3, the cross field #17 and #20 is a high strength pair; it takes 11 units of cache and 2 \u00d7 11 FLOPs during inference; in contrast, a low strength pair, field #18 and #22, only takes 2 units of cache and 2 \u00d7 2 FLOPs.\nWhen we drop a field pair in the traditional pruning, its signal was lost totally; while in this method, a field pair still keeps the major information with minimal cost. It is a soft version of pruning, which is similar to Softmax. It is more efficient and sees less performance drop during soft pruning. Figure 6 shows a heat-map of mutual information scores between field pairs and labels in the Criteo dataset, which represents the strength of field pairs in prediction. Figure 7 shows the cross field dimensions, which is the lower dimension between two fields (explanation in Section 4.3); it represents the parameters and computational cost for each field pair. Obviously, these two heat-maps are highly related to each other, which means the optimized FmFM model allocates more parameters and more computation on those higher strength field pairs, and fewer parameters and less computation on lower strength field pairs. ", "publication_ref": ["b4"], "figure_ref": ["fig_3", "fig_4"], "table_ref": []}, {"heading": "Linear Terms", "text": "There is a linear part in Eq.8:\n\ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56(11)\nwhich requires an extra scalar \ud835\udc64 \ud835\udc56 to be learned for each feature. However the learned embedding vector \ud835\udc63 \ud835\udc56 should contain more information, and the weight \ud835\udc64 \ud835\udc56 can be learned from \ud835\udc63 \ud835\udc56 by a simple dot product. Another benefit from the learned \ud835\udc63 \ud835\udc56 is that, it can help to learn the embedding vector from the linear part.\nWe follow the method from the work of [16], by learning a field specific vector \ud835\udc64 \ud835\udc39 (\ud835\udc56) so that all features from the same field \ud835\udc39 (\ud835\udc56) will share the same linear weight vector. Then the linear terms can be rewritten to:\n\ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \u27e8\ud835\udc97 \ud835\udc56 , \ud835\udc64 \ud835\udc39 (\ud835\udc56) \u27e9(12)\nWe apply this linear term optimization to FwFM, FvFM and FmFM by default in the rest of the paper.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "In this section we present our experimental evaluation results. We will first describe the data sets and implementation details in Section 5.1 and 5.2 respectively. In Section 5. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data sets", "text": "We use 2 public data sets to evaluate our model performance:\n(1) The first one is the Criteo CTR data set; it is a well-know benchmark data set which used for the Criteo Display Advertising Challenge [12]. The Criteo data set is already label balanced, the positive to the negative ratio is about 1:3. There are 45 million samples and each sample has 13 numerical fields and 26 categorical fields. (2) The second data set is the Avazu CTR data set; it was used in the Avazu CTR prediction competition, which predicts whether a mobile ad will be clicked. The positive to the negative ratio in the Avazu data set is about 1:5. There are 40 million samples and each sample has 23 categorical fields.\nWe follow those existing works [5,7,13,21,22,24,26,28], for each data set, we split it into 3 parts randomly, 80% for training, 10% for validation, and 10% for testing. 2Regarding to those numerical features in the Criteo data set, we adopt the log transformation of \ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc65) 2 if \ud835\udc65 > 2, which proposed by the winner of the Criteo competitionfoot_3 to normalize the numerical features. This method was also used by [5] and [22]. Regarding the date/hour feature in the Avazu data set, we transfer it into 2 features: day_of_week(0-6) and hours(0-23) to consume the feature better.\nWe also remove those infrequent features that are less than a threshold in both data sets and replace their values with the default \"unknown\" feature in that field. We set the threshold to 8 for the Criteo data set, and to 5 for the Avazu data set.\nThe statistics of the normalized data sets are shown in Table 5. ", "publication_ref": ["b11", "b1", "b4", "b6", "b12", "b20", "b21", "b23", "b25", "b27", "b4", "b21"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Data set", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment Setup", "text": "We have implemented the LR (logistic regression) and all factorization machine models (FM, FwFM, FFM, FvFM and FmFM) with Tensorflow. 4 . We follow the implementation of LR and FM in [17], and implement FFM, FwFM following the work [16].\nWe evaluate all models performance by AUC (Area Under the ROC Curve) and Log Loss on the test set. It is noticeable that a slightly higher AUC or lower Log Loss at 0.001-level is regarded a significant improvement for CTR prediction task, which has also been pointed out in existing works [4,22,24].\nFor those state-of-the-art models, they are all DNN models and may need more hyper-parameters tuning, we pull their performance (AUC and Log Loss) from their original papers, in order to keep their results optimal. It is fair to compare our results with theirs, since we use more strict data splittings; while their implementations may have slight differences, e.g. feature processing, optimizer (Adam or Adagrad), we list their results just for reference. The Deep & Cross Network is an exception, since their paper only listed the Log Loss but not AUC. Thus, we implemented their model and got a similar performance.", "publication_ref": ["b16", "b15", "b3", "b21", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Performance Comparisons", "text": "In this section we will conduct performance evaluation for FmFM. We will compare it with LR, FM, FwFM and FFM on the two data sets mentioned above. We always use the full size model to compare in the results; that means we don't use any optimization methods mentioned in Section 4. For the parameters such as regularization coefficient \ud835\udf06, and learning rate \ud835\udf02 in all models, we select those which lead to the best performance on the validation set and then use them in the evaluation on the test set. Experiment results can be found in Table 6 for the Criteo data set, and We observe that FvFM and FmFM can achieve better performance than LR, FM, and FwFM on both data sets, which is in our expectation. Surprisingly, the FmFM can achieve better performance than FFM constantly in both test sets. As we mentioned before, even though FFM is a model dozens times larger than FmFM, our FmFM model still get the best AUC in the test set among all shallow models. Additionally, if we compare the differences in AUC between training and test, we found that the \u0394\ud835\udc34\ud835\udc48\ud835\udc36 \ud835\udc39\ud835\udc5a\ud835\udc39 \ud835\udc40 = 0.0074 is the lowest one among those factorization machine models, which affirms our hypothesis in section 3.2: those low frequency features are also trained well with the help of the interaction matrix, and this mechanism helps FmFM to avoid over-fitting.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Embedding Dimension Optimization", "text": "In this part we implement the method described in 4.1, whereby we have a full size model, we can extract its embedding tables for each field, then we utilize a standard PCA dimensionality reduction. Here we do several experiments and compare how the dimensionality reduction impact the model performance, and try to find a trade-off between the model size, speed and its performance.\nWe use the full size FmFM model from experiment 5.3 on the Criteo data set to evaluate our metrics. We keep 99%, 97%, 95%, 90%, 85% and 80% variance in PCA dimensionality reduction respectively, and estimate the average embedding dimensions and float operations (FLOPs, with cached intermediate vectors). With the new dimensions setting, we train those FmFM models the second pass respectively, and observe the AUC and Log Loss change in test set.\nTable 8 shows the summary of experiments. The average embedding dimension was reduced significantly when we keep less variance from PCA: there is only less than 1/2 embedding dimensions and 1/3 computation cost when we keep 95% variance, while there is no significant change on the model's performance compare to the full size model. Thus, 95% variance is a good trade-off when we optimize the embedding dimensions in FmFM.\nFigure 8 shows these models' performance (in AUC) and their computational complexity (in FLOPs). As a shallow model, the optimized FmFM model gets higher AUC as well as lower FLOPs, compared with all the baseline models except Deep & Cross and DeepLight. While its computational cost is much lower than these two complex models which ensembled DNN module and shallow module, its FLOPs is only 1.76% and 8.78% of them, respectively. The lower FLOPs makes it preferable when the computation latency is strictly limited, which is the common scenario in the real-time online ads CTR prediction and recommender systems.   ", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": ["tab_10"]}, {"heading": "CONCLUSION AND FURTHER WORKS", "text": "In conclusion, we propose a novel approach FmFM to model the interactions of field pairs as a matrix. We prove that FmFM is a unified framework of factorization machine model family, in which both FM and FwFM can be treated as special cases. We devise a few optimizations to FmFM, including field-specific embedding dimensions and caching intermediate vectors. These optimizations make the FmFM lightweight and faster during inference, taking only thousands of floating-point operations to make a prediction. We have done comprehensive experiments to verify the effectiveness and efficiency of the proposed model. It achieves state-of-the-art performance among all shallow models, including FM, FFM and FwFM, and its performance is even comparable to those complex DNN models.\nWith regard to future work, there are a few potential research directions:\n\u2022 The FmFM is still a linear model, since the field interaction are matrices, and embedding vectors are transformed linearly. We can introduce the non-linear layers to the field interaction and let the model become a non-linear model, which is more flexible.\n\u2022 All the factorization machine models are actually Degree-2 models, which allows up to 2 fields interactions. This restriction is majorly because the dot product. In the future, we can introduce the 3D tensor and allows the 3 fields interaction, or even higher ranks. This work may require more model optimization since there are too much Degree-3 interactions. \u2022 We can combine the DNN models like the Wide and Deep [4],\nDeepFM [7], DeepLight [5], and try FmFM as a building block in DNN models to further improve their performances. We believe this method should be more competitive in the model performance, when compare to those deep learning based models in Section 2.", "publication_ref": ["b3", "b6", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "A MATH PROOF", "text": "Lemma A.1. Given two row vectors \ud835\udc63 \ud835\udc56 and \ud835\udc63 \ud835\udc57 whose lengths are \ud835\udc58 and \ud835\udc59 respectively, there is a matrix \ud835\udc40 \u2208 R \ud835\udc58,\ud835\udc59 , then:\nwhere \u00d7 denotes matrix multiplication, and \u2022 denotes dot product.\nProof. Since \ud835\udc97 \ud835\udc56 \u00d7 \ud835\udc40 \u2022 \ud835\udc97 \ud835\udc57 is a scalar, we denote it as \ud835\udc4e, and the dot product can be rewrite to a matrix multiplication, we rewrite the left of the equation:\nwhile the transpose of a scalar equals to itself:\nHence, the left equals the right. \u25a1", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "OFF-set: onepass factorization of feature sets for online recommendation in persistent cold start settings", "journal": "ACM", "year": "2013", "authors": "Michal Aharon; Natalie Aizenberg; Edward Bortnikov; Ronny Lempel; Roi Adadi; Tomer Benyamini; Liron Levin; Ran Roth; Ohad Serfaty"}, {"ref_id": "b1", "title": "Training and testing low-degree polynomial data mappings via linear SVM", "journal": "Journal of Machine Learning Research", "year": "2010-04", "authors": "Yin-Wen Chang; Cho-Jui Hsieh; Kai-Wei Chang; Michael Ringgaard; Chih-Jen Lin"}, {"ref_id": "b2", "title": "Simple and scalable response prediction for display advertising", "journal": "ACM Transactions on Intelligent Systems and Technology (TIST)", "year": "2015", "authors": "Olivier Chapelle; Eren Manavoglu; Romer Rosales"}, {"ref_id": "b3", "title": "Wide & deep learning for recommender systems", "journal": "ACM", "year": "2016", "authors": "Heng-Tze Cheng; Levent Koc; Jeremiah Harmsen; Tal Shaked; Tushar Chandra; Hrishi Aradhye; Glen Anderson; Greg Corrado; Wei Chai; Mustafa Ispir"}, {"ref_id": "b4", "title": "DeepLight: Deep Lightweight Feature Interactions for Accelerating CTR Predictions in Ad Serving", "journal": "", "year": "2020", "authors": "Wei Deng; Junwei Pan; Tian Zhou; Aaron Flores; Guang Lin"}, {"ref_id": "b5", "title": "Pricing ad slots with consecutive multi-unit demand", "journal": "Autonomous Agents and Multi-Agent Systems", "year": "2017", "authors": "Xiaotie Deng; Paul Goldberg; Yang Sun; Bo Tang; Jinshan Zhang"}, {"ref_id": "b6", "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction", "journal": "", "year": "2017", "authors": "Huifeng Guo; Ruiming Tang; Yunming Ye; Zhenguo Li; Xiuqiang He"}, {"ref_id": "b7", "title": "Neural Factorization Machines for Sparse Predictive Analytics", "journal": "", "year": "2017", "authors": "Xiangnan He; Tat-Seng Chua"}, {"ref_id": "b8", "title": "Field-aware factorization machines in a real-world online advertising system", "journal": "", "year": "2017", "authors": "Yuchin Juan; Damien Lefortier; Olivier Chapelle"}, {"ref_id": "b9", "title": "Fieldaware factorization machines for CTR prediction", "journal": "ACM", "year": "2016", "authors": "Yuchin Juan; Yong Zhuang; Wei-Sheng Chin; Chih-Jen Lin"}, {"ref_id": "b10", "title": "Matrix factorization techniques for recommender systems", "journal": "Computer", "year": "2009", "authors": "Yehuda Koren; Robert Bell; Chris Volinsky"}, {"ref_id": "b11", "title": "Display Advertising Challenge", "journal": "Criteo Labs", "year": "2014", "authors": ""}, {"ref_id": "b12", "title": "xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems", "journal": "", "year": "2018", "authors": "Jianxun Lian; Xiaohuan Zhou; Fuzheng Zhang; Zhongxia Chen; Xing Xie; Guangzhong Sun"}, {"ref_id": "b13", "title": "Ad click prediction: a view from the trenches", "journal": "ACM", "year": "2013", "authors": "Gary H Brendan Mcmahan; David Holt; Michael Sculley; Dietmar Young; Julian Ebner; Lan Grady; Todd Nie; Eugene Phillips;  Davydov;  Daniel Golovin"}, {"ref_id": "b14", "title": "Predicting different types of conversions with multi-task learning in online advertising", "journal": "", "year": "2019", "authors": "Junwei Pan; Yizhi Mao; Alfonso Lobos Ruiz; Yu Sun; Aaron Flores"}, {"ref_id": "b15", "title": "Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising", "journal": "", "year": "2018", "authors": "Junwei Pan; Jian Xu; Alfonso Lobos Ruiz; Wenliang Zhao; Shengjun Pan; Yu Sun; Quan Lu"}, {"ref_id": "b16", "title": "Product-based neural networks for user response prediction", "journal": "IEEE", "year": "2016", "authors": "Yanru Qu; Han Cai; Kan Ren; Weinan Zhang; Yong Yu; Ying Wen; Jun Wang"}, {"ref_id": "b17", "title": "Factorization machines", "journal": "IEEE", "year": "2010", "authors": "Steffen Rendle"}, {"ref_id": "b18", "title": "Factorization machines with libfm", "journal": "ACM Transactions on Intelligent Systems and Technology (TIST)", "year": "2012", "authors": "Steffen Rendle"}, {"ref_id": "b19", "title": "Predicting clicks: estimating the click-through rate for new ads", "journal": "ACM", "year": "2007", "authors": "Matthew Richardson; Ewa Dominowska; Robert Ragno"}, {"ref_id": "b20", "title": "Deep Crossing: Web-scale modeling without manually crafted combinatorial features", "journal": "ACM", "year": "2016", "authors": "Ying Shan; Ryan Hoens; Jian Jiao; Haijing Wang; Dong Yu; J C Mao"}, {"ref_id": "b21", "title": "AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks", "journal": "", "year": "2018", "authors": "Weiping Song; Chence Shi; Zhiping Xiao; Zhijian Duan; Yewen Xu; Ming Zhang; Jian Tang"}, {"ref_id": "b22", "title": "On the convergence and robustness of reserve pricing in keyword auctions", "journal": "", "year": "2012", "authors": "Yang Sun; Yunhong Zhou; Ming Yin; Xiaotie Deng"}, {"ref_id": "b23", "title": "Deep & Cross Network for Ad Click Predictions", "journal": "", "year": "2017", "authors": "Ruoxi Wang; Bin Fu; Gang Fu; Mingliang Wang"}, {"ref_id": "b24", "title": "Optimal Reserve Prices in Weighted GSP Auctions: Theory and Experimental Methodology", "journal": "", "year": "2011", "authors": "Sun Yang; Zhou Yunhong; Deng Xiaotie"}, {"ref_id": "b25", "title": "Deep learning over multi-field categorical data", "journal": "Springer", "year": "2016", "authors": "Weinan Zhang; Tianming Du; Jun Wang"}, {"ref_id": "b26", "title": "Rocket launching: A universal and efficient framework for training well-performing light net", "journal": "", "year": "2018", "authors": "Guorui Zhou; Ying Fan; Runpeng Cui; Weijie Bian; Xiaoqiang Zhu; Kun Gai"}, {"ref_id": "b27", "title": "Deep interest network for click-through rate prediction", "journal": "", "year": "2018", "authors": "Guorui Zhou; Xiaoqiang Zhu; Chenru Song; Ying Fan; Han Zhu; Xiao Ma; Yanghui Yan; Junqi Jin; Han Li; Kun Gai"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "\ud835\udc65the LR model parameters \ud835\udc98 are estimated by minimizing the following loss function:", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: An explanation of FwFM and FvFM with FmFM framework", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: An example of FFM", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 :6Figure 6: An example of Mutual Information Score between field pairs and label in Criteo Dataset", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 :7Figure 7: An example of cross fields dimensions -\ud835\udc5a\ud835\udc56\ud835\udc5b(\ud835\udc37 \ud835\udc56 , \ud835\udc37 \ud835\udc57 ) in Criteo Dataset", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "3 we compare FmFM with other baseline models like LR, FM, FwFM and FFM, as well as the state-of-the-art methods like Wide & Deep, Deep & Cross network, xDeepFM, AutoInt, FiBiNET and DeepLight. In Section 5.4, we did a few experiment on the Criteo dataset and observe the model performance change when we apply the field-specific dimension in embedding.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Variance", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure 8: AUC and FLOPs comparison among all models on the Criteo dataset", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The feature embedding vectors \ud835\udc63 \ud835\udc56 , \ud835\udc63 \ud835\udc57 , and \ud835\udc63 \ud835\udc58 are looked up from the embedding table, and \ud835\udc63 \ud835\udc56 will be shared between those 2 pairs. (2) Transformation: Then \ud835\udc63 \ud835\udc56 is multiplied by the matrices \ud835\udc40 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) and \ud835\udc40 \ud835\udc39 (\ud835\udc56)\ud835\udc39 (\ud835\udc58) respectively, here we get the intermediate vector \ud835\udc63 \ud835\udc56,\ud835\udc39 ( \ud835\udc57) = \ud835\udc63 \ud835\udc56 \u00d7 \ud835\udc40 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) for the field \ud835\udc39 ( \ud835\udc57), and \ud835\udc63 \ud835\udc56,\ud835\udc39 (\ud835\udc58) = \ud835\udc63 \ud835\udc56 \u00d7 \ud835\udc40 \ud835\udc39 (\ud835\udc56)\ud835\udc39 (\ud835\udc58) for the field \ud835\udc39 (\ud835\udc58).", "figure_data": "Dot Product\u2022\u2022TransformationEmbedding v iEmbeddingLookupEmbedding v i \u2026\u2026 \u2026\u2026Figure 1: An example of FmFM interaction terms calculationThe calculation can be decomposed into 3 steps:(1) Embedding Lookup:"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "A summary of model complexities (ignoring the bias term \ud835\udc64 0 ). The estimate of the total \ud835\udc41 of the model in the settings of Section 5.3", "figure_data": "N of ParametersEstimated N in Criteo DatasetLR\ud835\udc5a1.33MPoly2\ud835\udc5a + \ud835\udc3b45MFM\ud835\udc5a + \ud835\udc5a\ud835\udc3e14.63MFwFM\ud835\udc5a + \ud835\udc5a\ud835\udc3e +\ud835\udc5b (\ud835\udc5b-1) 214.63MFmFM \ud835\udc5a + \ud835\udc5a\ud835\udc3e +\ud835\udc5b (\ud835\udc5b-1) 2 \ud835\udc3e 214.63MFFM\ud835\udc5a + \ud835\udc5a(\ud835\udc5b -1)\ud835\udc3e859.18M"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "The optimized dimensions for each field in Criteo data set, 95% variance kept", "figure_data": "Table"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "A summary of Floating Point Operations by model", "figure_data": ", and estimate it with typicalsettings."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Statistics of training, validation and test sets of the Criteo data sets.", "figure_data": "Samples Fields Features Pos:NegTrain36,672,493CriteoValidation4,584,062391,327,180~1:3Test4,584,062Train32,343,173AvazuValidation4,042,897231,544,257~1:5Test4,042,897"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": "for the Avazu"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Comparison among models on the Criteo CTR data sets.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Comparison among models on Avazu CTR data sets.", "figure_data": "AUCLog LossModelsTraining ValidationTest(Test Set)LR0.75260.75210.75170.3953FM0.77440.76960.76950.3857FFM0.80120.77610.77610.3826FwFM0.78220.77300.77310.3835FvFM(ours)0.78360.77320.77330.3834FmFM(ours)0.79430.77640.77630.3822Deep & Cross0.81090.78250.78260.3791AutoInt--0.77520.3823Fi-GNN--0.77620.3825FGCNN+IPNN--0.78830.3746DeepLight--0.78970.3748"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Compare among FmFM optimized models with embedding dim optimization, an example of the Criteo Data Set", "figure_data": "Emb DimFLOPsAUCLog Loss%(Average)Estimated #(Test Set)(Test Set)Full16(100%) 24,531(100%)0.81090.441099%10.56(66.0%) 12,884(52.5%)0.81090.441097%8.69(54.3%) 10,280(41.9%)0.81070.441195%7.72(48.2%) 8,960(36.5%)0.81080.441190%6.26(39.1%)7,202(29.4%)0.81030.441585%3.82(23.9%)4,716(19.2%)0.80840.443280%3.36(21.0%)4,392(17.9%)0.80800.4436"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03a6 \ud835\udc3f\ud835\udc45 (\ud835\udc98, \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56(2)", "formula_coordinates": [2.0, 126.13, 172.77, 167.92, 24.75]}, {"formula_id": "formula_1", "formula_text": "\u03a6 \ud835\udc43\ud835\udc5c\ud835\udc59 \ud835\udc662 (\ud835\udc98, \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \ud835\udc64 \u210e (\ud835\udc56,\ud835\udc57)(3)", "formula_coordinates": [2.0, 80.26, 317.38, 213.78, 24.75]}, {"formula_id": "formula_2", "formula_text": "\u03a6 \ud835\udc39 \ud835\udc40 ((\ud835\udc98, \ud835\udc97), \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 \u27e9 (4)", "formula_coordinates": [2.0, 75.05, 461.63, 218.99, 24.75]}, {"formula_id": "formula_3", "formula_text": "\u03a6 \ud835\udc39 \ud835\udc39 \ud835\udc40 ((\ud835\udc98, \ud835\udc97), \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56,\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc57,\ud835\udc39 (\ud835\udc56) \u27e9 (5)", "formula_coordinates": [2.0, 58.15, 629.19, 235.9, 34.34]}, {"formula_id": "formula_4", "formula_text": "\ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 \u27e9\ud835\udc5f \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57)", "formula_coordinates": [2.0, 400.49, 167.24, 73.82, 9.43]}, {"formula_id": "formula_5", "formula_text": "\u03a6 \ud835\udc39 \ud835\udc64\ud835\udc39 \ud835\udc40 ((\ud835\udc98, \ud835\udc97), \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 \u27e9\ud835\udc5f \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) (6)", "formula_coordinates": [2.0, 318.26, 253.43, 239.94, 34.34]}, {"formula_id": "formula_6", "formula_text": "\ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56 \ud835\udc40 \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc57 \u27e9", "formula_coordinates": [2.0, 398.58, 573.23, 78.37, 9.43]}, {"formula_id": "formula_7", "formula_text": "\u03a6 \ud835\udc39\ud835\udc5a\ud835\udc39 \ud835\udc40 ((\ud835\udc98, \ud835\udc97), \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \u27e8\ud835\udc97 \ud835\udc56 \ud835\udc40 \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc57 \u27e9 (7)", "formula_coordinates": [2.0, 317.96, 651.11, 240.25, 34.34]}, {"formula_id": "formula_8", "formula_text": "v i,F(j) = v i \u00d7M F(i)F(j) v i,F(k) =v i \u00d7M F(i)F(k) Embedding v k Embedding v j Matrix M F(i)F(k) Matrix M F(i)F(j)", "formula_coordinates": [3.0, 92.04, 151.32, 155.69, 99.86]}, {"formula_id": "formula_9", "formula_text": "\ud835\udc63 \ud835\udc56 = \ud835\udc63 \ud835\udc56 \ud835\udc3c \ud835\udc3e ,", "formula_coordinates": [3.0, 156.58, 642.62, 34.36, 7.79]}, {"formula_id": "formula_10", "formula_text": "Embedding v k Embedding v j \u2022 \u2022 Embedding v i \u2026\u2026 \u2026\u2026 Embedding v i Embedding v i 1 1 1 1 1", "formula_coordinates": [3.0, 351.53, 88.16, 178.14, 128.12]}, {"formula_id": "formula_11", "formula_text": "\u27e8\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 \u27e9\ud835\udc5f \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) = \u27e8\ud835\udc97 \ud835\udc56 \ud835\udc5f \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc57 \u27e9", "formula_coordinates": [3.0, 374.29, 310.53, 127.58, 9.43]}, {"formula_id": "formula_12", "formula_text": "\ud835\udc63 \ud835\udc56,\ud835\udc39 ( \ud835\udc57) = \ud835\udc63 \ud835\udc56 \ud835\udc5f \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) = \ud835\udc63 \ud835\udc56 (\ud835\udc5f \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) \ud835\udc3c \ud835\udc3e )", "formula_coordinates": [3.0, 370.14, 370.49, 135.21, 9.43]}, {"formula_id": "formula_13", "formula_text": "v i r F(i)F(j) v i r F(i)F(k) Embedding v k Embedding v j \u2022 \u2022 Embedding v i,F(j) \u2026\u2026 \u2026\u2026 r F(i)F(j) r F(i)F(k)", "formula_coordinates": [3.0, 335.84, 476.03, 192.27, 160.04]}, {"formula_id": "formula_14", "formula_text": "\ud835\udc63 \ud835\udc56,\ud835\udc39 ( \ud835\udc57) = \ud835\udc63 \ud835\udc56 \ud835\udc37 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) = \ud835\udc63 \ud835\udc56 \u2299 \ud835\udc51 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) ,", "formula_coordinates": [4.0, 105.99, 149.99, 135.55, 9.43]}, {"formula_id": "formula_15", "formula_text": "\u03a6 \ud835\udc39\ud835\udc5a\ud835\udc39 \ud835\udc40 ((\ud835\udc98, \ud835\udc97), \ud835\udc99) = \ud835\udc64 0 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56 + \ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc5a \u2211\ufe01 \ud835\udc57=\ud835\udc56+1 \ud835\udc65 \ud835\udc56 \ud835\udc65 \ud835\udc57 \ud835\udc5d (\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 , \ud835\udc7e \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) ) (8)", "formula_coordinates": [4.0, 53.8, 527.46, 243.46, 34.34]}, {"formula_id": "formula_16", "formula_text": "\ud835\udc5d (\ud835\udc97 \ud835\udc56 , \ud835\udc97 \ud835\udc57 , \ud835\udc7e \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57) ) = \ud835\udc3e \u2211\ufe01 \ud835\udc58=1 \ud835\udc3e \u2211\ufe01 \ud835\udc58 \u2032 =1 \ud835\udc63 \ud835\udc58 \ud835\udc56 \ud835\udc63 \ud835\udc58 \u2032 \ud835\udc57 \ud835\udc64 \ud835\udc58,\ud835\udc58 \u2032 \ud835\udc39 (\ud835\udc56),\ud835\udc39 ( \ud835\udc57)(9)", "formula_coordinates": [4.0, 91.13, 591.25, 202.92, 26.61]}, {"formula_id": "formula_17", "formula_text": "Embedding v k,F(i) Embedding v j,F(i) \u2022 \u2022 \u2026\u2026 \u2026\u2026 Embedding v i,F(j) Embedding v i,F(k) \u2026\u2026 \u2026\u2026 Embedding v i,F(j) Embedding v i,F(k)", "formula_coordinates": [4.0, 345.8, 225.87, 183.87, 113.53]}, {"formula_id": "formula_18", "formula_text": "\ud835\udc5b (\ud835\udc5b-1)", "formula_coordinates": [5.0, 217.34, 307.3, 22.36, 6.25]}, {"formula_id": "formula_19", "formula_text": "Embedding v j Matrix M F(i)F(j) Embedding v i Embedding v i \u2026\u2026 \u2026\u2026 v i \u00d7 M F(i)F(k) v i \u00d7 M F(i)F(j) \u2022 \u2022 Matrix M F(i)F(k) Embedding v i Embedding v k", "formula_coordinates": [5.0, 352.57, 314.07, 180.22, 177.98]}, {"formula_id": "formula_20", "formula_text": "\u27e8\ud835\udc97 \ud835\udc56 \ud835\udc40 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc57 \u27e9 = \u27e8\ud835\udc97 \ud835\udc57 \ud835\udc40 \ud835\udc47 \ud835\udc39 (\ud835\udc56)\ud835\udc39 ( \ud835\udc57) , \ud835\udc97 \ud835\udc56 \u27e9(10)", "formula_coordinates": [6.0, 372.05, 515.23, 186.15, 11.46]}, {"formula_id": "formula_21", "formula_text": "\ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \ud835\udc64 \ud835\udc56(11)", "formula_coordinates": [7.0, 423.88, 429.19, 134.32, 24.75]}, {"formula_id": "formula_22", "formula_text": "\ud835\udc5a \u2211\ufe01 \ud835\udc56=1 \ud835\udc65 \ud835\udc56 \u27e8\ud835\udc97 \ud835\udc56 , \ud835\udc64 \ud835\udc39 (\ud835\udc56) \u27e9(12)", "formula_coordinates": [7.0, 409.56, 568.74, 148.65, 24.75]}], "doi": "10.1145/3442381.3449930"}
