{"title": "COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval", "authors": "Xirong Li; Chaoxi Xu; Xiaoxu Wang; Weiyu Lan; Zhengxiong Jia; Gang Yang; Jieping Xu", "pub_date": "2019-01-15", "abstract": "This paper contributes to cross-lingual image annotation and retrieval in terms of data and baseline methods. We propose COCO-CN, a novel dataset enriching MS-COCO with manually written Chinese sentences and tags. For more effective annotation acquisition, we develop a recommendationassisted collective annotation system, automatically providing an annotator with several tags and sentences deemed to be relevant with respect to the pictorial content. Having 20,342 images annotated with 27,218 Chinese sentences and 70,993 tags, COCO-CN is currently the largest Chinese-English dataset that provides a unified and challenging platform for cross-lingual image tagging, captioning and retrieval. We develop conceptually simple yet effective methods per task for learning from crosslingual resources. Extensive experiments on the three tasks justify the viability of the proposed dataset and methods. Data and code are publicly available at https://github.com/li-xirong/coco-cn.", "sections": [{"heading": "I. INTRODUCTION", "text": "A UTOMATED description of multimedia content, let it be image or video, is among the core themes for multimedia analysis and retrieval. For a long time, efforts on describing multimedia have limited the form of description to be individual words [1], e.g., car, road, and people, due to the semantic gap between low-level features and high-level semantics. Only recently we have witnessed noticeable advances in producing natural language-like descriptions, e.g., cars racing on a road surrounded by lots of people, that indicate not only the presence of specific concepts but also their interactions [2]. While these advances can certainly be attributed to technical breakthroughs, mostly deep learning, the importance of welllabeled datasets shall not be underestimated. Exemplars are ImageNet [3] for visual class recognition, NUS-WIDE [4] for image tagging, MS-COCO [5] for image captioning, and more recently Twitter100k [6] for cross-media retrieval, to name a few. Notice that annotations of these datasets are all written in the English language. To facilitate research that goes beyond the monolingual setting, this paper presents a new dataset called COCO-CN. It extends MS-COCO with manually written Chinese sentences and tags, see Table I. Although the importance of multimedia annotation and retrieval in a cross-lingual setting has been recognized early [7], only recently has the topic gained increasing attention [8]- [15]. Cross-lingual image tagging is important as it studies how to exploit data labeled in a source language to obtain image tagging models for a target language. Consequently, common users are provided with the possibility to have their images automatically tagged in a native language even when training examples in that language are in short supply. In [10], for instance, Miyazaki and Shimizu exploit a transfer learning strategy, initializing the visual embedding matrix of a Japanese captioning model using its counterpart from a trained English captioning model. Tsutsui and Crandall propose to generate multilingual sentences in a single network, using artificial tokens to control the language [8]. Lan et al. demonstrate the possibility of training an image captioning model by exploiting English resources to generate fluent Chinese sentences [14]. Elliott et al. show that using images as a side information improves multi-modal machine translation [15]. Cross-lingual applications are crucial for the majority of the world's population who do not speak English.\nTo support this line of research, several image datasets with non-English annotations have been developed. Depending on their applications, the target languages of these datasets differ, including German for multi-modal machine translation [9], [11], German and French for image retrieval [12], Japanese for cross-lingual document retrieval [16] and image captioning [10], [17], and Chinese for image captioning [14], [18], [19]. Except for IAPR TC-12 [7] and AIC-ICC [19] which consist of images collected from the Internet, each of these datasets is built on top of an existing English dataset, with MS-COCO as the most popular choice.\nDespite the encouraging progress, we see two deficiencies in the current research in terms of datasets and methods. While Chinese is the most spoken language in the world, existing datasets for this language are either small in scale (Flickr8k-CN [18]) or strongly biased for describing human activities and monolingual (AIC-ICC [19]). As for methods, there is a paucity of literature on cross-lingual image tagging. Initially set up for cross-lingual purposes, the ImageCLEF series have factually become monolingual [20]. The only work published in the last five years 1 is by Wei et al. [13], where the authors assume zero-availability of training images in their target language. Clarifai [21], an online image tagging service, is capable of predicting tags in multiple languages. But the non-English tags appear to be simply machine translation of the predicted English tags. How to effectively perform crosslingual image tagging is unclear.\nWorks on cross-lingual image captioning consistently report that for training an image captioning model for a target language, machine-translated sentences are less effective than manually written sentences [10], [14], [17], [18]. No attempt is made to answer whether and in what ways machinetranslated data can be effectively exploited together with manually-written sentences in the target language for better performance. For matching images and sentences in two different languages, Gella et al. propose a neural model to learn multilingual multi-modal representations with image as pivot [22]. A shortcoming of that model is it requires many triplets for training, where each image is associated with bilingual descriptions. Cross-lingual image retrieval requires not only cross-modal matching between a query and an image, but also cross-lingual matching between the same query and the description associated with the image, but in another language.\nTo attack the above two deficiencies, this paper makes three contributions as follows:\n\u2022 We present COCO-CN, a new dataset extending MS-COCO with manually written Chinese sentences and tags. It thus differs from previous cross-lingual efforts on MS-COCO that target Japanese [10], [17], German [11] or French [12]. With over 20k images described and tagged, COCO-CN is the largest Chinese-English dataset for cross-lingual image annotation and retrieval. The development of COCO-CN and its usage in the varied tasks are illustrated in Fig. 1. Data and code are available at https://github.com/li-xirong/coco-cn.\n\u2022 For COCO-CN construction a novel recommendationassisted annotation system is developed. While much progress has been made for image auto-tagging [23]- [25] and caption retrieval [26]- [28], whether these techniques can be used to assist dataset construction is unexplored. The new system 1 Paper [13] was the only hit when querying Google Scholar with \"crosslingual image annotation\" or \"cross-lingual image tagging\".", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b6", "b7", "b14", "b9", "b7", "b13", "b14", "b8", "b10", "b11", "b15", "b9", "b16", "b13", "b17", "b18", "b6", "b18", "b17", "b18", "b19", "b12", "b20", "b9", "b13", "b16", "b17", "b21", "b9", "b16", "b10", "b11", "b22", "b24", "b25", "b27", "b12"], "figure_ref": ["fig_0"], "table_ref": ["tab_0"]}, {"heading": "MS-COCO", "text": "Images with English sentences", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "COCO-CN", "text": "Images with Chinese tags and sentences allows us to assess the possibility of exploring the state-ofthe-art for reducing annotation workload. Moreover, with its source code released, the system provides a good starting point for peers who are interested in building similar datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Recommendation-assisted collective annotation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cross", "text": "\u2022 We show the applicability of COCO-CN for three tasks, i.e., cross-lingual image tagging, captioning and retrieval, and its superiority over alternate datasets. Moreover, we develop models specifically for the cross-lingual setting, i.e., Cascading MLP for tagging, Sequential Learning for captioning and Enhanced Word2VisualVec for retrieval. While conceptually simple, these models effectively learn from bilingual resources and consequently advance the state-of-the-art in the crosslingual context.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "II. RELATED WORK", "text": "We summarize in Table II an (incomplete) list of publicly available image datasets that are associated with manually annotated, non-English descriptions. The datasets closely related to ours in terms of the target language are Flickr8k-CN [18], Flickr30k-CN [14] and AIC-ICC [19], all focusing on Chinese.\nTo the best of our knowledge, Flickr8k-CN is the first dataset for image captioning in Chinese [18], adopting Flickr8k [31] as its data source. Each image has been reannotated with five Chinese sentences written by native speakers via a local crowd sourcing service. Probably because no annotation guideline or quality control was provided, the Flickr8k-CN descriptions tend to be short, containing on average 8 Chinese characters per sentence. As for Flickr30k-CN [14], its manual annotation covers only the test set of Flickr30k [30], by collectively translating 5,000 test sentences from English to Chinese. AIC-ICC is a big step forward, consisting of 240k images collected from Internet search engines and 1.2 million crowd-sourced sentences [19]. With an average number of 21.6 Chinese characters per sentence, the texts appear to be more descriptive than their Flickr8k- COCO-CN naturally inherits the visual diversity of MS-COCO. Its cross-lingual applicability is granted by jointly exploiting the new Chinese and the existing English annotations. Moreover, we collect tags to further enrich the annotations. The tags can be used for multi-label image classification. They can also be applied to multi-modal tasks, e.g., image captioning with tags. All this makes COCO-CN a versatile dataset for cross-lingual image tagging, captioning and retrieval.", "publication_ref": ["b17", "b13", "b18", "b17", "b30", "b13", "b29", "b18"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "III. COCO-CN CONSTRUCTION", "text": "The complexity of pictorial content makes it non-trivial to write a fluent sentence to fully describe an image. Consider the image in the first row of Table I for instance. The gist of the scene is a man flying a kite outdoor. If one tries to cover the entire context, e.g., sky, cloud, river, tree, and grass field, it will be a lengthy sentence that does not read smoothly. So we collect tags to complement natural-language descriptions.\nA. Recommendation-assisted Collective Annotation 1) User Interface: For the ease of participation, we develop a web based image annotation system that allows a user to annotate images remotely and independently. Fig. 2 provides a snapshot of the system. Our novel design is the deployment of two content-based recommendation modules, one for sentences and the other for tags, that assist manual annotation on the fly. When clicked, a sentence or tag from the recommendation board will instantly appear in the corresponding editable text form. User operations including clicking and editing are logged for data analytics.\nSince there is no off-the-shelf model available for recommending Chinese sentences or tags based on the pictorial content, we build our own models, as described in Section III-A2 and III-A3. Notice that we started the COCO-CN project in 2016 with the different modules developed in different periods, so the choice of image features was subject to the availability of the corresponding CNN models.\n2) Chinese Sentence Recommendation: Given an image to be annotated, our sentence recommendation module retrieves the sentence most likely to be relevant from a large Chinese sentence pool. The pool, consisting of 120k sentences, is obtained by performing machine translation on the MS-COCO English sentences. Li et al. make an empirical comparison between two public machine translation services, i.e., Google and Baidu, reporting the better performance of Baidu for translating image descriptions from English to Chinese [18]. So we use Baidu translation.\nGiven that each image is associated with five machinetranslated sentences, a straightforward strategy would be to simply allow the annotators to select the most appropriate one and edit it as they see fit. We do not choose this strategy for the following considerations. First, we encourage the annotators, while being assisted, to describe the images from their (personal) viewpoints, rather than just make a good translation. Learning from images with unaligned bilingual descriptions is more challenging yet more practical. Second, we want to keep the system flexible to handle novel images without any descriptions. Last but not least, to justify the usability of sentence retrieval for manual annotation.\nFor sentence retrieval we adopt Word2VisualVec (W2VV) [28], which demonstrates state-of-the-art performance on multiple datasets. W2VV is designed to predict deep visual features from just an (English) textual input. In particular, the mapping from a given text to a visual feature vector is achieved by first encoding the text into a fixed-length vector. The vector then goes through a Multilayer Perceptron (MLP) to produce the visual feature vector. Consequently, the crossmodal relevance between a given image and a given sentence is computed as cosine similarity in the visual feature space. While W2VV is originally developed for retrieving English captions, it can be easily extended to the Chinese scenario by substituting its English based encoding layer for a Chinese counterpart. Paired images and Chinese sentences are required to train the Chinese version of W2VV. To that end, we split at random the pool of 120k sentences into two disjoint subsets, with 90% for training and 10% for validation. We train a Fig. 2. A snapshot of our web based image annotation system. Rather than starting from scratch, an annotator is provided with five tags and five sentences automatically recommended by the system based on the pictorial content. When clicked, the tags and sentences will instantly appear in the editable text forms to assist manual annotation. Source code of the annotation system is publicly available at https://tinyurl.com/cococn-system.\nW2VV model that uses the bag-of-words based encoding and predicts a 2,048-dim ResNet-152 feature. As shown in Fig. 2, for a given image we compute its relevance to each sentence in the pool on the fly, and recommend the top-5 retrieved sentences to an annotator.\n3) Chinese Tag Recommendation: Our tag recommendation module is to predict multiple Chinese tags based on the visual content. Since the MS-COCO images are quite diverse, a large vocabulary is required. We need a large-scale training set wherein each image is labeled with multiple Chinese tags. Since a dataset as such is not publicly available, we consider Flickr images associated with user tags in different languages. However, we observe that the number of Flickr images labeled with Chinese tags is much less than its English counterpart. This is in line with [32], where Koochali et al. report that the majority of the Flickr tags are in English. Also we note that the quality of Chinese tags is much lower. Therefore, we use Flickr images but manually translate the tag vocabulary from English to Chinese. In particular, we adopt one million Flickr images provided by [23]. To construct a Chinese tag vocabulary, we first sort all the English tags in the 1M set in descending order by the number of distinct users who have used them for image tagging. We manually go through the ranked tag list, performing one-by-one manual translation. Tags lacking correspondence to specific objects, scenes or events are excluded. Note that synonyms are grouped. For instance, 'car', 'auto' and 'automobile' are all translated into the Chinese word '\u6c7d\u8f66'. Consequently, images labeled with 'car', 'auto' or 'automobile' are considered as positive training examples of '\u6c7d\u8f66'. A word with multiple meanings is translated to a list of Chinese words. For instance, 'glass' is mapped to {'\u73bb\u7483', '\u73bb\u7483\u676f', '\u773c\u955c'}. We let the annotators decide which translation is the most appropriate. Finally we obtain a Chinese vocabulary of 1,951 classes, covering 2,085 English and 2,367 Chinese words.\nAs Flickr data is known to be noisy [1], [23], [33], we de-noise with the following rule of thumb. We remove overlytagged images (associated with more than 20 tags) and images having less than two tags from the constructed vocabulary. This results in a training set of 800k images. For image representation we employ a pre-trained GoogLeNet model [34], taking the output of its pool5 layer as a 1,024-dim visual feature. An MLP with a network structure of 1024-1024-2048-1951 is trained on the 800k images. The top-5 Chinese tags predicted by the MLP model are presented to each annotator.", "publication_ref": ["b17", "b27", "b31", "b22", "b0", "b22", "b32", "b33"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "B. COCO-CN Annotation Process", "text": "Sentences of existing datasets such as MS-COCO, STAIR captions, Flickr8k-CN and AIC-ICC were gathered by distributing the annotation task on a specific crowd-sourcing platform. However, how to measure and thus control the quality of crowd-sourced sentences remains unresolved. Consider MS-COCO for instance. During manual translation we found a number of typos and misspellings in its sentences, e.g., \"ready to hi [hit] a ball\", \"smile son [on] their faces\", \"airplane with a striped tale [tail]\", and \"a giraffe standing by a palm team [tree]\". In order to gather high-quality annotations, we did not fully rely on crowd sourcing. Our annotation team was comprised of 17 volunteers (staff and graduate students in our lab) and 22 paid undergraduate students from our department.\nBefore they started, the team were instructed with the following guidelines. A sentence shall cover the main objects, actions and scene in a given image. The annotators were asked to provide at least one tag that complements the image description. For a better understanding of the guidelines, we provided a number of well / badly labeled examples. For quality control, an inspection committee comprised of paper authors regularly sampled a small proportion of the annotation results by distinct annotators for a manual check. Annotators responsible for unqualified results were put into a watch list and informed to review and re-edit their annotations. An annotator entering the watch list three times would be excluded from the team, with his/her annotations removed as well.\nFor effective acquisition of labeled data, our goal is to have more images annotated than to collect more sentences per image. This strategy is inspired by [28] where the authors empirically show that given the same number of image-sentence pairs for training, having more images results in a better model for cross-modal matching. To that end, we generated for each annotator a unique list of images by randomly shuffling the MS-COCO images. Consequently, the chance of two annotators labeling the same image was low.\nC. Data Analytics 1) Progress Overview: We have 20,342 images annotated with 22,218 sentences and 70,993 tags in total. Notice that we did not select these 20k images in advance. They were obtained as a consequence of manual annotation. Since images were randomly assigned to each annotator, the 20k images in COCO-CN are factually a representative subset of MS-COCO. In addition, we have 5,000 MS-COCO English sentences manually translated to Chinese for evaluating image captioning (c.f. Section IV-C). This adds the number of manually written Chinese sentences up to 27,218. Different from an English sentence where words are separated by whitespace characters, a Chinese sentence has no marker as word boundaries. It has to be segmented into a sequence of meaningful words before further analysis. We employ the BosonNLP toolkit [35] for sentence segmentation and part-of-speech tagging. Basic statistics of the sentences and tags are listed in Table III. The average number of annotators per image is 1.1, while the average number of images annotated per person is 521. 6.\nChinese tag vocabulary. The sentences contribute a vocabulary of 7,096 distinct words, while the manual tags contribute a vocabulary of 4,867 distinct words, among which 1,918 words are outside the sentence vocabulary. At the image level, on average nearly 45% of the manual tags are not covered by the associated sentences. It is clear that only tags with a reasonable amount of training images can be effectively modeled. So we expand the manual tags with words automatically extracted from Chinese sentences as follows. We consider only nouns, adjectives and verbs. Adjectives and verbs consisting of single Chinese characters, e.g., '\u6ee1', '\u4eae' and '\u7a7f', are over generic and thus removed. We further exclude rare tags that occur less than 20 times in the COCO-CN training set (see Fig. 1 and Section IV-A for data partition). We then manually go through the remaining tags, deleting those we consider unsuited for describing the pictorial content. Consequently, we obtain a vocabulary of 655 Chinese tags. The number of positive training images ranges from 20 to 1,876, with an averaged number of 123.\nCOCO-CN tags versus MS-COCO object classes. The images in the MS-COCO dataset are already annotated with 80 object classes such as person, bicycle and toothbrush. After manually translating these classes into Chinese tags, we find that there are 70 tags in common. In other words, we have enriched the MS-COCO annotations with 655 -70 = 585 novel concepts.\nCOCO-CN sentences versus MS-COCO sentences. For the ease of cross-lingual comparison, for each image in COCO-CN we employ machine translation to convert its five English captions from MS-COCO to Chinese. Given a COCO-CN sentence we investigate the overlap between its keywords, i.e., nouns, verbs and adjectives, and those in the translated sentences. On average a COCO-CN sentence brings in 45.3% novel keywords, with 39.9% nouns, 51.9% verbs, and 69.5% adjectives. The usage of adjectives is more subjective and personalized. Consider for instance describing a dog as small or big and judging if a cat is cute.\nAnnotation quality. Concerning the annotation team, no one entered the watch list three times. The annotators were asked to perform the annotation task independently, which helped improve the diversity of human annotations. The regular inspection mechanism, with nearly 10% of the images double-checked, helped maintain the overall annotation quality. Concerning the variety of description for each image and the vocabulary for the description, we evaluate in two aspects as follows. First, for each image we use its five machine translated sentences as references, and compute four metrics, i.e., BLEU-4, METEOR, ROUGE-L and CIDEr, that have been de facto for evaluating caption quality. The corresponding scores are 16.1, 27.6, 42.9, and 60.0, respectively, suggesting a substantial divergence between human annotations and machine translations. Second, we compare their vocabulary size. Not surprisingly, the human vocabulary, with a size of 7,096, is smaller than the machine vocabulary which has 14,219 distinct words. Notice however that the number of machinetranslated sentences is five times as large as the number of manually written sentences. So the human vocabulary is reasonably diverse. Moreover, we checked low-level typos in each manually written sentence by matching each of its words with a vocabulary of nearly 350k words from the Jieba Chinese text segmentation tool. For 942 out of the 22,218 sentences, they contain words outside the vocabulary. A manual check shows that only 56 sentences have true typos, e.g., incorrectly typing '\u897f\u5170\u82b1' (broccoli) as '\u897f\u84dd\u82b1' or '\u725b\u4ed4\u88e4' (jeans) as '\u725b\u5b50\u88e4'. For the other 886 sentences, their mismatched words such as '\u77ee\u67dc' (low cabinet) and '\u5e7c\u8c61' (baby elephant) are actually valid. The accuracy of the human annotations will be further verified by image captioning experiments, where COCO-CN is compared against current alternatives including Flickr8k-CN and AIC-ICC as training data. We also compare statistics of the sentences respectively written by the volunteer group and the paid group, and find no considerable difference.\n2) The Influence of Recommendation: We now analyze if the two recommendation modules are helpful during the annotation process.\nTag recommendation. The annotators accepted approximately 2.7 out of the five suggested tags per image, with an acceptance rate of 54%. Fig. 3 shows the top 30 recommended tags and their acceptance counts. The best-predicted tag is '\u732b' (cat), got accepted 649 out of 828 times.\nSentence recommendation. Among the 22,218 sentences, 31% of them were typed by the annotators after clicking (and optionally re-editing) the recommendations. The number of edited sentences is 5,796. To measure the manual effort saved per image, we propose to calculate the Levenshtein distance [36] between the recommended sentence clicked by an annotator and the final submitted sentence. As a string metric, the Levenshtein distance between two Chinese sentences is the minimum number of single Chinese character edits (insertions, deletions or substitutions) required to change one sentence into the other. Our baseline is sentence annotation without recommendation, where the distance between the annotation and the recommendation (which is null) is actually the length of the annotation. The baseline has an average distance of 16.8, while the recommendation counterpart is 15.0. Fig. 4 shows the distance distribution curves with and without recommendation. These results suggest that sentence recommendation reduces the annotation workload to some extent. Nevertheless, sentence recommendation is less effective than tag recommendation. ", "publication_ref": ["b27", "b34", "b5", "b35"], "figure_ref": ["fig_0", "fig_1"], "table_ref": ["tab_2"]}, {"heading": "IV. APPLICATIONS A. Common setup", "text": "Before detailing the individual tasks, we first describe an experimental setup shared by the tasks.\nData partition. We randomly select 1,000 images from COCO-CN as a held-out test set, and another set of 1,000 images as a validation set for hyper parameter optimization. The remaining 18,342 images are used for training. When evaluating image tagging and captioning in an automatic manner, limitations exist due to vocabulary discrepancy and incomplete ground truth. Therefore, we randomly select 100 images from the test set, which we term COCO-CN test100, for human evaluation. In addition, for human evaluation we construct another test set of 100 images, which are randomly sampled from NUS-WIDE other than MS-COCO.\nImage feature. For the three tasks, we employ the state-ofthe-art ResNeXt-101 model [37] with input size of 224 \u00d7 224. The model, provided by the MediaMill team at University of Amsterdam, is pre-trained on the full ImageNet dataset with a bottom-up reorganization [34] of the 22k ImageNet classes. The reorganization balances the classes by grouping or deleting over-specific and rare classes and down-sampling training examples for over-frequent classes, and consequently reduces the number of classes to 12k. We take the output of the last pooling layer, obtaining a 2,048-dim feature vector. For each image we resize it to 256 \u00d7 256, and extract its feature using an oversampling strategy. That is, we extract features from its 10 sub images, obtained by clipping the image and its horizontal flip with a window of 224 \u00d7 224 at their center and four corners. The 10 features are averaged as the image feature. Note that the ResNeXt-101 feature is better than the GoogLeNet and ResNet-152 features previously used in our annotation system, see the appendix.", "publication_ref": ["b36", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "B. Task I: Cross-Lingual Image Tagging", "text": "We describe how to exploit the Chinese annotations from COCO-CN and the English annotations from MS-COCO for building an image tagging model that predicts Chinese tags for a given image. We train the first MLP that predicts English tags. The output of this MLP is concatenated with the image CNN feature to form the input for the second MLP, which predicts Chinese tags.\n1) Setup: Tagging model. For learning from the crosslingual resources, we propose Cascading MLP. As illustrated in Fig. 5, the model consists of two MLPs. The first MLP is trained on MS-COCO for English tag prediction. A vocabulary of 512 English tags is constructed from the original MS-COCO captions by sorting nouns, verbs and adjectives in descending order in terms of their occurrence frequency. As such, this MLP essentially transforms the visual feature of a given image into a 512-dim semantic feature, with each dimension corresponding to a specific English tag. The visual and semantic features are concatenated to form a semanticenhanced input of the second MLP. This MLP is trained using COCO-CN to predict the probability of each Chinese tag being relevant with respect to a given image. As shown in Fig. 5, each MLP has one hidden layer consisting of 1,024 neurons. A rectified linear unit (ReLU) is followed to increase the nonlinearity of the model. As an image can be described by multiple tags, we use sigmoid (rather than softmax commonly used for visual recognition) as the activation function of the two output layers. The loss function is binary cross entropy.\nDuring training we apply a common dropout trick (with a rate of 0.5) to reduce the risk of overfitting.\nBaselines. In order to justify the effectiveness of the proposed model, we consider the following four alternates. Evaluation criteria. We report the popular Precision, Recall and F-measure at top 5. Each metric is computed per image and averaged over all test images.\n2) Results: Automated evaluation. Table IV shows automated evaluation results of the five tagging models. The Multi-task MLP is better than MLP trained on COCO-CN, suggesting a positive effect of multi-task regularization. Nonetheless, it is less effective than Cascading MLP. Moreover, in contrast to Multi-task MLP, Cascading MLP, by learning two monolingual MLPs sequentially, does not require aligned bi-lingual annotations per training image, and is thus more flexible to exploit partially annotated yet larger training data. Therefore, the cascading architecture is more suited for learning from unaligned cross-lingual resources.\nWe also compare Multi-task MLP and Cascading MLP on Flickr8k-CN, where the result shows that the latter is better. For more details we refer to https://tinyurl.com/tag-flickr8kcn. Human evaluation. It is possible that the lower performance of Clarifai is caused by a discrepancy between the Chinese vocabulary of the online service and our groundtruth vocabulary. To resolve this uncertainty, we performed a user study as follows. For each of the 100 pre-specified images, we collected the top 5 predicted tags by each model. Eight subjects participated in the user study. Each image together with the collected tags was shown to two subjects, who independently rated each of the tags as relevant, irrelevant or unsure. To avoid bias, the tags were randomly shuffled in advanced. Only tags rated as relevant by both subjects were preserved. Table V shows the tagging performance given the new ground truth. As the ground truth is more complete, all the scores improve. Cascading MLP again performs the best. Moreover, the qualitative conclusion concerning which model performs better remains the same.\nTo prevent dataset bias, we repeated the human study on another test set of 100 images randomly sampled from NUS-WIDE [4], which is independent of all the training sets, i.e., Flickr8k-CN, AIC-ICC, COCO-MT and COCO-CN, used in this work. We term this additional test set NUS-WIDE100. As Table VI shows, Cascading MLP is again the best. Captioning model. We adopt the Show and Tell network [38], originally developed for English caption generation and later found to be effective for Chinese caption generation [14], [18], [19]. The network computes the posterior probability of a sentence S given an input image x by combining a CNN based image encoder and an LSTM based sentence decoder. Consequently, the image is annotated with the sentence yielding the maximal probability. The size of LSTM, i.e., the dimensionality of its hidden vector, is empirically set to 512.\nTraining strategies. As mentioned above, Flickr8k-cn [18] and AIC-ICC [19] already exist for image captioning in Chinese. One more alternate is COCO-MT, a machine translation version of MS-COCO. We compare COCO-CN with these three datasets. For a fair comparison the Show and Tell model is separately trained using each dataset. The model is trained using standard supervised learning that minimizes the cross entropy loss. The initial learning rate is set to be 0.0005, which decays every three epochs with a decaying factor of 0.8.\nWhile the machine-translated sentences in COCO-MT tend to contain grammatical errors and do not read naturally, this dataset has a much larger vocabulary (with 10,144 words occurring at least 5 times) for training. In order to effectively exploit the large-scale property of COCO-MT and the highquality property of COCO-CN, we introduce a Sequential Learning strategy. Using the joint vocabulary of the two datasets, we first train the captioning model using COCO-MT. After 30 epochs, we pick up the model that has the highest CIDEr score on the COCO-CN validation set. We continue to train this model using COCO-CN, with a learning rate of 0.00005. Sequential Learning is in some way similar to fine tuning, with its focus on appropriate utilization of machinetranslated and manually written training sentences. From this viewpoint, Sequential Learning is not new by itself. Rather, it provides a novel and more effective way to exploit crosslingual resources, when compared to existing works [8], [10].\nBaselines. To the best of our knowledge, the Transfer learning model [10] and the Artificial token model [8] are the only two existing alternatives that target at learning from cross-lingual resources, both originally proposed for generating Japanese captions. In our implementation, the former initializes the visual embedding matrix of a Chinese captioning model using its counterpart in a trained English captioning model, while the latter learns a bilingual model from both manually written Chinese captions and original MS-COCO English captions. In addition, one might consider having COCO-MT and COCO-CN simply blended and used for training the image captioning model in one stage. We term this strategy COCO-Mixed, a necessary baseline to investigate whether the sequential learning strategy really matters.\nEvaluation criteria. We use standard automatic evaluation metrics to evaluate caption quality, namely BLEU, METEOR, ROUGE-L and CIDEr. While being convenient, the automated metrics have weak correlations with human judgments of caption quality. We perform a user study, in terms of relevance and fluency, on COCO-CN test100 and NUS-WIDE100. Fluency reflects the extent to which a sentence reads naturally. The same eight subjects participated in the user study. Each sentence was independently rated by two subjects on a Likert scale of 1 to 5 (higher is better) for relevance and fluency. To help a subject give more comparable scores, each test image and sentences generated by different models were presented together to the subject. To avoid bias, the sentences were randomly shuffled in advance.\n2) Results: Automated evaluation. Table VIII shows automated evaluation results of different captioning models. Models trained on Flickr8k-CN and AIC-ICC have fairly low scores. The relatively limited size of Flickr8k-CN makes it lack the ability for training a good captioning model. As for AIC-ICC, although it is the largest Chinese captioning dataset, it is strongly biased that all the images are about human beings. Consequently, the model trained on this dataset is unsuitable for describing general images. The model trained on COCO-CN is on par with the model trained on machine translated sentences of the full MS-COCO set. As for COCO-Mixed, since manually written sentences from COCO-CN are overwhelmed by machine translated sentences from COCO-MT during training, the benefit of COCO-CN appears to be marginal. By contrast, Sequential learning is quite effective, performing best under all metrics.\nHuman evaluation. Table IX shows human evaluation of distinct models on the two test sets. The COCO-MT model receives the lowest fluency score of 4.50 on COCO-CN test100 and 4.36 on NUS-WIDE100. Errors in machine translation impair the accuracy and grammar of the sentences generated by the COCO-MT model. With sentences carefully written by native Chinese speakers and covering a wide variety of visual content, COCO-CN is suitable for generic Chinese  image captioning. Sequential learning allows us to effectively leverage the manual annotations. While its fluency score of 4.76 is lower than the COCO-CN counterpart on COCO-CN test100 and that of AIC-ICC on NUS-WIDE100, sequential learning gives the best relevance score in terms of both automatic and human evaluation, suggesting that the generated captions are the most descriptives.\nExamples of captions generated by different models are shown in Table VII. Consider the second row for instance. The manually written sentence is \u4e00\u53ea\u9ed1\u732b\u8db4\u5728\u4e00\u53f0\u7b14\u8bb0\u672c \u7535\u8111\u4e0a (A black cat is lying on a laptop). The AIC-ICC model predicts \u623f\u95f4\u91cc\u6709\u4e00\u4e2a\u5750\u5728\u6905\u5b50\u4e0a\u7684\u5973\u4eba\u5728\u770b\u7535\u8111 (There is a woman sitting on the chair in the room watching the computer), showing a strong bias of the model on describing human actions. Compared to the COCO-CN model which describes the image as \u4e00\u53ea\u732b\u8db4\u5728\u7b14\u8bb0\u672c\u7535\u8111\u4e0a (A cat is lying on the laptop), both COCO-Mixed and Sequential Learning is more descriptive by predicting \u9ed1\u732b (black cat). The result shows the benefit of learning from the cross-lingual resources. Moreover, compared to COCO-Mixed which uses the verb \u5750 (sit), Sequential Learning uses the verb \u8db4 (lying), which is more precise and vivid in the Chinese language. D. Task III: Cross-Lingual Image Retrieval 1) Setup: The bilingual property of COCO-CN supports cross-lingual image retrieval. We consider in this paper the following setting. Given a Chinese sentence as a query, the goal is to find amidst a set of images the one best matches the query. Each of the images is described by an English sentence. The proposed cross-lingual image retrieval differs from a monolingual task as the former has to look into not only similarities between the visual and textual modalities but also similarities between texts presented in two distinct languages. To accomplish the proposed task, one needs to effectively represent Chinese queries, images and their English descriptions in a common space by cross-modal, cross-lingual representation learning. The task is thus of more research interest than its monolingual counterpart. While cross-modal matching is well known to be difficult, image retrieval by cross-lingual matching is also challenging. Even if the Chinese and English sentences are meant for describing the same image, they were independently written by distinct subjects, presumably with varied educational and cultural background. So they are not necessarily semantically aligned. A desirable method shall exploit cross-modal matching between the query and a given image and cross-lingual matching between the query and the English description of the given image.\nTest set. We again use the test set of COCO-CN. For each image, its manually written Chinese sentence is taken as a query, resulting in a query set of size 1,000. Recall that each image has five English sentences from MS-COCO. We take the first sentence as the English description of the image.\nRetrieval models. We see three approaches. One, crosslingual (CL) matching between the Chinese query and the English description, with the visual content ignored. Two, cross-modal (CM) matching between the query and the image, with the English description ignored. Third, cross-lingual and -modal (CLM) matching that combines the previous two approaches. W2VV previously used for sentence recommendation can project a given sentence into a visual feature space. Having sentences of distinct languages projected into the same visual feature space enables both cross-lingual and crossmodal matching. Hence, we adopt W2VV as a unified solution.\nWe start with the best configuration of W2VV [28]. An input sentence is vectorized in parallel by three strategies: bag-ofwords (BoW), Word2Vec (w2v) and a Gated Recurrent Units (GRU) network, as Fig. 6 shows. For GRU based vectorization, W2VV uses the hidden vector at the last time step, which is now known to be over compact to represent the entire sentence [15], [39]. Consider a query sentence \"\u4e09\u4e2a\u5851\u6599\u996d\u76d2\u4e2d\u88c5\u7740 \u9762\u6761\uff0c\u852c\u83dc\u548c\u6c34\u679c\" (Three plastic lunch boxes containing noodles, vegetables and fruits) for instance. While the query requires co-presence of multiple objects such as \u5851 \u6599 \u996d \u76d2 (plastic lunch boxes), \u9762\u6761 (noodles), \u852c\u83dc (vegetables) and \u6c34\u679c (fruits), W2VV tends to over-emphasize the ending part of the query, i.e., fruits. Moreover, W2VV is trained using the Mean Squared Error (MSE), which does not consider negative pairs, i.e., images and sentences that are irrelevant. Based on these two observations, we enhance the encoding module and the loss of W2VV as follows.\nFirst, we improve its GRU part by introducing a soft attention mechanism [39]. As shown in Fig 6, the attention layer is placed right after GRU, summing up the hidden vectors over all time steps with an adaptive weight assigned to each vector. More formally, let h i be the hidden vector at time step i, with i = 1, . . . , n and n is the maximum time step. The corresponding weight \u03b1 i is computed as Fig. 6. W2VV and our improvement. We enhance W2VV [28] by 1) adding an attention layer after GRU to take into account all the hidden vectors, and 2) substituting the original mean square error for the contrastive loss. The enhanced W2VV is used for cross-lingual image retrieval.\nm i = W T h i + b, [\u03b1 1 , \u03b1 2 , . . . , \u03b1 n ] = sof tmax([m 1 , m 2 , . . . , m n ]),(1)\nwhere W and b parameterize the affine transformation within the attention layer. Accordingly we substitute h n in W2VV for n i=1 \u03b1 i h i . Second, in order to exploit the many negative pairs, we substitute the MSE loss for the Contrastive Loss [40]. Given an image x and a sentence S, we use a binary variable y to indicate their relevance, i.e., y = 1 if they are relevant and 0 otherwise. The new loss is\ny \u2022 D(S, x) 2 + (1 -y) \u2022 max(margin -x), 0) 2 , (2)\nwhere D(S, x) indicate the Euclidean distance between the image and sentence vectors and margin is a hyper-parameter, empirically set to be 2. Minimizing Eq. 2 means to minimize (maximize) the distance between relevant (negative) pairs.\nWe use COCO-CN to train our Chinese model that transforms a given Chinese sentence into a ResNeXt-101 feature. In a similar manner, an English version of our model is trained using the images from the same training set but with MS-COCO sentences. The cross-lingual (CL) similarity between a Chinese sentence and an English sentence is computed as the cosine similarity between their ResNeXt-101 features. Similarly, the cross-modal (CM) similarity between a Chinese sentence and an image is computed.\nGiven a Chinese query, image retrieval by cross-lingual matching is achieved by sorting the images in descending order according to the CL similarities of their English sentences to the query. Then, image retrieval by cross-modal matching is to sort images in descending order according to their CM similarities to the query. Finally, for image retrieval by cross-lingual and -modal (CLM) matching, the similarity between the query and a given image is computed as a convex combination of the CL and CM similarities. The combination weight is optimized on the validation set.\nEvaluation criterion. Per query there is only one image known to be relevant with respect to the query. Hence, we report Mean Reciprocal Rank (MRR) at which the relevant item is found. Higher MRR means better performance.\n2) Results: Table X shows performance of the different approaches. The Enhanced W2VV outperforms the original W2VV, justifying the effectiveness of the joint use of the attention mechanism and the contrastive loss in the new context. Image retrieval by CM is less effective than its CL counterpart. The best performance is achieved by CLM  matching, scoring the highest MRR value of 0.511. This is a relative improvement of 6.2% over the original W2VV. Some qualitative results are given in Table XI. The Enhanced W2VV better models relatively complex queries that require cooccurrence of multiple visual concepts. Notice the exception in the second row, where the ending part of the query, i.e., \u7ea2\u8272 \u73af (red ring), is more important. In this case, the shortcoming of W2VV of over-emphasizing the ending part helps.", "publication_ref": ["b3", "b37", "b13", "b17", "b18", "b17", "b18", "b7", "b9", "b9", "b7", "b27", "b14", "b38", "b38", "b27", "b39"], "figure_ref": ["fig_2", "fig_2"], "table_ref": ["tab_4", "tab_5", "tab_5", "tab_7", "tab_8", "tab_6", "tab_8", "tab_9"]}, {"heading": "E. Discussion", "text": "To justify the viability of COCO-CN for cross-lingual image tagging, captioning and retrieval, we have developed baseline methods per task. While being conceptually simple, much engineering efforts are required to properly implement these methods. For reproducible research, we have released our source code at the same github URL as the dataset.\nThere is much room for future exploration from an algorithmic perspective. We discuss a few recent methods related to image tagging, captioning and retrieval, albeit in a monolingual setting. Wang et al. [41] propose a novel generative model, called Bilateral Correspondence Latent Dirichlet Allocation (BC-LDA), to find conditional relationships between images and texts based on good low-dimensional representations. BC-LDA is shown to be effective for microblog image tagging. In order to leverage partially labeled data for image and video annotation, Song et al. [42] propose to learn an optimized graph from multiple cues. Relationships between data points are embedded into the graph more precisely. For action recognition in videos, Wang et al. [43] introduce two-stream 3-D ConvNet fusion that handles video clips with arbitrary size and length by spatial temporal pyramid techniques. To attack the data selection bias between the training and testing stages, Shen et al. [44] introduce causally regularized learning, which results in image classifiers with better generalization ability. In the context of video captioning, Gao et al. [2] propose a novel attention-based LSTM model with semantic consistency to explore the correlation between multi-modal representations for generating sentences with rich semantic content. A more recent work by Song et al. [45] improves video captioning by implementing the decoding module using multi-modal stochastic RNNs networks, modeling the uncertainty observed in the data using latent stochastic variables. For image retrieval, Cui et al. [46] present an interesting study on how to transfer social knowledge sensed from social media platforms for reranking and consequently personalizing image search results from generic image search engines. For content-based video retrieval at large-scale, Song et al. [47] propose self-supervised video hashing that simultaneously encodes video temporal and visual information using an endto-end hierarchical binary auto-encoder and a neighborhood structure. For cross-modal retrieval, Peng et al. [48] propose to jointly employ coarse-grained instances and fine-grained patches within a multi-grained fusion network. Based on the observation that different modalities often contain an unequal amount of information when describing the same semantics, a modality-specific cross-modal similarity measurement approach is developed in [49], where an independent semantic space is constructed for each modality. The aforementioned algorithms have shown encouraging results for their own tasks, but all in a monolingual scenario. Repurposing and redesigning these algorithms to let them fit for the cross-ligual setting are beyond the scope of this paper.\nConcerning the dataset size, we could have chosen crowd sourcing, which is actually more cheaper and more scalable, to make the numbers more impressive. However, as we noted in the paper and supported by our observations about MS-COCO annotations, crowd sourcing lacks quality control. We thus progressed with the proposed annotation approach, which produced high-quality annotations as demonstrated by the data analytics in Section III-C and the multi-task experiments in this section. Compared to the existing datasets, COCO-CN results in noticeably better models for multiple tasks.", "publication_ref": ["b40", "b41", "b42", "b43", "b1", "b44", "b45", "b46", "b47", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "V. CONCLUSIONS AND PERSPECTIVES", "text": "The development of COCO-CN indicates that our recommendation-assisted annotation system helps reduce an-notators' workload. In particular, 54 percent of the recommended tags were accepted by the annotators. Sentence recommendation is also helpful, albeit less effective than tag recommendation.\nCOCO-CN has enriched MS-COCO, not just in terms of the language. Compared to the 80 MS-COCO object classes, there are 585 novel concepts. On average a COCO-CN sentence brings in 45.3% keywords not covered by machine translation of MS-COCO sentences. While the number of its images seems to be small, its diverse visual content associated with high-quality annotations makes the dataset superior to Flickr8k-CN and AIC-ICC. Compared to NUS-WIDE, a popular multi-label dataset that has 81 labels and 2.4 labels per image, COCO-CN provides 655 labels with 4.4 labels per image. With the rich and (unaligned) bilingual tags and sentences, COCO-CN is unique to the community, providing a unified and challenging platform for advancing image annotation and retrieval in the cross-lingual direction.\nEven though the ground truth of the test set is incomplete, it is adequate for drawing qualitative conclusions about which model is better for cross-lingual image tagging. Cascading MLP outperforms MLP trained on the monolingual resources, multi-task MLP trained on bi-lingual resources, and Clarifai, a commercial image tagging service. Our sequential learning strategy, while being a common practice and thus not new in the monolingual setting, is the first solution that has successfully combined machine-translated and manual annotations for cross-lingual image captioning. The enhanced W2VV establishes a new baseline for cross-lingual image retrieval. COCO-CN, provided with these good baseline methods, opens up interesting avenues for future research on cross-lingual multimedia tasks. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APPENDIX", "text": "Choice of Image Features. As shown in Table XII, for all the three tasks models using the ResNeXt-101 feature perform the best, when compared to their counterparts using either the GoogLeNet or ResNet-152 feature.\nImage Captioning with Tags. As we have summarized in Table II, a novel property of COCO-CN is that its images are annotated with tags, in addition to sentence descriptions. We conduct an extra experiment, termed image captioning with tags, to demonstrate that the tags can complement the image captioning task. The image captioning model previously trained on COCO-CN is re-used as a baseline. To construct a multi-modal input for a given image, we represent tags associated with this image by a bag-of-words (BoW) feature vector. Each dimension corresponds a specific tag, with its value set to one if the tag is present and zero otherwise. A multi-modal input is formed by concatenating the ResNeXt-101 feature and the BoW feature. As Table XIII shows, image captioning with tags performs better.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "General knowledge embedded image representation learning", "journal": "IEEE Trans. Multimedia", "year": "2018", "authors": "P Cui; S Liu; W Zhu"}, {"ref_id": "b1", "title": "Video captioning with attention-based lstm and semantic consistency", "journal": "IEEE Trans. Multimedia", "year": "2017", "authors": "L Gao; Z Guo; H Zhang; X Xu; H T Shen"}, {"ref_id": "b2", "title": "ImageNet: a large-scale hierarchical image database", "journal": "", "year": "2009", "authors": "J Deng; W Dong; R Socher; L.-J Li; K Li; L Fei-Fei"}, {"ref_id": "b3", "title": "NUS-WIDE: A real-world web image database from National University of Singapore", "journal": "CIVR", "year": "2009", "authors": "T.-S Chua; J Tang; R Hong; H Li; Z Luo; Y Zheng"}, {"ref_id": "b4", "title": "Microsoft COCO captions: Data collection and evaluation server", "journal": "CoRR", "year": "2015", "authors": "X Chen; H Fang; T.-Y Lin; R Vedantam; S Gupta; P Doll\u00e1r; L Zitnick"}, {"ref_id": "b5", "title": "Twitter100k: A realworld dataset for weakly supervised cross-media retrieval", "journal": "IEEE Trans. Multimedia", "year": "2018", "authors": "Y Hu; L Zheng; Y Yang; Y Huang"}, {"ref_id": "b6", "title": "The IAPR benchmark: A new evaluation resource for visual information systems", "journal": "", "year": "2006", "authors": "M Grubinger; P Clough; H M\u00fcller; T Deselaers"}, {"ref_id": "b7", "title": "Using artificial tokens to control languages for multilingual image caption generation", "journal": "", "year": "2017", "authors": "S Tsutsui; D Crandall"}, {"ref_id": "b8", "title": "Multi30k: Multilingual English-German image descriptions", "journal": "", "year": "2016", "authors": "D Elliott; S Frank; K Sima'an; L Specia"}, {"ref_id": "b9", "title": "Cross-lingual image caption generation", "journal": "", "year": "2016", "authors": "T Miyazaki; N Shimizu"}, {"ref_id": "b10", "title": "Multimodal pivots for image caption translation", "journal": "", "year": "2016", "authors": "J Hitschler; S Schamoni; S Riezler"}, {"ref_id": "b11", "title": "Bridge correlational neural networks for multilingual multimodal representation learning", "journal": "", "year": "2016", "authors": "J Rajendran; M Khapra; S Chandar; B Ravindran"}, {"ref_id": "b12", "title": "Harvesting deep models for cross-lingual image annotation", "journal": "CBMI", "year": "2017", "authors": "Q Wei; X Wang; X Li"}, {"ref_id": "b13", "title": "Fluency-guided cross-lingual image captioning", "journal": "ACMMM", "year": "2017", "authors": "W Lan; X Li; J Dong"}, {"ref_id": "b14", "title": "Imagination improves multimodal translation", "journal": "", "year": "2017", "authors": "D Elliott; \u00c1 K\u00e1d\u00e1r"}, {"ref_id": "b15", "title": "Image-mediated learning for zero-shot cross-lingual document retrieval", "journal": "", "year": "2015", "authors": "R Funaki; H Nakayama"}, {"ref_id": "b16", "title": "STAIR captions: Constructing a large-scale japanese image caption dataset", "journal": "", "year": "2017", "authors": "Y Yoshikawa; Y Shigeto; A Takeuchi"}, {"ref_id": "b17", "title": "Adding Chinese captions to images", "journal": "ICMR", "year": "2016", "authors": "X Li; W Lan; J Dong; H Liu"}, {"ref_id": "b18", "title": "AI challenger : A large-scale dataset for going deeper in image understanding", "journal": "CoRR", "year": "2017", "authors": "J Wu; H Zheng; B Zhao; Y Li; B Yan; R Liang; W Wang; S Zhou; G Lin; Y Fu; Y Wang; W Y "}, {"ref_id": "b19", "title": "ImageCLEF 2013: The vision, the data and the open challenges", "journal": "", "year": "2013", "authors": "B Caputo; H M\u00fcller; B Thomee; M Villegas; R Paredes; D Zellhofer; H Goeau; A Joly; P Bonnet; J Gomez; I Varea; M Cazorla"}, {"ref_id": "b20", "title": "Clarifai image and video recognition API", "journal": "", "year": "2018", "authors": " Clarifai"}, {"ref_id": "b21", "title": "Image pivoting for learning multilingual multimodal representations", "journal": "", "year": "2017", "authors": "S Gella; R Sennrich; F Keller; M Lapata"}, {"ref_id": "b22", "title": "Socializing the semantic gap: A comparative survey on image tag assignment, refinement and retrieval", "journal": "CSUR", "year": "2016", "authors": "X Li; T Uricchio; L Ballan; M Bertini; C Snoek; A Del Bimbo"}, {"ref_id": "b23", "title": "Advances in deep learning approaches for image tagging", "journal": "APSIPA Transactions on Signal and Information Processing", "year": "2017", "authors": "J Fu; Y Rui"}, {"ref_id": "b24", "title": "Ranking-preserving lowrank factorization for image annotation with missing labels", "journal": "IEEE Trans. Multimedia", "year": "2018", "authors": "X Li; B Shen; B.-D Liu; Y.-J Zhang"}, {"ref_id": "b25", "title": "An overview of cross-media retrieval: Concepts, methodologies, benchmarks and challenges", "journal": "TCSVT", "year": "2017", "authors": "Y Peng; X Huang; Y Zhao"}, {"ref_id": "b26", "title": "Generalized semisupervised and structured subspace learning for cross-modal retrieval", "journal": "IEEE Trans. Multimedia", "year": "2018", "authors": "L Zhang; B Ma; G Li; Q Huang; Q Tian"}, {"ref_id": "b27", "title": "Predicting visual features from text for image and video caption retrieval", "journal": "IEEE Trans. Multimedia", "year": "2018", "authors": "J Dong; X Li; C Snoek"}, {"ref_id": "b28", "title": "Collecting image annotations using Amazon's Mechanical Turk", "journal": "", "year": "2010", "authors": "C Rashtchian; P Young; M Hodosh; J Hockenmaier"}, {"ref_id": "b29", "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "journal": "TACL", "year": "2014", "authors": "P Young; A Lai; M Hodosh; J Hockenmaier"}, {"ref_id": "b30", "title": "Framing image description as a ranking task: Data, models and evaluation metrics", "journal": "JAIR", "year": "2013", "authors": "M Hodosh; P Young; J Hockenmaier"}, {"ref_id": "b31", "title": "Which languages do people speak on Flickr? a language and geolocation study of the YFCC100m dataset", "journal": "", "year": "2016", "authors": "A Koochali; S Kalkowski; A Dengel; D Borth; C Schulze"}, {"ref_id": "b32", "title": "Joint latent dirichlet allocation for social tags", "journal": "IEEE Trans. Multimedia", "year": "2018", "authors": "J Yao; Y Wang; Y Zhang; J Sun; J Zhou"}, {"ref_id": "b33", "title": "The ImageNet shuffle: Reorganized pre-training for video event detection", "journal": "ICMR", "year": "2016", "authors": "P Mettes; D Koelma; C Snoek"}, {"ref_id": "b34", "title": "BosonNLP: An ensemble approach for word segmentation and POS tagging", "journal": "NLPCC", "year": "2015", "authors": "K Min; C Ma; T Zhao; H Li"}, {"ref_id": "b35", "title": "Binary Codes Capable of Correcting Deletions, Insertions and Reversals", "journal": "Soviet Physics Doklady", "year": "1966-02", "authors": "V I Levenshtein"}, {"ref_id": "b36", "title": "Aggregated residual transformations for deep neural networks", "journal": "", "year": "2017", "authors": "S Xie; R Girshick; P Doll\u00e1r; Z Tu; K He"}, {"ref_id": "b37", "title": "Show and tell: A neural image caption generator", "journal": "", "year": "2015", "authors": "O Vinyals; A Toshev; S Bengio; D Erhan"}, {"ref_id": "b38", "title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2015", "authors": "D Bahdanau; K Cho; Y Bengio"}, {"ref_id": "b39", "title": "Dimensionality reduction by learning an invariant mapping", "journal": "", "year": "2006", "authors": "R Hadsell; S Chopra; Y Lecun"}, {"ref_id": "b40", "title": "Bilateral correspondence model for words-and-pictures association in multimediarich microblogs", "journal": "ACM Trans. Multimedia Comput. Commun. Appl", "year": "2014", "authors": "Z Wang; P Cui; L Xie; W Zhu; Y Rui; S Yang"}, {"ref_id": "b41", "title": "Optimized graph learning using partial tags and multiple features for image and video annotation", "journal": "IEEE Trans. Image Processing", "year": "2016", "authors": "J Song; L Gao; F Nie; H T Shen; Y Yan; N Sebe"}, {"ref_id": "b42", "title": "Two-stream 3-d convnet fusion for action recognition in videos with arbitrary size and length", "journal": "IEEE Trans. Multimedia", "year": "2018", "authors": "X Wang; L Gao; P Wang; X Sun; X Liu"}, {"ref_id": "b43", "title": "Causally regularized learning with agnostic data selection bias", "journal": "ACMMM", "year": "2018", "authors": "Z Shen; P Cui; K Kuang; B Li; P Chen"}, {"ref_id": "b44", "title": "From deterministic to generative: Multi-modal stochastic rnns for video captioning", "journal": "IEEE Trans. Neural Networks and Learning Systems", "year": "2018", "authors": "J Song; H Zhang; X Li; L Gao; M Wang; R Hong"}, {"ref_id": "b45", "title": "Social-sensed image search", "journal": "ACM Trans. Inf. Syst", "year": "2014", "authors": "P Cui; S.-W Liu; W.-W Zhu; H.-B Luan; T.-S Chua; S.-Q Yang"}, {"ref_id": "b46", "title": "Selfsupervised video hashing with hierarchical binary auto-encoder", "journal": "IEEE Trans. Image Processing", "year": "2018", "authors": "J Song; H Zhang; X Li; L Gao; M Wang; R Hong"}, {"ref_id": "b47", "title": "CCL: Cross-modal correlation learning with multigrained fusion by hierarchical network", "journal": "IEEE Trans. Multimedia", "year": "2018", "authors": "Y Peng; J Qi; X Huang; Y Yuan"}, {"ref_id": "b48", "title": "Modality-specific cross-modal similarity measurement with recurrent attention network", "journal": "IEEE Trans. Image Processing", "year": "2018", "authors": "Y Peng; J Qi; Y Yuan"}, {"ref_id": "b49", "title": "", "journal": "", "year": "2005", "authors": "Xirong Li Received The; B S ; M E Degrees"}, {"ref_id": "b50", "title": "was recipient of the ACMMM 2016 Grand Challenge Award", "journal": "", "year": "2012", "authors": " Prof;  Li"}, {"ref_id": "b51", "title": "Computer Science from Renmin University of China", "journal": "", "year": "2017", "authors": "Chaoxi Xu; B S Degree"}, {"ref_id": "b52", "title": "Computer Science from Renmin University of China", "journal": "", "year": "2015", "authors": "Xiaoxu Wang Received Her; B S ; M E Degrees"}, {"ref_id": "b53", "title": "Computer Science from Renmin University of China", "journal": "", "year": "2015", "authors": "Weiyu Lan Received Her; B S ; M E Degrees"}, {"ref_id": "b54", "title": "Computer Science from Renmin University of China", "journal": "", "year": "2018", "authors": "Zhengxiong Jia; B S Degree"}, {"ref_id": "b55", "title": "D. degree in Innovative Life Science from University of Toyama", "journal": "Renmin University of China", "year": "2009", "authors": "Gang Yang;  Ph"}, {"ref_id": "b56", "title": "degree in Computer Science from the Institute of Acoustics, Chinese Academy of Sciences in", "journal": "", "year": "1999", "authors": "Jieping D Xu Received Her Ph"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 .1Fig. 1. The development of COCO-CN and its usage in three tasks.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 4 .4Fig. 4. Distribution of the Levenshtein distance with and without sentence recommendation. Sentence recommendation decreases the distance, and thus reduces annotation workload.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 5 .5Fig. 5. The proposed Cascading MLP for cross-lingual image tagging.We train the first MLP that predicts English tags. The output of this MLP is concatenated with the image CNN feature to form the input for the second MLP, which predicts Chinese tags.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "1 )1COCO-MT. An MLP is trained using tags extracted from the machine-translated Chinese sentences of MS-COCO. Leaving out 2k images covered by the COCO-CN validation and test sets, COCO-MT has 121,287 training images. 2) COCO-CN. An MLP is trained on COCO-CN alone. 3) Clarifai. An online image tagging service [21] that predicts for a given image up to 200 tags in multiple languages including Chinese. The service claims to be a market leader since winning the top five places in image classification at the ImageNet 2013 competition, and is accessible via its API. 4) Multi-task MLP. A multi-task MLP is trained on COCO-CN with bi-lingual annotations to simultaneously predict English and Chinese tags. The model is a noncascaded version of Fig. 5, where the sum of the English and Chinese binary crossentropy losses is minimized during training.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "\ud835\udc46, \ud835\udc65 2 + 1 -\ud835\udc66 * max \ud835\udc5a\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc56\ud835\udc5b -\ud835\udc37 \ud835\udc46, \ud835\udc65 , 02   ", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "", "figure_caption": "", "figure_data": ""}, {"figure_label": "I", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "EXAMPLES FROM THE PROPOSED COCO-CN DATASET. THE NEW DATASET EXTENDS MS-COCO [5] WITH MANUALLY WRITTEN CHINESE SENTENCES AND TAGS. TEXT IN PARENTHESES IS ENGLISH TRANSLATION, PROVIDED FOR NON-CHINESE READERS.", "figure_data": "COCO-CNImageMS-COCO textSentencesTagsman is flying akite in a field"}, {"figure_label": "II", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "IMAGE DATASETS WITH MANUALLY ANNOTATED, NON-ENGLISH DESCRIPTIONS. WITH OVER 20K IMAGES DESCRIBED AND TAGGED, COCO-CN IS CURRENTLY THE LARGEST CHINESE-ENGLISH DATASET APPLICABLE FOR CROSS-LINGUAL IMAGE TAGGING, CAPTIONING AND RETRIEVAL.", "figure_data": "DatasetRelease Data sourceLanguagesImagesSentencesTagsApplicationsCN counterpart. Nevertheless, AIC-ICC has two downsides.First, it is a monolingual dataset, inapplicable in a cross-lingualsetting. Second, all its images are about human activities,requiring human figures to be clearly visible. Thereby the AIC-ICC sentences are tailored to human activity description. As aconsequence, an image captioning model trained on AIC-ICCalways generates sentences with a pattern of someone doingsomething, even if no person is present in test images."}, {"figure_label": "III", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "OF THE COCO-CN SENTENCES AND TAGS. THE SENTENCES CONTRIBUTE A VOCABULARY OF 7,096 DISTINCT WORDS, WHERE THE PERCENTAGES OF NOUNS, VERBS AND ADJECTIVES ARE 59.5%, 22.6% AND 5.1%. THE NUMBER OF DISTINCT TAGS IS 4,867. THE NUMBER OF TAGS OUTSIDE THE SENTENCE VOCABULARY IS 1,918.", "figure_data": "Chars per sentenceWords per sentenceTags per imagemin maxmeanmin maxmeanmin maxmean45916.824310.91153.5"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The top-30 recommended tags. Their English translations are provided in parentheses for non-Chinese readers.", "figure_data": "7RSUHFRPPHQGHGWDJVZLQWHU FURZG NLWFKHQ FDW KRUVH ELF\\FOH VHD UHG SODQH EOXH GRJ VWUHHW IRRG DQLPDO JLUO EHDFK VSRUW FDU VQRZ ZDWHU ]RR ZLOGOLIH VLJQ ZKLWH OLJKW WUHH UHVWDXUDQW VNL WUDLQ GLQQHUDFFHSWDQFHFRXQW5HFRPPHQGDWLRQFRXQWFig. 3."}, {"figure_label": "IV", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "EVALUATION OF DIFFERENT MODELS FOR IMAGE TAGGING. CASCADING MLP LEARNED FROM CROSS-LINGUAL DATA IS THE BEST.", "figure_data": "ModelPrecision Recall F-measureClarifai [21]0.2170.2610.228MLP trained on COCO-MT 0.4320.5250.456MLP trained on COCO-CN0.4770.5760.503Multi-task MLP0.4820.5830.508Cascading MLP0.4910.5940.517"}, {"figure_label": "V", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "HUMAN EVALUATION OF DIFFERENT MODELS FOR IMAGE TAGGING ON COCO-CN TEST100. CASCADING MLP AGAIN PERFORMS THE BEST.Naturally, COCO-CN is suitable for research on image captioning in Chinese and in a cross-lingual setting. To demonstrate the necessity of COCO-CN, we present the following experiments, comparing models trained on different datasets.1) Setup: Test set. Each of the 1,000 test images in COCO-CN is associated with one manually written Chinese sentence at least. To enrich the ground truth, we manually translated the existing five English sentences per test image. Consequently, each test image has six Chinese sentences as ground truth.", "figure_data": "ModelPrecision Recall F-measureClarifai0.6340.3580.451MLP trained on COCO-MT 0.7780.4530.563MLP trained on COCO-CN0.8360.4880.607Cascading MLP0.8580.5010.623TABLE VIHUMAN EVALUATION OF IMAGE TAGGING ON NUS-WIDE100, ARANDOM SUBSET OF 100 IMAGES FROM NUS-WIDE.ModelPrecision Recall F-measureClarifai0.6360.4480.515MLP trained on COCO-MT 0.6680.4720.542MLP trained on COCO-CN0.6940.4870.561Cascading MLP0.7260.5090.588C. Task II: Cross-Lingual Image Captioning"}, {"figure_label": "VII", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "TAGGING AND CAPTIONING RESULTS BY DIFFERENT MODELS. TEXTS IN PARENTHESES ARE ENGLISH TRANSLATIONS, PROVIDED FOR", "figure_data": ""}, {"figure_label": "VIII", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "EVALUATION OF IMAGE CAPTIONING MODELS TRAINED ON DIFFERENT DATASETS. THE PROPOSED CROSS-LINGUAL TRANSFER PERFORMS THE BEST.", "figure_data": "TrainingBLEU-4 METEOR ROUGE-L CIDErFlickr8k-CN10.114.933.822.9AIC-ICC7.421.334.224.6COCO-MT30.227.150.086.2COCO-CN31.727.252.084.6COCO-Mixed29.828.650.386.8Transfer learning [10]33.728.252.989.2Artificial token [8]31.826.951.585.4Sequential Learning36.729.555.098.4TABLE IXHUMAN EVALUATION OF IMAGE CAPTIONING MODELS ON TWO TESTSETS. THE NUMBERS AFTER \u00b1 ARE STANDARD DEVIATIONS.SEQUENTIAL LEARNING OBTAINS THE BEST OVERALL SCORE.COCO-CN test100NUS-WIDE100ModelRelevanceFluencyRelevanceFluencyFlickr8k-CN2.34 \u00b11.10 4.62 \u00b10.621.96 \u00b11.03 4.71 \u00b10.48AIC-ICC2.63 \u00b11.24 4.75 \u00b10.442.17 \u00b11.10 4.73 \u00b10.55COCO-MT3.89 \u00b10.77 4.50 \u00b10.772.92 \u00b11.18 4.36 \u00b10.86COCO-CN3.95 \u00b10.90 4.83 \u00b10.512.69 \u00b11.08 4.59 \u00b10.72Transfer Learning [10] 3.89 \u00b10.83 4.78 \u00b10.492.86 \u00b11.10 4.62 \u00b10.61Artificial token [8]3.78 \u00b10.99 4.78 \u00b10.692.61 \u00b11.10 4.46 \u00b10.90Sequential Learning4.13 \u00b10.72 4.76 \u00b10.59 3.02 \u00b11.15 4.67 \u00b10.60"}, {"figure_label": "X", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "PERFORMANCE OF DIFFERENT APPROACHES TO CROSS-LINGUAL IMAGE RETRIEVAL, MEASURED BY MRR.", "figure_data": "Matching approachesModelCLCMCLMW2VV [28]0.450 0.3780.481Enhanced W2VV (attention)0.466 0.4120.499Enhanced W2VV (attention + contrastive loss)0.468 0.4210.511"}, {"figure_label": "XI", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "W2VV VERSUS ORIGINAL W2VV FOR IMAGE RETRIEVAL BY CROSS-LINGUAL AND -MODAL (CLM) MATCHING. TEXTS IN PARENTHESES ARE ENGLISH TRANSLATIONS, PROVIDED FOR NON-CHINESE READERS AND NOT USED FOR IMAGE RETRIEVAL. WHILE W2VV TENDS TO OVER-EMPHASIZE THE ENDING WORDS OF A QUERY SENTENCE, ENHANCED W2VV WITH THE ATTENTION MECHANISM BETTER MODELS THE ENTIRE QUERY.", "figure_data": "Chinese sentence as queryTop hitby W2VVby Enhanced W2VV\u4e09\u4e2a\u5851\u6599\u996d\u76d2\u4e2d\u88c5\u7740\u9762\u6761\uff0c\u852c\u83dc\u548c\u6c34\u679c (Three plastic lunchboxes containing noodles, vegetablesand fruits)"}, {"figure_label": "XII", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "THREE CNN FEATURES. FOR A FAIR COMPARISON, ALL THE FEATURES ARE EVALUATED IN THE SAME SETTING, i.e., MLP TRAINED ON COCO-CN FOR IMAGE TAGGING, THE SHOW AND TELL MODEL TRAINED ON COCO-CN FOR IMAGE CAPTIONING, AND THE ENHANCED W2VV FOR IMAGE RETRIEVAL. MODELS USING THE RESNEXT-101 FEATURE ARE THE BEST.TABLE XIII AUTOMATED EVALUATION OF IMAGE CAPTIONING WITH AND WITHOUT TAGS. COCO-CN IS USED AS TRAINING DATA. INCLUDING TAGS AS INPUT GIVES BETTER PERFORMANCE.", "figure_data": "Image FeatureImage TaggingImage CaptioningImage RetrievalPrecision RecallF-measureBleu-4 METEOR ROUGE-LCIDErCLCMCLMGoogLeNet0.3940.4730.41426.127.349.673.50.458 0.336 0.463ResNet-1520.4070.4960.43026.527.349.874.40.450 0.362 0.479ResNeXt-1010.4770.5760.50331.727.252.084.60.468 0.421 0.511InputBLEU-4 METEORROUGE-LCIDErImage31.727.252.084.6Image + tags31.330.153.290.0"}], "formulas": [{"formula_id": "formula_0", "formula_text": "m i = W T h i + b, [\u03b1 1 , \u03b1 2 , . . . , \u03b1 n ] = sof tmax([m 1 , m 2 , . . . , m n ]),(1)", "formula_coordinates": [10.0, 331.28, 727.44, 231.76, 23.68]}, {"formula_id": "formula_1", "formula_text": "y \u2022 D(S, x) 2 + (1 -y) \u2022 max(margin -x), 0) 2 , (2)", "formula_coordinates": [11.0, 58.74, 336.55, 241.29, 11.03]}], "doi": ""}
