{"Federated Learning is Better with Non-Homomorphic Encryption": "KONSTANTIN BURLACHENKO, AI Initiative, KAUST, Saudi Arabia ABDULMAJEED ALROWITHI, Saudi Data and AI Authority, Saudi Arabia FAHAD ALI ALBALAWI, Saudi Data and AI Authority, Saudi Arabia AI Initiative, KAUST, Saudi Arabia PETER RICHT\u00c1RIK, Traditional AI methodologies necessitate centralized data collection, which becomes impractical when facing problems with network communication, data privacy, or storage capacity. Federated Learning ( FL ) offers a paradigm that empowers distributed AI model training without collecting raw data. There are different choices for providing privacy during FL training. One of the popular methodologies is employing Homomorphic Encryption ( HE ) - a breakthrough in privacy-preserving computation from Cryptography. However, these methods have a price in the form of extra computation and memory footprint. To resolve these issues, we propose an innovative framework that synergizes permutation-based compressors with Classical Cryptography, even though employing Classical Cryptography was assumed to be impossible in the past in the context of FL . Our framework offers a way to replace HE with cheaper Classical Cryptography primitives which provides security for the training process. It fosters asynchronous communication and provides flexible deployment options in various communication topologies. Keywords: Federated Learning, Privacy Preserving Machine Learning, Asynchronous Training, Security, Optimization, AES, CKKS.", "1 INTRODUCTION": "Effective machine learning models necessitate vast training data from diverse sources [67]. However, such data, often dispersed across various entities, faces sharing restrictions due to privacy concerns [39, 51]. Federated learning ( FL ) provides a viable solution, enabling collaborative global model training without exposing sensitive information [40, 53]. FL algorithms bifurcate into cross-device FL , involving big amount of devices, and cross-silo FL , typically engaging several distinct organizations [51]. In both cases, preserving the privacy of clients' datasets is significant. FL strives to ensure confidentiality by retaining private data on the client side, yet it falls short of providing substantial privacy assurance. Model parameters and derived quantities from it, transmitted from clients to a server, may embed sensitive data (see Appendix E.1). Detailed information about privacy methods for FL is presented in Appendix D. We briefly discuss two of them. Differential Privacy ( DP ) is an approach to assess the privacy of a specific Algorithm provided by Dwork et al. [24, 25]. DP algorithms ensure that, for any set of training examples, no attacker, no matter how powerful, can not learn much more information about a single training example than they could if this example had been excluded from the training data. Despite offering privacy protection for individual users, integrating DP with FL could amplify communication overhead, diminish accuracy. When striving for strong privacy guarantees in scenarios where the user count is small (which is possible in cross-silo settings) the noise impact from DP technics makes training challenging. Understanding the convergence of SGD with DP is an active research topic [17]. While DP statistically hides the training data, Homomorphic Encryption ( HE ) provides means to perform computation operations under encrypted numbers from Z \ud835\udc51 , R \ud835\udc51 , B \ud835\udc51 without decryption, protecting input and output from execution part. Therefore HE can be used for aggregating encrypted gradients [5, 50] in a server that clients do not trust, given that master has a public key. Classical Cryptography in FL. Classical Cryptography operates on the binary representation of data. It does not preserve even linear relations in a homomorphic way. The fact of exhibiting poor algebraic properties is an underlying reason why using Classical Cryptography was considered challenging by previous research papers [36, 41]. In these works, authors stated that Advanced Encryption Standard ( AES ) [22] is not suitable for FL or is challenging. Communication Compression in FL. Numerous compression methods such as quantization [62, 66], sparsification [4, 65], and dithering [3, 35] have been explored to mitigate communication cost during FL training. However, these techniques necessitate secure server-side aggregation, and there are no guarantees that communication reduction techniques and techniques aimed to preserve privacy or security are combinable. Research Contributions. We discovered that recently proposed permutated correlated compressors [64] PermK exhibit properties essential for using Classical Cryptography in FL while preserving the ability of Communication Compression . In our work, we introduce a framework that provides privacy and secure preserving training process to FL applications in which previously HE methods have been used. Summary of our contributions: (1) We addressed the challenge previously mentioned in [36, 41] that usage of AES is challenging in FL . (2) We demonstrated the operational advantages of the proposed privacy and secure aware optimization Alg. 3 over HE in the setting in which HE is typically applied in FL . (3) Weillustrated the framework's capability to train ResNet-18 [32] DL model in CIFAR-10 , and carried the discussion of potential benefits for DL in Appendix I. (4) We demonstrated a possibility of computation communication overlap and handling compute heterogeneity in Appendix H.2. Deployment flexibility in communication topologies is discussed in Appendix J. To support readers with various backgrounds we provide (i) glossary in Appendix A; (ii) details about AES in Appendix F; (iii) overview of privacy mechanisms in FL in Appendix D, discussion about the difference between privacy and security in Appendix E.2; (iv) overview of CKKS in Appendix G (see Table of Content at p.16).", "2 PROBLEM FORMULATION": "In this work, we develop a practical communication efficient privacy and secure aware framework for FL training. Requirements for Optimization Objective. From the perspective of machine learning ( ML ), our objective is to select a function from a parameterized function class F indexed by \ud835\udc65 \u2208 R \ud835\udc51 , by solving the following optimization problem: Here, \ud835\udc5b \u2208 N represents the number of clients, and \ud835\udc65 \u2208 R \ud835\udc51 denotes the \ud835\udc51 parameters or weights of a model \u02c6 \ud835\udc39 (\u00b7 ; \ud835\udc65 ) \u2208 X \u2192 Y that need to be learned across all \ud835\udc5b clients. The function \ud835\udc53 \ud835\udc56 : R \ud835\udc51 \u2192 R provides the score criteria for using the model \u02c6 \ud835\udc39 (\u00b7 ; \ud835\udc65 ) on the client's \ud835\udc56 data. In the context of FL the functions \ud835\udc53 \ud835\udc56 typically represented as: Here, \ud835\udc5b \ud835\udc56 \u2208 R denotes the number of data points at client \ud835\udc56 \u2208 [ \ud835\udc5b ] , and ( \ud835\udc4e \ud835\udc56 \ud835\udc57 , \ud835\udc4f \ud835\udc56 \ud835\udc57 ) \u2208 X \u00d7 Y represent the input-output pairs at client \ud835\udc56 . The function L \ud835\udc56 \ud835\udc57 ( \ud835\udc66 real , \ud835\udc66 pred ) : Y\u00d7Y \u2192 R is a loss function that scores prediction, \ud835\udc45 \ud835\udc56 : R \ud835\udc51 \u2192 R is the regularization function used for parameter \ud835\udc65 at client \ud835\udc56 . The weight \ud835\udc64 \ud835\udc56 \u2208 R encodes knowledge about the role of client \ud835\udc56 . Federated Learning is Better with Non-Homomorphic Encryption For example: (i) \ud835\udc64 \ud835\udc56 def = ( \ud835\udc5b \ud835\udc56 \u00b7 \ud835\udc5b )/ GLYPH<0>\u02dd \ud835\udc5b \ud835\udc56 = 1 \ud835\udc5b \ud835\udc56 GLYPH<1> corresponds to a case when all data point s are equally important; (ii) \ud835\udc64 \ud835\udc56 def = 1 corresponds to a case when all devices are equally important. We aim to solve Problem 1 under the next requirements: Security Requirements for the FL Training Procedure. (A) Clients never transfer the training data to the master. (B) Clients do not trust communication devices. (C) Detect attempts of message tampering by adversaries. (D) Clients distrust a server to store information that could compromise the training data. (E) Preventing competitor worker interference. (F) Limit memory traffic from clients to the master. (G) Overlapping communication and computing in clients because separate physical devices implement it. Assumptions for applying our framework. (i) \ud835\udc53 ( \ud835\udc65 ) is differentiable in training variable \ud835\udc65 \u2208 R \ud835\udc51 ; (ii) Clients trust each other through the established key; (iii) dom ( \ud835\udc53 ) = R \ud835\udc51 . Real-world scenarios for applying our framework. (a) An individual using multiple IoT devices and wants to train an ML model via servers provided by a third-party company. In reality, engineers in the companies may have unlimited access to server software facilities and this social aspect should be considered while making a decision about sending plain messages to servers; (b) In cross-device settings several IoT mutually trusted clients perform FL training, but still there is a need to have a master: some client is temporarily unavailable and updates should be buffered; applied steps need to be stored to reconstruct the optimization trajectory for further statistical tests, but IoT devices do not have the needed storage capacity; the training should be restarted in critical system failures; (c) We want to train FL model without a physical master in a situation when communication topology effectively supports broadcasting. Our framework can be instantiated in this regime (see Appendix J).", "3 FRAMEWORK OF SECURITY AWARE FL WITH PERMUTED COMPRESSORS": "The Distributed Compressed Gradient Descent ( DCGD (Baseline) ) [43], presented as (Algorithm 1, Option B), enables the use of independent unbiased compressors C \ud835\udc56 if E GLYPH<2> \u2225C \ud835\udc56 ( \ud835\udc65 ) -\ud835\udc65 \u2225 2 GLYPH<3> \u2264 \ud835\udc64 \u2225 \ud835\udc65 \u2225 2 . If \ud835\udc64 \u2260 0 this algorithm does not induce variance for a \ud835\udf07 -strongly convex objective \ud835\udc53 only in a specific ( overparameterized ) mode: \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) = 0 , \u2200 \ud835\udc56 \u2208 [ \ud835\udc5b ] . If C \ud835\udc56 (\u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 )) def = \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) , it reconstitutes distributed Gradient Descent ( GD ). One line of research involves removing variance from the use of compressors. For instance, MARINA [30] and COFIG/FRECON [69] do not induce variance when using unbiased independent compressors, while EF21 [59] avoids inducing variance from any independent contractive compressors. In all these algorithms, the logic in the master starts to include extra state updates based on obtaining messages from clients. But this is what we aim to avoid. Thus, the development of our framework is based on stateless DCGD (Baseline) . For plain DCGD (Baseline) we have two issues: (a) In Line 6, the algorithm sends sparsified information about \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) to the master, which is insecure; (b) In Line 8, the master computes the average of \ud835\udc54 \ud835\udc58 \ud835\udc56 \u2208 R \ud835\udc51 , but the master should not obtain \ud835\udc54 \ud835\udc58 \ud835\udc56 = \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 \ud835\udc58 ) or quantities that are subject to reconstruction attacks (see Appendix E.1). One way to address these problems is to use the HE schemes. The current practical state-of-the-art HE scheme which works R \ud835\udc51 is the CKKS [18]. The CKKS scheme is the most prevalent for ML applications [47] and is a practical one. As we will demonstrate in experiments, the CKKS carries an approximation error that arises due to the scheme being lossy by design. See Appendix G, D for details about CKKS and HE .", "Algorithm 2 Sampling of Correlated Permutation Compressors ( PermK ) configuration ( \ud835\udc51 > \ud835\udc5b )": "1: Parameters: Dimension of opt. problem \ud835\udc51 > 0, number of clients \ud835\udc5b > 0. 2: U.a.r. generate permutation \ud835\udc67 of sequence [ \ud835\udc51 ] = { 1 , 2 , . . . ,\ud835\udc51 } 3: Split z into \ud835\udc5b buckets, where each bucket has a size at least \ud835\udc35 = GLYPH<4> \ud835\udc51 \ud835\udc5b GLYPH<5> 4: Each bucket \ud835\udc4f \ud835\udc56 is initialized with { \ud835\udc67 ( \ud835\udc56 \u00b7 \ud835\udc35 ) -\ud835\udc35 + 1 , . . . , \ud835\udc67 ( \ud835\udc56 \u00b7 \ud835\udc35 ) } , 1 \u2264 \ud835\udc56 \u2264 \ud835\udc5b 5: Compute the residual \ud835\udc61 = \ud835\udc51 -\ud835\udc5b \u00b7 GLYPH<4> \ud835\udc51 \ud835\udc5b GLYPH<5> 6: Sample without replacement \ud835\udc61 clients from \ud835\udc5b as a set \ud835\udc46 , | \ud835\udc46 | = \ud835\udc61 = { } = \u222a 7: Scan the set \ud835\udc46 \ud835\udc60 1 , . . . ,\ud835\udc60 \ud835\udc58 , . . . , \ud835\udc60 \ud835\udc61 and update \ud835\udc4f \ud835\udc60 \ud835\udc58 \ud835\udc4f \ud835\udc60 \ud835\udc58 \ud835\udc67 \ud835\udc51 -\ud835\udc61 + \ud835\udc58 8: Setup compression C = ( C 1 , . . . , C \ud835\udc5b ) : [ C \ud835\udc56 ( \ud835\udc65 ) ] \ud835\udc57 = \ud835\udc5b \u00b7 \ud835\udc65 \ud835\udc57 \u00b7 \ud835\udc3c ( \ud835\udc57 \u2208 \ud835\udc4f \ud835\udc56 )", "Algorithm 3 DCGD/PermK/AES , \ud835\udc51 \u2265 \ud835\udc5b": "The clients need to send in parallel \ud835\udc5b encrypted messages to the master. Since clients trust each other, symmetric key encryption is a natural option to use. In the world of symmetric ciphers, we have decided to use a block cipher, specifically the industry-standard AES [21], instead of stream ciphers like SALSA [10]. The choice was made due to the advantages of block ciphers, as: (i) hardware support within CPU 1 ; (ii) the provision of multiple security levels (128 , 192 , 256 bits); (iii) the necessary flexibility to work with available parts of the vector \ud835\udc3a \ud835\udc58 (In coming Algorithm 3). The AES block-cipher provides security for a single 16-byte block. The Modes of Operation provide a way to use AES for more than one block. Message Authentication Code ( MAC ) provides guarantees from tampering. In our work, we used AES/EAX mode of operation. For details about AES see Appendix F. The naive way to use AES is to use Algorithm 1, (Option A). It solves problems with an untrusted server and channels. Without knowing \ud835\udc60\ud835\udc58 , the untrusted party cannot join the training procedure. Due to the use of MAC (Lines 5, 13), the training process verifies the integrity and protects against malicious attacks. To provide semantic security against a chosen-plaintext attack ( CPA ) and have the ability to use \ud835\udc60\ud835\udc58 in a distributed way, we employ random \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52 based selection. It eliminates the need for client coordination in a \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52 selection if \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52 is big enough such as 128 bits. Details about the role of \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52 in block ciphers are presented in Appendix F. With this Algorithm 1 Option-A, solves (A)-(E). However, this strategy has two big downsides: (1) Computation at Lines 13 and 14 is repeated by all clients with complexity per client O( \ud835\udc51\ud835\udc5b ) ; (2) Amount of information from the master in Line 9 is O( \ud835\udc51\ud835\udc5b ) , not O( \ud835\udc51 ) . We pay this price due to the use of AES , which does not allow to perform aggregation in the server. From one point of view, the natural way is to perform averaging for vectors in R \ud835\udc51 in the master (Line 10), but on another side, we perform bit-wise operations (Line 5) inside AES cipher. There is no way to connect it from the first point of view, but we have discovered how to do it! The PermK correlated compressor from work [64] possesses a compelling property that has yet to be recognized. The correlated compressors operate interdependently for clients and the schema was outlined in Algorithm 2. Let C 1 , . . . , C \ud835\udc5b : R \ud835\udc51 \u2192 R \ud835\udc51 be randomized PermK compressors, then If \ud835\udc51 \u2265 \ud835\udc5b , then 1 \ud835\udc5b \u02dd \ud835\udc5b \ud835\udc56 = 1 C \ud835\udc56 ( \ud835\udc63 \ud835\udc56 ) fulfills the variance bound: . \uf8f0 \uf8fb This compressor is used in our DCGD/PermK Algorithm 3. Its unbiasedness property ensures the absence of systematic errors and the variance bound guarantees that if we are in an overparameterized setting and \ud835\udc65 \ud835\udc58 \u2192 \ud835\udc65 \u2217 , then the estimator's variance will decay to zero, which should remove any oscillation behavior near the solution. The crucial property of DCGD/PermK is that the aggregation of compressed gradients can be replaced by concatenation, which is algebraic monoid for ( R \u2217 , concat ) , and not the algebraic group. Scaling an encrypted vector by 1 \ud835\udc5b is unfeasible in the master, but this can be delegated to the iterate update Algorithm 3, Line 17. We can use non-secure pseudo-random generators as [52] for sampling indices because they are independent of the input.", "4 THE RESILIENCE FOR ATTACKS": "In chosen-plaintext-attack ( CPA ), an attacker may adaptively ask for the encryption ( \ud835\udc52 1 , . . . ) of arbitrary messages ( \ud835\udc5a 1 , . . . ) to obtain the ability to correctly guess from the set of two encrypted cipher-texts { \ud835\udc38 1 , \ud835\udc38 2 } which encryption belongs to which message in the set { \ud835\udc40 1 , \ud835\udc40 2 } , given that attacker knows the set { \ud835\udc40 1 , \ud835\udc40 2 } . CKKS and AES in EAX mode of operation [8] are secure against CPA . In a chosen-ciphertext-attack ( CCA ), the attacker has unlimited access to a decryption oracle, with the same goal as in CPA . All HE schemas cannot achieve CCA security [28]. It has been proved that AES / EAX is secure against CCA [8]. The algorithms executed in general-purpose microprocessors from an execution point of view are a stream of instructions. However, the energy consumption for different operations is different [34]. It leads to the possibility of side-channel attacks, where the attacker can use information about physical leakages when the number (or type) of operations is some function of secret key or plaintext. The AES block cipher and Operation Modes perform the same stream of operation types (see Appendix F). In CKKS polynomial multiplication, encoding, decoding, and randomization can lead to a trace to plain message [6] using side-channel attacks. During the past time from standardization of AES the works targeted to protect AES from side-channel attacks have been carried [57], [31]. Protecting HE libraries from side-channel attacks is an open question. For example, according to [6] the HE implementation in a reference SEAL library [16] and any derived libraries such as TenSEAL [9] requires protection against side-channel attacks. The work describes how to exploit an asymmetry in ciphertext generation and decrease the security level from 2 128 to 2 4 . 4 for Brakerski/Fan-Vercauteren( BFV ) [27] Ring Learning With Error ( RLWE ) (see Appendix G.1). Therefore, protection from side-channel attacks for AES can be considered to be more well-developed.", "5 EXPERIMENTS": "We made an experimental comparison of several optimization algorithms with different compression and security methods. In Table 6 presented in Appendix B we summarized their qualitative aspects.", "5.1 Synthetic Experiments": "We're going to illustrate the advantages of employing block cipher AES (see Appendix F), as opposed to the CKKS (see Appendix G). The experiments were conducted on a simulated environment using FL_PyTorch [15]. Details on the computing environment are in Appendix E.1. We configured the CKKS to offer security guarantees as AES with 128 bit (16 bytes) key. In this case, the size of public and private keys for CKKS has a lower bound 420 000 bytes (see Appendix G for underlying reason). During usage of CKKS , the public key should be reported somehow to the master once. As we will see while using DCGD/PermK/AES no key at all should be reported to master for master to operate. Therefore the key size can be a problem for CKKS already when the volume of communicated information is far smaller than 0 . 42 \u00b7 10 6 Bytes. In our experiments, we ignore the overhead from key negotiations between parties. Optimization Problem and Experimental Setup. In our synthetically controlled experiments, we consider a specific smooth convex optimization problem which is obtained from Equation 1 via \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) = 1 \ud835\udc5b \ud835\udc56 \u2225 \ud835\udc34 \ud835\udc56 \ud835\udc65 -\ud835\udc4f \ud835\udc56 \u2225 2 , \ud835\udc34 \ud835\udc56 \u2208 R \ud835\udc5b \ud835\udc56 \u00d7 \ud835\udc51 , \ud835\udc4f \ud835\udc56 \u2208 R \ud835\udc51 . Case 1: Distributed GD with/without AES/CKKS. We conducted experiments with \ud835\udc51 = 1000, \ud835\udc5b = 50, and \ud835\udc5b \ud835\udc56 = 12. In our designed experimental setup we filled the Hessian \u2207 2 \ud835\udc53 ( \ud835\udc65 ) such that its nonzero eigenvalues lie uniformly in [ 1 . 0 , 10 . 0 ] , therefore \ud835\udc3f \ud835\udc53 = 10. We used the maximum theoretical constant step size \ud835\udefe = 1 / \ud835\udc3f \ud835\udc53 for GD . Fig. 1 (a) shows the impact of IEEE-754 FP16 , FP32 , and FP64 formats. It shows that GD/AES does not hurt float arithmetic. However, GD/AES 3000 0 500 1000 1500 2000 2500 3000 Rounds 0 500 1000 1500 2000 2500 Rounds GD [fp64] AES-128 GD [fp32] AES-128 GD [fp16] AES-128 GD [fp64] GD [fp32] GD [fp16] 0 500 1000 1500 2000 2500 3000 Rounds 10 22 10 15 10 8 10 1 || f(x)|| 2 (a) 0 500 1000 1500 2000 2500 3000 Rounds 10 5 10 7 10 9 # of bits from master (b) 0 500 1000 1500 2000 2500 3000 Rounds 0 10 20 30 Time (minutes) (c) increases traffic between the master and clients by a factor of \ud835\udc5b as we see from Fig. 1 (b) and increases wall clock time by a factor of 1.7 as in Fig. 1 (c). Next, Fig. 2 compares GD with AES and CKKS . According to Fig. 2 (a), GD/CKKS and FP64 arithmetic, it's possible to obtain |\u2207 \ud835\udc53 ( \ud835\udc65 \ud835\udc58 )| 2 \u2248 10 -9 , when GD/AES attains |\u2207 \ud835\udc53 ( \ud835\udc65 \ud835\udc58 )| 2 \u2248 10 -23 . From Fig. 2 (b), (c) we see that GD/CKKS increases load from master slightly compared to GD/AES , but increases the load from clients by \u00d7 10 4 . As per Fig. 2 (d), CKKS is approximately \u00d7 3 slower compared to AES . Using GD/AES is reasonable only if \ud835\udc51 and \ud835\udc5b are small. Case 2: DCGD with AES/CKKS. In this experiment, we employ DCGD with RandK sparsification compressor to analyze the possibility of gradient sparsification while preserving client's privacy from the master. To compress \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) each client creates a set \ud835\udc46 \ud835\udc56 \u2282 { 1 , 2 , . . . ,\ud835\udc51 } of size \ud835\udc3e chosen uniformly at random, and compute \ud835\udc36 (\u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 )) def = \ud835\udc51 \ud835\udc3e \u02dd \ud835\udc57 \u2208 \ud835\udc46 \ud835\udc56 [\u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 )] \ud835\udc57 \u00b7 \ud835\udc52 \ud835\udc57 , where \ud835\udc52 \ud835\udc57 are unit vectors of standard basis of R \ud835\udc51 . Results are presented in Fig. 3, and Fig. 4. Fig. 3 (a) and Fig. 4 (a) demonstrates that using AES does not lead to numerical issues, whereas CKKS for FP64 does. In GD , the master broadcasts a vector from R \ud835\udc51 in each round. For DCGD/RandK/AES , if clients can reconstruct sparsified indices, then the master has to broadcast \ud835\udc51 = \ud835\udc3e \u00b7 \ud835\udc5b = 1 5 \u00b7 \ud835\udc5b encrypted scalars and 32 \ud835\udc5b bytes from employing nonce , and mac for privacy and integrity. This process reduces the number of bits transmitted from the master to the clients by a factor \u00d7 5, compared to standard GD/AES , as depicted in Fig. 3 (b) and Fig. 1 (b). Fig. 4 (c) shows that CKKS does not leverage the sparsity. For HE schemas, encoding of any two vectors (e.g. sparse and dense) should be indistinguishable due to semantic security requirements. However, from a computational perspective, ignoring sparsity is sometimes highly impractical, and this gap presents an open research question for HE . Fig. 4 (b) highlights that AES adapts to any bit representation of scalars. In contrast, TenSEAL [9] implementation of CKKS does not exhibit this property. Case 3: DCGD with PermK. Both GD/AES and DCGD/AES require a significant amount of data to be sent from the master to the clients. In the case of using correlated compressors as PermK , all clients do not intersect in supports of sparsified 10 3 10 1 10 5 2 f(x)|| 10 9 0 500 1000 1500 2000 2500 3000 Rounds 10 25 10 21 10 17 10 13 || GD [fp64] AES-128 GD [fp32] AES-128 GD [fp16] AES-128 GD [fp64] CKKS GD [fp32] CKKS GD [fp16] CKKS 0 500 1000 1500 2000 2500 3000 Rounds 10 22 10 15 10 8 10 1 || f(x)|| 2 (a) 0 500 1000 1500 2000 2500 3000 Rounds 10 7 10 9 # of bits from master (b) 0 500 1000 1500 2000 2500 3000 Rounds 10 5 10 7 10 9 #bits/n (c) 0 500 1000 1500 2000 2500 3000 Rounds 0 50 100 Time (minutes) (d) gradients by design. The encryption of the global direction can be obtained by concatenating encrypted messages from clients. When using PermK , clients do not need to perform any aggregation on their side, and decryption of the whole global direction obtained from master can be done in O( \ud835\udc51 ) independent on \ud835\udc5b . We aim to find an approximate \ud835\udefe for DCGD/PermK . We generated 5 problems with matrices \ud835\udc34 \ud835\udc56 \u223c \ud835\udc48 [ 0 , 1 ) \ud835\udc5b \ud835\udc56 \u00d7 \ud835\udc51 , projected \ud835\udc34 = [ \ud835\udc34 1 , . . . , \ud835\udc34 \ud835\udc5b ] \u22a4 to have \ud835\udc3f \ud835\udc53 = 10, and computed \ud835\udc4f \ud835\udc56 def = \ud835\udc34 \ud835\udc56 \ud835\udc65 fixed . We tested various step sizes demonstrated in Fig. 5. We found that using a step size \ud835\udefe \u2265 1 2 \ud835\udc3f \ud835\udc53 = 0 . 05 led to divergence, as shown in Fig. 5 (a), (b). From Fig. 5 (a), we see that the method exhibits linear convergence without oscillation near the solution, similar to DCGD with RandK (Here \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 \u2217 ) = 0 , \u2200 \ud835\udc56 \u2208 [ \ud835\udc5b ] , because \ud835\udc51 > \ud835\udc5b \ud835\udc56 \u00b7 \ud835\udc5b ). From Fig. 5 (c), we see that the variance of the optimization path using fixed step size \ud835\udefe = 0 . 007 and fixed \ud835\udc51, \ud835\udc5b \ud835\udc56 , \ud835\udc5b, \ud835\udc3f \ud835\udc53 is negligible. Comparison of DCGD/PermK/AES and GD/CKKS. Fig. 6 compares GD , GD/CKKS , GD/PermK with GD/AES , GD/PermK/AES . We see that the CKKS schema does not leverage the sparsity of vectors, making sparsification-based compression ineffective in reducing communication during privacy-preserving training. Therefore, there is no benefit from using RandK for DCGD/CKKS , and it's better to use vanilla GD . For GD , we used theoretical step size, which in practice is extremely tight. If communication isn't free, Fig. 6 (a) and Fig. 6 (d) suggest that CKKS is impractical in settings where client-master communication is a bottleneck. Fig. 6 (b) shows that GD/AES does not increase client-to-master traffic but does significantly increase master-to-client traffic, as seen in Fig. 6 (d). If communication is free, Fig. 6 (c) shows that the best convergence in terms of rounds is attained for GD or GD/AES with preserving security. If communication is free, GD/CKKS remains suboptimal due to the approximate nature of floating-point operations in CKKS . Suppose the key metric is convergence in \u2225\u2207 \ud835\udc53 ( \ud835\udc65 \ud835\udc58 )\u2225 relative to the number of bits from client to master. Fig. 6 (b) shows that GD/CKKS ndK 0 10 3 10 1 10 5 2 10 9 DCGD [fp16] AES-128, RandK 0 500 1000 10 25 10 21 10 17 10 13 || DCGD [fp64] AES-128, RandK DCGD [fp32] AES-128, RandK DCGD [fp16] AES-128, RandK 1000 1500 2000 2500 3000 Rounds DCGD [fp64], RandK DCGD [fp32], RandK DCGD [fp16], RandK 0 500 1000 1500 2000 2500 3000 Rounds 10 22 10 15 10 8 10 1 || f(x)|| 2 (a) 0 500 1000 1500 2000 2500 3000 Rounds 10 5 10 7 10 9 # of bits from master (b) 0 500 1000 1500 2000 2500 3000 Rounds 10 4 10 5 10 6 10 7 #bits/n (c) DCGD [fp64], RandK 1500 500 Rounds uses approximately 0 . 6 \u00b7 10 7 bits per client after the first round of optimization. DCGD/PermK/AES with this transfers can attain \u2225\u2207 \ud835\udc53 ( \ud835\udc65 )\u2225 2 \u2248 10 -20 .", "5.2 Image Classification Application": "We evaluated the applicability of using DCGD/PermK/AES on Deep Neural Networks training. We used the ResNet-18 architecture [33] and trained it on the CIFAR-10 dataset [46], which consists of 60 000 images across 10 classes with a resolution of 32 \u00d7 32 pixels. We used ResNet-18 implementation from TorchVision library, part of the PyTorch [55]. The model size \ud835\udc51 = 11 , 181 , 642. For Optimization Problem 1, we used a standard cross-entropy loss for L \ud835\udc56 \ud835\udc57 terms in Equation 2. Next, we distributed the dataset uniformly across \ud835\udc5b = 10 clients, with each client participating in every round. Global learning rate 0 . 1, local learning rate 0 . 1, local weight decay 5 \u00b7 10 -4 , number of rounds 5 000. During each round, clients evaluate \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) using a fixed 10% subset of each client's local data points. In FL , there are cases where clients cannot store a large number of data points (e.g., high-resolution images, recorded voice in IoT devices) due to storage limitations. Our experiment simulates this scenario. We compared our implementation with FedAvg [53], where no compression or encryption is performed. We tracked during training communicated message size, accuracy, and convergence. Our reported metrics in Fig. 7 are computation framework and communication topology independent. In these experiments, we did not carry out an analysis of the spent time for communication, compression, and AES encryption/decryption, because the actual numbers are highly implementation dependent quantities. In DCGD/PermK/AES , the message size from the client to the master contains l \ud835\udc51 10 m = 1 , 118 , 165 parameters with 4 bytes each one. The overhead from AES-128/EAX Operation Mode is 32 bytes per single message, which is negligible. Fig. 7 shows that the message size was reduced from 42 . 65 (for FedAvg ) MBytes to 4 . 26 MBytes (for DCGD/PermK ). We f(x)|| DCGD [f 2000 28, RandK 2 f(x)|| || 10 3 10 1 10 10 10 10 10 10 5 9 DCGD [fp16] AES-128, RandK 13 17 21 25 0 10 3 10 1 10 5 2 f(x)|| 9 0 500 10 25 10 21 10 17 10 13 || DCGD [fp64] AES-128, RandK DCGD [fp32] AES-128, RandK DCGD [fp16] AES-128, RandK 500 1000 1500 2000 2500 3000 Rounds DCGD [fp64] CKKS, RandK DCGD [fp32] CKKS, RandK DCGD [fp16] CKKS, RandK 0 500 1000 1500 2000 2500 3000 Rounds 10 22 10 15 10 8 10 1 || f(x)|| 2 (a) 0 500 1000 1500 2000 2500 3000 Rounds 10 7 10 9 # of bits from master (b) 0 500 1000 1500 2000 2500 3000 Rounds 10 5 10 7 10 9 #bits/n (c) 0 500 1000 1500 2000 2500 3000 Rounds 0 50 100 Time (minutes) (d) DCGD [fp64] CKKS, RandK 1000 cannot launch CKKS with security guarantees as AES-128 in our environment due to memory overhead. In Appendix 5.1, we demonstrated that problems start appearing in CKKS with AES-128 security guarantees already for \ud835\udc51 = 10 6 . For a discussion about extra flexibility in Training DL models see Appendix I.", "6 DEPLOYMENT FLEXIBILITY": "The Physical Network Topologies describes the arrangement of the computation and routing devices. In a Mesh topology, every pair of nodes is connected with a dedicated link. It has high bandwidth and fault tolerance but requires a lot of cables. In this setting, the DCGD/PermK is the natural choice. The Algorithm 3 can be observed as a series of broadcast operations to reconstitute the optimization step. The benefits to other topologies are discussed in Appendix J. In Appendix H.2, we present the refined scheduled communication and computation plan that utilizes the possibility of computation communication overlap and handling computation heterogeneity during training Linear Regression . The modeled situation that we studied contained 5 clients (one of them is a straggler) which all are connected to the master with a shared channel. We demonstrated that the actual execution plan can be refined. The possible gained speedup for GD is \u00d7 1 . 31, for DCGD/PermK is \u00d7 3 . 33. The DCGD/PermK has flexibility to start several compute operations during waiting the straggler, which are limited for GD .", "7 CONCLUSIONS": "We proposed a novel secure FL framework that uses symmetric-key encryption with permutation compressors to protect the gradients during communication and simultaneously compress them. We conducted experiments on real and synthetic data. Additional studies about the effect of problem dimension, overlapping communication and computation, 10 05 30000 25000 =0.0003 20000 15000 10000 Rounds 5000 0 0 5000 0 5000 10000 15000 20000 25000 Rounds =1e 05 =3e 05 =5e 05 =7e 05 =0.0003 =0.001 0 5000 10000 15000 20000 25000 30000 Rounds =0.001 =0.003 =0.005 =0.007 =0.01 =0.05 0 5000 10000 15000 20000 25000 30000 Rounds 10 19 10 11 10 3 10 5 10 13 || f(x)|| 2 (a) 0 5000 10000 15000 20000 25000 Rounds 10 0 10 1 10 2 |x| at round start (b) 0 1000 2000 3000 4000 5000 Rounds 10 21 10 15 10 9 10 3 10 3 || f(x)|| 2 =0.007, launch 1 =0.007, launch 2 =0.007, launch 3 =0.007, launch 4 =0.007, launch 5 (c) and deployment options in various network topologies are presented in Appendices H, J. Our work opens a new possibility for applying Classical Cryptography to FL and challenges some existing claims about its limitations. Possible future research and current limitations are described in Appendix M. 30000 =0.003 10000 =0.005 15000 Rounds 10 4 10 4 10 3 10 3 10 2 10 2 Rounds 10 1 Rounds f(x) 0 5000 10000 15000 20000 25000 30000 Rounds 10 0 DCGD [fp64], AES-128, PermK,  =0.005 DCGD [fp64], AES-128, PermK,  =0.007 GD [fp64] AES-128 0 5000 10000 15000 20000 25000 30000 Rounds 10 0 10 1 DCGD [fp64] PermK,  =0.005 DCGD [fp64] PermK,  =0.007 GD [fp64] GD [fp64] HE/CKKS 0.0 0.5 1.0 1.5 2.0 2.5 3.0 #bits/n 1e9 10 23 10 17 10 11 10 5 10 1 || f(x)|| 2 (a) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 #bits/n 1e7 10 23 10 17 10 11 10 5 10 1 || f(x)|| 2 (b) 0 500 1000 1500 2000 2500 3000 Rounds 10 11 10 7 10 3 10 1 10 5 || f(x)|| 2 (c) 0 500 1000 1500 2000 2500 3000 Rounds 10 6 10 8 10 10 # of bits from master (d) 0 1000 2000 3000 4000 5000 Rounds DCGD [fp32], AES-128, PermK FedAvg [fp32] 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 #bits/n 1e12 10 3 10 2 10 1 10 0 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 #bits/n 1e12 10 20 30 40 50 Top1 acc (validation) 0 1000 2000 3000 4000 5000 Rounds 2\u00d710 1 3\u00d710 1 4\u00d710 1 Top1 acc (validation)", "REFERENCES": "[1] 2014. PyCryptodome. https://pypi.org/project/pycryptodome/. Accessed: 2023-05-10. [2] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. { TensorFlow } : A System for { Large-Scale } Machine Learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16) . 265-283. [3] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. 2017. QSGD: Communication-efficient SGD via gradient quantization and encoding. Advances in neural information processing systems 30 (2017). [11] Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan Rogers. 2018. Protection against reconstruction and its applications in private federated learning. arXiv preprint arXiv:1812.00984 (2018). [15] Konstantin Burlachenko, Samuel Horv\u00e1th, and Peter Richt\u00e1rik. 2021. Fl_pytorch: optimization research simulator for federated learning. In Proceedings of the 2nd ACM International Workshop on Distributed Machine Learning . 1-7. [30] Eduard Gorbunov, Konstantin P Burlachenko, Zhize Li, and Peter Richt\u00e1rik. 2021. MARINA: Faster non-convex distributed learning with compression. In International Conference on Machine Learning . PMLR, 3788-3798. [31] Hannes Gross, Stefan Mangard, and Thomas Korak. 2017. An Efficient Side-Channel Protected AES Implementation with Arbitrary Protection Order. In Topics in Cryptology - CT-RSA 2017 , Helena Handschuh (Ed.). Springer International Publishing, Cham, 95-112. [35] Samuel Horv\u00f3th, Chen-Yu Ho, Ludovit Horvath, Atal Narayan Sahu, Marco Canini, and Peter Richt\u00e1rik. 2022. Natural compression for distributed deep learning. In Mathematical and Scientific Machine Learning . PMLR, 129-141. [38] Zhihao Jia, Matei Zaharia, and Alex Aiken. 2019. Beyond Data and Model Parallelism for Deep Neural Networks. Proceedings of Machine Learning and Systems 1 (2019), 1-13. [39] Zhifeng Jiang, Wei Wang, and Yang Liu. 2021. Flashe: Additively symmetric homomorphic encryption for cross-silo federated learning. arXiv preprint arXiv:2109.00675 (2021). [43] Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. 2018. Distributed learning with compressed gradients. arXiv preprint arXiv:1806.06573 (2018). [48] Chen Li, Yifan Sun, Lingling Jin, Lingjie Xu, Zheng Cao, Pengfei Fan, David Kaeli, Sheng Ma, Yang Guo, and Jun Yang. 2019. Priority-based PCIe scheduling for multi-tenant multi-GPU systems. IEEE Computer Architecture Letters 18, 2 (2019), 157-160. [50] Changchang Liu, Supriyo Chakraborty, and Dinesh Verma. 2019. Secure model fusion for distributed learning using partial homomorphic encryption. Policy-Based Autonomic Data Governance (2019), 154-179. [52] Makoto Matsumoto and Takuji Nishimura. 1998. Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator. ACM Transactions on Modeling and Computer Simulation (TOMACS) 8, 1 (1998), 3-30. Federated Learning is Better with Non-Homomorphic Encryption [60] Ronald L Rivest, Len Adleman, Michael L Dertouzos, et al. 1978. On data banks and privacy homomorphisms. Foundations of secure computation 4, 11 (1978), 169-180. [61] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by back-propagating errors. nature 323, 6088 (1986), 533-536.", "Appendix": "", "Contents": "", "A GLOSSARY": "Our work is multidiscipline. To make our paper more readable for researchers with different backgrounds, we have constructed this Glossary. Federated Learning is Better with Non-Homomorphic Encryption", "B COMPARED OPTIMIZATION ALGORITHMS": "In our work, we made a comparison of several optimization algorithms with different compression and security methods. In Table 6 below we summarize their qualitative and qualitative features.", "C COMPUTING AND SOFTWARE ENVIRONMENT": "We conducted numerical experiments using the Python software suite FL_PyTorch [15], running on Python 3.9. The target machine is a server-grade system operating on Ubuntu 18.04 and Linux Kernel v5.4.0-148. It is equipped with a 48-core Intel(R) Xeon(R) Gold 6246 CPU (2 sockets with 24 cores per socket) running at 3.3 GHz. The machine is equipped with 256 GBytes of DDR4 DRAM system memory operating at 2.9GHz. The installed CPU does not support both the AVX512FP16 Instruction Set Architecture 2 and it does not support FP16 arithmetic. The machine also has an NVIDIA GeForce RTX 3090 GPU built on Ampere microarchitecture with 24 GBytes of DRAM@9.7GMHz GPU memory. This GPU supports the CUDA Compute Capability 8.6. and as a consequence, supports the FP16 arithmetic, which is supported for devices with Compute Capability 5.3. and higher. 3 . Numerical experiments with FP16 arithmetic were carried out in this GPU. We have patched FL_PyTorch to include necessary functionality from the TenSeal [9] library version 0.3.14 and PyCryptodome [1] version 3.17. The experiment's scheduler for computation and communication from Appendix H.2 generates text descriptions in dot graphviz format. Finally, we used Graphviz 8.0.5 [26] to generate a directed graph from this description. Federated Learning is Better with Non-Homomorphic Encryption", "D OVERVIEW OF EXISTING PRIVACY MECHANISMS IN CONTEXT OF FL": "Trusted Execution Environments ( TEE ). The TEE brings the idea of handling self-isolation of processes more seriously than it has been done in the Operation Systems before. An example of the particular implementation of this idea is ARM TrustZone [56] and Intel SGX [20], where both technologies have been implemented at the hardware level. Such technologies allow the protection of memory from reading and writing operations not only from another process in the Operation System (OS) but also from the kernel of the OS by itself. The goal of TEE : TEE protects the execution environment from illegal intervention, which will defect training . Differential Privacy (DP). The goal of DP is to ensure statistical analysis does not hurt the privacy aspect. The intuitive definition can be as follows: \"An algorithm A is differentially private if an observer seeing its output cannot tell if a particular individual's data was used in the computation.\" DP criteria assess the private property of the algorithm, but it does not dictate how this functionality should be implemented. In general, as more noise is appending the data or derived quantities from the data that are released publicly, the algorithm starts to be more DP , but from another side, the statistical properties of the data for the purpose of solving original task start to be loosed. Definition 1 (Approximate Differential Privacy) . An algorithm \ud835\udc40 : X \ud835\udc5b \u2192 Y satisfies approximate ( \ud835\udf00, \ud835\udeff ) - DP if \u2200 \ud835\udc4b,\ud835\udc4b \u2032 \u2208 X \ud835\udc5b such that datasets are different in one point (neighboring datasets denoted as \ud835\udc4b \u223c \ud835\udc4b \u2032 ) and \u2200 \ud835\udc47 \u2286 Y the following holds: The definition 1 is a relaxation of pure-DP, first proposed by Dwork et al. in [24] in 2006. The pure-DP was given by in [25], requiring \ud835\udeff = 0. The Approximate DP possesses weaker privacy guarantees but allows the addition of less noise. DP-ERM aims to output \u02c6 \ud835\udf03 , which is ( \ud835\udf00, \ud835\udeff ) - DP with respect to the training dataset \ud835\udc37 . To do that, there are three types of approaches: (1) Output perturbation; (2) Objective perturbation; (3) Gradient perturbation. Algorithms that solves that DP-ERM problem are quantified by expected excess empirical risk E [L \ud835\udc52\ud835\udc5f\ud835\udc5a ( \u02c6 \ud835\udf03, \ud835\udc37 ) - L \ud835\udc52\ud835\udc5f\ud835\udc5a ( \ud835\udf03 \u2217 \ud835\udc52\ud835\udc5f\ud835\udc5a , \ud835\udc37 )] , and for DP-RM problem by expected population risk E [L \ud835\udc5f\ud835\udc5a ( \u02c6 \ud835\udf03, \ud835\udc37 ) - L \ud835\udc5f\ud835\udc5a ( \ud835\udf03 \u2217 \ud835\udc5f\ud835\udc5a , \ud835\udc37 )] . These quantities are sometimes named as utility . The goal of DP : DP protects output of algorithms so that users' data are not leaking from Algorithm execution . One important classification in DP algorithms targeted to distributed environments is their separation into two classes: Centralized and Local Differential Private settings. In the centralized model, a client trusts curator (or master) and DP protection mechanism are applied in master. In the local model, each individual applies a differential private mechanism to their own data before sending it to an untrusted curator (or master). Centralized models have lower privacy loss, since the noise is added only once at the end of the process such as aggregation at master. However, the centralized model also requires more trust in the aggregator. Aggregation with Multi-Party Computation (MPC). The MPC is a sub-field of Cryptography concerned with the problem of having a set of parties that compute an agreed function of their private inputs. The goal of secure multi-party computation ( MPC ) is to enable independent data owners who do not trust each other or any common third party to jointly compute a function that depends on all of their private inputs. MPC protocols are typically implemented with (a) Secret sharing - in this case, it requires a lot of total communication rounds to compute the average across \ud835\udc5b clients; (b) Garbled circuits - in this case there both communication and computation overhead is added to the training [68]. The goal of MPC : MPC allows for protecting inputs for the algorithm at the cost of communication. Homomorphic Encryption ( HE ). Homomorphic Encryption ( HE ) enables numerical computation on encrypted data, e.g. aggregating vectors from R \ud835\udc51 . However, the result can only be decrypted with the private key. The concept dates back to 1978 when R. L. Rivest et al. [60] proposed this but without a complete solution. HE allows any device party to compute functions on encrypted data using only a public key and encrypt the result. HE aims to hide the plaintext input and output from the executor, who only sees the encrypted versions of it. This is possible with HE . On the other hand, obfuscation is the process of encrypting the program (function \ud835\udc53 ), not the input or output from the caller. Obfuscation is impossible under weak technical conditions [7]. Fully Homomorphic Encryption ( FHE ) allows any computable algorithm to be executed on encrypted data without any restrictions on the binary operations. The first FHE scheme was proposed by Craig Gentry [29], based on lattices and a novel bootstrapping technique. Let \ud835\udc50 \ud835\udc56 be the encryption of message \ud835\udc5a \ud835\udc56 , and \ud835\udc50 be the result of evaluating a function \u02c6 \ud835\udc53 ( \ud835\udc50 1 , . . . \ud835\udc50 \ud835\udc5b ) , which should decrypt to \ud835\udc5a = \ud835\udc53 ( \ud835\udc5a 1 , . . . \ud835\udc5a \ud835\udc5b ) . In his Ph.D. thesis, C. Gentry listed some requirements that any FHE scheme should satisfy, which we summarize below: (1) Correctness: Decryption should always recover the correct evaluation of the function, i.e., \ud835\udc43 ( \ud835\udc37\ud835\udc52\ud835\udc50\ud835\udc5f\ud835\udc66\ud835\udc5d\ud835\udc61 ( \ud835\udc50 ) = \ud835\udc53 ( \ud835\udc5a 1 , \ud835\udc5a 2 , . . . ) = 1 ) . (2) Semantic security: The encryption of any two messages should be computationally indistinguishable. (3) Efficiency: Decryption should not be more expensive than evaluating the function itself. (4) Compactness: Ciphertexts should have a polynomial size in the security parameter, independent of the size of the function evaluated. (5) Security: The best-known attack should have exponential complexity in the security parameter, i.e., \u03a9 ( 2 \ud835\udc58 ) ( \ud835\udc58 is a security parameter). (6) Feasibility: Key generation, encryption, and decryption should have polynomial complexity in the security parameter. By C.Gentry, only algorithms that allow executing any computable algorithm on encrypted data that satisfies properties (1) - (6) can be called FHE . The FHE with this requirement can firstly be hard to construct, and secondary in practice, they may not be computationally efficient. To mitigate these issues, two main strategies are the following: \u00b7 Somewhat Homomorphic Encryption ( SWHE ). One way to make FHE more efficient in practice is to restrict the class of functions that can be evaluated. This leads to Somewhat Homomorphic Encryption, which can handle functions from restricted classes (e.g. they are low-degree polynomials). \u00b7 Leveled FHE ( LFHE ). Another way is to limit the depth of the binary circuit that represents the function. This leads to a notion of Leveled FHE, which can handle arbitrary functions represented by boolean circuits but with a fixed bound on the circuit depth. Challenges of applying HE for training Machine Learning models and scientific computation: (1) The HE adds noise to the plaintext to ensure security. The challenge is to manage it with error-correction techniques, as it typically grows after each arithmetic operation. (2) The HE cannot perform random access on encrypted data without revealing information. (3) The HE cannot exploit the advantages of Random Access Machines, which can compute some algorithms faster than binary circuits, e.g., Binary Search. (4) The HE does not support multiple keys natively. Originally, it was designed as a single-private-key system. (5) The HE does not obfuscate the function itself, only the input and output. As mentioned, obfuscation is impossible under weak conditions [7]. (6) The HE works on binary circuits or functional schemas. This class of representation is Turing Complete, assuming we can create circuits of different levels for different inputs. The pure FHE operations are computationally intensive and currently for practical purposes the SWHE and LFHE schemas should be considered instead. (7) The HE methods require large ciphertext sizes. (8) Choosing the right HE scheme for a given machine learning task is not trivial. The goal of HE : HE allows meaningful manipulation under encrypted data without revealing it.", "E DISCUSSIONS": "", "E.1 The Imperative of Safeguarding Against Eavesdropping": "Assume that during training with first-order optimization method, each client \ud835\udc56 discloses the following information at each iteration \ud835\udc58 \u2208 1 , . . . , \ud835\udc3e : If \ud835\udc53 \ud835\udc56 is twice differentiable and has bounded Hessians near point \ud835\udc65 \ud835\udc58 , the knowledge of partial derivative approximates the following equality: \ud835\udf15\ud835\udc53 \ud835\udc56 ( \ud835\udc65 \ud835\udc58 ) \ud835\udf15\ud835\udc65 \ud835\udc57 \u00b7 \ud835\udc51\ud835\udc65 \ud835\udc57 \u2248 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 \ud835\udc58 + \ud835\udc52 \ud835\udc57 \u00b7 \ud835\udc51\ud835\udc65 \ud835\udc57 ; \ud835\udc37 \ud835\udc56 ) -\ud835\udc53 ( \ud835\udc65 \ud835\udc57 ; \ud835\udc37 \ud835\udc56 ) . In the last approximate equality, \ud835\udc52 \ud835\udc57 is a unit norm vector of the standard basis of R \ud835\udc51 . Let's assume that: (1) The \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) is linear with respect to \ud835\udc37 \ud835\udc56 in original form, or there exists a bijective change of variable \ud835\udc37 \ud835\udc56 \u2192 \ud835\udc37 \ud835\udc56 \u2032 such that \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) is linear in \ud835\udc37 \u2032 \ud835\udc56 . (2) Iterates \ud835\udc65 \ud835\udc58 are uniformly distributed in R \ud835\udc51 . Under these assumptions, a training process might reveal sensitive information. Indeed, if an adversary obtains this information, then information about partial derivative provides noisy response linear function in \ud835\udc37 \ud835\udc56 . This general setting has been studied by [23]. Authors demonstrated that there exist linear attacks that could, with high probability, expose \ud835\udc37 \ud835\udc56 . Specifically, let's denote the length of \ud835\udc37 \ud835\udc56 is equal to \ud835\udc41 bits. Let's assume adversarial obtains \ud835\udc3e noisy answers (for a single partial derivative) with an additive error of \ud835\udc38 = \ud835\udc5c ( \u221a \ud835\udc41 ) to each answer. Having \ud835\udc3e \u2265 \ud835\udf03 ( \ud835\udc41 2 / \ud835\udc38 2 ) , such adversarial queries can reveal information about \ud835\udc37 \ud835\udc56 with high probability. The hidden constant is around 256 but can be improved. A single full gradient \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ; \ud835\udc37 \ud835\udc56 ) provides \ud835\udc51 equations instead of 1 for each response. This demonstrates that in specific modes the pretty big amount of partial derivative already reveals enough information to reconstruct the client's dataset. Such attacks are not only in theoretical interest but can be practically mounted [19], [42].", "E.2 Privacy and Security": "As we have described in Appendix D, different mechanisms can be used for FL to provide privacy for different aspects training process. Each mechanism has a specific target of protection (such as input, output, execution, or communication channel) and a specific adversary model. The precise meaning of protection is defined by rigorous formalization and the details are crucial. For example, DP protects the output of an algorithm so that it does not reveal sensitive information about the input data, while MPC protects the input data from being exposed to other parties during computation. Recent research papers on FL deployments have mostly used Local DP [11], or a combination of MPC and centralized DP . In contrast, the proposed design in our work can be understood as a mechanism for providing security for the training process rather than privacy. The response obtained from the server by the client is computed securely and accurately with protection against attacks on the server or the communication channels to the server. The notion of privacy covers the guarantee that the target of protection can participate in an algorithm without being observed by unauthorized parties. Security, on the other hand, can be considered as a more holistic concept that specifies: the algorithm or protocol, how personal information and derived quantities are protected, the type and strength of attacks that are resisted, and the requirement of having a secret key to access the data. Sometimes, a complex security protocol may not be feasible or desirable for certain situations, and a privacy mechanism that is less comprehensive but more flexible may be a better choice. Federated Learning is Better with Non-Homomorphic Encryption", "F USAGE OF AES CIPHER DURING DISTRIBUTED TRAINING": "This section overviews the current state-of-the-art block cipher AES and explains how it can be used for symmetric key encryption in scenarios where multiple client messages must be encrypted and decrypted securely.", "F.1 AES Block Cipher": "A block cipher is a fundamental cryptographic primitive that transforms a fixed-length block of bits into another block of the same length using a secret key. The AES is today's most widely used secure block cipher. Some CPUs have hardware support for it. Examples of CPUs with x86 instruction set architecture that support AES in the hardware level are Intel Westmere, AMD Bulldozer, and ARM AARCH64 instruction set architecture example is ARM Cortex-A53. AES block cipher supports three key sizes: 128, 192, or 256. The key size determines the level of security and the computational cost of the encryption and decryption operations. The key space \ud835\udc3e for AES-128 has size of | \ud835\udc3e | = 2 128 . The input message space \ud835\udc40 and the output cipher text space \ud835\udc36 of the block cipher AES for all types of keys have the same size equal to 128 bits. Therefore cardinality of \ud835\udc40 and \ud835\udc36 is equal to | \ud835\udc40 | = | \ud835\udc36 | = 2 128 . For each key \ud835\udc58 \u2208 \ud835\udc3e , the AES block cipher maps \ud835\udc40 \u2192 \ud835\udc36 with a bijective function, with \ud835\udc40 = \ud835\udc36 , and | \ud835\udc40 | = | \ud835\udc36 | \u2264 \u221e . Essentially, the key \ud835\udc58 is a selector of bijective mapping or permutation. Each key realizes the permutation of the input message. If we assume that each of two distinct keys \ud835\udc58 1 , \ud835\udc58 2 \u2208 \ud835\udc3e implements different bijective mappings \ud835\udc40 \u2192 \ud835\udc36 , then the number of permutations that AES can realize is | \ud835\udc3e | = 2 128 . Even though it's a big number, this is much smaller than 2 128 !, all possible permutations if input and output are 128 bits in length. In the Cryptography community, the block cipher AES is sometimes observed as a primitive that implements a Pseudo Random Permutation ( PRP ). In other words, this means that for a fixed key, it defines a permutation. The discrepancy between the number of all possible permutations and the number of PRPs that AES can realize is elegantly resolved in the Cryptography community. It is resolved by introducing the notion of a Secure PRP . The algorithm which implements PRP implements a Secure PRP if an adversary from observing the realization of permutation \ud835\udc53 cannot distinguish by using an arbitrarily tractable algorithm between two events: (1) The permutation function \ud835\udc53 : \ud835\udc40 \u2192 \ud835\udc40 is chosen uniformly at random from all possible \ud835\udc40 ! permutations. (2) The permutation function \ud835\udc53 : \ud835\udc40 \u2192 \ud835\udc40 is chosen as one of the permutations that block cipher BlockCipher (\u00b7 ,\ud835\udc58 ) : \ud835\udc40 \u2192 \ud835\udc40 with \ud835\udc58 \u223c \ud835\udc62.\ud835\udc4e.\ud835\udc5f \ud835\udc3e can realize. By the current status in Cryptography, the AES is believed to be a secure PRP . Having two input and output pairs, adversarial may wish to derive the secret key. The brute force search on AES-128 is computationally infeasible because 2 128 is too large to enumerate. The best-known attack on the full version of AES-128 that can recover the complete key has a complexity of 2 126 [13]. The AES-128 is secure against brute force search and also against linear and quantum attacks [37].", "F.2 Internals of AES Block Cipher": "The AES block cipher has 10 rounds for AES-128 and 14 rounds for AES-256 . The key is expanded into 11 or 15 subkeys of 128 bits in length each. Subkeys are used in each round. Each round consists of four invertible steps: key addition (in the sense of exclusive boolean or , which we will denote as XOR), byte substitution, row shift, and column mix. These steps transform a 4 \u00d7 4 matrix of bytes representing each round's input. Byte substitution adds non-linearity, row shift rotates each row cyclically, and column mix applies a linear transformation to each column. The last subkey is used for a final key addition (XOR) to mask the output. The AES decryption reverses the encryption steps. The AES has different implementations for different devices and code size requirements. All occurred transformations are stateless; consequently, the AES block cipher is stateless.", "F.3 Apply AES Block Cipher for more than one input block": "The AES is a secure block cipher, but it is a bad idea to use the same key \ud835\udc58 to encrypt multiple blocks deterministically. This breaks the notion of Semantic Security, which essentially means that adversarial can deduce some information from analyzing sequences of ciphertexts from the block cipher (e.g. adversarial can, from observing \ud835\udc50 1 = \ud835\udc50 2 conclude that \ud835\udc5a 1 = \ud835\udc5a 2 , even adversarial does not know exactly values \ud835\udc5a 1 ,\ud835\udc5a 2 ). To preserve Semantic Security, the encryption should have protection from chosen-plaintext-attack, which informally denotes this kind of attack. Formally, in chosen-plaintext-attack ( CPA ) an attacker may adaptively ask for the encryption ( \ud835\udc52 1 , \ud835\udc52 2 , . . . ) of arbitrary messages ( \ud835\udc5a 1 ,\ud835\udc5a 2 , . . . ) of his choice. The attacker's goal is to obtain the ability to correctly guess from two obtained encryption { \ud835\udc52 \ud835\udc4e ,\ud835\udc52 \ud835\udc4f } from experiment \ud835\udc34 and experiment \ud835\udc35 which encryption belongs to which message from the set { \ud835\udc5a \ud835\udc4e , \ud835\udc5a \ud835\udc4f } . The attacker does not know encryption of { \ud835\udc5a \ud835\udc4e , \ud835\udc5a \ud835\udc4f } in advance, and the attacker does not know which plain message has been used in which experiment. In the context of this attack, the advantage of adversarial is defined as: \ud835\udc34\ud835\udc37\ud835\udc49 \ud835\udc50\ud835\udc5d\ud835\udc4e = | \ud835\udc43 ({ ExperimentA uses \ud835\udc5a \ud835\udc4e }) -\ud835\udc43 ({ ExperimentB uses \ud835\udc5a \ud835\udc4f })| . Fundamentally, there are two ways to provide security against CPA attacks: (1) Increase ciphertext space and allow encryption to work randomly. This is the underlying reason why CKKS is CPA secure. The downside of this is that ciphertext space is increasing. (2) Augment key space \ud835\udc3e with extra counter, named as a nonce from a nonce space \ud835\udc41 . The clients who perform encryption guarantee that the pair ( \ud835\udc58\ud835\udc52\ud835\udc66,\ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52 ) is unique during the life of the \ud835\udc58 . The \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52 is open to everybody. Next, there are two different ways to select nonce in nonce-based protection against CPA attacks: (1) Deterministic counter. In this mode, a single deterministic integer counter is used. Sometimes there is no need to send nonce itself with ciphertext during communication if the receiver of ciphertext can recover counter from other information. (2) Randomized counter. If encryption happens on several devices, the coordination of nonce complicates the process. One way around this is to select \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52 \u223c \ud835\udc62.\ud835\udc4e.\ud835\udc5f. \ud835\udc41 , where \ud835\udc41 is sufficiently large, e.g. | \ud835\udc41 | > 2 128 . Such nonce space size will guarantee that we expect to obtain during the sampling at least one collision after sampling \u221a\ufe01 2 | \ud835\udc41 | + 1 = 2 64 due to the famous Birthday Paradox. The two popular ways to use nonce-based encryption for AES , also known as Operation Modes , are the following: (1) Nonce-based Cipher Block Chaining Mode ( CBC ). In this schema, the input message is split into buckets. These inputs are substituted into AES block cipher, but the inputs themselves are masked before encryption. The first plaintext bucket is masked (via XOR) with nonce . Other plaintext buckets are masked (via XOR) with the output from AES block cipher operated in the previous block. The output of this mechanism is the public nonce and sequence of encrypted input buckets. It can be proved that AES/CPA secure with advantage \ud835\udc34\ud835\udc37\ud835\udc49 \ud835\udc50\ud835\udc5d\ud835\udc4e \u2264 1 / 2 32 if use CBC for 2 48 AES (128 bits) blocks. If nonce is generated with non-secure PRG, then it should be firstly encrypted by itself with secure AES Block to generate secure nonce . (2) Randomized Counter Mode (CTR) . The input message is split into buckets. But in this mode, the mask for each bucket \ud835\udc56 is generated with AES block cipher AES ( \ud835\udc58, \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52 + \ud835\udc56 ) . The input bucket is masked with this value AES ( \ud835\udc58, \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52 + \ud835\udc56 ) via XOR operation. It can be shown to guarantee CPA security with the advantage 1 / 2 32 for it the number of transferred messages with AES should be at most 2 64 AES Blocks. In addition, this operation mode is more flexible in contrast to CBC because this mode does not require any padding and can be implemented by discarding not needed bits in the last bucket. The CKKS and AES in EAX mode of operation [8] are secure against CPA attacks.", "F.4 Apply AES with Integrity Guarantees": "The CPA security and operations mode solve security against eavesdropping, but they don't provide integrity of the delivered messages. Message Authentication Code( MAC ) is Cryptography protected analogous to (non-secure) Cyclic Redundancy Check ( CRC ), which is used in Networking communication to protect from random (not adversarial) noise in communication channels. The goal of MAC is that the value of MAC coupled with a ciphertext guarantees that the message has not been modified during the transfer by the adversary. The signing algorithm for generating MAC tag takes a plain message \ud835\udc5a and secret key \ud835\udc58 as input. The signature is added to the transferred message with the public nonce. Adversarial without a secret key \ud835\udc58 cannot produce valid ( \ud835\udc40\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52, \ud835\udc40\ud835\udc4e\ud835\udc50 ) pairs in such a way that it will pass the verification algorithm . The verification algorithm takes as input the secret key \ud835\udc58 , decrypted message \ud835\udc5a , and MAC value \ud835\udc61\ud835\udc4e\ud835\udc54 . In practice, nonce and MAC are typically 16 bytes long. If the optimization algorithm already requires sending far more than 2 elements of R encoded in IEEE-754 FP64 format then this overhead is negligible during the training. Two popular constructions to provide integrity in the context of AES are the following: (1) Encrypted CBC-MAC . The input message is split into 16 bytes buckets. After this, the first bucket is plugged into AES as input. Output from this block is masked with XOR with the next input 16 byte bucket. After the XOR operation is finished, the output of XOR is plugged into the next AES block as input, and the process repeats in a chaining fashion. It is important to notice that all AES blocks use the same key \ud835\udc58 . Finally, the last output is plugged into one more AES block as input, which uses another key \ud835\udc58 \u2032 and schema releases MAC tag. (2) Nested MAC . In this approach, the message is split into 16 bytes buckets. After this, the first AES block obtains as input this first bucket and \ud835\udc58 is used as a secret key. The output of this AES block is used as a key for the next block, and data for this AES block is the next input bucket. After processing all messages, the last output is plugged into the last AES block as input, but for the last AES block, another secret key \ud835\udc58 \u2032 should be used similarly as in CBC-MAC . For using CBC-MAC and Nested MAC securely, the number of signed messages (of arbitrary length) should be on the order of 2 64 AES 128-bits blocks. For details, please check the analysis of these schemes.", "F.5 Chosen Ciphertext Attack CCA and Authentication": "In a chosen ciphertext attack ( CCA ), the attacker has access to a decryption oracle and can decrypt any ciphertext of his choice. Also, the attacker can access encryption oracles and obtain encryption of arbitrary messages of his choice. Adversarial aims to break semantic security in the CPA sense. To provide the CCA security, the two processes Encrypt and MAC should be performed sequentially, and typically it's preferable to perform them in this order. In our paper, we have used EAX , which is CTR mode for encryption combined with CMAC for integrity purposes.", "F.6 Using AES with EAX Operation Mode in our paper": "We use AES encryption with EAX mode, which produces a triple as output: \u27e8 \ud835\udc5a = \ud835\udc36 \ud835\udc56 (\u2207 \ud835\udc53 ( \ud835\udc65 \ud835\udc61 ) \u2208 R \ud835\udc51 ( 16 / 32 / 64 \u00b7 \ud835\udc51 bits ) , \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52 \u2208 \ud835\udc41 ( 128 bits ) , \ud835\udc47 \ud835\udc4e\ud835\udc54 ( 128 bits , \ud835\udc50\ud835\udc56\ud835\udc5d\u210e\ud835\udc52\ud835\udc5f\ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 )\u27e9 . The ciphertext has the same bit length as the original message. Because EAX mode employs CTR for stream encryption, it does not require any padding. The \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52 is a random 16-byte value, and the \ud835\udc47\ud835\udc4e\ud835\udc54 is a 16 byte message authentication code. As it has been mentioned in Section 4, the CKKS and all HE schemas cannot achieve CCA security [28]. For AES / EAX , it has been proved in [8] that it's secure against CCA attacks. Cryptography is an area of science by itself. Readers for can gain more information about Classical Cryptography from [14]. Federated Learning is Better with Non-Homomorphic Encryption", "G HOMOMORPHIC ENCRYPTION WITH CKKS": "", "G.1 Learning With Errors Problem and REGEV09 Algorithm as a Concrete Example of SWHE": "The security of the CKKS scheme depends mainly on the hardness of the Learning With Errors( LWE ) problem which we will overview next. The \"Learning with errors\" search problem represents was analyzed in [58]. For this work Oded Regev has obtained a G\u00f6del prize in 2018: Parameters have the following properties: \u00b7 The \ud835\udc5e is a prime number. \u00b7 \ud835\udc5a > \ud835\udc5b . \u00b7 A \u2208 Z \ud835\udc5b \u00d7 \ud835\udc5a \ud835\udc5e is a random matrix drawn u.a.r. from Z \ud835\udc5b \u00d7 \ud835\udc5a \ud835\udc5e . \u00b7 \ud835\udefd \u2208 R . \u00b7 \ud835\udc52 \u2208 Z \ud835\udc5a \ud835\udc5e is small (in terms of \ud835\udc3f 2 norm) r.v. For r.v. \ud835\udc52 there is no assumption about its distribution. The only one constraint is that \ud835\udc52 \u2260 0. As can be observed from the description LWE problem operates on a system of over-determined sets of equations. When \ud835\udc52 = 0 the system can be solved via Gaussian elimination , but when \ud835\udc52 \u2260 0 we believe it's a hard search problem. Specifically, Theorem 1 from [58] makes a connection between LWE and the search problem in integer lattices via the following theorem: Instance of LWE problems with parameter \ud835\udc5b is as hard as the Short Integer Solution Problem . The best well-known solution for solving the LWE problem works in the following time: \ud835\udc5e O( \ud835\udc5b / log ( \ud835\udc5b ) ) [12]. There exists a variation of the LWE problem named as Decision Learning With Errors problem. In theory, it has been shown that Decisional LWE is as hard as LWE. In usual LWE we can not find a solution for a noisy set of linear equations effectively based on knowledge of \ud835\udc34 and \ud835\udc4f . In Decisional LWE the adversaries can not from observing tuple ( A ,\ud835\udc4f ) distinguish the following two scenarios: (1) The tuple ( A ,\ud835\udc4f ) has been generated completely uniformly at random. In some sense, there are no real hidden linear dependence structures between columns of \ud835\udc34 and \ud835\udc4f . (2) The tuple ( A ,\ud835\udc4f ) in fact has a specific structure ( A = A ,\ud835\udc4f = A \ud835\udc47 \ud835\udc60 + \ud835\udc52 ) . The LWE problem provides a way to publish a lot of perturbated linear equations in variable \ud835\udc60 in the form of \ud835\udc60 \ud835\udc47 A + \ud835\udc52 \ud835\udc47 = \ud835\udc4f , and essentially hide \ud835\udc60 \u2208 Z \ud835\udc5b from parties who do not know exactly \ud835\udc52 . Next, we will describe concrete examples of using this idea. The method described next represents a symmetric Somewhat Homomorphic Encryption ( SWHE ) Learning With Errors (LWE) based scheme known as REGEV09 . The method was proposed in 2009 at work [58]. Its security properties are based on hardness to solve the decisional LWE. KeyGen. \ud835\udc60 \u2208 Z \ud835\udc5b \ud835\udc5e \u223c \ud835\udc48 is a secret key, \ud835\udc5e is a prime, \ud835\udc5b is a secure parameter. Encrypt. Encryption is working bit by bit. The message that we encrypt without loss of generality can be considered as \ud835\udc5a \u2208 { 0 , 1 } . The algorithm for encryption produces ciphertext as \ud835\udc50 = ( \ud835\udc4e,\ud835\udc4f ) \u2208 Z \ud835\udc5b \ud835\udc5e \u00d7 Z \ud835\udc5e via following rules: (1) \ud835\udc52 \u2208 Z is a \"short\" noise generated from some distribution, satisfied constraint | \ud835\udc52 | < \ud835\udc5e / 4). (2) \ud835\udc4e \u2208 Z \ud835\udc5b \ud835\udc5e is a random vector sampled uniformly from its domain. (4) The released ciphertext \ud835\udc50 = ( \ud835\udc4e,\ud835\udc4f ) \u2208 Z \ud835\udc5b \ud835\udc5e \u00d7 Z \ud835\udc5e Decrypt. Decryption happens via using the following formula which involves function \ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc51 \ud835\udc4e ( \ud835\udc65 ) which round to 0 or \ud835\udc4e depends on what is more close to \ud835\udc65 . For decryption to work correctly, we need to have \ud835\udc52 \u2208 (- \u230a \ud835\udc5e / 4 \u230b , \u230a \ud835\udc5e / 4 \u230b) . Now let's verify that this formula works correctly: If \ud835\udc5a = 0, and because | \ud835\udc52 | < \u230a \ud835\udc5e / 4 \u230b = \u21d2 \u02c6 \ud835\udc5a = 0. If \ud835\udc5a = 1, and because | \ud835\udc52 | < \u230a \ud835\udc5e / 4 \u230b = \u21d2 \ud835\udc52 > - \u230a \ud835\udc5e / 4 \u230b = \u21d2 \u02c6 \ud835\udc5a = 1. HE Add. We add two ciphertext messages \ud835\udc50 \ud835\udc56 = ( \ud835\udc4e \ud835\udc56 ,\ud835\udc4f \ud835\udc56 ) ,\ud835\udc56 \u2208 { 0 , 1 } by components: This schema is SWHE and homomorphic additive. In the worst case error is doubled in each addition operation of each bit. If the adversary listens to the channel, he can collect ( \ud835\udc4e,\ud835\udc4f ) for each transferred bit and construct A = [ \ud835\udc4e 1 , \ud835\udc4e 2 , . . . ] and the right-hand side \ud835\udc4f . But encoding of ciphertext directly follows LWE problem description with additive error correction code \ud835\udc52 + \ud835\udc5a \u00b7 \u230a \ud835\udc5e / 2 \u230b . By properties of the Decision LWE Problem, adversaries can not get any information from it because data distribution ( A ,\ud835\udc4f ) is indistinguishable from a uniform.", "G.2 Introduction to CKKS": "There are different variants of FHE , LFHE , and SWHE , but most of them operate on boolean or integer arithmetic. In our paper, we compare DCGD/PermK/AES against GD with Cheon-Kim-Kim-Song ( CKKS ) [18] schema. This schema allows approximate arithmetic on encrypted real and complex numbers and dense linear vectors. The CKKS schema violates property (1) of FHE from Appendix D, and it is specialized to work with these two linear spaces R \ud835\udc51 and C \ud835\udc51 . Therefore, CKKS is an SWHE scheme. Still, it is sufficient for most Machine Learning problems that operate on real spaces during the training phase, and it is the most popular schema in Machine Learning applications that require HE . Supporting multiple keys (MK) with privacy guarantees is an active research topic [44]. None of MK-CKKS possible schemas are currently implemented in TenSEAL [9] or SEAL [54] libraries. In our work, we used the classical single-key CKKS scheme for comparison because it's suitable for our setting. As we mentioned, the CKKS scheme allows us to perform HE computations on vectors of complex and real values. CKKS provides ways to perform element-wise addition, multiplication, and rotation of elements in encrypted form. The CKKS uses three different keys: \u00b7 Public key. The public key in CKKS schema is used for the encryption of plain messages. \u00b7 Relinearization key . Relinearization key reduces the size of a ciphertext after arithmetic operations on ciphertexts. \u00b7 Private(secret) key . The private (secret) key is used for decryption of encrypted messages. To perform arithmetic operations on encrypted vectors on the master device, the master should know the public key and the master should know the relinearization keys to reduce the size of a ciphertext after arithmetic operations. The public key in CKKS schema is used for encryption and can be shared with the master. However, a private(secret) key is used for decryption and must be kept confidential from the master if clients do not trust the master. The security of the scheme depends on the hardness of the Ring Learning With Errors ( RLWE ) problem. The RLWE is a generalization of LWE problem, but instead of using Z \ud835\udc5b \ud835\udc5e field, it is based on underlying algebra which is polynomials over a ring. RLWE inherits useful properties of hardness from LWE , but it is more space efficient. For an overview of LWE see Appendix G.1.", "G.3 Description of processes inside CKKS": "The schematic process of how CKKS operates is depicted in Fig.8. The general schema of CKKS consists of several steps. First Step. Firstly, a vector of values \ud835\udc5a \u2208 R \ud835\udc41 / 2 on which we want to perform certain computations is encoded into a plaintext polynomial \ud835\udc5d ( \ud835\udc65 ) . This is necessary because encryption, decryption, and other mechanisms work on polynomial commutative rings. In the case of encoding into polynomials from Z [ \ud835\udc4b ]/( \ud835\udc4b \ud835\udc41 + 1 ) , the encoding process is reversible. In the original paper, [18] this process is carried out using canonical embedding. This embedding is carried in such a way that if evaluate the obtained polynomial from Z [ \ud835\udc4b ]/( \ud835\udc4b \ud835\udc41 + 1 ) at the roots of cyclotomic polynomial \ud835\udc4b \ud835\udc41 + 1 we will recover entries of the original message of \ud835\udc5b \u2208 R \ud835\udc41 / 2 . Second Step. Next, the actual encryption part encrypted the plain polynomial with integer coefficients via a public key into polynomial ( \ud835\udc50 0 , \ud835\udc50 1 ) \u2208 ( \ud835\udc4d \ud835\udc5e [ \ud835\udc65 ]/( \ud835\udc4b \ud835\udc41 + 1 )) 2 . The \ud835\udc5e is a modulus number in the CKKS scheme. Here \ud835\udc5e = \u02db \ud835\udc3f \ud835\udc56 = 1 \ud835\udc5e \ud835\udc56 , where \ud835\udc5e \ud835\udc56 are prime numbers. So \ud835\udc5e is not necessarily a prime number, but it is chosen to be a product of several prime numbers. To carry encryption the public key in the form of \ud835\udc4d \ud835\udc5e [ \ud835\udc4b ]/( \ud835\udc4b \ud835\udc41 + 1 )) after specific transformation is used. It means that the size of the public key in bits is approximately equal to \ud835\udc41 \u00b7 \ud835\udc5e . To have AES-128 security level for CKKS the key size is at least equal to 420 KBytes. Third Step. Next, the algebraic operations are carried on encrypted messages ( \ud835\udc50 0 , \ud835\udc50 1 ) by specific rules which we will not go deep into. Fourth Step. After carrying out the arithmetic operation, the result will be from the same space ( \ud835\udc50 0 , \ud835\udc50 1 ) \u2208 ( \ud835\udc4d \ud835\udc5e ( \ud835\udc65 )/( \ud835\udc65 \ud835\udc41 + 1 )) 2 . To carry decryption the private key in the form of \ud835\udc4d \ud835\udc5e [ \ud835\udc4b ]/( \ud835\udc4b \ud835\udc41 + 1 )) after a specific transformation is used. Therefore it means that the size of the private key in bits is approximately equal to \ud835\udc41 \u00b7 \ud835\udc5e . To have AES-128 security level the key size at least equal should be equal to 420 KBytes. Fifth Step. Finally during decoding, a message from space ( \ud835\udc4d \ud835\udc5e ( \ud835\udc65 )/( \ud835\udc65 \ud835\udc41 + 1 )) 2 to plain text in \ud835\udc4d [ \ud835\udc65 ]/( \ud835\udc65 \ud835\udc41 + 1 ) and finally perform decoding into R \ud835\udc41 / 2 .", "G.4 CKKS Configuration Equivalent to AES-128": "To provide privacy grantees similar to AES-128 encryption: \ud835\udc41 should satisfy this condition \ud835\udc41 > 16384, and \ud835\udc5e = \u02db \ud835\udc3e \ud835\udc56 = 1 \ud835\udc5e \ud835\udc56 should be at least 438 bits long. This ensures that the security parameter \ud835\udc41 , \ud835\udc5e is large enough to provide sufficient security. These details can be found in the reference implementation of Microsoft Research SEAL Library 4 . This is an underlying reason why CKKS has a more memory requirement compared to AES-128 . The encoding consists of 2 polynomials with 2 max ( \ud835\udc51, \ud835\udc41 ) coefficients, and each coefficient is not 32 (for FP32) or 64 (for FP64) bits long, but it's essentially \ud835\udc5e = 438 bits long. In addition, when input/output vectors do not match \ud835\udc41 CKKS/HE requires performing chunking of input and output - it requires additional operations to maintain the correctness and efficiency of the computation. In all our experiments, we used the following CKKS configuration inside TenSeal [9] library version 0.3.14. For this library to obtain AES-128 , we have used the recommended configuration: (1) Polynomial degree: 2 14 = 16 384 (2) Coefficient modulus: \ud835\udc5e 1 , \ud835\udc5e 2 , \ud835\udc5e 3 , \ud835\udc5e 4 , \ud835\udc5e 5 = ( 60 , 30 , 30 , 30 , 60 ) bits, which corresponds to \ud835\udc5e size of 210 bits. This configuration is recommended, by TenSeal for AES-128 security level. (3) Scale factor: 2 30 (4) Scheme type: CKKS Input message \ud835\udc5a \u2208 R \ud835\udc41 / 2 Encode message \ud835\udc5a \u2208 R \ud835\udc41 / 2 into \ud835\udc5a \u2032 \u2208 Z [ \ud835\udc65 ]/( \ud835\udc65 \ud835\udc41 + 1 ) Encrypt message \ud835\udc5a \u2032 \u2208 Z [ \ud835\udc65 ]/( \ud835\udc65 \ud835\udc41 + 1 ) into \ud835\udc5a \u2032\u2032 \u2208 GLYPH<16> Z \ud835\udc5e [ \ud835\udc65 ]/( \ud835\udc65 \ud835\udc41 + 1 ) GLYPH<17> 2 Computation on encrypted messages GLYPH<0> \ud835\udc5a \u2032\u2032 1 , \ud835\udc5a \u2032\u2032 2 , . . . GLYPH<1> . CKKS allows to perform: addition, multiplication, and rotation. Decrypt \ud835\udc5a \u2032\u2032 \u2208 GLYPH<16> Z \ud835\udc5e [ \ud835\udc65 ]/( \ud835\udc65 \ud835\udc41 + 1 ) GLYPH<17> 2 into \ud835\udc5a \u2032 \u2208 Z [ \ud835\udc65 ]/( \ud835\udc65 \ud835\udc41 + 1 ) Decode message \ud835\udc5a \u2032 \u2208 Z [ \ud835\udc65 ]/( \ud835\udc65 \ud835\udc41 + 1 ) into \ud835\udc5a \u2208 R \ud835\udc41 / 2 Output message \ud835\udc5a \u2208 R \ud835\udc41 / 2 0.0 0.5 1.0 1.5 2.0 2.5 #bits/n 1e7 10 22 10 16 10 10 10 4 10 2 || f(x)|| 2 DCGD, AES-128, PermK, d=1K DCGD, PermK, d=1K GD, d=1K GD, HE/CKKS, d=1K 0 500 1000 1500 2000 2500 3000 Rounds 10 20 10 12 10 4 10 4 10 12 || f(x)|| 2 DCGD, AES-128, PermK, d=1K DCGD, PermK, d=1K GD, d=1K GD, HE/CKKS, d=1K 0 500 1000 1500 2000 2500 3000 Rounds 10 4 10 6 10 8 10 10 #bits/n DCGD, AES-128, PermK, d=1K DCGD, PermK, d=1K GD, d=1K GD, HE/CKKS, d=1K 0.0 0.5 1.0 1.5 2.0 2.5 #bits/n 1e8 10 22 10 16 10 10 10 4 10 2 || f(x)|| 2 DCGD, AES-128, PermK, d=10K DCGD, PermK, d=10K GD, d=10K GD, HE/CKKS, d=10K 2 500 1000 1500 2000 2500 3000 Rounds 10 22 10 15 10 8 10 1 10 6 || f(x)|| DCGD, AES-128, PermK, d=10K DCGD, PermK, d=10K GD, d=10K GD, HE/CKKS, d=10K #bits/n 0 500 1000 1500 2000 2500 Rounds 10 5 10 7 10 9 DCGD, AES-128, PermK, d=10K DCGD, PermK, d=10K GD, d=10K GD, HE/CKKS, d=10K 0.0 0.5 1.0 1.5 2.0 2.5 #bits/n 1e9 10 22 10 16 10 10 10 4 10 2 || f(x)|| 2 DCGD, AES-128, PermK, d=100K DCGD, PermK, d=100K GD, d=100K GD, HE/CKKS, d=100K 0 500 1000 1500 2000 2500 3000 Rounds 10 21 10 14 10 7 10 0 10 7 || f(x)|| 2 DCGD, AES-128, PermK, d=100K DCGD, PermK, d=100K GD, d=100K GD, HE/CKKS, d=100K #bits/n 0 500 1000 1500 2000 2500 3000 Rounds 10 6 10 8 10 10 DCGD, AES-128, PermK, d=100K DCGD, PermK, d=100K GD, d=100K GD, HE/CKKS, d=100K 0 500 1000 1500 2000 2500 3000 Rounds 10 6 10 8 10 10 # of bits from master DCGD, AES-128, PermK, d=1K DCGD, PermK, d=1K GD, d=1K GD, HE/CKKS, d=1K (a) \ud835\udc51 = 1 000 0 500 1000 1500 2000 2500 3000 Rounds 10 6 10 7 10 8 10 9 10 10 # of bits from master DCGD, AES-128, PermK, d=10K DCGD, PermK, d=10K GD, d=10K GD, HE/CKKS, d=10K (b) \ud835\udc51 = 10 000 0 500 1000 1500 2000 2500 3000 Rounds 10 7 10 8 10 9 10 10 10 11 # of bits from master DCGD, AES-128, PermK, d=100K DCGD, PermK, d=100K GD, d=100K GD, HE/CKKS, d=100K (c) \ud835\udc51 = 100 000", "H EXTRA EXPERIMENTS": "", "H.1 Exploring Problem Dimension": "This experiment investigates the impact of problem dimension \ud835\udc51 \u2208 { 10 3 , 10 4 , 10 5 } . Fig. 9 shows that the CKKS overhead from encryption is \u00d7 10 3 more both in master to client, and client to master communication direction compare to DCGD/PermK/AES . With \ud835\udc51 = 10 6 the memory footprint for CKSS configured to guarantee the same guarantees as AES-128 in the master to store \ud835\udc5b = 50 encrypted gradients is 46 GBytes, rendering storage of such information in the master challenging. The best convergence relative to the volume of information sent to the master is achieved with DCGD/PermK . The behavior of DCGD/PermK and DCGD/PermK/AES is indistinguishable for \ud835\udc51 > 10 \ud835\udc3e . Despite the ciphertext size being the same as the input when using AES , proper use of AES block ciphers for communication requires the addition of a Message Authentication Code ( MAC ) for protection against malicious errors and a unique pseudo-random identifier (nonce). Each of these adds an overhead of 16 bytes. It explains different behavior observed for DCGD/PermK and DCGD/PermK/AES at \ud835\udc51 = 1 \ud835\udc3e in Fig. 9. Given that, DCGD/PermK/AES emerges as a more viable alternative to CKKS in FL context, in the setting when HE previously has been applied. Federated Learning is Better with Non-Homomorphic Encryption", "H.2 Compute and Communication Overlap for DCGD/PermK": "In this experiment, we simulated a compute and communication environment with realistic assumptions and evaluated the performance of DCGD/PermK . We ignored overheads from the Operating System (OS) and its components (drivers, kernel, services). Firstly, we provide background on Network Communication and System Architecture to justify our modeling choices.", "H.2.1 Background in Network Communication.": "Communication Bandwidth During Master Client Communication. We consider a scenario where clients are connected to the master via the Internet. These devices are called end systems or hosts in the context of a communication network. The communication between end systems is facilitated by intermediate routing devices , such as routers and switches, connected by links . Internet communication is based on packet switching , which means that exchange messages \ud835\udc5a are divided into smaller units called packets . Then, they are delivered with the best effort through the communication network without reserving a path(circuit) from sender to receiver in advance. The bandwidth of a link is the maximum rate at which data can be transferred over it and is measured in bits/second. This rate depends on various factors and generally can vary over time. It depends on: (1) The distance between the linked devices. (2) The type of physical medium for carrying the signal. (3) The number of packets in the queue to transfer in routing devices. (4) The probability that clients on the Internet will use the same link simultaneously. (5) The probability that a packet will be dropped in a routing device. (6) The number of clients attached to the link. We can distinguish between two types of bandwidth: (1) Instantaneous bandwidth - bandwidth at a specific moment of time. (2) Average bandwidth - average bandwidth over a complete message transfer. If the information flows from client to master via a directed path across \ud835\udc41 links, then the effective bandwidth of the path is determined by the minimum bandwidth among all links: . The link with this minimum bandwidth is called the bottleneck link . Communication Delays During Master Client Communication. The delay of a single link device is the actual time it takes for a bit of data to travel from one link device to the next. The delay consists of several components: (1) Processing delay is the time required to examine the packet header and determine where to forward the packet in a link device with several output ports. (2) Transmission delay , which is the time required for the router to push out the packet into the link, without considering the propagation of signal by itself. (3) Propagation delay is the time required for the signal to travel along the physical medium. (4) Queuing delay is the time required for a packet to wait in the output buffer if the link output port is busy. (5) Loss delay is the time required for a packet to be retransmitted if it is dropped due to buffer overflow or errors due to medium. Suppose a client and a server are connected by \ud835\udc41 links sequentially, and the information flows through them in a directed path. In that case, the total delay for transferring a single package over this path is given by the sum of all delays along each link over the path: H.2.2 Network Communication Model. As we have seen, the total delay and bandwidth vary, even for the fixed topology of connecting the master and clients. According to 5 in the average download bandwidth across the globe is \ud835\udc35 = 41 . 54 MBps and communication latency is \ud835\udc45\ud835\udc47\ud835\udc47 = 28 ms. The delay time for communication with messages \ud835\udc5a \u2208 R \ud835\udc51 where each component is represented by \ud835\udc4f\ud835\udc5d\ud835\udc5d bits per component from master to client is modeled in the following way: Here, \ud835\udc35 is the bandwidth (or throughput) of the communication channel between the master and the client. We assume it's constant. Next, \ud835\udc45\ud835\udc47\ud835\udc47 is the time for a small packet to travel from client to server and back. It is only an approximation; for example, it does not model Packet Loss Delay. The worst-case maximum delay is very difficult even to estimate. We assume that \ud835\udc45\ud835\udc47\ud835\udc47 / 2 and \ud835\udc35 are not changing during training. However, in real communication networks, the path from client to master the exact communication path can change over time. In reality, a Network Interface Controller ( NIC ) implements network communication in a local computing device, and it is typically connected with a PCI-Express bus as an external input/output device to the whole local computation system. For modern PCI-Express buses such as PCI-E v5, the bandwidth, even for a single physical lane x1, is in the order of 4000 . 0 MBps, and latency for this bus and latency is on the order of 35 \u00b7 10 -3 ms. It means that in context when devices are communicated via the Internet, the effect of delays from PCI-Express is negligible, or at least it does not represent the bottleneck both in terms of bandwidth and latency. The same holds for involving communication time to transfer data from CPU to DRAM memory. Modern DDR5 memory has a bandwidth of 51200 MBps, and latency is measured in the order of nanoseconds. It means that what is represented as a bottleneck from a communication point of view (both in terms of latency and bandwidth) with the server is the connection to though NIC installed in the client. H.2.3 CPU-based Computation in Clients. To evaluate the compressed gradient \ud835\udc36 \ud835\udc56 (\u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 )) in a client, different algorithms can be used, such as analytical, numerical, symbolic, and methods that leverage automatic differentiation. These algorithms need to run on some device that can execute them efficiently. We will focus on the execution aspects of modern Central Processing Units (CPUs), which are the most flexible devices from a programming perspective. CPUs have multiple cores that contain various components that work together to execute algorithms.", "H.2.4 Background in Modern Central Processing Unit.": "Instruction Decode. In this stage, the CPU decodes the instructions and obtains information about the input, output, and operation type. Then, it splits the instructions into micro operations and puts them into the Operation Issue Queue. This stage may introduce some complexities. One complexity is that some CPUs can decode multiple instructions simultaneously and issue them in parallel. This is called a multi-issue (or superscalar ) design, which aims to exploit Federated Learning is Better with Non-Homomorphic Encryption instruction-level parallelism by executing independent instructions concurrently. Another complexity is that the CPU with out of order issue capability can execute instructions whenever they are ready, regardless of the original order. Operation Issue Queue. This is a hardware queue where decoded instructions, in the form of micro-operations , are stored. The queue has a minimum length that can accommodate the longest sequence of micro-operations for any instruction in the CPU's instruction set. Control Unit (CU). The CU operates at the level of micro-operations and performs the following functions: (a) selecting the way to connect electrical components using multiplexers and demultiplexers; (b) turning on/off different electronic components; and (c) controlling the control lines of electronic components. Multiplexers and Demultiplexers. During pipeline execution, intermediate inputs and results are stored in the latches of electrical components. To route signals within the CPU, multiplexers, and demultiplexers are utilized. Signals propagate through the component once all the components are connected, and data is applied to the input ports. The results are then produced when the enabled control signal reaches the electrical component. Adder and Multiplier. The adders and multipliers are electrical circuits that perform addition or multiplication when turned on. The input for these devices is read from the intermediate buffer of the electronic component(latches). The typical input source (after intermediate routing with multiplexers and demultiplexers) is the Register File. Register File. The Register File is a storage unit that holds all the registers in the CPU. It has multiple ports that allow parallel access to it. The Load and Store Units may have direct access to the Register File. Load and Store Units (LS). The Execution pipeline sends requests to the LS units for memory access. The LS units can access the Register File, the TLB for address translation, and the Memory Cache. Translation Lookaside Buffer (TLB). To read code or data from memory in the user space or kernel space of the OS, the first step is to find the actual physical address of the specific memory location. This operation occurs for every instruction of a program. Without the TLB, the virtual addressing mechanism would require several accesses to different page tables, significantly increasing the time needed. The TLB is a cache that stores the mapping between virtual page numbers and physical frame numbers, speeding up the address translation process for memory access. The TLB relies on the locality of code and data in most algorithms. CPUMemoryCache. The CPU Memory Cache is a fast storage unit that holds frequently accessed data and instructions. It is used to reduce the latency of accessing the DRAM memory. The Load and Store Units have access to the Cache. Modern high-end systems support three levels of Cache. The L1 cache is typically split between data and instructions, and it is the closest to the CPU. The L2 and L3 caches are larger and slower, and they can be shared by multiple cores. The cache implementation varies across different CPUs, depending on the trade-offs between speed, potential conflicts, hardware complexity, cache replacement policy, and power consumption. If the data is not available in the Cache, then the CPU Cache requests a block of memory from the Memory Controller (MU), which accesses the DRAM memory. When data from DRAM is stored in multiple CPU caches, it fundamentally means that data may be stored in several places. In this situation, another aspect becomes important: (a) cache consistency , which essentially means that all copies of DRAM cache lines should be the same in all caches in the system; (b) cache coherence which essentially means that any read of memory returns the most recent update anywhere in the system. DRAM Memory Controller (MC). The DRAM Memory Controller is a device that manages access to the main DRAM memory Chips. It is used to fetch data from the Main Memory when it is not available in the CPU cache. The Memory Controller returns the data to the caches in blocks of a fixed size, called Cache Lines, and typically it is 64 bytes. The Memory Controller is also responsible for running the memory bus transactions, which are the transfers of data between the MC and DRAM memory chips. The Memory Controller is typically implemented in the hardware as a device that is shared by multiple cores. DRAM Memory Chips. Memory chips are devices that store data in binary form. They are usually specified by the number of bits stored and the number of bits accessed in one read or write operation. For example, a common DRAM chip 4Gbx1 means that it can store 4G bits and access one bit at a time. To protect data from corruption, extra logic may store bits for Error Correction Codes (ECC). Input and Output Buses. Input and Output buses serve as pathways connecting external devices to the CPU. Examples of I/O buses include SATA, USB, and PCI-Express. For example, PCI-Express is commonly used to communicate with devices such as graphics cards and network cards. Communication with such external devices can be achieved using Direct Memory Access (DMA) or Programmed Input-Output (PIO). DMA enables devices to transfer data directly to or from memory without involving the CPU, while PIO requires the CPU to issue commands and wait for data. DMA is more efficient and faster than PIO but necessitates additional hardware support. H.2.5 Modeling of Computation. We model clients' compute capability by assuming that they have a computation device similar to Intel-Xeon-E5-2666-v3 CPU 6 . We assume that the computation device of the client and the master is represented by a CPU with 10 CPU cores, working at frequency 3 . 2 \ud835\udc3a\ud835\udc3b\ud835\udc67 , CPU support hyper-threading with executing 2 computation works per core, we assume that Multiply - Add (MAD) operation is possible which effectively doubles compute throughput. There are 2 functional units (FU) or compute ports per core for Floating Point arithmetic in such a device. We assume that add, subtract, and multiply operations for float numbers require a throughput of 1 operation/clock and latency of 1 clock per execution unit for FP32 arithmetic, which is realistic. With these assumptions, this device has a peak computation throughput of 238 . 41 \ud835\udc3a\ud835\udc39\ud835\udc59\ud835\udc5c\ud835\udc5d\ud835\udc60 @ \ud835\udc39\ud835\udc43 32. And we suppose all \ud835\udc5b = 4 clients and masters are equipped with it. In our model, we will assume that, on average, the train data is located in the L2 cache, and all memory operations in the client can be executed via accessing the L2 cache by utilizing one of 3 Load/Store Units per Core. We assume access latency to read a cache line of size 64 bytes requires 10 clocks. Next, we assume that the Network Interface Controller(NIC) in the master and clients have data-direct I/O access to the L3 cache. Access latency to it is 40 CPU cycles for 64 byte cache line size. The effect of DRAM and caches can be ignored during inter-node communication but not during memory operation during gradient oracle computation. H.2.6 The Optimization Problem. For modeling purposes, we consider solving a Linear Regression in the form: Federated Learning is Better with Non-Homomorphic Encryption For this problem the value of \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) = 1 \ud835\udc5b \ud835\udc56 \ud835\udc34 \u22a4 \ud835\udc56 ( \ud835\udc34 \ud835\udc56 \ud835\udc65 -\ud835\udc4f \ud835\udc56 ) . In implementing gradient oracles, we utilize dense matrix and matrix-vector operations. To add two vectors, we execute \ud835\udc51 scalar additions and 2 \ud835\udc51 memory access operations for reading. In modern computing, hardware loads are more expensive because writes essentially can be queued. The need time of inner product operation of two vectors of dimension \ud835\udc51 is equal to ( \ud835\udc51 -1 ) \u00b7 addcost + \ud835\udc51 \u00b7 multcost + 2 \ud835\udc51 \u00b7 memaccesscost . To estimate computing time for the Matrix-Vector and Matrix-Matrix Operations, we have assumed that their calculation is a sequence of inner products. In reality, not all data may fit into the Cache. If we go one step further with modeling, then Cache misses effects should modeled as well. H.2.7 Implementation Benefits of DCGD/PermK. There are several flexibility aspects of DCGD/PermK that we will utilize in our experiment: (1) The PermK operator compressor behaviorism is independent of the input. This means that clients can a prior sample need coordinates for sparsification and increase the speed of \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) in clients. (2) Next, we will exploit the fact that after the master receives the message, it can immediately broadcast it to all clients, regardless of whether the client has finished the work for the current round. This is possible in the context of using DCGD/PermK/AES , but it is impossible when using CKKS . (3) If during the FL process, there is a slow client with a slow CPU or with a big amount of samples, then DCGD/PermK allow other clients to start several operations which are impossible for GD : \u00b7 Obtain (partial) results from the master by using a communication network for current round h \u2207 \ud835\udc53 ( \ud835\udc65 \ud835\udc58 ) i \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61 \u00b7 Apply partial update for current model \ud835\udc65 \ud835\udc58 and obtain partially new model \ud835\udc65 \ud835\udc58 + 1 \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61 \u00b7 Client can start perform partial computations for next iteration h \u2207 \ud835\udc53 ( \ud835\udc65 \ud835\udc58 + 1 \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61 ) i \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61 H.2.8 Scheduling of Clients and Master Computation and Communication with Critical Path Method. We modeled the GD and DCGD Algorithms with unrolling \ud835\udc5f = 4 rounds of computation and communication over \ud835\udc5b = 4 clients and 1 master. We represented the optimization process as the directed graph of different elementary tasks. Task dependencies are represented in the form of weighted, orientated, directed, acyclic graph: The graph is constructed following the next rules: (1) Tasks are represented as vertices in graph \ud835\udc3a . (2) Task \ud835\udc60 is connected to task \ud835\udc52 with weight 0, if and only if task \ud835\udc52 can not start before task \ud835\udc60 . (3) If the task \ud835\udc52 obtains as input from task \ud835\udc60 , then the task \ud835\udc60 is connected to task \ud835\udc52 with the weight of duration of work (in seconds) which \ud835\udc60 should perform to produce input for task \ud835\udc52 . The algorithm for the scheduling process involves the following steps: (1) Introduce fake source and sink vertices. (2) Add zero weight directed edge from a fake source vertex to all \ud835\udc63 \u2208 \ud835\udc49 . (3) From all original vertices \ud835\udc63 \u2208 \ud835\udc49 add zero weight edge to fake sink vertex. (4) Add fake source and sink to set \ud835\udc49 . (5) Compute topological order of \ud835\udc3a starting from fake source \ud835\udc60 . (6) Compute the longest path in the directed acyclic graph of tasks. It is achieved via temporally changing weights of \ud835\udc3a = ( \ud835\udc49, \ud835\udc38 ) to negative, computing the topological order of \ud835\udc3a , and relaxing all vertices in topological order. (7) The longest path to all vertices \ud835\udc63 \u2208 \ud835\udc49 from fake source \ud835\udc60 creates a schedule for executing operation \ud835\udc63 . Essentially, this is a critical path method (CPM) to solve the parallel precedence-constrained scheduling problem. The running time of this algorithm is O ( \ud835\udc49 + \ud835\udc38 ) . Correctness Proof. Let us examine the longest path \ud835\udc60 \u2740 \ud835\udc63 . The previously scheduled jobs before \ud835\udc63 are vertices \ud835\udc65 \u2208 \ud835\udc49 , such that \ud835\udc65 \u2740 \ud835\udc63 . The construction of the longest path implies that \ud835\udc64 ( \ud835\udc60 \u2740 \ud835\udc63 ) \u2265 \ud835\udc64 ( \ud835\udc60 \u2740 \ud835\udc65 )+ \ud835\udc64 ( \ud835\udc65 \u2740 \ud835\udc63 ) \u2265 \ud835\udc64 ( \ud835\udc60 \u2740 \ud835\udc65 ) , \u2200 \ud835\udc60 \u2740 \ud835\udc65 .From this, we observe that the start times obtained by the longest paths are feasible because jobs \ud835\udc65 that need to be executed before \ud835\udc63 will be scheduled before \ud835\udc63 . Furthermore, the length of any path \ud835\udc60 \u2740 \ud835\udc63 is a lower bound on the actual time to start \ud835\udc63 because \ud835\udc63 cannot begin earlier than previous tasks due to dependency constraints. This proves that the longest path from \ud835\udc60 \u2208 \ud835\udc49 to \ud835\udc63 \u2208 \ud835\udc49 determines the start time for task \ud835\udc63 . Once task \ud835\udc63 can begin execution in the timeline, it can potentially activate the execution of all tasks ( \ud835\udc63, \ud835\udc67 ) \u2208 \ud835\udc38 , where \ud835\udc67 \u2208 \ud835\udc4e\ud835\udc51\ud835\udc57 ( \ud835\udc63 ) and \ud835\udc4a ( \ud835\udc63, \ud835\udc67 ) > 0. H.2.9 Critical Path Method Iterative Refinement. Parallel precedence-constrained scheduling using the CPM method determines the execution schedule for a graph of jobs. However, there may be cases where the duration of a task (which is the input for the CPM Algorithm) is defined by the number of other tasks during specific time intervals. This creates a circular dependency between the input and output of the CPM Algorithm. In our scenarios, we encounter this situation due to the following reasons: (1) If clients make a partial computation and the CPU is not busy with other works, parallelizable operations (such as Matrix-Vector multiply) can be parallelized across several CPU cores. Consequently, it increases clients' computational throughput for this partial update. (2) If clients share the same bottleneck link to the master and some are still busy with compute \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 \ud835\udc58 ) , then other clients can transmit data at a fast bandwidth because the bottleneck link is shared across a smaller number of clients. This observation leads to a situation in which effective bandwidth can be increased. H.2.10 Results of Scheduling and Refinement for GD and DCGD/PermK/AES. Scheduling results are presented in Fig. 10. In our experiment, all clients have the same compute power equal to 238 . 41 GFLOPS. Communication bandwidth and latency from all clients to the master and vice versa is the same, namely \ud835\udc35 \ud835\udc56 = 41 . 54 Mbps, and latency (or delay) for transmission is 28 \u00b7 10 -3 seconds. The assumption for computation is that each memory operation is carried with an L2 cache. In case there is a need to perform read and write, we treat it as two memory operations. We assume all clients during communication to master share the same bottleneck communication link. Also, we assume that the target CPU supports AES . The detailed generated execution plans can not be represented in this paper due to their size. However, they can be found in the source code attached to the paper. As illustrated in Fig. 10, overlapping communication, computation, and the refined scheduling due to time-varying into shared communication bus and time-varying load into computing devices in clients leads to different speedups in execution plans (Fig. 10, (d), (b)) for GD and DCGD/PermK . The total computation speedup per round when compared to refined GD is 6 . 578. We hope this simulation will prove valuable for those seeking to adopt DCGD/PermK/AES to align with hardware requirements closely. C4 0 20 40 60 80 Seconds Master C1 C2 C3 C4 0 10 20 30 40 50 60 70 Seconds Master C1 C2 C3 0 5 10 15 20 25 30 35 Seconds Master C1 C2 C3 C4 0 2 4 6 8 10 Seconds Master C1 C2 C3 C4 Fig. 10. Event-based modeling for training Linear Regression across \ud835\udc5b = 4 clients, \ud835\udc51 = 10 \u00b7 10 6 , \ud835\udc5b 1 = 55000 , \ud835\udc5b 2 = \ud835\udc5b 3 = \ud835\udc5b 4 = 11000 during 4 rounds. The uplink and downlink bandwidth is 41 . 54 MBps, latency 28 ms, and computational thought of modeled CPUs is 238 . 41 GFLOPS. Legend: - Computation and local memory access in the client (Client can use all available CPU cores), - Communication from client to the master (Clients share the same bottleneck link), - Communication from the master to client (Clients share the same bottleneck link).", "I FLEXIBILITY IN TRAINING DEEP LEARNING MODELS": "", "I.1 Introduction to Gradient Oracle Computation with Backpropagation": "In Appendix H.2 we have explored that DCGD/PermK exhibits practical flexibility for training linear models in detail. However, described communication and computation overlap can be important not only for linear models but also for DL models. The evaluation of the gradient of the score function in modern computational frameworks such as TensorFlow [2] or PyTorch [55] for complex computational graph is automatized via leveraging algorithms for the numeric evaluation of derivatives. In most cases, this is achieved using Automatic Differentiation(AD) in Reverse Accumulation mode [49] named as a Backpropagation algorithm [61] in ML literature. A composed Loss function for Deep Learning models typically has the following structure: The score function \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) is represented by a computation graph and has a nested structure where prediction is driven by a nested composition of functions \ud835\udc54 \ud835\udc56 and the predicted label is scored with a true label for training input-output pair ( \ud835\udc4e \ud835\udc57 , \ud835\udc4f \ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc59,\ud835\udc57 ) with a score loss function L . Each function \ud835\udc54 \ud835\udc56 has the following source of inputs: output from the prevision function (or layer) \ud835\udc54 \ud835\udc56 -1 , and input (trainable) scalar parameters \ud835\udc65 \ud835\udc5d where \ud835\udc5d \u2208 \ud835\udc44 1 , \ud835\udc44 2 , . . . , and \ud835\udc44 \ud835\udc56 \u2286 [ \ud835\udc51 ] . Assume that \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) differentiable in points where derivatives according to the chain rule are evaluated. In this case, the needed partial derivatives [\u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 )] \ud835\udc58 = \ud835\udf15\ud835\udc53 \ud835\udc56 \ud835\udf15\ud835\udc65 \ud835\udc58 can be computed with chaining Jacobians calculated with intermediate variables. The Backpropagation algorithm allows for fixed input samples \ud835\udc37 \ud835\udc56 = ( \ud835\udc4e 1 ,\ud835\udc4f 1 ) , . . . , ( \ud835\udc4e \ud835\udc5b \ud835\udc56 ,\ud835\udc4f \ud835\udc5b \ud835\udc56 ) stored in client number \ud835\udc56 and for fixed computational graph compute the required gradient \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) in two passes or phases : (1) Forward Pass . Compute all intermediate variables namely values of \ud835\udc54 \ud835\udc56 () for every training sample, for the whole compute graph. All these variables are stored in memory for the next pass. (2) Backward Pass . Compute the intermediate Jacobians and produce final partial derivatives of a full gradient.", "I.2 Examples of Parallelization inside Backpropagation": "In fact, there is no need to store all intermediate Jacobians simultaneously. They are stored implicitly in special variables typically denoted as \ud835\udeff in implementation of Backpropagation Algorithm . However, there is a need to store all intermediate outputs explicitly (named as activation in ML literature) after Forward Pass . Computationally Backpropagation is often a preferred strategy when the entire gradient needs to be computed. The compute scheduling and parallelism strategies for effective computation of \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) is an active line of research by itself in Deep Learning System literature right now [38], [45]. The standard strategies for performing parallelism inside computation of \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) involve the following: (1) Data Parallelism. Assign train samples ( \ud835\udc4e \ud835\udc57 , \ud835\udc4f \ud835\udc57 ) to different computation devices in client \ud835\udc56 . (2) Model Parallelism. Assign different functions \ud835\udc54 \ud835\udc56 to different computation devices available in client \ud835\udc56 . (3) Attributes Parallelism. Split the image of functions \ud835\udc54 \ud835\udc56 -1 into parts and process it by parts if function \ud835\udc54 \ud835\udc56 allows to do it. (4) Parameters Parallelism. Partition trainable variables for function \ud835\udc54 \ud835\udc56 ( \ud835\udc65 ) , i.e. [ \ud835\udc65 ] \ud835\udc44 \ud835\udc56 into smaller chunks. After partitioning [ \ud835\udc65 ] \ud835\udc44 \ud835\udc56 if \ud835\udc54 \ud835\udc56 ( \ud835\udc65 ) can be computed in parallel then compute it in parallel in different devices. Federated Learning is Better with Non-Homomorphic Encryption", "I.3 Research Opportunities for Parallelization in Backpropagation from DCGD/PermK": "Assume that clients know current iterate \ud835\udc65 \ud835\udc58 , however, if there is even one straggler \ud835\udc60 which still did not send \ud835\udc54 \ud835\udc60 \ud835\udc58 to the Master, then no clients can proceed in training. Master has to wait for gradient estimator \ud835\udc54 \ud835\udc60 \ud835\udc58 from straggler \ud835\udc60 , and what can be done for DCGD/RandK is at least challenging in these circumstances. However, DCGD/PermK exhibits useful properties that can partially helpful in dealing with this situation which we previously discussed in Appendix H.2: \u00b7 Clients can obtain (partial) results from the master for current round h \u2207 \ud835\udc53 ( \ud835\udc65 \ud835\udc58 ) i \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61 \u00b7 Clients can apply partial update for current model \ud835\udc65 \ud835\udc58 and obtain partially new model \ud835\udc65 \ud835\udc58 + 1 \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61 It depends on the specific situation, but during the waiting of a straggler, the forward pass ( forward pass in practice takes at most 50% of \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) computation) potentially can be started using the available \ud835\udc65 \ud835\udc58 + 1 \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61 . Assume that we have statistical information about stragglers, then potentially this information can be provided to Deep Learning schedulers and parallelization strategies described in the previous section Appendix I.2 to optimize \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 \ud835\udc58 + 1 ) . This opens new opportunities to refine parallelization strategies for training in general and Parameter Parallelism in particular.", "J FLEXIBILITY OF DCGD/PERMK FOR DIFFERENT COMMUNICATION TOPOLOGIES": "In High-Performance Computation and Network Communication, the Physical Network Topologies describes the physical arrangement of the devices and routing devices for organizing communication. Next, we will explain what benefits and flexibility DCGD/PermK can bring if this algorithm runs in the mentioned popular physical network topologies.", "J.1 Potential Benefits in Point-to-point Topology": "In this type of topology, some clients are directly connected with a single link pairwise. It is possible to instantiate DCGD/PermK so that clients do not coordinate during runtime for aggregation computation. It is possible because there is no need to perform reduction in the sense of averaging \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) . If some pair clients are connected with the fast link, this pair of clients can utilize this channel and deliver h \u2207 \ud835\udc53 ( \ud835\udc65 \ud835\udc58 + 1 \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61 ) i \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61 for his neighbor with a fast point-to-point link without obtaining it from a master.", "J.2 Potential Benefits in Bus Topology": "A single cable or bus connects all the nodes in this type of topology. In this type of topology, if some clients have slow computing, it removes them from competition for a shared bus and decreases communication contention. In a bus topology, the benefit of DCGD/PermK is that messages from each client can be effectively broadcast. In fact, DCGD/PermK can be implemented without a centralized server, and a shared bus is enough for organized communication. Employing AES based encryption guarantees protection from eavesdropping on the shared bus.", "J.3 Potential Benefits in Star Topology": "In this type, all the nodes are connected to a central master device with individual links. In the case of using DCGD/PermK , the master in this topology only plays the role of communication hub. Interestingly, while obtaining parts of the gradients from clients, computation is unnecessary, and synchronization is far more relaxed, as we have observed in Appendix H.2. While using DCGD/PermK , the server can potentially utilize communication links with Full Duplex mode. Examples of Full Duplex communication buses include PCI-Express, InfiniBand 7 , and some forms of Ethernet (see [63] Table 4.1 in Chapter 4). In our setting, we consider the situation when clients are connected to the Internet, and as it has been described in Appendix H.2, PCI-Express is not a bottleneck. However, in data centers, PCI-Express can also represent a bottleneck [48]. With DCGD/PermK , during some period, a single link can be used simultaneously to obtain information from clients and deliver information to the clients. This is not the case for DCGD/RandK or GD because they require explicit synchronization. But it is the case for DCGD/PermK . This doubles the maximum bandwidth during master client communication and decreases latency by factor two.", "J.4 Potential Benefits in Ring Topology": "This type of topology connects all the nodes circularly. The ring topology is sometimes preferable because it reduces the number of NIC and cables required for connecting multiple devices. Specifically, each device only needs one NIC to participate in all-reduce. Also, ring topology avoids congestion and collisions at the central hub as messages are distributed evenly along the ring. The time delay during performing aggregation for GD or DCGD/RandK in a ring topology represents a sum of delays along the circle and is equal to \ud835\udf0f ring delay = \u02dd \ud835\udc5b \ud835\udc56 = 1 \ud835\udc45\ud835\udc47\ud835\udc47 \ud835\udc56 2 + \ud835\udc51 \ud835\udc35 \ud835\udc56 \u00b7 \ud835\udc4f\ud835\udc5d\ud835\udc5d . Fundamentally, it is because during employing GD or DCGD/RandK , clients can not start any computation for the next iteration until they Federated Learning is Better with Non-Homomorphic Encryption do not compute the whole gradient. For DCGD/PermK clients, after obtaining h \u2207 \ud835\udc53 ( \ud835\udc65 \ud835\udc58 \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61 ) i \ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61 from the neighbor, can start computation for the next iteration.", "J.5 Potential Benefits in Mesh Topology": "This topology connects every node to every other node with dedicated links. It has high bandwidth and fault tolerance but requires a lot of cables and ports and is complex to install. In this form of topology, the DCGD/PermK is the most natural choice because underlying communication topology naturally maps to communication pattern in DCGD/PermK , which can observed if view Algorithm 3 as a series of broadcast operations to reconstitute the global direction for optimization step.", "J.6 Potential Benefits in Tree Topology": "This topology combines multiple star topologies into a hierarchical structure with a root node. In this topology, the broadcasting can be implemented very effectively. One possible vision of our algorithm is that it is implemented without a central server; instead, each client broadcasts information to other clients in Line 6 of Algorithm 3.", "K ACKNOWLEDGEMENTS": "The work of Peter Richt\u00e1rik and Konstantin Burlachenko was supported by the KAUST Baseline Research Scheme (KAUST BRF) and also supported by the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence (SDAIA-KAUST AI). We acknowledge two members of Peter Richt\u00e1rik's Optimization and Machine Learning Laboratory, Alexander Tyurin and Egor Shulgin, for useful and insightful discussions before the start of this project. Federated Learning is Better with Non-Homomorphic Encryption", "L REPRODUCIBILITY": "To ensure reproducibility, we use the following FL_PyTorch simulator [15] features: random seeds were fixed for data reshuffling, and random seeds were fixed for the runtime pseudo-random generators involved in variants of DCGD for randomized compressors. The source code of our experiments is a part of our submission to \"4th International Workshop on Distributed Machine Learning, co-located with CoNEXT 2023\" . If you are interested in the source code for experiments, please either find this publication and supplementary materials on a dedicated conference website 8 9 or contact the authors.", "M LIMITATIONS AND FUTURE RESEARCH": "One limitation of our work is the assumption that clients trust each other. However, it is also presented in HE . The second limitation is that our work studies the case \ud835\udc51 > \ud835\udc5b . Our paper did not provide a rigorous theoretical analysis and focused on practical aspects of the proposed framework. Our method achieves strong privacy guarantees without compromising efficiency or accuracy via leveraging existing Cryptography protocols. For future research, in addition to developing rigorous theory and developing strategy for \ud835\udc51 > \ud835\udc5b , we believe that our work provides a bridge to utilizing other fields of science that work only on the level of bits. In addition to Cryptography another class of methods that operate on a bitwise representation of information is lossless data compression. This line can be investigated within DCGD/PermK framework in future research. Next, as we have described in Appendix I our work potentially opens an extra degree of freedom for research in Systems and Compilers for Deep Learning that investigates different parallelism scheduling strategies for optimize gradient oracles \u2207 \ud835\udc53 \ud835\udc56 ( \ud835\udc65 ) . Our work opens the opportunity to refine Parameter Parallelism schedulers (see Appendix I.3)."}
