{
  "Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction": "",
  "Xiang Xu": "",
  "Hao Wang ∗": "demon@mail.ustc.edu.cn University of Science and Technology of China, State Key Laboratory of Cognitive Intelligence Hefei, China Wei Guo guowei67@huawei.com Huawei Noah's Ark Lab Singapore Luankang Zhang zhanglk5@mail.ustc.edu.cn University of Science and Technology of China, State Key Laboratory of Cognitive Intelligence Hefei, China Yong Liu liu.yong6@huawei.com Huawei Noah's Ark Lab Singapore wanghao3@ustc.edu.cn University of Science and Technology of China, State Key Laboratory of Cognitive Intelligence Hefei, China",
  "Wanshan Yang": "wanshan.yang@colorado.edu Consumer Cloud Service Interactive Media BU, Huawei Shenzhen, China",
  "Defu Lian": "liandefu@ustc.edu.cn University of Science and Technology of China, State Key Laboratory of Cognitive Intelligence Hefei, China",
  "Abstract": "Click-through Rate (CTR) prediction is crucial for online personalization platforms. Recent advancements have shown that modeling rich user behaviors can significantly improve the performance of CTR prediction. Current long-term user behavior modeling algorithms predominantly follow two cascading stages. The first stage retrieves subsequence related to the target item from the long-term behavior sequence, while the second stage models the relationship between the subsequence and the target item. Despite significant progress, these methods have two critical flaws. First, the retrieval query typically includes only target item information, limiting the ability to capture the user's diverse interests. Second, relational information, such as sequential and interactive information within the subsequence, is frequently overlooked. Therefore, it requires to be further mined to more accurately model user interests. To this end, we propose Multi-granularity Interest Retrieval and Refinement Network (MIRRN). Specifically, we first construct queries based on behaviors observed at different time scales to obtain subsequences, each capturing users' interest at various granularities. We then introduce an noval multi-head Fourier transformer ∗ Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '25, Toronto, ON, Canada © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1245-6/25/08 https://doi.org/10.1145/3690624.3709438",
  "Runlong Yu": "yrunl@mail.ustc.edu.cn University of Science and Technology of China, State Key Laboratory of Cognitive Intelligence Hefei, China",
  "Enhong Chen": "cheneh@ustc.edu.cn University of Science and Technology of China, State Key Laboratory of Cognitive Intelligence Hefei, China to efficiently learn sequential and interactive information within the subsequences, leading to more accurate modeling of user interests. Finally, we employ multi-head target attention to adaptively assess the impact of these multi-granularity interests on the target item. Extensive experiments have demonstrated that MIRRN significantly outperforms state-of-the-art baselines. Furthermore, an A/B test shows that MIRRN increases the average number of listening songs by 1.32% and the average time of listening songs by 0.55% on the Huawei Music App. The implementation code is publicly available at https://github.com/USTC-StarTeam/MIRRN.",
  "CCS Concepts": "· Information systems → Recommender systems .",
  "Keywords": "Click-Through Rate Prediction; User Interest Modeling; Long Sequential User Behavior; Recommendation System",
  "ACMReference Format:": "Xiang Xu, Hao Wang, Wei Guo, Luankang Zhang, Wanshan Yang, Runlong Yu, Yong Liu, Defu Lian, and Enhong Chen. 2025. Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1 (KDD '25), August 3-7, 2025, Toronto, ON, Canada. ACM, New York, NY, USA, 11 pages. https://doi.org/ 10.1145/3690624.3709438",
  "1 Introduction": "Click-Through Rate (CTR) prediction is a crucial task in modern online personalization platforms, such as recommendation systems KDD '25, August 3-7, 2025, Toronto, ON, Canada Xiang Xu, et al. and online advertising [3, 12]. Its objective is to estimate the probability of a user clicking on a specific item within a given contextual situation [7, 14]. Recent advancements in CTR prediction have shown that user behavior modeling which incorporates the user's historical behavior sequence to learn implicit interests results in significant performance gain [50, 52, 53]. As online platforms evolve, the length of user behavior sequences has experienced a surge.For instance, on Taobao, one of China's largest e-commerce platforms, over 23% of users have recorded more than 1,000 clicks in the past five months alone [33]. Effectively modeling this massive and intricate user behavior data has emerged as a focal point for both academia and industry. Due to the strict online inference time constraints, previous methods [20, 39, 52, 53] considering their high time complexity, can only utilize the most recent behaviors with the sequence length seldom exceeding 100. This limitation leads to suboptimal results. To address this, several models have been proposed to leverage long behavior sequences for modeling user interests. Most of these models follow two cascading stages [29]. In the first stage, the General Search Unit (GSU) identifies a small number of behaviors most relevant to the target item from thousands of long-term behaviors using fast and coarse search methods. In the second stage, the Exact Search Unit (ESU) captures the relationships between behaviors in these subsequences and the target item to precisely learn user interests. The most critical difference among these models such as SIM [29], UBR4CTR [30], ETA [8], SDIM [4], UBCS [49] and TWIN [5], lies in the different search strategies adopted by GSU. Despite significant improvements in performance and efficiency, most two-stage models [8, 29, 30] focus on limited contextual information in the first stage and neglect to mine the internal relations of subsequences in the second stage. In the first stage, we argue that the retrieval process, where the retrieval query contains only the target item information, accounts solely for target-aware interest. However, if the retrieval query includes relevant information about behaviors at different time scales, more diverse user interests can be mined. Ignoring these \"multi-granularity interests\", weakens the representation power of the learned user interest. For example, consider the user behavior sequence shown in Figure 1. Most models capture target-aware interests, favoring \"shoes\" with a high probability when focusing on the target item \"black sneakers\". However, we believe that coarser-grained user interests, such as \"local-aware\" and \"global-aware\" interests, also impact the final prediction results. Recent clicks on \"blue yoga pants\" and \"blue T-shirt\" indicate the user's local-aware interest in \"sports\". Additionally, broader user behavior sequence indicates the user's global interest in \"black\" items. In this example, these interests in shoes, sports, and black all positively influence the click rate on black sneakers. In the second stage, we believe that the relational information in the subsequence, such as sequential and interactive information, is as valuable as that in the original sequence [20]. It needs to be further mined to model more accurate user interests. In fact, we identify two key challenges in the long-term behavior modeling that require further investigation: (1) Effectively capturing diverse user interests under stringent online inference time constraints presents a non-trivial challenge. (2) Efficiently mining the sequential and interactive information in the subsequences to learn more accurate user interests remains a critical challenge. Figure 1: The left side of the picture depicts a user's behavior sequence, with black sneakers as the target item. The middle column represents the microcosms of the user's multigranularity interests, corresponding to the diverse queries we aim to construct. The right column displays the subsequences related to the multi-granularity interests. black color shoes sports black rain boots black bag black dress red high heels black jeans blue yoga pants blue T-shirt black sneakers Target-aware Interest Local-aware Interest Global-aware Interest blue yoga pants blue T-shirt red high heels black rain boots black jeans User Behavior Sequence Subsequence Subsequence Subsequence black rain boots Target Item To address the aforementioned challenges, we propose M ultigranularity I nterest R etrieval and R efinement N etwork (MIRRN) . Firstly, we propose Multi-granularity Interest Retrieval Module (MIRM) to capture diverse user interests. MIRM constructs queries based on the behaviors observed at different time scales, which contain the target, local, and global information of the behavior sequence, respectively. Using these queries, MIRM adopts a fast search method based on SimHash to handle behavior sequences of thousands of lengths, obtaining subsequences that each contains interest of different granularities. Secondly, we propose Behavior Sequence Refinement Module (BSRM) to extract relational information in subsequences. To incorporate sequential information and efficiently mine interactive information, BSRM employs TargetAware Position Encoding (TAPE) and Multi-Head Fourier Transformer (MHFT), respectively. MHFT utilizes the fast Fourier transform (FFT) to replace the complex convolution operation in the time domain, thereby enabling interaction with a low time complexity of 𝑂 ( 𝑛𝑙𝑜𝑔𝑛 ) . Additionally, MHFT introduces matrix multiplication and multi-head mechanism to enhance interaction and focus on information in different subspaces, while greatly reducing the number of parameters. Finally, we propose Multi-granularity Interest Activation Module (MIAM) using the multi-head target attention to adaptively learn the influence of different granular interests on the target item, obtaining the final user representation. In summary, our main contributions are as follows: · In our proposed MIRRN, MIRM construct diverse queries based on behaviors observed at different time scales to obtain subsequences. This approach enables MIRM to effectively capture multi-granularity interests. To the best of our knowledge, we are the first to successfully capture, refine, and integrate user's multi-granularity interests in long-term user behavior modeling. · In our proposed MIRRN, BSRM adopts the target-ware position encoding and the noval multi-head Fourier transformer (MHFT) to mine sequential and interactive information. MHFT innovatively utilizes the fast Fourier transform, matrix multiplication Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction KDD '25, August 3-7, 2025, Toronto, ON, Canada and multi-head mechanism to efficiently and comprehensively mine interactive information. · Extensive experiments have demonstrated that MIRRN achieves better performance compared to state-of-the-art baselines. We verify the validity of each module through comprehensive ablation studies. Furthermore, an online A/B testing conducted on the Huawei Music App further reinforces the effectiveness of MIRRN in real industrial scenarios.",
  "2 Related Works": "",
  "2.1 Click-Through-Rate Prediction": "Early CTR models primarily concentrate on exploiting feature interactions. Pioneering studies are relatively shallow such as factorization machines(FM) [34] and its variants [19, 26, 27]. With the great success of deep learning, various studies such as Wide&Deep [9], PNN [31], DeepFM [14] and DCN [40, 41], successfully combine deep and shallow models. Moreover, several models [18, 24, 42, 47, 48] have explored more complex neural networks, such as xDeepFM [24] further introduces compressed interaction networks to explicitly model high-order feature interactions. With the increasing demand for personalization, researchers are focusing on user behavior modeling [36, 46, 52-54], which derives the latent representation of users' interest from their behavior sequences. YoutubeDNN [11] performs mean pooling on the entire user sequence. DIN [53] proposes an attention mechanism to identify target-relevant interests. DIEN [52] introduces a novel GRU [10] with an attention update gate (AUGRU) to capture the evolving process of user interests. DSIN [13] observes that user behaviors are highly homogeneous in each session, and heterogeneous cross sessions. DMIN [43] and MIND [22] employ self-attention mechanisms and capsule networks, respectively, to represent user interests as multiple vectors. CAN [51] captures the feature co-action by directing user behavior features into a compact multi-layer perceptron generated by target features.",
  "2.2 Long-Term User Behavior Modeling": "As user behavior modeling [15, 44, 45, 53] has demonstrated remarkable performance in industrial applications, researchers start to focus on modeling increasingly longer behaviors. Due to strict limitations on inference time and model parameter count, DIN-like models [13, 52, 53] are less suitable for long sequence modeling. Recently, various models [4, 5, 8, 28, 29, 49] have been proposed to leverage long behavior sequences for modeling user interests. MIMN [28] stores user behaviors in the memory matrix of the User Interest Center (UIC). However, MIMN does not pay much attention to the target item information. SIM [29] introduces a two-stage cascading framework to overcome above issue. In the first stage, the General Search Unit (GSU) uses a rapid and coarse search method to retrieve several behaviors related to the target item from thousands of long-term behaviors, forming subsequences. In the second stage, the Exact Search Unit (ESU) captures the relationships between behaviors in these subsequences and the target item to precisely learn user interests. The most critical difference between later models lies in the different search strategies adopted by GSU. For example, SIM Hard [29] selects behaviors in the same category as the target item. SIM Soft [29] employs the inner product of pre-trained item embeddings as the relevance measure. UBR4CTR [30] utilizes BM25 algorithm to search for related behaviors. ETA [8] proposes an endto-end model that leverages locality-sensitive hashing [6] (LSH) to encode item embeddings and retrieve relevant items via Hamming distance. SDIM [4] directly gathers behavior items that share the same hash signature with the candidate item, then performs a linear aggregation on them. UBCS [49] adopts a sampling method that considers relevance and temporal information and accelerates the sampling process through clustering. TWIN [5] designed the Consistency-Preserved GSU (CP-GSU) by behavior feature splitting, using the same target behavior correlation measure as ESU. Additionally, several approaches attempt to directly shorten the length of user behaviors. ADFM [23] uses hierarchical aggregation to compress raw behavior sequences. DGIN [25] groups behaviors using a relevant key to remove abundant items. Based on TWIN, TWIN-V2 [37] further employs a hierarchical clustering method to group items with similar characteristics into a cluster. Nevertheless, these models overlook the diversity of user interests and the relationships among behaviors in the subsequences. Our MIRRN focuses on learning multi-granular user interests and efficiently mining sequential and interactive information .",
  "3 Methodology": "The framework of MIRRN is depicted in Figure 2. Our model comprises Embedding Layer, Multi-granularity Interest Retrieval Module, Behavior Sequence Refinement Module, Multi-granularity Interest Activation Module and Prediction Layer. The details of these modules will be introduced in the subsequent subsections.",
  "3.1 Embedding Layer": "Depending on the nature of the input features, we utilize diverse embedding techniques. We categorize input features into two main types: categorical and numeric features. We perform one-hot encoding and multi-hot encoding on categorical features. For numerical features, we first assign them to different numerical buckets and then perform one-hot encoding to distinguish these buckets. It's typical to map all one-hot feature vectors into a lower-dimensional latent vector space. Specifically, we define the embedding vector of the 𝑖 -th behavior in the user behavior sequence 𝑏 𝑖 as 𝑒 𝑖 ∈ R 𝑑 , where 𝑑 represents the embedding size. We consolidate all the embedding vectors of user behavior items into a matrix 𝑆 ∈ R 𝐿 × 𝑑 , representing the embedding representation of the user behavior sequence:  where 𝐿 denotes the length of the user behavior sequence. We denote the embedding of the target item as 𝑒 𝑡 . For other features, we abbreviate them as 𝐸 𝑜𝑡ℎ𝑒𝑟 .",
  "3.2 Multi-granularity Interest Retrieval Module": "Considering the strict limitations on inference time and the number of model parameters for long sequence modeling, we follow previous work [8] using a simple and fast search method to filter out relevant items from the user's long-term behavior sequence to obtain subsequence (SBS). We then accurately model the user's interests within SBS to reduce training and inference costs. However, in previous work, the query in the retrieval stage considers KDD '25, August 3-7, 2025, Toronto, ON, Canada Xiang Xu, et al. Figure 2: The Architecture of our proposed MIRRN. Fast Fourier Transform AVG Inverse Fast Fourier Transform …… X only the target information, resulting in SBS that contain only the user's target-relevant interest. We construct queries based on behaviors observed at different time scales, containing target, local, and global information from the behavior sequence, respectively. These queries serve as microcosms of the user's diverse interests, allowing the retrieved SBSs to contain interests of different granularities. Below, we will introduce the methods for constructing different queries and the fast search units in detail. where the 𝑆𝑒𝑎𝑟𝑐ℎ function receives the behavior sequence and query as input, and outputs related SBS. 3.2.1 Target-Aware Search Unit (TASU). The information contained in the target item is crucial for accurately predicting the user's click rate on it. Therefore, in this unit, we directly use the embedding of the target item as the query 𝑞 𝑡 . Using this query, we can obtain the SBS 𝑆 𝑡 ∈ R 𝐾 × 𝑑 through a fast search method. 𝐾 is the predefined length of the SBS. 𝑆 𝑡 contains the user's most fine-grained interest, which we refer to as target-aware interest.   In our paper, we adopt a Simhash-based search method for its simplicity and efficiency. The SimHash algorithm takes an item's embedding vector as input and transforms it through a hash function into a binary signature as output. Simhash adheres to the locality-sensitive property: if the input vectors are similar, then the output binary signatures will also be similar. Specifically, we employ a fixed random matrix 𝐻 ∈ R 𝑑 × 𝑚 where the elements are generated randomly around 0. Each row of 𝐻 is served as a random hash function. Through matrix multiplication and the sign function, item embedding 𝑒 𝑖 and query embedding 𝑞 𝑡 can be transformed into m-dimensional binary signatures ℎ 𝑖 , ℎ 𝑞 ∈ R 1 × 𝑚 .   Due to the locality-sensitive property, the similarity between an item and a query can be replaced by the similarity between Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction KDD '25, August 3-7, 2025, Toronto, ON, Canada their binary signatures. Therefore, we can use the Hamming distance between the query and item binary signatures as a similarity indicator instead of the inner product of the embedding vectors, thereby reducing the time complexity of calculating similarity. By comparing the Hamming distances of all behaviors to the query, we obtain the indices 𝐼 𝑡 ∈ N 𝐾 of the 𝐾 behaviors with the smallest Hamming distances to the query signature.  Thesub-sequence 𝑆 𝑡 is obtained through 𝐼 𝑡 whenemployingSimhash as the search algorithm:  Let's compare the time complexity of computing the Hamming distance and the inner product of vectors. Assuming that multiplication is considered an atomic operation, the complexity of computing the inner product of two d-dimensional vectors is 𝑂 ( 𝑑 ) . In contrast, computing the Hamming distance between two m-dimensional signatures is approximated as 𝑂 ( 1 ) because this calculation can leverage the XOR operation. Therefore, our method significantly reduces the time complexity from 𝑂 ( 𝑑 ) to 𝑂 ( 1 ) . Additionally, when model training updates item embeddings, the locality-sensitive property of SimHash ensures that the new binary signatures align with the updated item embeddings. Therefore, our search method improves the computational efficiency and ensures accuracy. 3.2.2 Local-Aware Search Unit (LASU). In this unit, we broaden our focus beyond a single item. We make the granularity of observation coarser by using the user's recent behavior sequence to construct the query. Based on this query, which contains recent behavior information, we obtain a subsequence that represents the user's short-term interest, referred to as local-aware interest. It is worth noting that we do not directly use the recent behavior sequence as the SBS, as it may contain noise such as accidental clicks. Additionally, the user's long-term behavior sequence may include other behaviors that align with the local-aware interest. Specifically, we represent the user's recent behavior sequence as 𝑆 𝑟 = [ 𝑒 𝐿 -𝐽 , . . . , 𝑒 𝐿 ] , where 𝐽 is the length of recent behavior sequence. Using average pooling to construct queries is a good choice. Additionally, considering that 𝐽 is very small, we can further model 𝑆 𝑟 to enhance the representation ability of the query with slightly increasing the computational cost. Considering that each behavior is a carrier of current potential interest, we use a GRU to extract a series of hidden states 𝐻 𝑟 ∈ R 𝐽 × 𝑑 from 𝑆 𝑟 . We then combine 𝐻 𝑟 and 𝑆 𝑟 to get the query 𝑞 𝑙 . 𝐴𝑣𝑔 () is the average pooling.   The query 𝑞 𝑙 guides the retrieval of the relevant SBS 𝑆 𝑙𝑜𝑐𝑎𝑙 ∈ R 𝐾 × 𝑑 using the same SimHash method employed in the targetaware search unit as described in eqs. (4) to (7):  Since 𝑞 𝑙 mines the semantic information of the user's recent behaviors, 𝑆 𝑙 retrieved through 𝑞 𝑙 align with the user's local-aware interest, which is a coarser-grained interest. 3.2.3 Global-Aware Search unit (GASU). In this unit, we expand the focus to encompass the entire behavior sequence to consider the user's more global and stable interest. Constructing a query using the user's entire long-term behavior sequence is challenging. On the one hand, directly average pooling the long sequence may result in poor query representation due to its length and the noise within the sequence. On the other hand, constructing the query with more complex modeling of the long sequence cannot meet the strict time limits of an online system. Additionally, for new users with relatively short behavior sequences, it is difficult to obtain their global and stable preferences. To address the above issues, we employ clustering methods to aggregate the item set 𝑀 into several clusters using pre-trained item embeddings. Then we map the target item to the nearest cluster centroid in order to construct a query, which effectively and approximately captures the global preferences. Additionally, leveraging the semantic information of pre-trained embeddings helps to partially solve the user cold start problem. We adopt a simplified Density Peak Clustering [35] (DPC) algorithm that doesn't require multiple iterations or additional parameters, efficiently identifying clusters of arbitrary shapes based on density connectivity. DPC assumes that a cluster center is surrounded by low-density neighbors and maintains a relatively large distance from high-density points. For each item, we calculate its density 𝜌 and the minimum distance 𝛿 to items with higher density.   The cluster center score of an item is set to 𝜌 × 𝛿 . DPC selects 𝐶 items with the highest scores as cluster centers and assigns the remaining items to the closest cluster center. In this unit, we construct the query 𝑞 𝑔 using the aforementioned clustering algorithm and average pooling. The query 𝑞 𝑔 guides the retrieval of the relevant SBS 𝑆 𝑔 ∈ R 𝐾 × 𝑑 using the same SimHash method as described in eqs. (4) to (7):   where 𝐷𝑃𝐶 () means assigning 𝑒 𝑡 to the nearest cluster center. Since the cluster assignment for each item is precomputed, the complexity of it is 𝑂 ( 1 ) during inference. Since 𝑞 𝑔 contains the semantic information of the user's global preferences, 𝑆 𝑔 retrieved through 𝑞 𝑔 align with the user's global-aware interest, which is the most coarse-grained interest.",
  "3.3 Behavior Sequence Refinement Module": "After obtaining the subsequences that contains the user's multigranularity interests, we aim to further mine sequential and interactive information to more accurately learn the user's diverse interests. For convenience, we unify 𝑆 𝑡 , 𝑆 𝑙 , 𝑆 𝑔 as 𝑆 ∗ ∈ R 𝐾 × 𝑑 . KDD '25, August 3-7, 2025, Toronto, ON, Canada Xiang Xu, et al. 3.3.1 Target-Aware Position Encoding (TAPE). Previous work has rarely emphasized the order of behaviors in SBSs, possibly sorting them by relevance in the retrieval module. However, we believe that the sequential information within SBSs plays a crucial role, much like the original sequence, in helping us better understand and learn the user's interests. Therefore, we preserve the original sequence of behaviors in SBS and use position encoding to supplement the sequential relationship, compensate for the information loss of the retrieval module, and enhance the expressive power of 𝑆 ∗ . We use the relative position of each behavior in 𝑆 ∗ to the target item to construct the position encoding. Rather than the widely used chronological order encoding [20], this method better emphasizes the attention on the target information. Specifically, we propose the Target-Aware Position Encoding 𝑇𝐴𝑃𝐸 ∈ R 𝐿 × 𝑑 . We then convert the retrieved indices 𝐼 in equation (6) to target-aware indices 𝐼 ′ . Finally, we obtain the target-aware position embeddings through a look-up process and incorporate them into 𝑆 ∗ :  3.3.2 Multi-Head Fourier Transformer (MHFT). After applying the above TAPE, our model has obtained the sequential information in SBSs. To efficiently learn the interactive information with less additional computational cost, we leverage the efficient Fourier Transformer . According to the convolution theorem [38], the product operator in the frequency domain is equivalent to global circular convolution in the time domain, which is able to extract interactive information and refine the sequence representation. It also implies that complex convolution operations can be implemented through fast Fourier transform (FFT) with a time complexity of 𝑂 ( 𝐾𝑙𝑜𝑔𝐾 ) and multiplication operations [21, 32]. Compared with the selfattention mechanism with a time complexity of 𝑂 ( 𝐾 2 ) , this method explores interactive information while improving efficiency. Specifically, we perform Fourier transform along the sequence length dimension to transform 𝑆 ∗ to 𝑋 ∈ C 𝐾 × 𝑑 which represents the frequency domain representation of 𝑆 ∗ . We then implement the convolution fusion of behaviors and mine interactive information by performing the element-wise multiplication with a complex parameter matrix 𝑊 ∈ C 𝐾 × 𝑑 [55]. Finally, we obtain the full convolution sequence representation e 𝑆 ∗ ∈ R 𝐾 × 𝑑 through inverse fast Fourier transform (IFFT). The formulas are as follows:  Additionally, inspired by the multi-head self attention mechanism, we further propose the Multi-Head Fourier Transformer . To achieve better information fusion, we first use matrix multiplication instead of element-wise multiplication in equation (16), using 𝑊 ∈ C 𝐾 × 𝑑 × 𝑑 instead of 𝑊 ∈ C 𝐾 × 𝑑 . Then we shares weights 𝑊 ∈ C 𝑑 × 𝑑 among different tokens, to adaptively handle sequences of different lengths while reducing the parameter count. To mine the interactive information from different subspaces, we divide 𝑋 ∈ C 𝐾 × 𝑑 into 𝑛 parts 𝑥 𝑖 ∈ C 𝐾 × 𝑑 / 𝑛 and diagonalize the weight 𝑊 ∈ C 𝑑 × 𝑑 into 𝑛 blocks 𝑊 𝑖 ∈ C 𝑑 / 𝑛 × 𝑑 / 𝑛 . Each block of 𝑋 and the diagonal matrix is equivalent to a head of the multi-head self attention. The diagonal weights can be computed in parallel. We implement them through an MLP with an activation function 𝜎 to introduce nonlinear relationships. The formulas are as follows:   In this way, we further mine the interactive information from different subspaces adequately and efficiently. Following previous work [20], we also integrate skip connections [17] and layer normalization [2] to alleviate gradient vanishing and improve network stability. The formulas are as follows:  Through Target-Aware Position Encoding and the Multi-Head Fourier Transformer, we efficiently and comprehensively extract the sequential and interactive information in SBSs, resulting in refined sequence representations e 𝑆 ∗ . Subsequently, we perform an average pooling aggregation operation to obtain the vector representation of the user's interest 𝐸 ∗ ∈ R 𝑑 :",
  "3.4 Multi-granularity Interest Activation Module": "After applying the above modules, we acquire the user's interest vectors 𝐸 𝑡 , 𝐸 𝑙 , 𝐸 𝑔 at different granularities, capturing target-aware, local-aware, and global-aware interest, respectively. However, it is unclear how much each interest contributes to the click on the target item. Therefore, we utilize multi-head target attention to adaptively learn the influence of interests at different granularities on the target item, obtaining the user representation 𝐸 𝑢 :     where ℎ donates the number of heads and 𝑊 𝑄 𝑖 , 𝑊 𝐾 𝑖 , 𝑊 𝑉 𝑖 , 𝑊 𝑂 are all learnable parameters.",
  "3.5 Prediction Layer": "For the final CTR prediction, we simply concatenate the target item embedding 𝑒 𝑡 , user interest representation 𝐸 𝑢 and other feature vectors 𝐸 𝑜𝑡ℎ𝑒𝑟 into the MLP layer:  As CTR prediction is typically formulated as a binary classification problem, we train our model under the Cross-Entropy loss:  where 𝑁 is the number of instances.",
  "3.6 Complexity Analysis": "We conduct a detailed analysis and comparison of the complexity of the two core modules of our MIRRN in this subsection. Assume that a user request with a batch size of 𝐵 and the multiplication is treated as an atomic operation. Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction KDD '25, August 3-7, 2025, Toronto, ON, Canada Table 1: Time complexity and parameter count for MHSA, FT and MHFT. 3.6.1 Multi-granularity Interest Retrieval Module (MIRM). The time complexity of query construction for TASU, LASU and GASU is approximately 𝑂 ( 1 ) , 𝑂 ( 𝐽𝑑 2 ) and 𝑂 ( 1 ) respectively. Given the slow training speed of GRU, we omit it in practical applications, allowing the time complexity for LASU to be regarded as 𝑂 ( 1 ) . Then we employ the SimHash-based search method within these units, which requires the hash signatures of the user behaviors and the candidate items. Notably these hash signatures can be stored in a table before inference, enabling a lookup operation during inference to obtain the hash signatures of behaviors in 𝑂 ( 1 ) time. The hashing of candidate items has a time complexity of 𝑂 ( 𝐵𝑚𝑑 ) which can be optimized to 𝑂 ( 𝐵𝑚𝑙𝑜𝑔𝑑 ) [1]. Finally, the time complexity for filtering behaviors is 𝑂 ( 𝐵𝐿 ) as we can calculate the Hamming distance in 𝑂 ( 1 ) through the XOR operation. Overall, the time complexity of MIRM is 𝑂 ( 𝐵𝐿 + 𝐵𝑚𝑙𝑜𝑔𝑑 ) , a level that has been proven to be acceptable in real industrial scenarios [4, 8]. 3.6.2 Behavior Sequence Refinement Module (BSRM). The time complexity of Target-Aware Position Encoding is 𝑂 ( 1 ) due to the lookup operation. And the time complexity of Multi-Head Fourier Transformer (MHFT) is 𝑂 ( 𝐾𝑑𝑙𝑜𝑔𝐾 + 𝐾𝑑 2 / 𝑛 ) . While MIRRN introduces additional time complexity due to BSRM, compared to other long-term user behavior modeling methods, the increase in inference time is relatively small since 𝐾 ≪ 𝐿 . This will be further verified in Section 4.4. Additionally, we analyze the complexity of MHFT in comparison to Fourier Transformer (FT) and Multi-Head Self Attention (MHSA), as shown in Table 1. Our findings reveal that FT and MHFT exhibit lower time complexity than the widely used MHSA. Moreover, MHFT has the advantage of requiring the fewest parameters among these three methods.",
  "4 Experiments": "",
  "4.1 Experimental Settings": "4.1.1 Datasets. We conduct comprehensive experiments on three real-word and large-scale datasets. The statistical details of these three datasets are presented in Table 2. · Taobao 1 : This dataset encompasses user behavior records from November 25 to December 3, 2017, drawn from one of China's largest e-commerce platforms. · Alipay 2 : The dataset is collected through Alipay, an online payment application. It covers users' online shopping behaviors spanning from July 1, 2015, to November 30, 2015. 1 https://tianchi.aliyun.com/dataset/dataDetail?dataId=649 2 https://tianchi.aliyun.com/dataset/dataDetail?dataId=53 Table 2: The dataset statistics. · Tmall 3 : The dataset is provided by Alibaba Group which contains user behavior history on Tmall e-commerce platform from May 2015 to November 2015. 4.1.2 Evaluation Metrics. In our experiments, we use the AUC (Area Under ROC) score as the evaluation metric, which is the most commonly used metric in CTR tasks. AUC measures the ability of the model to prioritize positive samples over negative samples. A larger AUC value indicates better performance in predicting CTR. 4.1.3 Baselines. To evaluate the effectiveness of MIRRN, we conduct a comprehensive comparison with 14 state-of-the-art CTR prediction baseline models. We divide them into three categories. The first category is the feature interaction models, including DNN [11], Wide&Deep [9], and PNN [31]. The second category is the shortterm behavior modeling models, including DIN [53], DIEN [52], DSIN [13], DMIN [43], and CAN [51]. The third category is the long-term behavior modeling models, including MIMN [28], SIM Hard [29], SIM Soft [29], ETA [8], SDIM [4], and TWIN [5]. 4.1.4 Implementation Details. For the three datasets, we adopt the same data preprocessing method as MIMN [28]. All samples are divided into a training set (80%), a validation set (10%), and a test set (10%) based on the click time of the target item. For the short-term user behavior modeling model, we use the user's most recent 100 behaviors. For the long-term user behavior modeling model, we use the user's most recent 300 behaviors. For a fair comparison, except for the behavior sequence modeling module, our model and all baselines retain the same network structure. It is worth noting that in our proposed MIRRN, there are three search units. To compare fairly with the two-stage retrieval models, we ensure that the total number of behaviors retrieved by MIRRN is the same as theirs. The MLPstructure of the final prediction layer of all models is 200×80×2, and the activation function is prelu [16]. Each feature dimension is set to 16. The batch size for all models is 256. During model training, we use the Adam optimizer with a fixed learning rate of 0.001 and train for 1 epoch. To ensure the fairness of the experiment, we optimize all baselines according to their original papers and repeat the experiments 10 times to obtain a more stable evaluation.",
  "4.2 Overall Performance": "In Table 3, we present a thorough performance comparison on three public datasets. The following conclusions can be drawn: (1). Our proposed MIRRN consistently outperforms all baselines across three public datasets, illustrating its effectiveness. Specifically, MIRRN achieves AUC improvements of 0.67%, 1.35%, and 0.51% improvements over the strongest baseline on the Taobao, Alipay and Tmall datasets, respectively. 3 https://tianchi.aliyun.com/dataset/dataDetail?dataId=42 KDD '25, August 3-7, 2025, Toronto, ON, Canada Xiang Xu, et al. Table 3: Overall performance of various methods on three public datasets. \"Imp.\" denotes the improvement rate of MIRRN over the strongest baseline. An asterisk (*) indicates statistical significance (with p<0.05) when comparing MIRRN to the best baseline results. (2). The top two strongest baselines on the three datasets predominantly come from models on long-term behavior modeling. Among the three types of baselines, long-term behavior modeling models generally deliver the best performance, followed by short-term behavior modeling models. The experiments demonstrate that modeling user interest significantly enhances CTR model performance. Furthermore, leveraging more extensive user historical behaviors can lead to substantial performance gains. (3). Among the short-term behavior modeling models, DMIN achieved the best performance on two datasets. DMIN emphasizes that users typically have multiple interests. It uses multi-head selfattention to implicitly extract multiple user interests. The strong performance of DMIN highlights the effectiveness of learning users' multiple interests, which is similar to the motivation behind our model. Our proposed MIRRN further enhances performance by efficiently utilizing users' long-term behaviors and explicitly capturing diverse interests, thereby surpassing DMIN. (4). Among the long-term behavior modeling baselines, SIM Soft and TWIN achieve strong results. SIM Soft uses the dot product of pre-trained embeddings as the relevance metric, while TWIN maintains consistency in relevance metrics during the two-stage interest modeling. However, both methods focus solely on the user's target-relevant behaviors during the first stage and lack the mining of relational information within subsequences during the second stage. In contrast, MIRRN considers the user's multi-granularity interests during the retrieval stage and effectively mines sequential and interactive information within subsequences, resulting in superior performance. These points will be further verified in the subsequent ablation study. Table 4: Ablation study of different search units. Table 5: Ablation study of core components in MIRRN.",
  "4.3 Ablation Study": "We conduct comprehensive ablation experiments to verify the effectiveness of each module and support our ideas. (1). We assess the effectiveness of capturing multi-granularity interests by evaluating the three search units in MIRM, which are denoted as TASU, LASU and GASU. The experimental results, presented in Table 4, are obtained with a constant total number of retrieved behaviors. Notably, the model using only TASU performs relatively well, which may explain why previous models focusing solely on target-aware interest still achieved notable results. In contrast, models using only LASU or GASU exhibit poor performance. Moreover, any model lacking TASU experiences a significant performance decline, underscoring the critical importance of TASU. Nevertheless, adding any of the search units consistently enhances performance, demonstrating the value of capturing users' diverse interests. Our proposed MIRRN, which employs all three search units, effectively captures interests at different granularities, leading to the best performance. (2). We validate the effectiveness of BSRM and MIAM by separately removing each component. The results, as shown in Table 5, reveal that removing either BSRM or MIAM results in a performance decrease. It indicates that both mining the relational information of behaviors and adaptively learning the impact of multi-granularity interests on the target item are crucial. (3). We further verify the effectiveness of two key components, TAPE and MHFT, in BSRM, which are responsible for mining sequential and interactive information respectively. The results in Table 5 demonstrate that both sequential information in TAPE and interactive information in MHFT positively influence performance. (4). We conduct comparative experiments on three methods for mining interactive information in sequences: FT, MHFT and MHSA. Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction KDD '25, August 3-7, 2025, Toronto, ON, Canada Figure 3: Inference time on the test set of three datasets. DIN DIEN SIM ETA SDIM MIRRN 0 5 10 15 20 25 Inference Time (s) Taobao DIN DIEN SIM ETA SDIM MIRRN 0 5 10 15 20 25 30 Inference Time (ms) Alipay DIN DIEN SIM ETA SDIM MIRRN 0 5 10 15 20 25 30 35 40 45 Inference Time (ms) Tmall The results in Table 5 indicate that our proposed MHFT achieves performance comparable to MHSA and consistently outperforms FT. MHFT even achieves the best results on two datasets. Notably, MHFT exhibits lower time complexity and requires fewer parameters as explained in Section 3.6.2. Therefore, MHFT demonstrates superiority among these three methods.",
  "4.4 Inference Time Analysis": "To demonstrate the practical value of our model, we compare the inference time across various models, including DIN, DIEN, SIM, ETA, SDIM, and MIRRN. As shown in Figure 3, SIM and SDIM exhibit relatively low inference times among the long sequence modeling methods. Our method introduces additional modules, leading to a slight increase in inference time. However, compared to the fastest baseline, our model only increases inference time by 12.0%, 11.9%, and 10.1% across the three datasets, while significantly improving AUC by 1.45%, 1.51%, and 0.58% over the strongest baseline.",
  "4.5 Online A/B Test": "We further conduct an online A/B test on the core playlist recommendation channel of the Huawei Music App, which is utilized by millions of users daily. Figure 4 gives a simple illustration of the deployed system, which encompasses both the offline training and online inference processes. During the offline phase, the system logs hundreds of millions of user interactions to refine the prediction model, subsequently uploading the updated model to the online server for real-time user inference. In the online phase, upon receiving a new request, the online servers retrieve the relevant features and forward them to the ranking server, which then generates and presents a personalized recommendation list to users. The baseline employed is a highly optimized deep CTR model. To ensure a fair comparison, identical input features are utilized across both the baseline model and our proposed model. An online A/B test is conducted over one week, involving 20% of users selected at random. Of these users, half are designated to the experimental group while the remaining half are reserved as the control group. The evaluation is based on two metrics: the average number of listening songs (ANoLS) and the average time of listening songs (AToLS). The outcomes indicated that our model achieved an improvement of 1.32% in ANoLS and 0.55% in AToLS. Considering that an increment of 1% in listening time is regarded as a substantial enhancement, our model realizes a significant business gain. Figure 4: A simple illustration of the deployed playlist recommendation system in Huawei. Rank Sever Model Server LOGS OFFLINE ONLINE User ID User Feature Interaction Impression Request Retrieve User Behavior User Portrait Item ID Item Feature Item Side Info Item ID",
  "5 Conclusion": "In this paper, we proposed the Multi-granularity Interest Retrieval and Refinement Network (MIRRN) for long-term user behavior modeling in CTR prediction. To the best of our knowledge, MIRRN is the first to successfully capture, refine, and integrate the user's multigranularity interests. Specifically, we constructed multi-granularity queries, extracting subsequences that capture diverse interests. We then utilized Target-Aware Position Encoding and Multi-Head Fourier Transformer to efficiently and effectively mine sequential information and interactive information within these subsequences, obtaining the refined interest vectors. Finally, we employed multihead target attention to adaptively learn the influence of different granular interests on the target item. Experiments on three public datasets, along with an online A/B test validated the effectiveness of MIRRN. In the future, we plan to consider user interests at additional granularities tailored to different scenarios and explore more efficient retrieval methods to further reduce time complexity.",
  "6 Acknowledgments": "This work is supported by the National Natural Science Foundation of China (No. U23A20319, 62472394, and 62202443). And we thank MindSpore for the partial support of this work, which is a new deep learning computing framework. We would like to express our gratitude to FuxiCTR [56, 57] for providing an open-source library for CTR prediction.",
  "References": "[1] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. 2015. Practical and optimal LSH for angular distance. Advances in neural information processing systems 28 (2015). [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 (2016). [3] Giorgio Brajnik and Silvia Gabrielli. 2010. A review of online advertising effects on the user experience. International Journal of Human-Computer Interaction 26, 10 (2010), 971-997. [4] Yue Cao, Xiaojiang Zhou, Jiaqi Feng, Peihao Huang, Yao Xiao, Dayao Chen, and Sheng Chen. 2022. Sampling is all you need on modeling long-term user behaviors for CTR prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 2974-2983. [5] Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, et al. 2023. TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou. arXiv preprint arXiv:2302.02352 (2023). [6] Moses S. Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the Thiry-Fourth Annual ACM Symposium on Theory of KDD '25, August 3-7, 2025, Toronto, ON, Canada Xiang Xu, et al. [50] Chang Zhou, Jinze Bai, Junshuai Song, Xiaofei Liu, Zhengchao Zhao, Xiusi Chen, and Jun Gao. 2018. Atrank: An attention-based user behavior modeling framework for recommendation. In Proceedings of the AAAI conference on artificial intelligence , Vol. 32. Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction KDD '25, August 3-7, 2025, Toronto, ON, Canada [51] Guorui Zhou, Weijie Bian, Kailun Wu, Lejian Ren, Qi Pi, Yujing Zhang, Can Xiao, Xiang-Rong Sheng, Na Mou, Xinchen Luo, et al. 2020. CAN: revisiting feature coaction for click-through rate prediction. arXiv preprint arXiv:2011.05625 (2020). [52] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 5941-5948. [53] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1059-1068. [54] Haolin Zhou, Junwei Pan, Xinyi Zhou, Xihua Chen, Jie Jiang, Xiaofeng Gao, and Guihai Chen. 2024. Temporal Interest Network for User Response Prediction. In Companion Proceedings of the ACM on Web Conference 2024 . 413-422. [55] Kun Zhou, Hui Yu, Wayne Xin Zhao, and Ji-Rong Wen. 2022. Filter-enhanced MLP is all you need for sequential recommendation. In Proceedings of the ACM web conference 2022 . 2388-2399. [56] Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, and Rui Zhang. 2022. BARS: Towards Open Benchmarking for Recommender Systems. In SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 , Enrique Amigó, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 2912-2923. doi:10.1145/3477495.3531723 [57] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. Open Benchmarking for Click-Through Rate Prediction. In CIKM '21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021 , Gianluca Demartini, Guido Zuccon, J. Shane Culpepper, Zi Huang, and Hanghang Tong (Eds.). ACM, 27592769. doi:10.1145/3459637.3482486",
  "keywords_parsed": [
    "Click-Through Rate Prediction",
    "User Interest Modeling",
    "Long Sequential User Behavior",
    "Recommendation System"
  ],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "Practical and optimal LSH for angular distance"
    },
    {
      "ref_id": "b2",
      "title": "Layer normalization"
    },
    {
      "ref_id": "b3",
      "title": "A review of online advertising effects on the user experience"
    },
    {
      "ref_id": "b4",
      "title": "Sampling is all you need on modeling long-term user behaviors for CTR prediction"
    },
    {
      "ref_id": "b5",
      "title": "TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou"
    },
    {
      "ref_id": "b6",
      "title": "Similarity estimation techniques from rounding algorithms"
    },
    {
      "ref_id": "b50",
      "title": "Atrank: An attention-based user behavior modeling framework for recommendation"
    },
    {
      "ref_id": "b51",
      "title": "CAN: revisiting feature coaction for click-through rate prediction"
    },
    {
      "ref_id": "b52",
      "title": "Deep interest evolution network for click-through rate prediction"
    },
    {
      "ref_id": "b53",
      "title": "Deep interest network for click-through rate prediction"
    },
    {
      "ref_id": "b54",
      "title": "Temporal Interest Network for User Response Prediction"
    },
    {
      "ref_id": "b55",
      "title": "Filter-enhanced MLP is all you need for sequential recommendation"
    },
    {
      "ref_id": "b56",
      "title": "BARS: Towards Open Benchmarking for Recommender Systems"
    },
    {
      "ref_id": "b57",
      "title": "Open Benchmarking for Click-Through Rate Prediction"
    }
  ]
}