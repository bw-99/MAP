{"PEACE: Prototype lEarning Augmented transferable framework for Cross-domain rEcommendation": "Chunjing Gan \u2217 Ant Group cuibing.gcj@antgroup.com Binbin Hu Ant Group bin.hbb@antfin.com Jian Ma Ant Group mj.mj@antgroup.com Jun Zhou Ant Group jun.zhoujun@antfin.com Bo Huang \u2217 Ant Group yunpo.hb@antgroup.com Ziqi Liu Ant Group ziqiliu@antgroup.com Guannan Zhang Ant Group zgn138592@antgroup.com", "ABSTRACT": "Zhiqiang Zhang Ant Group lingyao.zzq@antfin.com Wenliang Zhong \u2020 Ant Group yice.zwl@antgroup.com", "KEYWORDS": "To help merchants/customers to provide/access a variety of services through miniapps, online service platforms have occupied a critical position in the effective content delivery, in which how to recommend items in the new domain launched by the service provider for customers has become more urgent. However, the non-negligible gap between the source and diversified target domains poses a considerable challenge to cross-domain recommendation systems, which often leads to performance bottlenecks in industrial settings. While entity graphs have the potential to serve as a bridge between domains, rudimentary utilization still fail to distill useful knowledge and even induce the negative transfer issue. To this end, we propose PEACE , a Prototype lEarning Augmented transferable framework for Cross-domain rEcommendation. For domain gap bridging, PEACE is built upon a multi-interest and entity-oriented pre-training architecture which could not only benefit the learning of generalized knowledge in a multi-granularity manner, but also help leverage more structural information in the entity graph. Then, we bring the prototype learning into the pre-training over source domains, so that representations of users and items are greatly improved by the contrastive prototype learning module and the prototype enhanced attention mechanism for adaptive knowledge utilization. To ease the pressure of online serving, PEACE is carefully deployed in a lightweight manner, and significant performance improvements are observed in both online and offline environments.", "CCS CONCEPTS": "", "\u00b7 Information systems \u2192 Recommender systems .": "* Equal contribution. \u2020 Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM '24, March 4-8, 2024, Merida, Mexico \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0371-3/24/03...$15.00 https://doi.org/10.1145/3616855.3635781 Service Platforms; Cross-domain Recommendation; Prototype Learning", "ACMReference Format:": "Chunjing Gan, Bo Huang, Binbin Hu, Jian Ma, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Guannan Zhang, and Wenliang Zhong. 2024. PEACE: Prototype lEarning Augmented transferable framework for Cross-domain rEcommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining (WSDM '24), March 4-8, 2024, Merida, Mexico. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3616855.3635781", "1 INTRODUCTION": "Recently, online service platforms ( e.g., Alipay, WeChat and Tiktok) have become increasingly prevalent, which attract merchants/customers to provide/access a variety of services through miniapps. However, making proper recommendations [4, 12, 13, 19, 32] to satisfy the effective content delivery requirements on real-world service platforms still encounter several intractable issues. In the case of Alipay, which is known as an integrated platform involving multifarious miniapps, various online providers are able to effortlessly launch new scenarios in the form of miniapps ( e.g., Travel and Rental). Towards new domains, it is time-consuming and even impractical to train a recommender from scratch with extremely scarce interactions. Fortunately, service platforms generally accumulate considerable global behaviors ( e.g., search and click on the homepage feed), which shed some light on extracting universal and transferable knowledge for warming up brand-new scenarios. As a promising direction, cross-domain recommendation (CDR) has attracted a surge of investigations, which enables the effective learning of a data-sparser domain by transferring useful knowledge from data-richer domains. Existing CDR methods often assume the existence of shared information so that a mapping function can be learned across different domains [27, 31, 39, 58] or availability of source and target domains for joint optimization [20, 23, 56]. However, this assumption may not hold in real-world applications due to the considerable gap between source and target domains or even unavailability of target domains data during training, which severely hinders the application of existing CDR approaches. Recently, several approaches seek to bridge the domain gap via side information such as entity (knowledge) graph and bring the superiority of pre-training [6, 11, 53] into cross-domain recommendation. Specifically, [8, 14, 26, 37, 64] learn a universal user representation by bridging user's behaviors in multiple source domains via entity graph, text and so on, with the expectation that the obtained universal user representations contain rich information, and can be transferred to various downstream target scenarios directly or with careful fine-tuning. Despite considerable performance improvement in social and e-commerce platforms, rudimentary utilization of multiplex knowledge may fail to distill useful knowledge and even induce the negative transfer issue in online service platforms integrated with a wide variety of services/miniapps. Given the above limitations in current approaches, we comprehensively explore and effectively exploit rich interactions associated side information in multiple source domains to learn universal and transferable knowledge for a variety of unseen downstream domains. However, the solution is quite non-trivial, which needs to tackle the following essential challenges in industrial environments: i) The online service platform includes multifarious scenarios from various service providers with unsharing information and related items without identical properties. Therefore, it is essential to close this gap and utilize multiplex data sources to develope multi-granularity representations to enhance the capability of pre-training. ii) The prevalence of entity (knowledge) graph in real-world service platforms encourages us to incorporate it as the bridge for domain gap. However, entity graph in real-world applications is huge, containing considerable ambiguous entities or related entities without identical properties, simply aggregating entity-wise context for universal and transferable knowledge may easily result in a performance bottleneck and render the approach vulnerable to underlying noises. iii) The scenario-based context that implies varying semantics w.r.t. different interaction scenarios, plays a vital role in capturing users' underlying preferences, which is expected to be incorporated into user representation in an adaptive manner. Tothis end, we propose PEACE , a Prototype lEarning Augmented transferable framework for Cross-domain rEcommendation. In particular, the pre-training stage of PEACE is built upon multiinterest based and entity-oriented architecture, in which users and entities are paired for learning generalized knowledge and multigranularity representations distilled from multiplex interactions in source domains. To further enhance the capability of learning universal and transferable knowledge, PEACE introduces the prototype learning, so that i) the derived contrastive prototype learning component greatly improves the universal knowledge learning by attracting entities with similar characteristics and repelling unrelated entities; ii) the user-side representation learning is aware of scenario-based context in an adaptive manner through the proposed prototype enhanced attention mechanism. In the fine-tuning stage, after effortlessly loading pre-trained parameters, PEACE represents users with the universal encoding and demystifies target items in target domains with specific entities, followed by carefully fine-tuning for normal and strictly zero-shot recommendation in a lightweight manner. To sum up, we make the following main contributions: i) We highlight the practical challenges of transferable recommendation in real-world service platforms, which are mainly caused by the large gap between source and target domains, coupled with multiplex knowledge involved. To our knowledge, we take the first step to employ prototype learning for cross-domain recommendation for the extraction of universal and transferable knowledge. ii) We devise a novel transferable framework PEACE in a multi-interest based and entity-oriented pre-training manner, which is equipped with contrastive prototype learning and prototype enhanced attention to substantially facilitate the universal knowledge learning for entities and the scenario-aware representation for users, respectively. Moreover, to ease the pressure of online serving, PEACE is carefully deployed in a lightweight manner. iii) We conduct extensive experiments in both offline and online environments, which demonstrate that PEACE significantly surpasses a series of stateof-the-art approaches for cross-domain recommendation in both normal and zero-shot setting. And in-depth analysis also reveals the superiority of prototype learning based components, with which the distance of similar entities in the representation space can be potentially narrowed down.", "2 RELATED WORK": "Graph based Recommendation Recently, a surge of attention has been dedicated to graph based recommendation, which flexibly characterizes various kinds of auxiliary context ( e.g., collaborative graphs [12, 41], social graphs [7, 51], knowledge graphs [17, 40] and heterogeneous graphs [15, 28]) for facilitating recommendation. Early studies carry structural information based on extracting and encoding of numerous paths in graphs [15, 43]. By taking full advantage of the information propagation, graph neural networks [16, 45, 49] help graph-based recommenders explicitly model the high-order connectivities in graphs in an end-to-end fashion [48]. More recently, several works encourage the independence of different user intents/interests behind graphs in representation learning for better model capability [22, 42]. Although these methods usually achieve promising performance, they either require the availability of data from both source and target domains or cannot bridge isolated items from different domains, especially for unknown items in anonymous target domains, thus failing to make transferable recommendation in real-world service platforms when target domain items are unavailable. Cross-domain Recommendation Cross-domain recommendation [54, 59] aims at alleviating the long-standing cold-start issue in recommendation by leveraging the rich interactions in source domains to help the learning of recommenders in target domains. A series of studies propose to learn a shared mapping matrix, and then perform alignment and transformation between source and target users/items [18, 24, 25, 31, 58]. Recently, due to the impressive and rapid adaptability of meta learning to new tasks with only few data, recent attentions have been shifted towards the effective incorporation of meta networks, which focus on bridging user preference in different domains [63], including TMCDR [61] that is based on the popular Model-Agnostic Meta-Learning (MAML) and PTUPCDR [63] that models personalized bridge with users' preferences and characteristics. However, all the methods above are incapable of leveraging graph structural information to enhance the representation of both source and target domains. Moreover, Source / Target domain Entity graph Item ID: ELE5879436 Item Name: Hot-selling Apple computers with limited-time discount Category: Electronics Price: \u00b7 25000 Apple Samsung Computer Electronics 5G Phone Phone Related Product Related Brand Screen Required Component Chip Mobile device \u2026 ! Related Brand Related Technology Related Technology Related Product Required Component Sub-category \u2026 most of the current approaches commonly involve interactions of target domains in the training process, which ignore a practical fact in real-world service platforms that items are completely nonoverlapping across different domains and items in target domains are totally unavailable in the training stage. Pre-training for Recommendation Given the unprecedented success in learning useful representations with self-supervised signals, the pre-training technique has attracted a surge of investigation on recommendations. Relying on the recent development of self-supervised learning, many efforts have been made to pretrain a powerful recommender based on various auxiliary tasks, including sequence-level masking [9, 29, 33], item-level [50, 57] and graph-level [47, 52] contrastive learning. In terms of the pretraining for cross-domain recommendation, several works have attempted to learn transferable interest based on sequence [53], collaborative graph [37] and knowledge graph [64]. However, existing methods cannot handle the large gap between source and target domains, coupled with multiplex behaviors across domains. Although knowledge graphs are introduced in current approaches to connect isolated items, the entity-wise context is still insufficient to learn universal and transferable knowledge with the huge and noisy graph in real-world service platforms.", "3 PRELIMINARIES": "Let S and T denote the set of source and target domains, we define each source domain as S \u2208 S with the user set U S and the item set I S , comprised of interaction records H S = { \ud835\udc62, \ud835\udc56, B \ud835\udc62\ud835\udc56 , \ud835\udc66 \ud835\udc62\ud835\udc56 | \ud835\udc62 \u2208 U S , \ud835\udc56 \u2208 I S } . Here, B \ud835\udc62\ud835\udc56 \u2282 I S denotes historical behaviors ( i.e., item list) for user \ud835\udc62 when item \ud835\udc56 is recommended and \ud835\udc66 \ud835\udc62\ud835\udc56 \u2208 { 0 , 1 } is the feedback of user w.r.t. item i ( e.g., \ud835\udc66 \ud835\udc62\ud835\udc56 = 1 when clicking, otherwise 0). Analogically, we define each target domain as T \u2208 T with the user set U T and the item set I T , containing interactions records H T = { \ud835\udc62, \ud835\udc56, B \ud835\udc62\ud835\udc56 , \ud835\udc66 \ud835\udc62\ud835\udc56 | \ud835\udc62 \u2208 U T , \ud835\udc56 \u2208 I T } , where B \ud835\udc62\ud835\udc56 \u2282 I T . The goal of our task is to comprehensively explore and flexibly exploit universal knowledge from multiple source domains S , and then perform knowledge transfer for facilitating recommendation in each target domain T \u2208 T . Since new scenarios are commonly launched in real-world service platforms, the transferable recommendation task may have following essential properties: i) Interactions in target domains are extremely scarce ( i.e., |H T | \u226a |H S | ) and even totally unavailable ( i.e., H T = \u2205 ), which presses for enough universal and transferable knowledge without noises. ii) Completely non-overlapping items across domains ( i.e., ... EMB EMB Item profile Deep FM ... User behaviors in multiple domains ! Entity S-Attn PE-Attn Prototypes Target entity Masking Mean pooling ... ... Target item as entity list in domain \" Pre-training Masking ... PE-Attn Prediction User profile #$% #&\" #'\" Fine-tuning () *+ , ,\u2208. *+ * / \" *+ \" 0+ \" 0 / \" Embedding Prototype EMB Embedding look-up Data flow in fine-tuning Data flow in pre-training I S \u2229I T = \u2205 ) encourage our approach to perform item-level alignment with inductive ability. iii) Anonymity of target domains means the procedure of model training cannot involve any target domain T \u2208 T , where transferable capability with fast adaptation is urgent. To effectively bridge the domain gap in multiple scenarios, we naturally leverage the generality of entity graph [55], denoted as G = {E , R} with an entity set E and a relation set R , to encode universal user/item representation. Specifically, [55] is extracted from contents/items on the platform of Alipay 1 and its construction pipeline is a mixture of multiple text mining techniques ( e.g., NER [21] and RE [60]). To embrace broader applications in multiple tasks such as recommender system in online service platforms where items are the basic existence forms, [55] provides a unified API to associate entities with items, i.e., \ud835\udf19 : I \u2192 E 2 . A toy example of mapping between items and entities and the following entity graph reasoning process is illustrated in Figure 1.", "4 THE PROPOSED APPROACH": "In this section, we present PEACE, a domain-agnostic cross-domain framework, whose overall architecture is presented in Figure 2.", "4.1 Multi-interest based and Entity-oriented Recommender Pre-training": "Essentially, the online service platform is a composite of multifarious scenarios launched by a variety of service providers, where information is commonly unshared, making related items ( e.g., same categories and brands) without identical properties. To help close the distances and effectively leverage multiplex interactions, we draw support from the well-established entity graph to characterize multiple interests with entity-wise context for generalized recommender pre-training. Firstly, taking full advantage of information propagation, we perform graph convolution operation [36] over the entity graph to encode entities in the entity graph G as the \ud835\udc51 -dimensional matrix H G = { \ud835\udc89 \ud835\udc52 } \ud835\udc52 \u2208E \u2208 R | E | \u00d7 \ud835\udc51 . 4.1.1 Hierarchical-attentive User Representation across Domains. To allow the entity graph to serve as the critical 'bridge' across multiple domains, we propose to deconstruct the user behaviors in each domain into a series of entity list, which could be utilized to comprehensively characterize user interests across domains in a unified manner. Given a user \ud835\udc62 in source domain S , the deconstruction process with the item-entity alignment function \ud835\udf19 go on as follows: \ud835\udc48\ud835\udc60\ud835\udc52\ud835\udc5f B \ud835\udc62,\ud835\udc56 - - -\u2192 \ud835\udc3c\ud835\udc61\ud835\udc52\ud835\udc5a \ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61 \ud835\udf19 - \u2192 \ud835\udc38\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc61\ud835\udc66 \ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61 . With straightforward retrieval from the entity representations H G , we can obtain the representation for user \ud835\udc62 in domain S , denoted as the matrix H S \ud835\udc62 \u2208 R \ud835\udc3b \u00d7 \ud835\udc51 , where \ud835\udc3b is the maximum sequence length in each domains. To comprehensively understand user's interests among multiple source domains, we use a hierarchical-attentive mechanism to effectively absorb universal and transferable knowledge into user encodings. Entity encodings \u2192 Interest-specific user encodings. Given the representation of user \ud835\udc62 in multiple domains S , expressed as a sequence set of entity representations derived from graph learning ( i.e., { H S \ud835\udc62 } S\u2208 S ), we naturally perform multi-interest exploration with a sequence encoder. Inspired by the compelling design and superior performance of Transformer [35], we adopt a self-attention layer with \ud835\udc40 interest kernels ( i.e., { \ud835\udc98 1 , \u00b7 \u00b7 \u00b7 , \ud835\udc98 \ud835\udc40 } ) for multi-interest extraction. Specifically, the sequence encoder starts with concating entity representations of user \ud835\udc62 from multiple domains as H \ud835\udc62 = | | S\u2208 \ud835\udc46 H S \ud835\udc62 \u2208 R \ud835\udc3b | S | \u00d7 \ud835\udc51 , and extracting the \ud835\udc56 -th interest as follows: where W A is the weight matrix for interest extraction. After \ud835\udc40 times of interest extraction, we re-denote the all obtained interest representations as { \ud835\udc9b A \ud835\udc62 } A\u2208 A , where A is the interest set. Interest-specific encodings \u2192 Universal encodings. Here, we dive into the integration of user encodings derived from multiple interests from various source domains, and at the same time characterizing the fine-grained differentiation of importance or relevance across domains. Given the interest-specific representation set { \ud835\udc9b A \ud835\udc62 } A\u2208 A , we obtain the universal encoding for user \ud835\udc62 with adaptive fusion as follows: where \ud835\udc97 and W are the weight vector and matrix, respectively. 4.1.2 Entity-oriented Recommender Pre-training. As mentioned above, items between source and target domains are unshared, making it infeasible to pre-train a recommender aiming at the matching between user interests and specific items. In addition, entities usually involve more generalized knowledge ( i.e., brand and category), which potentially benefits the transferable recommendation. Hence, in this paper, we adopt an entity-oriented pre-training strategy to explore and exploit universal knowledge in multiple source domains. Firstly, given a record \u27e8 \ud835\udc62, \ud835\udc56, \ud835\udc66 \ud835\udc62\ud835\udc56 \u27e9 \u2208 H \ud835\udc46 in domain S 3 , with the help of the alignment function \ud835\udf19 (\u00b7) , we map it to the entity-level forms \u27e8 \ud835\udc62, \ud835\udc52, \ud835\udc66 \ud835\udc62\ud835\udc52 \u27e9 with \ud835\udc52 = \ud835\udf19 ( \ud835\udc56 ) and \ud835\udc66 \ud835\udc62\ud835\udc52 = \ud835\udc66 \ud835\udc62\ud835\udc56 . It is noteworthy that one original record would be mapped to multiple entity-level forms since one item may be associated with multiple entities. In sum, we organize the entity-level form as \u02c6 H S = { \ud835\udc62, \ud835\udc52, \ud835\udc66 \ud835\udc62\ud835\udc52 | \ud835\udc62 \u2208 U S , \ud835\udc52 \u2208 E} . For each training pair \u27e8 \ud835\udc62, \ud835\udc52 \u27e9 , we could easily obtain the corresponding representations \ud835\udc9b \ud835\udc62 and \ud835\udc89 \ud835\udc52 , and adopt a multi-layer perceptron based decoder ( i.e., MLP( \u00b7 )) for prediction, followed by cross-entropy ( i.e., C(\u00b7 , \u00b7) ) based optimization. where ' | | ' is the concatenation operation and \ud835\udf0e (\u00b7) is the sigmoid function. In industrial scenarios, the huge scale of graphs and the latency requirements usually restrict the recommender system to 2-hop graph learning, which makes the entity-oriented pre-training strategy more effective to learn universal knowledge from entity graph, i.e., entity-oriented strategy includes more structural neighbors ('entity \u2192 entity \u2192 entity' \ud835\udc63.\ud835\udc60. 'item \u2192 entity \u2192 entity').", "4.2 Improving Entity Representations with Contrastive Prototype Learning": "Until now, we have introduced the entity-oriented recommender pre-training which characterizes entity-wise context for learning universal and transferable knowledge that potentially benefits various downstream applications. Although the entity-wise context provides excellent capability for generalizing in the transferable recommendation, we can further improve the universal knowledge learning by attracting entities with similar characteristics ( e.g., 'Luckin' and 'Starbucks') and repelling unrelated entities ( e.g., 'Starbucks' and 'Hot pot'). Inspired by cluster analysis[1, 2], we wish to introduce the concept of prototype into the recommender pre-training for helping cluster semantically similar entities in the representation space, so that the pre-trained recommender would be not only more generalized with cluster-level semantic, but also invulnerable to inevitable noises in real-world environments. To this end, we propose a contrastive prototype learning module to improve entity representation. Specifically, we adopt a \ud835\udc51 -dimensional learnable matrix P \u2208 R \ud835\udc5b \u00d7 \ud835\udc51 to denote the latent prototypes in the entity graph, where \ud835\udc5b is the number of prototypes. For each entity \ud835\udc52 , we calculate the normalized similarity vector w.r.t. the \ud835\udc5b prototypes as follows: To well guide the learning process of prototypes, we wish the related prototype could help 'guess' the target entity based on the weighted sum over the similarity distribution. Hence, given the target entity \ud835\udc52 , we obtain its prototype-level view as follows: After that, the goal of the proposed contrastive prototype learning is to pull related views close and push others away, we define the loss based on InfoNCE [34] as follows: Request Online ranking server Source domains PEACE Embedding storage Target domains User          Entity \u2026 PEACE Fine-tuning Ranking list \u2026 Normal ranker Feedback data Zero-shot ranker Entity graph Entity alignment Entity alignment Feedback & profile PEACE Pre-training Pre-training flow Offline inference flow Fine-tuning flow Request & Response \u2026 where P \ud835\udc5b\ud835\udc52\ud835\udc54 is the noise distribution for generating negative views, which could be set as uniform distribution over the current batch for efficient training or other biased distributions for hard negatives. And \ud835\udf0f is the temperature parameter that controls the strength of penalties we enforced on hard negative samples [38] and \ud835\udeff (\u00b7 , \u00b7) measures the similarity of two target views with cosine function.", "4.3 Improving User Representations with Prototype Enhanced Attention": "In the pre-training stage, data collections in source domains involve rich user behaviors in various scenarios, i.e., visiting a travel scene with travelling plan while visiting a job hunting scene when seeking employment. However, aforementioned process of universal user encoding (Eq. 1 and Eq. 2) does not take the user and scenario-based context into consideration, which lacks the ability to capture varying semantics w.r.t. different interaction scenarios. Hence, we aim at improving user representations by facilitating the hierarchicalattentive mechanism with prototype enhanced attention. Formally, given the target user-entity pair \u27e8 \ud835\udc62, \ud835\udc52 \u27e9 in the source domain S , we could easily obtain the similarity distribution of target \ud835\udc52 towards \ud835\udc5b prototypes as \ud835\udc94 through Eq. 5. Then we use a mask vector \ud835\udc8e \u2208 R \ud835\udc5b to keep the most relevant prototypes w.r.t. target entity \ud835\udc52 , whose \ud835\udc56 -th value can be defined as follows: where \ud835\udf16 ( \ud835\udc94 , \ud835\udc3e ) aims at finding the \ud835\udc3e th-largest entries from \ud835\udc94 . Since prototypes serve as informative centers in the entity graph, wedemystify the scenario-based context with most relevant entities and capture varying semantics w.r.t. different related prototypes. In particular, given the interest-specific representation of user \ud835\udc62 as \ud835\udc9b A \ud835\udc62 (See Eq. 1), we produce the scenario-aware representation by weighing various underlying preferences among the target user \ud835\udc62 and prototypes P as follows: With the improved interest-specific representation, we feed it into Eq. 2 for universal encoding by replacing the original interestspecific representation set { \ud835\udc9b A \ud835\udc62 } A\u2208 A with { \u02c6 \ud835\udc9b A \ud835\udc62 } A\u2208 A . After that, we re-denote the improved universal encoding for user \ud835\udc62 as \u02c6 \ud835\udc9b \ud835\udc62 .", "4.4 Learning of PEACE": "Pre-training Stage in Source Domains By integrating entityoriented pre-training strategy with contrastive prototype learning module, the overall objective can be defined as follows: where \ud835\udefe \u2265 0 denotes the relative weight of contrastive term. Fine-tuning Stage in Target Domains The entity-oriented pretraining strategy endows our approach superior generalization ability for making fast adaptation to various target domains, since there is no need to care about anonymous items in target domains. Formally, with a target domain T \u2208 T , we easily initialize a recommender by utilizing the pre-training parameters, which is capable of producing representations of users and entities. And then, given a user-item pair \u27e8 \ud835\udc62, \ud835\udc56 \u27e9 in target domain T , the user representation could be naturally obtained through universal encoding (denoted as \ud835\udc9b T \ud835\udc62 ), while we obtain the final representation for item \ud835\udc56 (denoted as \ud835\udc9b T \ud835\udc56 ) by adopting a mean pooling operation 4 on the representations of entities associated with item \ud835\udc56 . Thus, the preference score is given by MLP and DeepFM [10] as follows: where \ud835\udc99 T \ud835\udc62 and \ud835\udc99 T \ud835\udc56 denote the profile information of user \ud835\udc62 and item \ud835\udc56 in target domain T , respectively. Similarly, we adopt a cross entropy loss as the final objective: In general, the fine-tuning stage can be extended to consider the CVR estimation [46] by adopting the ESSM-like architecture [30]. Our approach also can effortlessly adapt to strictly cold-start scenarios ( i.e., with no supervised/labeled data for some target domains), which are common in online service platforms, by calculating the preference score with the inner product of \ud835\udc9b T \ud835\udc62 and \ud835\udc9b T \ud835\udc56 and encouraging results are presented in the 'Experiments' part.", "4.5 Lightweight Deployment of PEACE": "To ease the pressure of online serving, we strive to deploy the proposed PEACE in a lightweight manner. We present the online deployment pipeline of PEACE in Figure 3, which firstly cold start a new scenario and then fine-tune an expressive recommender with accumulated interactions. In particular, the deployment of PEACE mainly contains three central flows: \u00b7 Pre-training flow. With the collected feedback data from multiple domains, coupled with a well-established entity graph, the proposed PEACE is pre-trained daily to keep universal and transferable knowledge stay up-to-date. \u00b7 Offline inference flow. To ease the burden graph neural network imposes on online serving system, we aim at decoupling the entire procedure by inferring embeddings for users and entities in advance. With the knowledge in Embedding Storage , only MLP based components are fine-tuned by PEACE for downstream applications, so that the complicated information propagation is unnecessary, which greatly eases the pressure of online serving. \u00b7 Fine-tuning flow. As no interaction is available when a new domain is launched by the service provider, PEACE provides recommendation services with the following two steps: i) A zero-shot ranker that helps to cold start the scenario with the direct inner product via embeddings of users and items. ii) With accumulated interactions, as well as the profile data, PEACE would load the pre-trained parameters for fine-tuning, and then upload the well-trained model to the normal ranker.", "5 EXPERIMENTS": "", "5.1 Experimental Setup": "5.1.1 Datasets. Since the proposed PEACE mainly focuses on the industrial cross-domain recommendation issue in online service platforms, we evaluate it by employing real-world industrial datasets 5 which are collected from different domains in Alipay. Specifically, for the source domains, we collect one month's data from Payment , Search and click behaviors on the HomepageFeed . In terms of the target domains, due to data sparsity, we collect two month's data from the following six scenarios: Rental service, Travel service, Food Delivery service, Digital Art service, Dining service, and Daily Necessity service. Moreover, an entity graph involving more than ten millions of entities and more than one hundred millions of triplets is incorporated for cross-domain recommendation, where we extract a 32-dimensional embedding from a pre-trained BERT for each entity as the original features. In each target domain, several attributes are also extracted for model training, i.e., user-side attributes include age, occupation, behaviors in current domains and so on while item-side attributes contain item id, category, brand and so on. Detailed statistics of our datasets are shown in Table 1. 5.1.2 Baselines. We compare PEACE with following SOTAs: \u00b7 Basic competitors : We train a classical DeepFM [10] for each target domain with basic features of users and items. \u00b7 Pre-trained competitors : Similar as PEACE, we pre-train a transferable and expressive model on multiple source domains, and then fine-tune 6 it on each downstream domain. They include Sequential learning , following the BERT-like pre-training strategy ( i.e., SASRec [19], BERT4Rec [33], PAUP [62], PeterRec [53] and ComiRec [3]), Contrastive graph learning ( i.e., SimGCL [52]) and Cross-domain graph learning ( i.e., PCRec [37] with LightGCN [12] and Disen-GNN [22] as the backbones, denoted as PCRec-L and PCRec-D , respectively). \u00b7 Competitors for zero-shot recommendation : We evaluate the performance of PEACE in strictly zero-shot ( i.e., cold-start) scenarios, where the state-of-the-art Tiger [64] is the competitor. We discuss the working details in the supplemental material, and adopt two widely-used metrics, i.e., Hit and NDCG in the experiments (we truncate the ranking list to 5 and 10).", "5.2 Overall Performance": "From the empirical results on the transferable recommendation task in normal (Table 2) and zero-shot (Table 3) settings, the major findings can be summarized as follows: \u00b7 Superior capability of PEACE for transferable recommendation. We observe significant performance gains on six target domains in both normal and zero-shot settings, demonstrating the remarkable strength of PEACE in characterizing universal and transferable knowledge. The substantial improvements w.r.t. Tiger in zero-shot recommendation further demonstrate the transferable capability of PEACE, which could greatly warm up a recommender without fine-tuning. \u00b7 Transferable recommendation benefits from pre-training manner. In most cases, pre-training & fine-tuning models beat the basic competitor, indicating the usefulness of rich behaviors in source domains, and at the same time suggesting that a well-defined pre-trained model can help the recommenders in target domains. However, we also notice that several pre-trained models perform worse in a few domains, i.e., so-called negative transfer [44], which are alleviated in PEACE by abstracting and incorporating prototype-level knowledge. \u00b7 Prototype learning helps avoid noise in entity graph. Generally, we find graph learning approaches based on contrastive signals and the cross-domain framework are both unable to yield promising performance, which is probably attributed to inevitable noises derived from the huge entity graph in real-world applications. Fortunately, our PEACE proposes to characterize prototype-level knowledge in a fine-grained manner, which renders it invulnerable to unsatisfying noises. \u00b7 Entity-oriented pre-training is better. Surprisingly, when compared with graph learning approaches, pre-trained sequential learning based approaches often achieve better performance, notably BERT4Rec and PAUP. Such improvements mainly lie in the compelling design of Transformer architecture, more importantly, the entity-oriented pre-training strategy, which directly matches entity-level semantic over sequence for more universal knowledge towards transferable recommendation.", "5.3 Further Probe": "In this section, we make a series of in-depth analysis to better understand the virtues of the proposed PEACE. In the following parts, we only present the performance comparison results w.r.t. the NDCG metric, and the similar trends could be observed under the Hit metric. Detailed comparison results are provided in the supplemental material. 5.3.1 Ablation Study. Weexamine the effectiveness of each component in PEACE by preparing following variants: i) PEACE w/o GL (removing the G raph L earning component for entity encoding), ii) PEACE w/o CPL (removing C ontrastive P rototype L earning component), and iii) PEACE w/o PEA (removing P rototype E nhanced A ttention). We plot the performance comparison results in Figure 4, which suggests the performance would drop a lot when one component is discarded. It is worthwhile to note that PEACE w/o CPL performs worst in comparison, which reveals the critical position of prototype learning in capturing universal and transferable knowledge to benefit downstream domains. 5.3.2 Qualitative Analysis of Contrastive Prototype Learning. Firstly, we randomly select 6000 entities in the entity graph, and visualize the learned entity embedding from PEACE w/o CPL and PEACE using t-SNE in Figure 5, where different colors denote the prototypes that each entity belongs to. We observe when compared with PEACE w/o CPL, the entity clusters from PEACE is more coherent w.r.t. prototypes. It suggests that the contrastive prototype learning and the derived prototypes, could help close the distance of similar Travel DA Rental FD DN Dining 0.00 0.20 0.40 0.60 0.80 1.00 PEACE PEACE w/o GL PEACE w/o CPL PEACE w/o PEA (a) NDCG@5 Travel DA Rental FD DN Dining 0.00 0.20 0.40 0.60 0.80 1.00 PEACE PEACE w/o GL PEACE w/o CPL PEACE w/o PEA (b) NDCG@10 (a) PEACE w/o CPL (b) PEACE 200 400 600 800 1000 0.68 0.70 0.72 0.74 NDCG@5 NDCG@10 (a) Travel 200 400 600 800 1000 0.10 0.20 0.30 0.40 0.50 0.60 NDCG@5 NDCG@10 (b) Digital Art 200 400 600 800 1000 0.77 0.78 0.79 0.80 0.81 0.82 NDCG@5 NDCG@10 (c) Rental 200 400 600 800 1000 0.10 0.12 0.14 0.16 0.18 NDCG@5 NDCG@10 (d) Food Delivery 200 400 600 800 1000 0.12 0.14 0.16 0.18 0.20 0.22 NDCG@5 NDCG@10 (e) Daily Necessity 200 400 600 800 1000 0.12 0.14 0.16 0.18 0.20 0.22 NDCG@5 NDCG@10 (f) Dining entities in the representation space, and improve performance of PEACE by better capturing noise-free and universal knowledge. Next, we investigate the impact of the number of prototypes by varying it from 100 to 1000. From Figure 6, we observe PEACE always achieve superior performance with a large number of prototypes ( i.e., nearly (900, 1000)), which indicates that a large number of prototypes is a proper choice in real-world scenarios by i) better mitigating the noise in the huge entity graph, and ii) endowing powerful capability for capturing complicated semantics. 5.3.3 Zooming into the Prototype Enhanced Attention. In the prototype enhanced attention, we utilize a mask vector to sparsely activate the most relevant \ud835\udc3e prototypes to characterize scenariobased context for user representations. Here, we investigate into 40 80 120 160 200 0.68 0.70 0.72 0.74 0.76 NDCG@5 NDCG@10 (a) Travel 40 80 120 160 200 0.30 0.36 0.42 0.48 0.54 NDCG@5 NDCG@10 (b) Digital Art 40 80 120 160 200 0.76 0.78 0.80 0.82 0.84 NDCG@5 NDCG@10 (c) Rental 40 80 120 160 200 0.08 0.12 0.16 0.20 0.24 NDCG@5 NDCG@10 (d) Food Delivery 40 80 120 160 200 0.12 0.16 0.20 0.24 NDCG@5 NDCG@10 (e) Daily Necessity 40 80 120 160 200 0.12 0.16 0.20 0.24 NDCG@5 NDCG@10 (f) Dining the impact of the number of prototypes by varying it from 20 to 200. From the performance comparison results in Figure 7, we observe that PEACE always achieves superior performance with \ud835\udc3e in (100, 160), while too large or small \ud835\udc3e would harm the model due to inadequate knowledge or unnecessary noises, respectively.", "5.4 Online Performance": "We evaluate the proposed PEACE via real traffic in six downstream scenarios by competing against the deployed DeepFM-based baseline. Specifically, we perform the online evaluation from '2022/11/28' to '2022/12/04' with the widely-adopted CTR ( i.e., click-through rate) as the core metric, and report the experimental results in Figure 8. We find that PEACE always keeps the best performance across multiple downstream tasks, which is consistent with the offline evaluations. Overall the proposed PEACE achieves statistically significant performance gains by 35.63%, 11.00%, 4.13%, 11.95%, 15.87% and 19.05% with a significance level of 95% in the six scenarios, respectively, which further demonstrates the effectiveness of PEACEoncross-domain recommendation task in real-world service platforms by adopting a more principled way to distill universal and transferable knowledge for a variety of downstream applications. 20221128 20221129 20221130 20221201 20221202 20221203 20221204 0 20 40 60 80 100 Relative Improvement (%) PEACE Baseline (a) Travel 20221128 20221129 20221130 20221201 0 10 20 30 Relative Improvement (%) PEACE Baseline (b) Digital Art 20221128 20221129 20221130 20221201 20221202 10 0 10 20 Relative Improvement (%) PEACE Baseline (c) Rental 20221128 20221129 20221130 20221201 20221202 20221203 20 0 20 40 60 80 Relative Improvement (%) PEACE Baseline (d) Food Delivery 20221128 20221129 20221130 20221201 20221202 20221203 20221204 0 20 40 60 Relative Improvement (%) PEACE Baseline (e) Daily Necessity 20221128 20221129 20221130 20221201 20221202 20221203 20221204 15 0 15 30 45 Relative Improvement (%) PEACE Baseline (f) Dining", "6 CONCLUSION": "In this paper, we propose PEACE, a prototype learning augmented transferable framework for cross-domain recommendation, which is built upon a multi-interest and entity-oriented pre-training architecture. By incorporating the prototype learning, representations of users and items are greatly improved by the contrastive prototype learning module and the prototype enhanced attention mechanism. Extensive experiments on real-world datasets demonstrate that PEACE significantly outperforms state-of-the-art baselines in various scenarios with both offline and online settings.", "REFERENCES": "[1] Charu C. Aggarwal and Chandan K. Reddy. 2014. Data Clustering: Algorithms and Applications. In CRC Press . [2] David Arthur and Sergei Vassilvitskii. 2007. K-Means++: The Advantages of Careful Seeding. In SIAM . 1027-1035. [3] Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang. 2020. Controllable multi-interest framework for recommendation. In KDD . 29422951. [4] Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. 2023. Blurring-Sharpening Process Models for Collaborative Filtering. In SIGIR . 10961106. [5] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using t-SNE. In JMLR . 2579-2605. [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL . 4171-4186. [7] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph neural networks for social recommendation. In WWW . 417-426. [8] Weibo Gao, Hao Wang, Qi Liu, Fei Wang, Xin Lin, Linan Yue, Zheng Zhang, Rui Lv, and Shijin Wang. 2023. Leveraging Transferable Knowledge Concept Graph Embedding for Cold-Start Cognitive Diagnosis. In SIGIR . 983-992. [9] Yulong Gu, Wentian Bao, Dan Ou, Xiang Li, Baoliang Cui, Biyu Ma, Haikuan Huang, Qingwen Liu, and Xiaoyi Zeng. 2021. Self-Supervised Learning on Users' Spontaneous Behaviors for Multi-Scenario Ranking in E-commerce. In CIKM . 3828-3837. [10] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In IJCAI . 1725-1731. [11] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross B. Girshick. 2022. Masked Autoencoders Are Scalable Vision Learners. In CVPR . 15979-15988. [12] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In SIGIR . 639-648. [13] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In WWW . 173-182. [14] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards Universal Sequence Representation Learning for Recommender Systems. In KDD . 585-593. [15] Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S Yu. 2018. Leveraging meta-path based context for top-n recommendation with a neural co-attention model. In KDD . 1531-1540. [16] Zepeng Huai, Yuji Yang, Mengdi Zhang, Zhongyi Zhang, Yichun Li, and Wei Wu. 2023. M2GNN: Metapath and Multi-Interest Aggregated Graph Neural Network for Tag-Based Cross-Domain Recommendation. In SIGIR . 1468-1477. [17] Chao Huang, Huance Xu, Yong Xu, Peng Dai, Lianghao Xia, Mengyin Lu, Liefeng Bo, Hao Xing, Xiaoping Lai, and Yanfang Ye. 2021. Knowledge-aware coupled graph neural network for social recommendation. In AAAI . 4115-4122. [18] SeongKu Kang, Junyoung Hwang, Dongha Lee, and Hwanjo Yu. 2019. SemiSupervised Learning for Cross-Domain Recommendation to Cold-Start Users. In CIKM . 1563-1572. [19] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In ICDM . 197-206. [20] Adit Krishnan, Mahashweta Das, Mangesh Bendre, Hao Yang, and Hari Sundaram. 2020. Transfer Learning via Contextual Invariants for One-to-Many CrossDomain Recommendation. In SIGIR . 1081-1090. [21] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural Architectures for Named Entity Recognition. In NAACL . 260-270. [22] Ansong Li, Zhiyong Cheng, Fan Liu, Zan Gao, Weili Guan, and Yuxin Peng. 2022. Disentangled Graph Neural Networks for Session-based Recommendation. In TKDE . [23] Chenglin Li, Yuanzhen Xie, Chenyun Yu, Bo Hu, Zang Li, Guoqiang Shu, Xiaohu Qie, and Di Niu. 2023. One for All, All for One: Learning and Transferring User Embeddings for Cross-Domain Recommendation. In WSDM . 366-374. [24] Pan Li and Alexander Tuzhilin. 2020. DDTCDR: Deep Dual Transfer Cross Domain Recommendation. In WSDM . 331-339. [25] Meng Liu, Jianjun Li, Guohui Li, and Peng Pan. 2020. Cross Domain Recommendation via Bi-Directional Transfer Graph Collaborative Filtering Networks. In CIKM . 885-894. [26] Peng Liu, Lemei Zhang, and Jon Atle Gulla. 2023. Pre-train, prompt and recommendation: A comprehensive survey of language modelling paradigm adaptations in recommender systems. In arXiv preprint arXiv:2302.03735 . [27] Jinhu Lu, Guohao Sun, Xiu Fang, Jian Yang, and Wei He. 2023. A Three-Layer Attentional Framework Based on Similar Users for Dual-Target Cross-Domain Recommendation. In DASFAA . 297-313. [28] Yuanfu Lu, Yuan Fang, and Chuan Shi. 2020. Meta-learning on heterogeneous information networks for cold-start recommendation. In KDD . 1563-1573. [29] Jianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu. 2020. Disentangled self-supervision in sequential recommenders. In KDD . 483491. [30] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate. In SIGIR . 1137-1140. [31] Tong Man, Huawei Shen, Xiaolong Jin, and Xueqi Cheng. 2017. Cross-Domain Recommendation: An Embedding and Mapping Approach. In IJCAI . 2464-2470. [32] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction. In KDD . 2671-2679. [33] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In CIKM . 1441-1450. [34] A\u00e4ron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. In arXiv preprint arXiv:1807.03748 . [35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NIPS . 5998-6008. [36] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR . [37] Chen Wang, Yueqing Liang, Zhiwei Liu, Tao Zhang, and S Yu Philip. 2021. Pretraining graph neural network for cross domain recommendation. In CogMI . 140-145. [38] Feng Wang and Huaping Liu. 2021. Understanding the Behaviour of Contrastive Loss. In CVPR . 2495-2504. [39] Jiaqi Wang and Jing Lv. 2020. Tag-informed collaborative topic modeling for cross domain recommendations. In KBS . 106-119. [40] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat: Knowledge graph attention network for recommendation. In KDD . 950-958. [41] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In SIGIR . 165-174. [42] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, and Tat-Seng Chua. 2021. Learning intents behind interactions with knowledge graph for recommendation. In WWW . 878-887. [43] Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-Seng Chua. 2019. Explainable reasoning over knowledge graphs for recommendation. In AAAI . 5329-5336. [44] Zirui Wang, Zihang Dai, Barnab\u00e1s P\u00f3czos, and Jaime G. Carbonell. 2018. Characterizing and Avoiding Negative Transfer. In CVPR . 11293-11302. [45] Yinwei Wei, Wenqi Liu, Fan Liu, Xiang Wang, Liqiang Nie, and Tat-Seng Chua. 2023. LightGT: A Light Graph Transformer for Multimedia Recommendation. In SIGIR . 1508-1517. [46] Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, and Keping Yang. 2020. Entire Space Multi-Task Modeling via Post-Click Behavior Decomposition for Conversion Rate Prediction. In SIGIR . 2377-2386. [47] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021. Self-supervised graph learning for recommendation. In SIGIR . 726-735. [48] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neural networks in recommender systems: a survey. In CSUR . 1-37. [49] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks. In TNNLS . 4-24. [50] Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H Chi, Steve Tjoa, Jieqi Kang, et al. 2021. Self-supervised learning for large-scale item recommendations. In CIKM . 4321-4330. [51] Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Self-supervised multi-channel hypergraph convolutional network for social recommendation. In WWW . 413-424. [52] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen. 2022. Are Graph Augmentations Necessary? Simple Graph Contrastive Learning for Recommendation. In SIGIR . 1294-1303. [53] Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. 2020. Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation. In SIGIR . 1469-1478. [54] Tianzi Zang, Yanmin Zhu, Haobing Liu, Ruohan Zhang, and Jiadi Yu. 2022. A Survey on Cross-Domain Recommendation: Taxonomies, Methods, and Future Directions. In TOIS . 1-39. [55] Xiaoling Zang, Binbin Hu, Jun Chu, Guannan Zhang, Zhiqiang Zhang, Jun Zhou, and Wenliang Zhong. 2023. Commonsense Knowledge Graph towards Super APP and Its Applications in Alipay. In KDD . 5509-5519. [56] Xiaoyun Zhao, Ning Yang, and Philip S Yu. 2022. Multi-sparse-domain collaborative recommendation via enhanced comprehensive aspect preference learning. In WSDM . 1452-1460. [57] Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021. Contrastive learning for debiased candidate generation in large-scale recommender systems. In KDD . 3985-3995. [58] Feng Zhu, Chaochao Chen, Yan Wang, Guanfeng Liu, and Xiaolin Zheng. 2019. DTCDR:AFrameworkfor Dual-Target Cross-Domain Recommendation. In CIKM . 1533-1542. [59] Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, and Guanfeng Liu. 2021. Cross-Domain Recommendation: Challenges, Progress, and Prospects. In IJCAI . 4721-4728. [60] Hao Zhu, Yankai Lin, Zhiyuan Liu, Jie Fu, Tat-Seng Chua, and Maosong Sun. 2019. Graph Neural Networks with Generated Parameters for Relation Extraction. In ACL . 1331-1339. [61] Yongchun Zhu, Kaikai Ge, Fuzhen Zhuang, Ruobing Xie, Dongbo Xi, Xu Zhang, Leyu Lin, and Qing He. 2021. Transfer-Meta Framework for Cross-Domain Recommendation to Cold-Start Users. In SIGIR . 1813-1817. [62] Yuehua Zhu, Bo Huang, Shaohua Jiang, Muli Yang, Yanhua Yang, and Wenliang Zhong. 2022. Progressive Self-Attention Network with Unsymmetrical Positional Encoding for Sequential Recommendation. In SIGIR . 2029-2033. [63] Yongchun Zhu, Zhenwei Tang, Yudan Liu, Fuzhen Zhuang, Ruobing Xie, Xu Zhang, Leyu Lin, and Qing He. 2022. Personalized Transfer of User Preferences for Cross-domain Recommendation. In WSDM . 1507-1515. [64] Jianhuan Zhuo, Jianxun Lian, Lanling Xu, Ming Gong, Linjun Shou, Daxin Jiang, Xing Xie, and Yinliang Yue. 2022. Tiger: Transferable Interest Graph Embedding for Domain-Level Zero-Shot Recommendation. In CIKM . 2806-2816.", "A SUPPLEMENTARY MATERIAL": "", "A.1 Environment": "Due to the large volume of data in our experiments, all models are implemented on parameter server based distributed learning systems. Specifically, we utilize 20 worker with 12 cores and 100GB memory, and 20 parameter servers with 5 cores and 15GB memory for the training of all models.", "A.2 Evaluation Protocol": "We mainly evaluate the proposed PEACE for transferable recommendation in the normal and zero-shot settings. After the pretraining stage, the proposed PEACE is evaluated as follows: \u00b7 Normal recommendation : For each target domain, we do a chronological train/validation/test split with 70%/15%/15% according to interaction timestamps. \u00b7 Zero-shot recommendation : For each target domain, we directly apply the pre-trained PEACE for prediction, i.e., all interactions in the domain are used for test.", "A.3 Parameter Settings": "For all methods, we set the batch size as 512, embedding size as 64 and adopt Adam as the optimizer with a learning rate of 1e-4. Moreover, we set the number of GNN layers as 2 for all GNN-based methods and the sequence length as 200 for all sequential methods. All MLP( \u00b7 ) components are commonly implemented with two hidden layers with ReLU as the activation function. It is worthwhile that these sequential learning methods ( i.e., SASRec, BERTRec,PeterRec, PAUP, ComiRec) cannot directly applied to our task since items in source and target domains are completely non-overlapping. Therefore, we utilize the entity-oriented pre-training strategy in these approaches, i.e., replacing the original item-wise sequences with entity-wise sequences through our item-entity mapping function. For a fair comparison, we also utilize the widely-adopted grid search strategy to carefully tune each approach in the validation set and briefly present the optimal parameters as follows:", "For baselines": "\u00b7 DeepFM : the network structure is 64-32-8 and the latent dimension of FM is 8. To be fair, in the fine-tuning stage, all pre-trained models are under the same setting. \u00b7 SASRec : the number of self-attention blocks is set to 2 and the learned positional embedding is utilized. \u00b7 BERT4Rec : the layer number \ud835\udc3f is set to 2, the number of heads \u210e is set to 2 and the learnable positional embedding is used. \u00b7 PeterRec : the kernel size, the number of dilation channels and dilations are set to 3, 64 and [ 1 , 4 , 1 , 4 , 1 , 4 , 1 , 4 ] respectively. \u00b7 PAUP : the layer \ud835\udc3f , head number \u210e and segment number \ud835\udc5a are set to 2, 2 and 5 respectively, and the unsymmetrical positional encoding is used. \u00b7 ComiRec : the number of interest embeddings is set to 8. \u00b7 SimGCL : the parameter for \ud835\udc3f 2 regularization and temperature \ud835\udf0f are set to 1e-4 and 0.2 respectively. \u00b7 PCRec-L : the parameter for \ud835\udc3f 2 regularization is set to 1e-4 and layer combination coefficient \ud835\udefc \ud835\udc58 is uniformly set to 1 3 given the predefined GNN layer number is 2. \u00b7 PCRec-D : the factor number is set to 8 with 2-layer GNN. \u00b7 Tiger : the number of discarded C is set to 1 with a 2-layer GNN. For PEACE , we empirically set the temperature \ud835\udf0f as 0.2. As for \ud835\udefe , the interest kernel number \ud835\udc40 , the number of prototypes \ud835\udc41 and prototype enhanced attention parameter \ud835\udc3e , we set them to 1.0, 10, 900 and 160 according to the grid search results.", "A.4 Additional Results": "In this section, we provide the additional results of the influence of the entity graph, the influence of multi-interest encoding, and performance comparison w.r.t. the Hit metric and finally the efficiency analysis of the proposed PEACE. 0.1 0.3 0.5 0.7 0.9 1.0 0.66 0.69 0.72 0.75 0.78 NDCG@5 NDCG@10 (a) Travel 0.1 0.3 0.5 0.7 0.9 1.0 0.30 0.40 0.50 0.60 NDCG@5 NDCG@10 (b) Digital Art 0.1 0.3 0.5 0.7 0.9 1.0 0.78 0.80 0.82 0.84 NDCG@5 NDCG@10 (c) Rental 0.1 0.3 0.5 0.7 0.9 1.0 0.05 0.10 0.15 0.20 0.25 NDCG@5 NDCG@10 (d) Food Delivery 0.1 0.3 0.5 0.7 0.9 1.0 0.12 0.16 0.20 0.24 NDCG@5 NDCG@10 (e) Daily Necessity 0.1 0.3 0.5 0.7 0.9 1.0 0.12 0.16 0.20 0.24 NDCG@5 NDCG@10 (f) Dining 0.1 0.3 0.5 0.7 0.9 1.0 0.50 0.55 0.60 0.65 0.70 0.75 Hit@5 Hit@10 (a) Travel 0.1 0.3 0.5 0.7 0.9 1.0 0.30 0.40 0.50 0.60 0.70 0.80 Hit@5 Hit@10 (b) Digital Art 0.1 0.3 0.5 0.7 0.9 1.0 0.54 0.60 0.66 0.72 0.78 Hit@5 Hit@10 (c) Rental 0.1 0.3 0.5 0.7 0.9 1.0 0.00 0.05 0.10 0.15 Hit@5 Hit@10 (d) Food Delivery 0.1 0.3 0.5 0.7 0.9 1.0 0.12 0.16 0.20 0.24 0.28 0.32 Hit@5 Hit@10 (e) Daily Necessity 0.1 0.3 0.5 0.7 0.9 1.0 0.12 0.16 0.20 0.24 0.28 0.32 Hit@5 Hit@10 (f) Dining A.4.1 Influence of the Entity Graph. Intuitively, the entity graph serves as a critical bridge for broadening user interests and discovering correlations between items. Hence, we measure how PEACE perform with different sparsity of the entity graph. Concretely, we randomly drop entity-entity relations to vary the sparsity of the entity graph in {0.1, 0.2, 0.5, 0.7, 0.9, 1.0}, and present the performance comparison in Figure 9 and Figure 10. From the plot we find that the performance of PEACE generally achieves positive gains with the growth of the entity graph, which demonstrates the usefulness of the graph data, worthy of being comprehensively exploited in industrial recommenders. 2 4 6 8 10 0.65 0.70 0.75 0.80 0.85 NDCG@5 NDCG@10 (a) Travel 2 4 6 8 10 0.30 0.35 0.40 0.45 0.50 NDCG@5 NDCG@10 (b) Digital Art 2 4 6 8 10 0.76 0.78 0.80 0.82 0.84 0.86 NDCG@5 NDCG@10 (c) Rental 2 4 6 8 10 0.00 0.10 0.20 0.30 NDCG@5 NDCG@10 (d) Food Delivery 2 4 6 8 10 0.12 0.16 0.20 0.24 NDCG@5 NDCG@10 (e) Daily Necessity 2 4 6 8 10 0.12 0.16 0.20 0.24 0.28 NDCG@5 NDCG@10 (f) Dining", "Figure 11: The study of multi-interest encoding w.r.t. NDCG.": "2 4 6 8 10 0.50 0.60 0.70 0.80 Hit@5 Hit@10 (a) Travel 2 4 6 8 10 0.30 0.45 0.60 0.75 0.90 Hit@5 Hit@10 (b) Digital Art 2 4 6 8 10 0.55 0.60 0.65 0.70 0.75 Hit@5 Hit@10 (c) Rental 2 4 6 8 10 0.00 0.10 0.20 0.30 0.40 Hit@5 Hit@10 (d) Food Delivery 2 4 6 8 10 0.12 0.16 0.20 0.24 0.28 Hit@5 Hit@10 (e) Daily Necessity 2 4 6 8 10 0.10 0.20 0.30 0.40 Hit@5 Hit@10 (f) Dining A.4.2 Influence of multi-interest encoding. In fact, user interests in multiple domains are quite complex and the multi-interest encoding can serve as a useful tool for describing the complex user interests. Concretely, we investigate into the influence of multi-interest encoding by varying the interest kernels \ud835\udc40 from 1 to 10 and present the performance comparison in Figure 11 and Figure 12. From Figure 11 and Figure 12 we find that the performance of PEACE generally improves as the number of interests grows, which demonstrates that the fine-grained characterization of user interests are helpful due to the multiplex data in real-world service platforms. A.4.3 Performance comparison results w.r.t. the Hit metric. In the paper we only present performance comparison w.r.t. the NDCG metric for the in-depth anaslysis of PEACE. To comprehensively understand the merits of PEACE, we further provide additional comparison results w.r.t. the Hit metric. Specifically, we report the ablation study in Figure 13, the influence of the number prototypes in Figure 14, and the study of the prototype enhanced attention in Figure 15. Overall, compared to the results w.r.t. the NDCG metric, the similar trends and findings are concluded. A.4.4 Efficiency Analysis. To illustrate the efficiency of PEACE, firstly we report the time cost (in minutes) of each method in the Pre-training stage as follows: SASRec (670), BERT4Rec (675), PeterRec (420), PAUP (577), ComiRec (400), SimGCL (618), PCRec-L (572), PCRec-D (607), and PEACE (545), from which the efficiency of PEACE in training stage is verified. Secondly, we show the decrease of response time (RT) in online serving, which plays a key role in measuring the efficiency of industrial recommender systems. Benefiting from our lightweight deployment, the RT has substantially reduced from 70ms to 40ms by minimizing the time taken to infer representation for users and entities during model training. Overall, this approach has empowered us to handle a wide range of requests efficiently, thereby augmenting the scalability of the system. Travel DA Rental FD DN Dining 0.00 0.20 0.40 0.60 0.80 1.00 PEACE PEACE w/o GL PEACE w/o CPL PEACE w/o PEA (a) Hit@5 Travel DA Rental FD DN Dining 0.00 0.20 0.40 0.60 0.80 1.00 PEACE PEACE w/o GL PEACE w/o CPL PEACE w/o PEA (b) Hit@10 200 400 600 800 1000 0.50 0.55 0.60 0.65 0.70 Hit@5 Hit@10 (a) Travel 200 400 600 800 1000 0.2 0.4 0.6 0.8 1.0 Hit@5 Hit@10 (b) Digital Art 200 400 600 800 1000 0.55 0.60 0.65 0.70 0.75 0.80 Hit@5 Hit@10 (c) Rental 200 400 600 800 1000 0.04 0.06 0.08 0.10 0.12 Hit@5 Hit@10 (d) Food Delivery 200 400 600 800 1000 0.12 0.16 0.20 0.24 0.28 0.32 Hit@5 Hit@10 (e) Daily Necessity 200 400 600 800 1000 0.15 0.20 0.25 0.30 0.35 Hit@5 Hit@10 (f) Dining 40 80 120 160 200 0.52 0.56 0.60 0.64 0.68 0.72 Hit@5 Hit@10 (a) Travel 40 80 120 160 200 0.3 0.4 0.6 0.8 0.9 Hit@5 Hit@10 (b) Digital Art 40 80 120 160 200 0.55 0.60 0.65 0.70 0.75 0.80 Hit@5 Hit@10 (c) Rental 40 80 120 160 200 0.04 0.08 0.12 0.16 Hit@5 Hit@10 (d) Food Delivery 40 80 120 160 200 0.12 0.16 0.20 0.24 0.28 Hit@5 Hit@10 (e) Daily Necessity 40 80 120 160 200 0.12 0.16 0.20 0.24 0.28 0.32 Hit@5 Hit@10 (f) Dining"}
