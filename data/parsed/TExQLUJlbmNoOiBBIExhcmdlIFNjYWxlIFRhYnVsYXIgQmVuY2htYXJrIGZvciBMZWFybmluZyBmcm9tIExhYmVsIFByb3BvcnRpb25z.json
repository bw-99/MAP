{"LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label Proportions": "Anand Brahmbhatt* Google Research India anandpareshb@google.com Rishi Saket Google Research India Mohith Pokala* Google Research India mohithpokala@google.com rishisaket@google.com Aravindan Raghuveer Google Research India araghuveer@google.com March 6, 2024", "Abstract": "In the task of Learning from Label Proportions (LLP), a model is trained on groups (a.k.a bags) of instances and their corresponding label proportions to predict labels for individual instances. LLP has been applied pre-dominantly on two types of datasets - image and tabular. In image LLP, bags of fixed size are created by randomly sampling instances from an underlying dataset. Bags created via this methodology are called random bags . Experimentation on Image LLP has been mostly on random bags on CIFAR-* and MNIST datasets. Despite being a very crucial task in privacy sensitive applications, tabular LLP does not yet have a open, large scale LLP benchmark. One of the unique properties of tabular LLP is the ability to create feature bags where all the instances in a bag have the same value for a given feature. It has been shown in prior research that feature bags are very common in practical, real world applications [7, 33]. In this paper, we address the lack of a open, large scale tabular benchmark. First we propose LLP-Bench, a suite of 70 LLP datasets (62 feature bag and 8 random bag datasets) created from the Criteo CTR prediction and the Criteo Sponsored Search Conversion Logs datasets, the former a classification and the latter a regression dataset. These LLP datasets represent diverse ways in which bags can be constructed from underlying tabular data. To the best of our knowledge, LLP-Bench is the first large scale tabular LLP benchmark with an extensive diversity in constituent datasets. Second, we propose four metrics that characterize and quantify the hardness of a LLP dataset. Using these four metrics we present deep analysis of the 62 feature bag datasets in LLP-Bench. Finally we present the performance of 9 SOTA and popular tabular LLP techniques on all the 62 datasets.", "1 Introduction": "In traditional supervised learning, training data consists of feature-vectors (instances) along with their labels. Amodel trained using such data is then used during inference to predict the labels of test instances. In recent times, primarily due to privacy concerns and relative rarity of high quality large-scale supervised data, the * - equal contribution 1 Figure 1: Dataset D1 is formed by randomly choosing without replacement from the instance dataset to form bags of size =3. Dataset D2 is feature bag formed by creating bags such that within a bag instances have the same value for features F1, F2. The fourth bag is removed because it has only one instance i7. Dataset D3 is similarly formed by using feature F3 as the grouping key. Notice the bags have become substantially larger than those in D1. Source Instance Dataset Derived Datasets id Fl F2 F3 Label RANDOM D1 BAG il,i8,i6 i3,14,i7 i2,i5,i9 i2 2 1 | 7 i3 2 D2 FEATURE 8 5 BAG (FI,F2) i5,i9,i6 i5 3 4 i6 3 FEATURE D3 i7 4 5 BAG (F3) 11,12,14 i6,17 i9 i8 i9 3 4 | 2 Bag weakly supervised framework of learning from label proportions (LLP) has gained importance [34, 33, 30, 41] In LLP, the training data is aggregated into bags . Each bag contains a bunch of instances (and their feature vectors) and their corresponding aggregated label count. The goal is to learn a classification model for predicting the class-labels of individual instances [10, 25]. Study of LLP has recently gained importance due to developments in the privacy landscape. In particular, restrictions on tracking of user events have led to an LLP formulation of user-modeling in online advertising [26]. Since only the average label for a bag of users is revealed, the size of the bags is a measure of the privacy afforded. Other applications include medical records anonymization [41], IVF prediction [18], image classification [3, 27], mass spectrometry [5], datasets with legal constraints [30, 41] and inadequate or costly supervision [11, 5]. Such LLP techniques have primarily been evaluated and studied on image [21, 43, 38, 23, 14, 2] and tabular [33, 4, 29] datasets. On images, well known datasets like CIFAR-10, CIFAR-100, MNIST are used - typically by randomly partitioning the dataset into bags - to create medium-large scale LLP datasets. On the other hand, tabular data consists of independent rows of feature vectors with one more labels attached to each feature vector. Often, previous works used tabular LLP datasets derived from small UCI [12] datasets which fail to simulate the diversity and scale of applications involving such data. Notably, tabular datasets are extremely common in real world classification and regression tasks for online advertising [26], health care research [30, 41] and scientific simulation studies [5]. Such applications tend to use very large scale data either from online user interaction [24, 16] or user studies [15]. Impact of privacy leaks due to inadvertent exposure of sensitive data is much higher in large scale datasets. Therefore LLP on large scale tabular datasets is a very critical application that is receiving increasing attention from the research community. While image LLP has large scale benchmark datasets derived from CIFAR-*, an equivalent benchmark does not exist for tabular data. Aunique property of tabular LLP as pointed out in recent literature [33, 7] is the notion of feature bags. In feature bags, bags are constructed such that all instances within the bag have the same value for given key(s) called the grouping key(s). Such bags occur in critical real-world applications such as user modelling in online advertising where the conversion labels are aggregated over pre-selected categorical features [7, 4, 26]. Figure 1 shows three datasets created from the same instance datasets. The first dataset is created like random sampling much like the ones created using CIFAR-* [21, 43, 38]. The second and third datasets are features bags created by using F1, F2 and F3 as the grouping keys respectively. Note that feature bags can also be made to have fixed size. Motivated by the above observations and the richness of the problem, we propose LLPBench a large scale, diverse benchmark for tabular LLP. We make three contributions in this paper as listed below. 2 1. We propose LLP-Bench, a suite of \u00b7 56 LLP datasets (52 feature bag and 4 random bag datasets) created from the popular Criteo CTR prediction dataset consisting of 45 million instances [9]. \u00b7 14 LLP datasets (10 feature bag and 4 random bag datasets) created from the more recently available Criteo Sponsored Search Conversion Logs (Criteo SSCL) containing **TBA** million instances [37]. These LLP datasets represent diverse ways in which bags can be constructed from underlying tabular data. Those feature bag datasets entail removing bags of extreme sizes, and contain 13.5 million to 24.75 million bags (**this should be instances, change numbers also**). To the best of our knowledge, LLP-Bench is the first large scale tabular LLP benchmark with an extensive diversity in constituent datasets (Section 4). 2. We propose four metrics that help characterize and quantify the hardness of an LLP dataset. Using these four' metrics we present deep analysis of the 56 datasets in LLP-Bench (Sections 3, 5). 3. We present the performance of 9 SOTA and popular tabular LLP techniques on all the 56 datasets of LLP-Bench derived from the Criteo CTR prediction dataset. Some of these techniques are not applicable to regression datasets, so we evaluate 3 of them on the 14 datasets of LLP-Bench corresponding to Criteo Sponsored Search Conversion Logs. To the best of our knowledge, our study consisting of more than 3000 experiments is the most extensive evaluation of popular tabular LLP techniques in literature (Sections 6, 7). Choice of base datasets. The Criteo CTR and SSCL datasets are relevant to our work - firstly they are impression-click and click-conversion datasets which correspond to natural applications where LLP aggregation can occur due to user privacy [26]. Another reason is that they are among the few publicly available large scale tabular dataset with several categorical features, which allow for a rich collection of feature bag datasets. Our techniques for feature-based grouping and LLP dataset analysis are more generally applicable. However, since we focus on a tabular LLP Benchmark, we restrict ourselves to these datasets in which either the feature-vectors represent impressions and are given binary { 0, 1 } -labels indicating a click, or they represent clicks and the label indicates the number of conversions for that click. The binary label setting is widely studied in supervised machine learning, and in the LLP setting as well with many real-world applications: see for e.g. references [18] and [11] in the paper for applications in IVF prediction and high-energy physics. There are, on the other hand, regression applications in remote sensing [39, 40] in which the real-valued labels are available only as aggregates. In particular, the important task of user-modeling on online advertising platforms has recently seen privacy related restrictions leading to an LLP formulation for it (see Section 1 of [4]), which is typically a classification or a regression problem.", "2 Related Work": "Several techniques for LLP have been studied over the years. The work of [10, 17] applied trained probabilistic models using Monte-Carlo methods. Subsequent works [25, 30] extended supervised learning techniques such neural nets, SVM and k -nearest neighbors to LLP, others adapted clustering based approaches [8, 36], while [42] proposed a novel \u221d -SVM method for LLP. The work of [29] estimated model parameters from label proportions for the exponential generative model with certain assumptions on label distributions of bags. Their method was further applied by [28] for more general models and relaxed distributional assumptions. More recent works have investigated deep neural network based LLP methods [3, 1, 20], techniques using bag combinations [34, 33], curated bags [6] and training on derived surrogate labels for instances [4]. Recently, [31, 32] initiated a theoretical study of LLP from the computational learning perspective. All of the previous works in LLP experimentally evaluate their methods on LLP datasets consisting of bags randomly created from some real-world supervised learning dataset. In these pseudo-synthetic LLP datasets, instances are randomly sampled/partitioned into the different bags, where in [28] and [33] this process also clusters feature-vectors to generate more complicated bag distributions. Almost all of the above works use limited scale data, typically small to medium scale UCI datasets [42, 28, 34], image datasets [20], social media data [1]. In general, there have been very few large scale tabular datasets created and used for LLP. To 3 the best of our knowledge, [33] and [6] are the only works that explore a large dataset (Criteo) to test their methodology. Since the primary contribution of these works is algorithmic, they do not justify their choice of how bags were created nor do they explore the many choices and tradeoffs of creating bags from a large scale instance dataset. In our work we precisely address this gap - we not only create benchmark with 56 diverse datasets, but we study in detail the tradeoffs involved and analyse the performance of 9 important baselines. We study the performance of the following 9 baselies on the LLP-Bench benchmark. DLLP : This a the standard LLP baseline method use in previous works [1], which optimizes a bag-level loss between the average label proportion and the predicted average label proportion. We evaluate DLLP -BCE and DLLP -MSE which use the bag-level BCE and MSE losses respectively using the minibatch training described above. GenBags : This is the generalized bags method of [33], and since we only have one collection of bags in a given experiment we use random Gaussian combining weights to construct 120 different generalized bags per mini-batch of 8 bags. Easy -LLP : In this technique proposed by [4], mean-bag labels along with the global average label are used to define a surrogate label for training instances over which the model is optimized using the BCE loss. OT methods: There are the optimal transport based techniques proposed in [22, 13]. The first variant OT -LLP included is the non-regularized optimal transport for disjoint bags which can be implemented via a greedy approach. We also have Hard -EROT -LLP and Soft -EROT -LLP - the hard and soft entropic regularized OT methods of [22]. SIM -LLP : In this method proposed by [19], the bag-level DLLP loss is augmented with a pairwise similarity based loss penalizing different predictions of geometrically close feature-vectors. Since the similarity based loss has number of terms which is square in the number of feature-vectors in a minibatch, we sample a random set of 400 feature-vectors from each minbatch to apply this loss. Mean -Map : This is the well-known technique of [29] for linearized exponential generative models, consisting of two steps: computing the quantity using the bag-label proportions followed by optimizing for the model parameters over all feature-vectors. While the first step is a straightforward calculation, the second step is implemented using a minibatch optimization. While all of the above techniques are applicable to the binary-label setting and hence are evaluated on the LLP datasets derived from Criteo CTR, only a subset of them, specifically DLLP -MSE , GenBags and SIM -LLP , are applicable to real-valued labels and are included as baselines on the LLP datasets corresponding to Criteo SSCL.", "3 LLP Dataset Characteristics": "In this section, we first establish some notation, define LLP terminology and then propose four metrics to characterize and quantify the hardnesss of an LLP dataset. In our exploration of LLP we shall only consider non-negative real-valued instance labels. Notation : X : = { x ( i ) \u2208 R n } m i = 1 is a dataset of m feature vectors in n -dimensional space with labels given by Y : = { y ( i ) \u2208 { 0, 1 }} m i = 1 . We denote by \u02c6 Y : = { \u02c6 y ( i ) \u2208 R \u2265 0 } m i = 1 the corresponding model predictions which are probabilities of the predicted label being 1. A bag B \u2286 [ m ] consists of feature vectors XB : = \u222a i \u2208 B x ( i ) and with the corresponding label sum yB : = \u2211 i \u2208 B y ( i ) . The label proportion of the bag is yB / | B | . Definition 3.1 (LLP Dataset) A learning from label proportions (LLP) dataset corresponding to a collection of bags B : = { Bj } N j = 1 is given by { ( XB , yB ) | B \u2208 B } . The label bias of training dataset is \u00b5 ( B , Y ) : = ( \u2211 B \u2208 B yB ) / ( \u2211 B \u2208 B | B | ) , while the average label proportion is \u02c6 \u00b5 ( B , Y ) : = 1 N ( \u2211 B \u2208 B yB / | B | ) . The LLP datasets considered in the paper have disjoint bags. In the following we define statistics comparing the separation among feature-vectors within bags and their separation across bags. using a natural notion of bag separation. 4 Definition 3.2 (Bag Separation) For a distance d on R n and collection of bags B = { B 1 , . . . , BM } the corresponding separation function is defined as BagSep ( B , B \u2032 , d ) : = 1 | B || B \u2032 | \u2211 x \u2208 B \u2211 x \u2032 \u2208 B \u2032 d ( x , x \u2032 ) . We define the M \u00d7 Mmatrix BagSepMatrix ( B , d ) whose ( i , j ) th element is given by BagSep ( Bi , Bj , d ) . We use BagSep to compute the average separation between pairs of bags and the average separation within each bag. If the feature-vectors in bags are clustered together and far away from those of other bags, we expect the former to be significantly greater than the later. \u0338 Definition 3.3 (Inter-Bag Separation for a bag) Given B , and metric d on R n , the average inter-bag distance for a bag B \u2208 B is defined as InterBagSep ( B , d ) : = 1 | B |-1 \u2211 B \u2032 \u2208 B , B \u2032 = B BagSep ( B , B \u2032 , d ) . For computing the average statistic for the entire dataset we define the following. Definition 3.4 The mean intra-bag separation of B is defined as MeanIntraBagSep ( B , d ) : = 1 | B | \u2211 B \u2208 B BagSep ( B , B , d ) . The mean of average inter-bag separation is defined as MeanInterBagSep ( B , d ) : = 1 | B | \u2211 B \u2208 B InterBagSep ( B , d ) .", "3.1 Hardness metrics for LLP datasets": "We are now ready to present four metrics that characterize the hardness of a LLP dataset: (i) standard deviation of label proportion (ii) inter vs intra bag separation ratio (iii) mean bag size, and (iv) cumulative bag size distribution. LabelPropStdev : This is the standard deviation of the label proportions of the collection of bags i.e., \u221a Var B \u2190 B [ yB / | B | ] . A higher LabelPropStdev typically provides more model training supervision. For e.g. consider ( X , Y ) with two different bag collections B 1 and B 2 with \u03b1 = \u02c6 \u00b5 ( B 1 , Y ) = \u02c6 \u00b5 ( B 1 , Y ) while 0 \u2248 \u03b2 1 = LabelPropStdev ( B 1 , Y ) \u226a LabelPropStdev ( B 2, Y ) = : \u03b2 2. Consider a model that predicts \u03b1 for every feature-vector. Since \u03b2 1 \u2248 0, the predicted label proportions of this model are will be close to the true label proportions for most bags in B 1 unlike in B 2 (since \u03b2 2 \u226b \u03b2 2). For many bag-level losses, this results in such a model being much closer to optimal for B 1 rather B 2. On the other hand, the model is only learning the average label proportion and not discriminating among the instances, therefore not desirable and is ruled out when LabelPropStdev is higher. InterIntraRatio : This denotes MeanInterBagSep ( B , d ) / MeanIntraBagSep ( B , d ) when d = \u2113 2 2 . Adataset with large InterIntraRatio has well separated bags and therefore the label proportion supervision provided per bag carries more information and hence easier to learn from compared to a dataset with a smaller InterIntraRatio . MeanBagSize : Since we have only have a label proportion for each bag, informally speaking, the larger the bag size the lower the amount of label supervision for that bag. The third and a simple metric to characterize this is MeanBagSize i.e., the mean size of all the bags in the dataset. Therefore, a dataset with larger mean bag size is a much harder dataset to learn from compared to one with a much smaller mean bag size. CumuBagSizeDist : The bag sizes for any dataset are characterized by their cumulative distribution function which plots the fraction of bags of size at most t for all t \u2265 1. We compute the bag sizes at the 50, 70, 85 and 95 percentile of cumulative distribution plot, for each dataset. Short-tailed distributions have most bags of small size and a very few large sized bags whereas Long-tailed distributions contain many bags of large sizes. Bags of large sizes provide a very little label information for a lot of feature level information. Therefore a LLP dataset with a long-tailed distribution of bag sizes is a much harder dataset to learn from than a short-tailed one. 5", "4 LLP Dataset: Bag creation": "Criteo CTR. This has 13 numerical and 26 categorical features and a binary label. Each of the approximately 45 million rows (instances) represents an impression (online ad) and the label indicates a click. The semantics of all the features are undisclosed and the values of all the categorical features hashed into 32-bits for anonymization. Additionally, the dataset has missing values. We use a preprocessed version of the dataset as done for the AutoInt ([35]) model, described and implemented in their provided code 1 We choose AutoInt because that is among the best performing models on the Criteo benchmark 2 . For convenience we label the numerical and categorical features (in their order of occurrence) as N1, . . . , N13 and C1, . . . C26. The preprocessing applies int ( log 2 ( x )) transformation when x > 2 on the numerical feature values x , and we further additively scale so that their values are non-negative integers. The categorical features are encoded as non-negative integers. Criteo SSCL. This has 3 numerical features ( \u02dc N1, \u02dc N2, \u02dc N3) and 17 categorical features ( \u02dc C1, . . . , \u02dc C17) and 1.7 million instances in total. Unlike Criteo CTR, the columns in Criteo SSCL are labeled, and we provide the mapping in Appendix L. The missing values of the dataset are handled by (i) removing the datapoints in which the target is missing, (ii) group all the infrequent categorical values (occurring at most 5 times) along with the missing value (which is -1) into one category for each categorical feature, and (iii) replace all missing values by the mean of that feature for each numerical column. Since this is a regression dataset we do not transform the numerical features, while the categorical features are encoded as non-negative integers. The label is a non-negative real value. For both Criteo CTR and SSCL, we create two types of LLP Data sets. We create 4 Random Bag datasets by randomly sampling without replacement (Similar to D1 in Figure 1) to create bags of fixed sizes of 64, 128, 256, 512. Let U denote the categorical columns { C1, . . . , C26 } in the case of Criteo CTR and { \u02dc C1, . . . , \u02dc C17 } for Criteo SSCL. Next, we create Feature Bag datasets by grouping instances by subsets C \u2286 U , of the categorical columns, where C \u2264 2. The feature grouping subset used for a dataset is called its grouping key. For each setting of the values of C we obtain a bag with instances with those values of C (Similar to D2 and D3 in Figure 1). Each such key grouping yields an LLP dataset 3 . Thus, we obtain ( 26 2 ) + 26 = 351 LLP datasets from Criteo CTR and ( 17 2 ) + 17 = 153 LLP datasets corresponding to Criteo SSCL, each referred to also as a dataset on C ( | C | \u2264 2). Note that for any dataset, the set of bags partition the dataset and therefore each instance occurs in exactly one bag. Next we describe two filtering strategies to remove datasets that are ineffective in practice. The first strategy removes very small or very large bags within a dataset. The second strategy drops a dataset entirely if the first filtering method results in pruning a large portion of the underlying dataset.", "4.1 Bag Filtering": "The feature bag creation step leads to bags of varying sizes. Very small bags are not practical because they do not preserve enough privacy. For instance, datasets on { C10, C16 } and { C4, C10 } from Criteo CTR each contain more than 8 \u00d7 10 6 bags. We introduce a hyper-parameter lowthresh which represents the size of the smallest bag that can be present in a LLP-Bench dataset. Similarly, a very large bag is almost useless since the information lost via aggregation is significant and hence the dataset cannot be used to build a useful classifier. For instance, the initial dataset on C9 creates only 3 bags and the dataset on C20 creates only 4 bags. Similarly, for the Criteo SSCL Dataset, the initial dataset on \u02dc C1 created only 11 bags and the dataset on \u02dc C2 creates only 4 bags. We introduce a hyper-parameter highthresh . which represents the size of the largest bag that can be present in a LLP-Bench dataset. For our experiments we set lowthresh to be 50 and highthresh to be 2500. 1 https://github.com/DeepGraphLearning/RecommenderSystems/tree/master/featureRec . 2 https://paperswithcode.com/sota/click-through-rate-prediction-on-criteo 3 Note that for model training purposes such bags may be created from only the train set portion of the entire dataset 6", "4.2 Dataset Filtering": "If the lowthresh and highthresh based filtering remove a significant fraction of bags then most of the underlying instances will be lost. This will cause the LLP dataset's performance to be poor because we are not left with enough signal to train on. We hence drop datasets that have less than instance thresh %of the original instance data size. We set instance thresh to be 30% in our experiments. After applying this filter, we are left with 52 LLP datasets from Criteo CTR and 10 from Criteo SSCL. All the datasets in single columns are filtered out as the maximum percentage of instances any of these datasets retains is 21.68% (C4) for datasets formed using Criteo CTR dataset and 28.84% ( \u02dc C5) for datasets formed using Criteo SSCL dataset. This analysis is presented in detail in Appendix E. For notational convenience in the rest of this paper, we shall call ( A , B ) where A , B \u2208 U the LLP dataset formed via grouping by the subset C = { A , B } . In Appendix F we demonstrate the creation and performance metrics of fixed size feature bag datasets as well in which for each of the 52 groupings from Criteo CTR and bag size q \u2208 { 64, 128, 256, 512 } , the train instances are ordered according to the grouping features and consecutive q -sized sequences are made into bags.", "5 Diversity of the Benchmark": "Figures 2 and 5 depict for each of the 52 feature bag LLP datsets from Crireo CTR and 10 from Criteo SSCL respectively chosen in Sec. 4.2, the values of three different bag-level metrics: (i) MeanBagSize - the average size of bags, (ii) LabelPropStdev - the standard deviation of the bag label proportions, and (ii) InterIntraRatio as given defined in Sec. 3.1. Apart from capturing the bag size and label proportion distribution, the third metric also quantifies the geometric distribution of the feature-vectors w.r.t the bags, in particular how clustered the feature-vectors in an average bag are. Criteo CTR. We see in Figure 2a that MeanBagSize values range from nearly 500 to around 200. While most of them are in [ 150, 250 ] , around one-fourths of the values are above 300, indicating significant diversity in the values of this metric. In Figure 2b we see a similar trend in LabelPropStdev - while most of the values are in the range [ 0.15, 0.18 ] , around 25% of the them are below 0.14. This unsurprising since larger bags would typically lead to more concentrated label proportions, and thus MeanBagSize is loosely anti-correlated to LabelPropStdev and our collection of datasets have similar diversity of the latter's values. Figure 2c has the values for InterIntraRatio showing that they are well spread across the range [ 1.1, 1.6 ] . While most values are below 1.4, there are around 17% of them which are above 1.5, indicating that the distribution has a fat tail and metric values are diverse. Criteo SSCL. Similar trends are also observed from Figure 5, albeit on a smaller number of points. In Figure 4a MeanBagSize ranges from 200 to around 325, with values evenly distributed in the approximate range 200-240 except two values exceeding 280 indicating the diversity of this metric. The values of LabelPropStdev in Figure 4b has a diverse distribution in the range around 12.5 to 14 barring one outlier value near 11.5 corresponding to a dataset which has the minimum MeanBagSize . Figure 4c also indicates a fairly uniform spread of InterIntraRatio values in its range around 1.18 to around 1.26. In Appendix J we include the 3-D scatter plot of the the three metrics MeanBagSize , LabelPropStdev and InterIntraRatio as well as three 2-D scatter plots for each of the three pairs to show that the metrics have significant variability and limited dependency with respect to each other. In Appendix K we compute the Cramer's V between grouping-key pairs ( A , B ) and the label for each feature-bag LLP-Bench datasets from Criteo CTR. We observe that there is a significant diversity in the values which are all bounded away from 1, indicating that the datasets are diverse in terms of the bag vs label correlations. 7", "6 Evaluation of baselines on LLPBench": "Training and test data setup. For each feature-grouping based dataset ( A , B ) available after filtering in Section 4.2, we create the a 5 fold train/test split as follows. We first create the dataset ( A , B ) over the entire Criteo dataset and filter out the bags as mentioned in Section 4.1. Using the feature-vectors in the remaining bags and their original labels, we recreate the instance-level dataset. This is then used to create a 5-fold train/test split. For each train split we recreate the bag-level training data via grouping by ( A , B ) . For the fixed-size random bag datasets, we first do a 5-fold train/test split of the entire data and partition the train splits into bags of the given fixed size. Training Methodology. All of our baselines (listed below) are trained on the LLP datasets using minibatch training. We sample 8 bags in each minibatch and do a forward-pass of the model on all the feature-vectors in the minibatch of bags. This allows us to obtain the predicted label proportions for each of the bags as well as the instance-level predicted-labels. Using these along with the true label proportions we compute the appropriate loss functions of the different methods. The back-propagation step then updates the model parameters. We evaluate the following baseline methods on Criteo CTR LLP-Bench datasets whose methdology was explained in Section 2. DLLP [1], GenBags : [33], Easy -LLP [4], OT methods:[22, 13] ( Hard -EROT -LLP and Soft -EROT -LLP , the hard and soft entropic regularized OT methods of [22]), SIM -LLP : [19] Mean -Map : [29]. On the LLP-Bench datasets from Criteo SSCL which is a regression dataset, we evaluate the applicable baselines: DLLP -MSE , DLLP -MAE , GenBags and SIM -LLP . Using each of the above methods, a 2-layer perceptron is trained using a fixed learning rate of 1 e -5. It has a multi-hot encoding layer which takes as input index encoded feature-vectors, followed by 128 and 64 node hidden layers respectively with relu activation. The final output node is sigmoid activated in case of Criteo CTR LLP datasets for which we report the area under the ROC curve (AUC) scores for each of the above LLP methods, while on Criteo SSCL LLP-Bench datasets MSE is the evaluation metric. We use an Adam optimizer with learning rate of 1 e -5. We also implement early-stopping with patience of 3 epochs which monitors accuracy on the test set. Additional details of the implementation of the above methods are included in Appendix B. The AUC and MSE scores for the LLP-Bench datasets and the baselines as applicable are included in Appendix D.", "6.1 Performance of baselines on Feature Bags": "", "6.1.1 Criteo CTR": "Fig. 3a presents the trend of AUC scores for the previously described LLP methods w.r.t the MeanBagSize metric where the x-axis has the datasets ordered by increasing MeanBagSize . Similarly in Figures 3b and 3c the datasets are ordered on the x-axis by increasing LabelPropStdev and InterIntraRatio respectively. First we observe that the SIM -LLP , DLLP -BCE , DLLP -MSE , and GenBags methods are the best performing on all of the datasets with AUC scores in the 72%-78% range. Within them, SIM -LLP performs the best on 41, DLLP -MSE on 6 and DLLP -BCE on 5 datasets, indicating that the additional similarity based loss helps on feature bag datasets. The AUC scores of GenBags is consistently lower than SIM -LLP and the DLLP methods, possibly due to the fact that our scenario does not have multiple bag distributions and therefore no corresponding convex programming solution to obtain the combining weights. This leads to undesirable combinations of large bags with smaller ones with roughly equal weights (ideally smaller bags should receive larger weights), leading to loss in the bag-label supervision. On the other hand, the AUC scores of Mean -Map are the lowest for nearly all the datasets, remaining below 67%. The Easy -LLP and the OT methods have scores typically in the range of 65%-70%. For reference, the same model trained on instance-level data yields around 80% AUC score (see Appendix C). Since the datasets are created by feature-based aggregation they may not satisfy the distributional and generative model assumptions of [29], possibly explaining the lower scores of Mean -Map . Similarly, Easy -LLP 8 Table 2: Range of AUC scores on feature-bag Criteo CTR datasets is tailored towards random-bags, and therefore may have lower scores on these datasets. The lower performance of the OT methods indicates that the pseudo-labels computed in their optimization step could significantly differ from the true labels. On the other hand, optimizing the bag-level losses as in DLLP based methods leads to more accurate model training. The trends w.r.t. the metrics are as expected. There is a moderately decreasing trend of AUC scores with increasing MeanBagSize which is unsurprising since larger bags provide lower label supervision. More interestingly, there is a clear increasing trend with increasing LabelPropStdev along with a moderate increasing trend with increasing InterIntraRatio . The latter two trends are also expected, given the explanations in Sec. 3.", "6.1.2 Criteo SSCL": "Fig. 5a depicts the MSE scores for the methods evaluated on the LLP datasets from Criteo SSCL, where the x-axis has the datasets ordered by increasing MeanBagSize , while Figures 5b and 5c have the datasets ordered by increasing LabelPropStdev and InterIntraRatio respectively. 9 Table 3: Dataset Statistics for Bag Datasets formed using Criteo SSCL Table 4: MSE on Random Bags Datasets formed using Criteo SSCL Table 5: Range of MSE on feature-bag datasets formed using Criteo SSCL 10 We observe that SIM -LLP an DLLP -MSE are the best performing methods overall, followed by DLLP -MAE and GenBags . Since the comparison is w.r.t. test MSE score, it is possible that SIM -LLP an DLLP -MSE which are MSE loss based methods optimize better for the evaluation metric. Further, as explained previously in this subsection, our scenario has just disjoint bags and no convex program is solved for GenBags possibly leading to its worse performance. Since the number of feature bag LLP datasets from Criteo SSCL is only 10, the trends w.r.t to the bag-level metrics are not as evident as in the Criteo CTR case which has 52 such datasets. Nevertheless, we do observe a decrease in the MSE loss (i.e., increase in model performance) with increasing MeanBagSize .", "6.2 Performance of Baselines on Random Bags": "", "6.2.1 Criteo CTR": "Table 1 reports the AUC scores obtained by running baselines on random bags of sizes 64, 128, 256, 512. We observe that the performance of Easy -LLP is better random bags as compared to its performance on feature bags. As mentioned above, the performance guarantees for Easy -LLP assume random bags therefore this improvement is expected. We also notice the SIM -LLP no longer outperforms DLLP -BCE and DLLP -MSE on random bags as it did on feature bags. The instances in feature bags are closer than those in random bags. As pairs of instances sampled in the same mini-batch are likely belong the same bag, the weight factor in the similarity loss (exp ( \u2225 x i -x j \u2225 2 2 ) ) is likely to be higher in case of feature bags leading to greater supervision as compared to random bags. Further, as the random bag size increases, in SIM -LLP due to lower label supervision the magnitude of the bag-loss could decrease, making the similarity loss more dominant which spuriously penalizes pairs of instances with different labels, leading to an overall moderation in performance. While the performance of Easy -LLP reduces noticeably with increase in bags size, the corresponding degradation of the OT methods is particularly significant. Notably, these techniques derive instance level surrogate or pseudo-labels as training labels for the model. These trends suggest that such pseudo-labeling techniques are more severely affected by increasing bag size.", "6.2.2 Criteo SSCL": "Table 4 provides the MSE scores for the DLLP -MSE , DLLP -MAE , GenBags and SIM -LLP methods on size 64, 128, 256, 512 random bags datasets from Criteo SSCL. Firstly, we observe that the scores for each of the methods expectedly degrade with increasing bag size. The main takeaway however is that, while the scores for each of the methods are fairly similar for bag sizes 64 and 128, the GenBags method outperforms the other methods by a wide margin on bag sizes 128 and 512. This is unlike feature bags (see previous subsection) on which GenBags has performance similar or slightly worse than other methods. One reason could be that while the lack of convex programming based weights for GenBags hurts on feature bags, since each random bag has the same distribution a random weight combination works much better in this case. We also do not observe such benefit with random bags on Criteo CTR, likely because the regression task on Criteo SSCL is more amenable to the GenBags methodology of linearly combining bags.", "7 Detailed Analysis": "In this section, we present more detailed analysis of various baselines and interesting feature bag datasets in the benchmark. In order to perform a subjective analysis of the datasets, we classify them - separately for Criteo CTR and SSCL derived datasets - based on their metric values according to the following criteria: Tail size : We perform k -Means clustering where each dataset is represented by a four tuple of bag size at x percentile where x \u2208 { 50, 70, 85, 95 } (See Sec 3.1). For Criteo CTR we take k = 4 and name these clusters as very short-tailed , short-tailed , long-tailed and very long-tailed in increasing order of mean bag size at 70 11 percentile of each cluster. On Criteo SSCL we take k = 3 and similarly name the clusters as short-tailed , medium-tailed and long-tailed . Label Variation : For Criteo CTR, we perform 4-Means clustering with each dataset represented by LabelPropStdev . We classify the datasets as low , medium , high and very high LabelPropStdev dataset in increasing order of the mean LabelPropStdev of the cluster. On Criteo SSCL 3-Means clustering is done and the corresponding labels are low , medium and high . Bag Separation : We perform 4-Means clustering on Criteo CTR derived datasets using InterIntraRatio classifying them as less-separated , medium-separated , well-separated and far-separated in increasing order of mean InterIntraRatio of each cluster. As above, for Criteo SSCL datasets 3-Means clustering is done and clusters are annotated less-separated , medium-separated and well-separated . The detailed classifications based on mean bag size, LabelPropStdev and InterIntraRatio of all LLP-Bench datasets are respectively reported Appendices E.1, E.2 and E.3.", "7.1 Best and worst performance of every baseline": "Tables 2 and 5 list the range of scores of the methods evaluated on the Criteo CTR (AUC) and Criteo SSCL (MSE) feature bag datasets respectively. From these a range of AUC scores of at least 4 percentage points for all the baselines on Criteo CTR, and an MSE range of 12-15% of the maximum MSE for each baseline on Criteo SSCL datasets is observed. This shows that LLP-Bench has enough diversity in the underlying datasets and that it can be used to find opportunities to improve SOTA algorithms to further LLP research on Criteo CTR/SSCL and other tabular datasets. Criteo CTR. We observe that all baselines perform the best on the dataset ( C3, C11 ) and perform the worst on the dataset ( C6, C10 ) . ( C3, C11 ) is a very short-tailed dataset with high standard deviation in the label proportion and it well-separated . This makes this dataset relatively easy to learn on. On the other hand, ( C6, C10 ) is a long-tailed dataset with very low standard deviation in the label proportion and it Less-Separated . This dataset represents the worst-case scenario of the three feature combination making it very hard to learn on. Criteo SSCL. Here as well there is one dataset ( \u02dc C 3, \u02dc C 8 ) having the best performance and one dataset ( \u02dc C 5, \u02dc C 6 ) giving the worst performance, for all the three baselines. The former -( \u02dc C 3, \u02dc C 8 ) - has the lowest MeanBagSize and also has short-tailed bag sizes making it relatively easier to learn on, while ( \u02dc C 5, \u02dc C 6 ) is less-separated with high LabelPropStdev which could make it a relatively intractable LLP dataset.", "7.2 Dataset Analysis": "In this section, we select a few datasets that do not perform as expected in the trend-lines of Figure 3. We analyse them further to explain the performance metrics that we observe. On the LLP datasets from Criteo CTR we have the following: 1. Datasets ( C4, C15 ) and ( C4, C10 ) are medium-separated but they perform better than other such datasets (Fig 3c). This is because they are short-tailed and very short-tailed respectively. 2. Datasets ( C7, C8 ) and ( C1, C7 ) are well-separated but they perform worse than other such datasets (Fig 3c) because they have very low LabelPropStddev and are long-tailed . Similarly, datasets ( C6, C7 ) , ( C7, C14 ) and ( C7, C20 ) are all well-separated and yet they perform poorly that other such datasets. They are all very long-tailed and have very low LabelPropStdDev (except ( C7, C14 ) which has a low LabelPropStdDev ). 3. It can also be observed from Fig 3b that the datasets ( C7, C20 ) , ( C1, C7 ) and ( C7, C8 ) actually perform better as compared to other very low LabelPropStdDev datasets because they are long-tailed and well-separated (except ( C7, C20 ) which is very long-tailed ) 4. Dataset ( C4, C15 ) performs poorly as compared to other medium LabelPropStddev datasets even when it is short-tailed as it is less-separated (Fig 3b) 5. Dataset ( C7, C26 ) is really interesting since it performs poorly as compared to other medium LabelPropStddev , far-separated and short-tailed datasets. 12 6. Datasets ( C2, C11 ) and ( C2, C13 ) perform slightly better than other datasets with comparable MeanBagSize (Fig 3a). They are both long-tailed , with low LabelPropStddev and medium-separated and hence their higher performance cannot be explained based on our metrics. On Criteo SSCL dataset we observe that GenBags performs significantly worse than the other baselines on ( \u02dc C 8, \u02dc C 16 ) . This has the highest InterIntraRatio and is the only well-separated LLP dataset from Criteo SSCL. Since GenBags takes linear combinations of bags it could diminish this well-separatedness of he bags, while the other baselines preserve it, leading to a comparative degradation of GenBags . Experimental Code. The code for the experiments included in this paper is available at https://github. com/google-research/google-research/tree/master/LLP_Bench", "8 Conclusion": "We present the design of LLP-Bench: a diverse collection of tabular LLP datasets from the Criteo CTR and SSCL datasets as a benchmark for evaluating LLP techniques on binary claasification and regression tasks. In this process, our work analyzes bag collections given by grouping on at most two categorical features, based on their distribution of bags as well as label proportions. We show that LLP-Bench has significant diversity in the nature of the datasets that are present in it. To the best of our knowledge, LLP-Bench is the first large scale tabular benchmark with extensive diversity in the underlying datasets. We presented a detailed analysis of 9 SOTA baselines on LLP-Bench and explained their performance by correlating it with the underlying dataset characteristics. Again, to the best of our knowledge no other study has compared SOTA techniques to this level of detail that we have. We believe our work addresses to a great extent the current lack of a large scale tabular LLP benchmark. LLP-Bench along with the four dataset hardness metrics can be used to systematically study and design new LLP techniques. Limitations and Future Work. While the performance and outlier analysis (Sections 6.1 and 7) use up to four metrics, a deeper explanation of outliers could be possible using additional metrics. Future work could also incorporate more LLP algorithms as well as additional model architectures. 13 Figure 2: Datasets vs. bag-level metrics: y -axis has the metric, x -axis has the datsets. (a) MeanBagSize (b) LabelPropStdev (c) InterIntraRatio (a) MeanBagSize (b) LabelPropStdev (c) InterIntraRatio 400 350 200 200 0 16 0 15 0.14 DLLP BCE DLLP MSE 0 13 GenBags Easy LLP 0 12 OT LLP SIM LLP Soft EROT LLP Hard EROT LLP Mean Map Figure 3: Datasets performance: AUC scores on the y-axis, x -axis has the datasets ordered according to increasing metric. 14 320 300 280 260 240 220 200 9 9 (a) MeanBagSize 14.0 13.5 13.0 12.5 12.0 11.5 9 (b) LabelPropStdev 125 124 123 122 121 120 119 9 9 9 (c) InterIntraRatio Figure 4: Criteo SSL Datasets vs. bag-level metrics: y -axis has the metric, x -axis has the datsets. 200 195 190 185 180 175 170 9 (a) MeanBagSize 200 195 190 185 180 175 dlp_mae dlp_mse 170 genbags sim Ilp 9 (b) LabelPropStdev 200 195 190 185 180 175 170 9 9 (c) InterIntraRatio Figure 5: Criteo SSL Datasets performance: Test MSE on the y -axis, x -axis has the datasets ordered according to increasing metric. 15", "References": "16 17 18", "A Proofs of Lemmas and Algorithms": "While BagSep is not a metric since BagSep ( B , B ) is not necessarily zero, the following lemma (proved in Appendix A.1) shows that it does satisfy the other metric properties. Lemma A.1 BagSep satisfies non-negativity, symmetry and triangle inequality. We have the following lemma proved in Appendix A.2. Lemma A.2 For any bag B, (i) InterBagSep ( B , d ) / BagSep ( B , B , d ) \u2265 1/2 when d is a metric, (ii) InterBagSep ( B , d ) / BagSep ( B , B , d ) \u2265 1/4 when d is the \u2113 2 2 distance. The following is a straightforward corollary of Lemma A.2. Corollary A.3 (i) When d is a metric: MeanInterBagSep ( B , d ) / MeanIntraBagSep ( B , d ) \u2265 1/2. (ii) When d is the \u2113 2 2 distance: MeanInterBagSep ( B , d ) / MeanIntraBagSep ( B , d ) \u2265 1/4. We expect this ratio to achieve values substantially less than 1 in adversarial cases. Appendix A.5 provides an example of such a case. For convenience, for B , we use InterIntraRatio to denote MeanInterBagSep ( B , d ) / MeanIntraBagSep ( B , d ) when d = \u2113 2 2 .", "A.1 Proof of Lemma A.1": "Proof. From Def. 3.2, the non-negativity and symmetry properties are obvious. Triangle Inequality : let B 1 , B 2, B 3 \u2208 B , and we use the following notation for convenience: B 1 = { xi | i \u2208 [ n ] } , B 2 = { yj | j \u2208 [ m ] } , B 3 = { z k | k \u2208 [ l ] } . As d is a metric, we know that for all i \u2208 [ n ] , j \u2208 [ m ] and k \u2208 [ l ] , d ( xi , z k ) \u2264 d ( xi , yj ) + d ( yj , z k ) . Hence,  \u25a1 \u25a1", "A.2 Proof of Lemma A.2": "Proof. Let B \u2208 B . Using triangle inequality and symmetry from Lemma A.1 : \u2200 B \u2032 \u2208 B , BagSep ( B , B , d ) \u2264 BagSep ( B , B \u2032 , d ) + BagSep \u21d2 \u2200 B \u2032 \u2208 B , BagSep ( B , B , d ) \u2264 2 BagSep ( B \u2032 , B , d ) \u0338 \u21d2 BagSep ( B , B , d ) \u2264 2 \u2211 B \u2032 \u2208 B , B \u2032 = B BagSep ( B \u2032 , B , d ) | B | -1 \u21d2 BagSep ( B , B , d ) \u2264 2 InterBagSep ( B , d ) \u21d2 InterBagSep ( B , d ) / BagSep ( B , B , d ) \u2265 1/2 ( B \u2032 , B , d ) 19 The squared euclidean distance is not a metric as it follows all properties other than the triangle inequality. Hence, we show the following  Theorem A.5 Given X, Y and B , for any B 1 , B 2, B 3 \u2208 B ,  Proof. Follows by replacing triangle inequality in Lemma A.1 with inequality in Lemma A.4 \u25a1  Proof. Follows by replacing inequality in proof of Lemma A.2 with inequality in Theorem A.5 \u25a1", "A.3 Proof of Corollary A.3": "Proof. Given X , Y and B , and metric d in R n . Starting with inequality in Lemma A.2 \u2200 B \u2208 B , BagSep ( B , B , d ) \u2264 2 InterBagSep ( B , d ) \u21d2 BagSep ( B , B , d ) \u2264 2 InterBagSep ( B , d B \u2211 \u2208 B \u2211 B \u2208 B  \u21d2 MeanInterBagSep ( B , d ) / MeanIntraBagSep ( B , d ) \u2265 1/2 Starting with inequality for \u2113 2 2 -distance in Lemma A.2 , we get MeanInterBagSep ( B , \u2113 2 ) / MeanIntraBagSep ( B , \u2113 2 ) \u2265 1/2 2 2 \u25a1", "A.4 Bag Distance Results using squared euclidean distance": "We use the squared euclidean distance to compute the bag distances as it makes the computation faster. Algorithm 1 is used to compute the Bag Separation for any general metric d . Theorem A.7 Assuming the Bags to be disjoint, the running time of Algorithm 1 is O ( m 2 n ) where m is the number of examples and n is the dimension of the input space.   Now, this computation can be simplified due to the following. Let \u2225 B \u2225 : = 1 | B | \u2211 x \u2208 B \u2225 x \u2225 2 2 and \u00b5 ( B ) : = 1 | B | \u2211 x \u2208 B x Lemma A.8 For any B , B \u2032 \u2208 B ,  ) 20", "Algorithm 1: Compute Bag Separation of a dataset": "Data: Set of bags B , metric d on R n Result: BagSepMatrix ( B , d ) BagSepMatrix \u2190 [ 0 ] | B | x | B | for B 1 \u2208 B do for B 2 \u2208 B do for i \u2208 B 1 do for j \u2208 B 2 do BagSepMatrix [ B 1 , B 2 ] \u2190 BagSepMatrix [ B 1 , B 2 ] + d ( x ( i ) , x ( j ) ) end end BagSepMatrix [ B 1 , B 2 ] \u2190 BagSepMatrix [ B 1 , B 2 ] / ( | B 1 || B 2 | ) end end   Lemma A.9 Given B and B \u2208 B ,  Proof. First part is trivial from Lemma A.8. If B = { Bi | i \u2208 [ m ] } , \u0338  \u0338     \u25a1 \u25a1 21 Algorithm 2 is used to compute the Bag Separation for squared euclidean distance. Algorithm 2: Compute Bag Separation with squared euclidean distance Data: Set of bags B Result: MeanIntraBagSep ( B , \u2113 2 2 ) , MeanInterBagSep ( B , \u2113 2 2 ) MeanIntraBagSep \u2190 0 MeanInterBagSep \u2190 0 AvgSqNorm \u2190 [ 0 ] | B | BagMeans \u2190 [ 0 ] | B | xn SumofAvgSqNorm \u2190 0 SumofBagMeans \u2190 [ 0 ] 1 xn SumofBagMeansNorms \u2190 0 for B \u2208 B do for i \u2208 B do AvgSqNorm ( B ) \u2190 AvgSqNorm ( B ) + \u2225 x ( i ) \u2225 2 2 BagMeans ( B ) \u2190 BagMeans ( B ) + x ( i ) end AvgSqNorm ( B ) \u2190 AvgSqNorm ( B ) / | B | BagMeans ( B ) \u2190 BagMeans ( B ) / | B | end for B \u2208 B do MeanIntraBagSep \u2190 MeanIntraBagSep + 2 [ AvgSqNorm ( B ) -\u2225 BagMeans ( B ) \u2225 2 2 ] SumofAvgSqNorm \u2190 SumofAvgSqNorm + AvgSqNorm ( B ) SumofBagMeans \u2190 SumofBagMeans + BagMeans ( B ) SumofBagMeansNorms \u2190 SumofBagMeansNorms + \u2225 BagMeans ( B ) \u2225 2 2 end MeanIntraBagSep \u2190 MeanIntraBagSep / | B | MeanInterBagSep \u2190 2 | B | SumofAvgSqNorms -2 | B | ( | B |-1 ) [ \u2225 SumofBagMeans \u2225 2 2 -SumofBagMeansNorms ] Theorem A.10 Assuming the Bags to be disjoint, the running time of Algorithm 2 is O ( mn + | B | n + | B | ) where m is the number of examples and n is the dimension of the input space.  \u25a1", "A.5 Adversarial Example of Bags with Ratio of Mean Inter to Intra Bag Separation as 1/2": "Consider X = { x ( 1 ) , x ( 2 ) , x ( 3 ) } which lie on a straight line. The distances are as follows: \u00b7 d ( x ( 1 ) , x ( 2 ) ) = d 1 \u00b7 d ( x ( 2 ) , x ( 3 ) ) = d 2 \u00b7 d ( x ( 1 ) , x ( 3 ) ) = d 1 + d 2 We have two bags B 1 = { x ( 1 ) , x ( 3 ) } and B 2 = { x ( 2 ) } . The Intra-bag separations for both of them are as follows: \u2022 BagSep ( B 1 , B 1 , d ) = ( d ( x 1 , x 1 ) ) + d ( x 1 , x 3 ) ) + d ( x 3 , x 1 ) ) + d ( x 3 , x 3 ) )) = 1 ( ) ( ( ) ( ( ) ( ( ) ( 1 2 2 2 ( d 1 + d 2 ) \u00b7 BagSep ( B 2, B 2, d ) = 0 Hence, MeanIntraBagSep ( B , d ) = 1 4 ( d 1 + d 2 ) . Now, the bag separation between the bags is as follows: 22 \u00b7 BagSep ( B 1 , B 2, d ) = 1 1 \u00d7 2 ( d ( x ( 1 ) , x ( 2 ) ) + d ( x ( 3 ) , x ( 2 ) )) = 1 2 ( d 1 + d 2 ) \u00b7 InterBagSep ( B 2, d ) = 1 2 -1 ( BagSep ( B 2, B 1 , d )) = 1 2 ( d 1 + d 2 ) \u00b7 InterBagSep ( B 1 , d ) = 1 2 -1 ( BagSep ( B 1 , B 2, d )) = 1 2 ( d 1 + d 2 ) Hence, MeanInterBagSep ( B , d ) = 1 ( d + d ) 2 1 2 . Hence, MeanInterBagSep ( B , d ) / MeanIntraBagSep ( B , d ) = 1/2", "B Additional Details of Experimental Setup": "We begin with additional details on the different baselines evaluated in our experiments. DLLP : For a bag B , the DLLP -BCE loss is given by bce ( yB / | B | , \u02c6 yB / | B | ) where yB and \u02c6 yB are the given and predicted label sums of bag B where bce is the binary cross-entropy, that of DLLP -MSE is ( yB -\u02c6 yB ) 2 , and that of DLLP -MAE is | yB -\u02c6 yB | . The minibatch loss is the sum of the per-bag losses over the 8 bags in the minibatch. GenBags : We divide the 8 bags in a minibatch into 2 blocks of 4 bags each. For each block 60 iid values of w = ( w 1 , . . . , w 4 ) are sampled from N ( 0 , \u03a3 ) where \u03a3 is the inner product matrix of the directions of the corners of the tetrahedron centered at origin. In particular, the diagonal entries of \u03a3 are 1 and all the off-diagonal entries are -1/3. This is a solution to the SDP of [33] for the case of the 4 bags in a minibatch being iid random, and we utilize this in our implementation. For each of the 60 samples of w we create a generalized bag with those weights. In total we have 120 generalized bags derived from a minibatch of 8 bags. Easy -LLP : We directly implement the soft-surrogate label loss given in Defn. 3.4 of [4] which is instantiated using the BCE loss at the instance-level. OT methods: There are pseudo-labeling techniques based on optimal transport proposed in [22]. We first train our model to convergence using DLLP -BCE . Then we begin pseudo-labeling by constructing an OT problem described in Eqn 7 of [22]. We implement this without Entropic Regularization which we call OT -LLP . We also implement it with Entropic Regularization. We call these Hard -OT -LLP and Soft -OT -LLP based on whether we use hard or soft pseudo-labels. SIM -LLP : In this method proposed by [19], the bag-level DLLP loss is augmented with a pairwise similarity based loss penalizing different predictions of geometrically close feature-vectors (Eqn 3 in [19]). Since the similarity based loss has number of terms which is square in the number of feature-vectors in a minibatch, we sample a random set of 400 feature-vectors from each minbatch to apply this loss. Mean -Map : The optimization given in Algorithm 1 of [29] is implemented in two steps. The quantities \u02c6 \u00b5 XY therein are first computed and then the computation for \u02c6 \u03b8 \u2217 is implemented using a minibatch optimization along the same lines as the above methods.", "C Instance-level Model Training Results": "We perform instance-level training of our model on Criteo CTR Dataset for comparison. We perform a train-test spilt of 80:20 on the dataset. We then train using instance level mini-batch gradient descent for the same number of epochs, using the same optimizer, model, learning rate schedule and the instance-level variant of the loss function. We obtain an AUC score of 80.1 with BCE loss and an AUC score of 79.94 with MSE loss. We also perform instance-level training of our model on Criteo SSCL Dataset performing the same train-test spilt. We obtain an MSE of 147 after training with MSE loss. 23", "D Baseline Training Results": "Table 8 reports the AUC scores of all the baselines on our bags created using Criteo CTR Dataset. We take the best test AUC score during each training configuration. The mean and standard deviation over 5 splits has been reported. Table 19 reports the best MSE scores of all the baselines on our bags created using Criteo SSCL Dataset. Again, the mean and standard deviation over 5 splits has been reported.", "E Bag creation and filtering Statistics": "The statistics of datasets created as described in Section 4 before and after clipping for all 349 datasets are in Table 14. We report the number of bags created, number of bags retained after clipping, percentage of instances left after clipping and the mean and standard deviation of bag size in each dataset. The datasets which are emboldened pass our filter and are used for training.", "E.1 CumuBagSizeDist for LLP-Bench datasets": "Table 9 contains the threshold bags sizes such that t %of the bags have at most that size, for t = 50, 70, 85, 95 for LLP-Bench datasets formed using Criteo CTR Dataset. The Tail size cluster to which each dataset is assigned is also listed. Table 16 contains the same for LLP-Bench datasets formed using Criteo SSCL Dataset.", "E.2 LabelPropStdev and Label Variation clusters for LLP-Bench datasets": "Table 10 contains the LabelPropStddev for all LLP-Bench datasets formed using Criteo CTR Dataset. The Label Variation cluster to which each dataset is assigned is also listed. Table 18 does the same for LLP-Bench datasets formed using Criteo SSCL Dataset.", "E.3 InterIntraRatio and Bag Separation clusters for all datasets": "Table 11 contains MeanInterBagSep , MeanIntraBagSep and InterIntraRatio for all datasets in LLP-Bench formed using Criteo CTR Dataset. It also contains the information of the Bag Separation cluster to which each of these datasets are assigned. Table 17 does the same for LLP-Bench datasets formed using Criteo SSCL Dataset.", "F Training results on Fixed size Feature-bags Datasets": "We also create and train our model on fixed size feature bags . To create these datasets, we first perform a 5-fold split of the Criteo CTR Dataset. Next, for each group key C corresponding to an LLP-Bench dataset, we construct a random ordering of the train set with the constraint that feature vectors with same values of attributes in C lie in a contiguous segment. We then assign contiguous segments of size k to the same bag to create fixed size feature bags for k \u2208 { 64, 128, 256, 512 } . We train our DLLP , GenBags , Easy -LLP , OT -LLP , SIM -LLP and Mean -Map baselines on these fixed size feature bags and report the mean and std of test AUC scores in Table 12. For each group key C , Table 12 contains 4 contiguous rows, one for bag size { 64, 128, 256, 512 } each in ascending order.", "G Training results on Random Bags": "Table 1 is replicated along with the standard deviation of AUC scores across 5 splits in Table 13. 24", "H Feature Bag Datasets with Group Key Size 3": "We also calculate the number of additional datasets which would have been created had we also considered group keys of size 3. Using instance thresh %as 30%, we retain 1195 of ( 26 3 ) datasets. It would be intractable to handle so many datasets and we believe that the current benchmark provides sufficient diversity.", "I Analysis of label proportions of large bags": "Large bags with label proportions close to 0 or 1 contain a considerable amount of information. In this section we calculate the percentage of instances which lie in large bags (bags with size greater than highthresh = 2500) with skewed label proportions. We calculate this for each feature bag dataset. Since we throw these bags out of our dataset, if the percentage of such instances is low then we do not lose much information. We say that the label proportion of a bag is skewed if either it is less than \u03b5 or greater than 1 -\u03b5 . Table 6 reports these percentages for \u03b5 = 0.1 and \u03b5 = 0.05 respectively. It can be seen that the maximum percentages over all datasets with \u03b5 = 0.1 and \u03b5 = 0.05 are 6.66% and 1.42% respectively. This shows that there are relatively low number of such datapoints and they can be dropped so that neural network training is tractable.", "J Dataset diversity analysis": "Figure 8a shows a scatter plot with the three metrics, MeanBagSize , LabelPropStddev and InterIntraRatio on its axes. Each point represents one of the datasets in our benchmark. Figures 8b, 8c and 8d shows the projections of Figure 8a on MeanBagSize vs LabelPropStddev plane, LabelPropStddev vs InterIntraRatio plane and InterIntraRatio vs MeanBagSize plane respectively. The diversity of LLP-Bench is more apparent from these scatter plots as diversity afforded due to combination of these metrics can be visualized. For better readability and ease of reference we replicate (magnified versions of) Figures 2 and 3 as Figures 6 and 7 respectively.", "K Cramer's V between grouping-key and label for LLP-Bench Datasets": "It is important to see how each grouping-key pair ( A , B ) of LLP-Bench feature-bag datasets is correlated with the label. If the correlation is high, then most bags of the dataset corresponding to ( A , B ) will have label proportions close to 0 or 1. On the other hand, bags have mixed labels if the correlation is low. Since both, the labels and grouping-key ( A , B ) are categorical, we compute Cramer's V between them as follows. Given two categorical features ( X , Y ) such that X \u2208 { 1, . . . , r } and Y \u2208 { 1, . . . , c } , let N be the total number of data points. Let Oi , j be the total number of times X takes the value i and Y takes the value j for ( i , j ) \u2208 { 1, . . . , r } \u00d7 { 1, . . . , c } . We define the expected occurence of this event assuming independence of X and Y as Ei , j = Npiqj where pi = \u2211 c j = 1 Oi , j / N and qj = \u2211 r i = 1 Oi , j / N . We define Cramer \u2032 sV as follows.  For our case, we use the pairs ( A , B ) as X and the label as Y . Thus, r will be the number of bags in that dataset and c = 2 since it is a binary classification dataset. Oi ,0 and Oi ,1 will be the number of instances in 25 the i th bag labeled 0 and 1 respectively. N will be the total number of instances in the dataset. We report the \u03c7 2 values, total number of instances and Cramer \u2032 s V for all LLP-Bench datasets in Table 7. We observe a minimum Cramer \u2032 s V of 0.22 and a maximum Cramer \u2032 s V of 0.39. The Cramer \u2032 s V is concentrated towards the maximium. We see sufficient diversity in LLP-Bench dataset with respect to Cramer \u2032 s V .", "L Criteo SSCL Column Names": "Table 20 has the names of the columns of the Criteo SSCL dataset. 26 Table 6: Percentage of large bags with label proportion less than \u03b5 or greater than 1 -\u03b5 . 27 (c) InterIntraRatio (a) MeanBagSize (b) LabelPropStdev 450 400 350 300 250 200 0.17 0.16 0.15 0.13 0.12 0.11 0.10 15 0.14 Figure 6: Datasets vs. bag-level metrics: y -axis has the metric, x -axis has the datsets. Replication of Figure 2. 28 vhk 90 Nr", "(a) MeanBagSize": "DLLP BCE DLLP MSE GenBags Easy LLP OT LLP SIM LLP Soft EROT LLP Hard EROT LLP Mean Map", "(b) LabelPropStdev": "99 (c) InterIntraRatio Figure 7: Datasets performance: AUC scores on the y-axis, x -axis has the datasets ordered according to increasing metric. Replication of Figure 3. 29 15 14 13 1 12 0 16 0 15 200 0 14 250 300 0 12 350 400 0 11 450 0 10 Label Prop Stddev Mcan Bag Size (a) MeanBagSize vs LabelPropStddev vs InterIntraRatio 15 14 1 13 12 0 10 0 11 0 12 0.14 0 15 0 17 Stddev Label Prop (b) MeanBagSize vs LabelPropStddev 0.17 0 16 0.15 0.14 0 13 0 12 0 10 200 250 300 350 400 450 Mean Bag Size (c) LabelPropStddev vs InterIntraRatio 450 350 { 300 250 200 12 13 14 15 InterlntraRatio (d) InterIntraRatio vs MeanBagSize Figure 8: Scatter plots for MeanBagSize vs LabelPropStddev vs InterIntraRatio and it's projections for Criteo CTR Dataset 30 125 124 1 129 14.0 13.5 13.0 200 220 12.5 240 260 280 12.0 300 320 11.5 Label Prop Stddev Mean Bag Size 125 124 123 1 122 121 120 119 11.5 12.5 13.0 13.5 14.0 Stddev Label Prop (a) MeanBagSize vs LabelPropStddev vs InterIntraRatio 14.0 13.5 13.0 12.5 12.0 11.5 200 220 240 260 280 300 320 Mean Bag Size (b) MeanBagSize vs LabelPropStddev (c) LabelPropStddev vs InterIntraRatio 320 300 8 280 260 { 240 220 200 119 120 121 122 123 124 125 InterlntraSep (d) InterIntraRatio vs MeanBagSize Figure 9: Scatter plots for MeanBagSize vs LabelPropStddev vs InterIntraRatio and it's projections for Criteo SSL Dataset 31 Table 7: Cramer's V between (Col1, Col2) and the label for each Dataset created with Criteo CTR. 32 Table 12: Test AUC scores of training baselines on fixed size feature bags dataset using Criteo CTR. Each grouping-key ( Col1 , Col2 ) corresponds to 4 contiguous rows, one for bag size { 64, 128, 256, 512 } each in ascending order. 33 34 35 36 37 Table 14: Bag Level Statistics of all the Groupings formed using Criteo CTR Dataset (Emboldened : Used for Training) 38 39 40 41 42 43 44 Table 15: Bag Level Statistics of all the Groupings with Criteo SSCL (Emboldened : Used for Training) 45 46 47 Table 8: Test AUC scores on training baselines on Datasets created using Criteo CTR Dataset. 48 Table 9: Bag size at 50, 70, 85, 95 %ile with tail size clusters for Datasets created with Criteo CTR. 49 Table 10: Label Variation cluster and LabelPropStdDev values for Datasets created using Criteo CTR. 50 Table 11: BagSep statistics along with the Bag Separation Cluster for each dataset created using Criteo CTR. 51 Table 13: AUC scores of training baselines of Random Bags created from Criteo CTR dataset with error bars. Table 16: Bag size at 50, 70, 85, 95 %ile for Criteo SSCL Dataset Table 17: BagSep statistics for each dataset formed with Criteo SSCL Dataset 52 Table 18: Label Variation cluster and LabelPropStdDev values for Datasets created using Criteo SSCL. Table 19: Test AUC scores on training baselines on Bag Datasets created using Criteo SSCL dataset 53 Table 20: Encoding of columns for the Criteo SSCL Dataset 54"}
