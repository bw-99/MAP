{
  "On-device Content-based Recommendation with Single-shot Embedding Pruning: A Cooperative Game Perspective": "Hung Vinh Tran h.v.tran@uq.edu.au The University of Queensland Brisbane, Queensland, Australia Tong Chen tong.chen@uq.edu.au The University of Queensland Brisbane, Queensland, Australia Guanhua Ye g.ye@bupt.edu.cn Beijing University of Posts and Telecommunications Beijing, China Quoc Viet Hung Nguyen henry.nguyen@griffith.edu.au Griffith University Gold Coast, Queensland, Australia Kai Zheng zhengkai@uestc.edu.cn University of Electronic Science and Technology of China Chengdu, China",
  "Abstract": "Content-based Recommender Systems (CRSs) play a crucial role in shaping user experiences in e-commerce, online advertising, and personalized recommendations. However, due to the vast amount of categorical features, the embedding tables used in CRS models pose a significant storage bottleneck for real-world deployment, especially on resource-constrained devices. To address this problem, various embedding pruning methods have been proposed, but most existing ones require expensive retraining steps for each target parameter budget, leading to enormous computation costs. In reality, this computation cost is a major hurdle in real-world applications with diverse storage requirements, such as federated learning and streaming settings. In this paper, we propose Sha pley V alue-guided E mbedding R eduction (Shaver) as our response. With Shaver, we view the problem from a cooperative game perspective, and quantify each embedding parameter's contribution with Shapley values to facilitate contribution-based parameter pruning. To address the inherently high computation costs of Shapley values, we propose an efficient and unbiased method to estimate Shapley values of a CRS's embedding parameters. Moreover, in the pruning stage, we put forward a field-aware codebook to mitigate the information loss in the traditional zero-out treatment. Through extensive experiments on three real-world datasets, Shaver has demonstrated competitive performance with lightweight recommendation models across various parameter budgets. The source code is available at https://github.com/chenxing1999/shaver .",
  "ACMReference Format:": "Hung Vinh Tran, Tong Chen, Guanhua Ye, Quoc Viet Hung Nguyen, Kai Zheng, and Hongzhi Yin. 2025. On-device Content-based Recommendation ∗ Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1274-6/25/04 https://doi.org/10.1145/3696410.3714921 Hongzhi Yin ∗ h.yin1@uq.edu.au The University of Queensland Brisbane, Queensland, Australia with Single-shot Embedding Pruning: A Cooperative Game Perspective. In Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 14 pages. https: //doi.org/10.1145/3696410.3714921",
  "1 Introduction": "In the modern era, recommender systems (RSs) play a vital role in assisting users to identify relevant information. Meanwhile, on top of user-item interactions, the increasingly available side features of users and items (e.g., user gender and product category) have made content-based RSs (CRSs) [38] a step above collaborative filtering-based RSs, especially in cold-start settings. According to the Statista report [53], in the US, online advertising - a major CRS application - has doubled its market size from $107.5 billion in 2018 to $225 billion in 2023. More recently, with privacy legislation and cyber security awareness on the rise, on-device deployment of CRSs is quickly gaining popularity, where examples include Brave Browser's federated news recommendation [39], as well as Kuaishou's short video recommendation on mobile devices [15]. Most modern CRSs depend on sparse categorical features, such as movie genres and user regions in movie recommendation [38], where a dense embedding table that hosts all possible features' embeddings (i.e., vector representations) is trained. Given the enormous amount of sparse categorical features, embedding tables have become the storage bottleneck for modern CRSs, which contradicts the surging demand for scalability and efficiency during on-device deployment [43, 68]. For illustration, in click-through rate (CTR) prediction, a typical CRS task that predicts users' clicking intention on an item, the RS deployed by Meta has been reported to consume 12T parameters and can demand up to 96TB of memory and multiple GPUs to train [41]. Consequently, lowering the parameter footprint has attracted immense attention in both industry applications [24, 43] and academic research [19, 45]. For instance, PEP [33] prunes the embedding table by searching a pruning mask through a learnable threshold. After the initial training of both the threshold and the model, the model is retrained from scratch to work under the pruned setting. Another example is AutoSrh [26], which adaptively groups features into blocks. After bi-level optimization, a sparse embedding parameterization can be selected according to the memory budget and then optimized via retraining. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. Tran et al. Generally, these methods focus on pruning the embedding table to a fixed size, hence an expensive training cycle is required for every single target sparsity. On one side, in most existing solutions, optimal trade-offs between recommendation accuracy and parameter efficiency require tailoring models according to each device's specific computational and storage capacities. However, on the flip side of the coin, such expensive, repetitive training procedures are not affordable in practical settings with varying on-device storage needs. We provide two scenarios below: · Scenario 1: Federated recommendation ensures privacy by assigning each user an on-device recommender. Such paradigms commonly suffer from hardware heterogeneity, where the variation of computing resources across devices (e.g., TV boxes, smart watches, and mobile phones) [7, 67, 68] prohibits a uniform model architecture from being used. This naturally calls for more efficient means of embedding pruning to provide multiple versions of a trained model. · Scenario 2: When deployed on-device, a model needs to operate under different computational resource budgets due to software parallelization and varying battery status [4, 69], and CRSs are no exception. Considering the time-sensitive nature of recommendation services, a sophisticated embedding pruning mechanism that requires reinforcement learning [45, 46] or even post-pruning retraining [33] will lead to a significant overhead, hindering the usability of the service pipeline. Ideally, considering those real-life scenarios, a competent pruning technique is expected to quickly adapt a well-trained CRS model to any specified parameter budget, without undergoing iterative search or retraining that are both time-consuming. Therefore, this paper focuses on a more practical, yet largely neglected setting of CRS compression for on-device deployment, which is to prune the embedding table of a recommender in one single shot 1 . To achieve single-shot pruning, a common practice is based on attribution scores [2, 40]. In a nutshell, by quantifying the contribution from each individual model weight (i.e., embedding parameter in our case) to a performance metric, parameters with the lowest scores can be nullified to meet an arbitrary size constraint. In content-based recommendation, existing solutions [44, 59] perform parameter attribution by measuring the loss value change after removing the parameter from the full embedding table, which can be approximated with first-order gradients [28]. If we treat all embedding parameters as cooperative 'players' in a game, then this is essentially asking the question: In a team formed by all players, what is each individual player's contribution to the whole team? However, this formulation hardly aligns with the pruning context, where a player (i.e., embedding parameter), if not pruned, will actually form a smaller team with only a subset of players. In a single-shot pruning setting, instead of retaining embedding parameters that are most important to the full parameter set, we should ultimately identify parameters that can be of high value to any parameter subsets. So, the right question to be asked is: What is the expected contribution of each individual player, when it joins any team formed by a subset of players? Due to the intertwined 1 To clarify, different from transfer learning, our single-shot setting refers to performing model pruning in only one forward (and sometimes backward) pass [28, 44]. dependency among parameters, it is clear that this question does not share the same answer with the former one. To answer this question, we aim to investigate embedding pruning for CRSs via the lens of cooperative games, where we employ Shapley value [50] to perform embedding parameter attribution. Shapley value is a well-defined method in the cooperative game theory for fairly allocating the total game revenue to players based on their individual contributions [12, 23, 36]. It quantifies each player's contribution by considering all possible team combinations, resulting in the expected contribution of the player in any group. As such, compared with existing practices [28, 44], Shapley value is a better fit for generating unbiased attribution scores of embedding parameters to facilitate fast recommendation model pruning. However, some major challenges are yet to be resolved before we can enjoy the benefit of Shapley values. Firstly, the computational costs of Shapley value are prohibitively expensive. In our context, when a CRS only activates a subset of embedding parameters, each parameter's contribution in this subset can be measured by comparing the CRS's recommendation performance with and without it. Then, a parameter's Shapley value is its average contribution to all possible subsets of parameters. To calculate all the exact Shapley values, we need to enumerate over all 2 𝑛𝑑 parameter subsets, where 𝑛 and 𝑑 are respectively the numbers of features and embedding dimensions. Considering that in CRSs 𝑛𝑑 is intractable (e.g., reaching 100 million in our biggest dataset), and reliably measuring the recommendation performance requires a sufficiently large dataset, the number of resulted forward passes has rendered the straightforward calculation of Shapley values an infeasible option. Secondly, most pruning methods employ a simple zero-out approach to remove less important parameters and create a sparse embedding matrix. While this reduces the model size, it can result in significant information loss as recommenders rely heavily on dense feature representations for accurate predictions. This further deteriorates in CRSs due to their extensive use of dot product [18, 49] and element-wise multiplication [57] between embeddings to model feature interactions. By setting parameters to zero, pruning can disrupt the complex relationships between features, leading to suboptimal performance. Motivated by these challenges, we propose Sha pley V alue-guided E mbedding R eduction ( Shaver ), a novel embedding pruning method for compressing a CRS model to any target size in one shot. Shaver assigns each embedding parameter a Shapley value as its attribution score, such that a well-trained model can prune its embeddings to any parameter budget in a single shot. To address the high computational costs of obtaining exact Shapley values, through a series of theoretical analyses, we develop an unbiased approximation method to reliably estimate the Shapley values of all embedding parameters. Efficiency-wise, Shaver reduces the computational complexity from 𝑂 (( 𝑛𝑑 ) ! × |D|) to the much lower 𝑂 ( 𝑚𝑑 × |D|) , with D and 𝑚 respectively being the dataset and number of feature fields (e.g., user gender and movie genre). Notably, 𝑚 ≪ 𝑛 in our case tens of feature fields versus millions of features (see Table 1). To reduce the information loss caused by the default zero-padding on pruned parameters, we propose an alternative that searches for an optimal placeholder value per feature field, termed the codebook, which is then used for padding nullified embedding parameters during inference. In addition, the codebook values can be solved in On-device Content-based Recommendation with Single-shot Embedding Pruning: A Cooperative Game Perspective WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. a closed form, making it better at retaining the recommendation performance with a negligible computational overhead. In summary, our main contributions are: · We revisit one important but largely neglected task of singleshot embedding pruning, which is more relevant in the realworld, on-device deployment of CRSs. · We propose Shaver, a novel embedding compression method that reduces the time complexity of Shapley value estimation for CTR models and provides a more effective alternative to the widely used zero-out approach in model pruning with a field-aware codebook. · We conduct extensive experiments on three datasets to compare Shaver with state-of-the-art embedding compression methods. The results show that even without an extra finetuning step, Shaver can achieve competitive performance.",
  "2 Related Work": "",
  "2.1 CTR Prediction": "CTRprediction plays a vital role in the development of online advertisements. One of the first and most influential backbones for CTR prediction is Factorization Machine (FM) [49], where the interaction between features are explicitly model upto 2nd order. Recently, with the rapid development of deep neural networks (DNN), various deep models have also been proposed. NeuMF [21] combines the introduced Generalized Matrix Factorization (GMF) with a DNN network branch. DeepFM [18] is another prime example, which adds a DNN branch into the FM model. Unlike DeepFM, which only attempts to model up to 2nd-order interaction, DCNv2 [57] explicitly models the 𝑙 + 1-th order interaction by 𝑙 crossnet layers. DCNv2 also incorporates a DNN to model more complex interactions. To further reduce the computation cost of DCNv2, Wang et al. [57] also propose integrating Mixture-of-Experts for modeling the cross-layers, creating DCN-Mix.",
  "2.2 Lightweight Recommender Systems": "A diverse set of research endeavors has proposed various methods for learning light-weight embeddings [30, 55, 70]. Compositional embedding methods decompose the original embedding table into multiple smaller meta-embedding tables and share parameters between features [8, 9, 25, 31, 43, 52, 62, 66]. Quantization approaches are also applied in light-weight RSs [16, 29, 63, 65], which focus on reducing the loss of model quality in lower precision representation. However, these approaches generally suffer from limited compression ratios [70]. Knowledge distillation [56, 60, 61] is employed to mitigate performance degradation in smaller models. Another line of research focuses on finding the best parameter allocation through Neural Architecture Search (NAS), which usually involves reinforcement learning [24, 46, 48], bi-level optimization [26, 72-74], or evolutionary search [7, 37]. Consequently, NASbased approaches generally have high computation costs. Pruning is another popular approach to reduce the amount of embedding parameters by finding a binary mask [33, 44, 47, 58, 64]. Recently, many hybrid methods [32, 37] have also been proposed to combine the benefit of various archetypes to achieve the best trade-off between performance and parameter budget. Our method falls within the pruning category. Unlike most of other pruning methods [33, 64], we eliminate the hassle of simultaneously optimizing both the pruning budget mask and model performance. Compared to [7, 48], which also can adapt to arbitrary parameter budgets but focus on collaborative filtering, our method tailored towards CRSs. Additionally, unlike SSEDS [44], which mandates uniform dimensions across all features within the same field, our method offers greater flexibility by allowing varied budget allocations without such constraints. Moreover, our singleshot pruning setting focuses on efficiently customizing a trained CRS to heterogeneous memory budgets, which is different from online pruning settings [27, 34, 71] where data arrives continually.",
  "2.3 Shapley Value for Machine Learning": "Shapley values have been widely adopted in the explainable AI community due to its high correlation with human intuition [36]. Because of its high computation cost, various researches have been proposed to tackle this challenge through random order sampling [5, 12], solving weighted least square problem [11, 36], being model specific [35, 36], and recently amortization approaches [10, 23]. Recently, the success of Shapley values in explainable AI has inspired its application in pruning deep models. To start with, Ancona et al. [2] pointed out several desirable properties of Shapley values in pruning neurons: · Null player: If the game (loss) doesn't depend on a player (parameter or group of parameters), its attribution is zero. · Symmetry: If the game depends on two players equally, then the two players should receive the same attribution. · Efficiency: Attributions sum up to the difference between the loss evaluated when all players are enabled and the loss evaluated when all players are removed. Later, NeuronShapley [14] considered Shapley value estimation as a multi-arm bandit problem, and applied truncated Monte Carlo to estimate Shapley values. Leveraging the multi-arm bandit settings, Guan et al. [17] employ an 𝜖 -greedy approach. Troupe [51] proposes using Shapley values to select models for an ensemble based on a specific data point. Last, although it is not applied for compressing embedding tables, Shapley values also have been applied in recommender system designs [3, 13].",
  "3 Preliminaries": "In this section, we first introduce the key concepts of content-based recommendation by defining a classic task, namely click-through rate (CTR) prediction. Then, we provide the definition of Shapley value, the theoretical foundation of our solution.",
  "3.1 Content-based Recommendation": "In CTR prediction, a dataset D is a set of samples ( x , 𝑦 ) , where x is a multi-dimensional vector combining both user and item features, and 𝑦 is a binary label indicating positive ( 𝑦 = 1, clicked) or negative ( 𝑦 = 0, not clicked) user interaction with the item. For x , its features are commonly a collection of sparse binary features from multiple fields (i.e., user occupation and product category) [38, 75], such as:  WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. Tran et al. where an example of x 1 and x 2 can be 𝑢𝑠𝑒𝑟 𝑔𝑒𝑛𝑑𝑒𝑟 = 𝑓 𝑒𝑚𝑎𝑙𝑒 in field 1 and 𝑚𝑜𝑣𝑖𝑒 𝑔𝑒𝑛𝑟𝑒 = 𝑎𝑐𝑡𝑖𝑜𝑛 in field 2. x is then a concatenation of all categorical feature encodings { x 𝑗 } 𝑚 𝑗 = 1 from all 𝑚 fields, where numerical features are commonly discretized via bucketing [75] in CTR prediction tasks. In real-world applications, the input vector x is extremely sparse, therefore each field 𝑗 's feature is mapped into a 𝑑 -dimensional, dense latent vector e 𝑗 ∈ R 𝑑 , called embedding:  where V 𝑗 ∈ R 𝑛 𝑗 × 𝑑 and x 𝑗 ∈ R 𝑛 𝑗 are respectively the embedding table and sparse encoding for field 𝑗 , with 𝑛 𝑗 the number of features in field 𝑗 . In practice, all 𝑚 field-specific embedding tables V 𝑖 are concatenated as a single embedding table E ∈ 𝑅 𝑛 × 𝑑 for storage, where E = [ V 1 , V 2 , . . . , V 𝑚 ] , and 𝑛 = ˝ 𝑚 𝑗 = 1 𝑛 𝑗 is the total number of features. After obtaining embeddings for x from all fields { e 1 , e 2 , . . . , e 𝑚 } , they are further processed by a recommendation backbone model:  with ˆ 𝑦 is a binary classification prediction whether a user will click on an item or not. Its primary target is to model the complex interactions between different features. For instance, the model can combine the user gender and the movie genre to create a 2nd-order interaction, enhancing model performance [18, 49]. These high order interactions are commonly modeled explicitly by dot product [18, 49] or element-wise multiplications [57], and implicitly by deep neural networks [18, 57]. Finally, 𝑓 (·) is trained with log loss:  which quantifies the prediction error between 𝑦 and ˆ 𝑦 .",
  "3.2 Shapley Value": "Shapley value originates from the cooperative game theory, whose aim is to assign each player a score that quantifies the player's contribution in a cooperative game. Formally, a cooperative game consists of a set of players denoted by N , where any arbitrary subset of players can form a 'team'. The coalitional values generated by player subsets are measured by function 𝑣 : 2 N → R , with 𝑣 (∅) = 0. For player 𝑖 ∈ N , its Shapley value 𝜙 𝑣 ( 𝑖 ) w.r.t. function 𝑣 (·) is:  where GLYPH<0> · · GLYPH<1> is the binomial coefficient, and 𝑣 (S ∪ 𝑖 ) -𝑣 (S) quantifies the marginal contribution of 𝑖 after joining team S . In a nutshell, 𝜙 𝑣 ( 𝑖 ) is computed by enumerating over all possible teams S ⊆ N\\ 𝑖 . With that, a commonly used, permutation-based formulation of the Shapley value is:  where the ordered set R is a possible permutation of all players, P R 𝑖 is the set of players that precede 𝑖 in R . Thus, the Shapley value 𝜙 𝑣 ( 𝑖 ) can be calculated as the mean/expectation of 𝑖 's contributions in all |N| ! possible permutations of N . Monte Carlo Approximation of Shapley Values. Based on Eq.(6), the default form of Shapley value calculation incurs a factorial time complexity w.r.t. the number of players. To scale it to a large N , approximating the Shapley value via Mote Carlo approach is a widely adopted workaround [2, 14], which estimates 𝜙 𝑣 ( 𝑖 ) by sampling 𝑝 permutations R 𝑘 and take the average marginal contribution in these samples to approximate Shapley value:  where 𝜙 𝑣 ( 𝑖 ) is now the Monte Carlo approximation [5] of player 𝑖 's Shapley value w.r.t. value function 𝑣 (·) .",
  "4 Shaver: The Proposed Method": "In this section, we unfold the design of Shaver, namely Shapley Value-guided Embedding Reduction for on-device CRSs, of which an overview is provided in Figure 1. We firstly formally define the problem, and subsequently provide corresponding theoretical analyses and pseudocode to efficiently estimate the Shapley value of each embedding parameter. Finally, we derive a codebook to replace the traditional zero-padding strategy, so as to compensate for the information loss brought by embedding parameter pruning and maintain maximum model performance.",
  "4.1 Shapley Values of Embedding Parameters": "Before defining the problem, we provide some clarifications for Shapley value in our specific task context. To facilitate selective embedding table pruning, we derive a Shapley value for every single parameter in the feature embedding table E . Let E [ 𝑖, 𝑐 ] denote the parameter at the 𝑖 -th row and 𝑐 -th column (i.e., the 𝑐 -th embedding dimension of feature 𝑖 ), the Shapley value of player ( 𝑖, 𝑐 ) is denoted as 𝜙 𝑣 ( 𝑖, 𝑐 ) with a value function 𝑣 (·) . Please note that the use case of Shapley value in our work differs from the mainstream, data-centric counterparts [23, 36] that focus on calculating values of individual data points or raw features, while we are specifically interested in the Shapley values of model parameters. Value Function 𝑣 (·) . For measuring 𝜙 𝑣 ( 𝑖, 𝑐 ) , we need to define a value function 𝑣 (·) that can quantify the contribution from a group of players (i.e., parameters) S = { E [ 𝑖 1 , 𝑐 1 ] , E [ 𝑖 2 , 𝑐 2 ] , . . . , E [ 𝑖 𝑠 , 𝑐 𝑠 ]} . Given a CTR model 𝑓 (·) , we define 𝑣 (·) as the log loss increase when parameters S become absent from E :  where 𝑓 w/oS (·) denotes the CTR model without embedding parameters S , 𝐿 is the log loss, and D denotes the dataset. With this definition, the Shapley value 𝜙 𝑣 ( 𝑖, 𝑐 ) of a single parameter E [ 𝑖, 𝑐 ] can be quantified via an analogous process as in Section 3.2. Definition 4.1 (Single-shot Pruning with Shapley Values). Given a trained CTR model 𝑓 (·) , our goal is to obtain the Shapley value of every single parameter 𝜙 𝑣 ( 𝑖, 𝑐 ) in the feature embedding table. Based on 𝜙 𝑣 ( 𝑖, 𝑐 ) , we can efficiently prune a CTR model for any on-device budget by directly removing the lowest ranked entries from its embedding table to meet the specified parameter size. On-device Content-based Recommendation with Single-shot Embedding Pruning: A Cooperative Game Perspective WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. Figure 1: The overview of Shaver. We calculate placeholder values (codebook C ), and then compute Shapley values 𝜙 𝑣 from the provided dataset D . On any required memory budget 𝐵 , we replace embedding parameters with the lowest attribution scores by placeholder values. 0.2 1.1 0.4 3.1 1.5 ② Calculate Shapley Value (Algorithm 1) ③ Prune ① Calculate Codebook 0.2 1.1 1.1 0.4 1.5 3.1 Field 1 (e.g. Gender) Field 2 (e.g. Movie Genre) Field 3 (e.g. User ID)",
  "4.2 Local and Global Values": "If straightforward Monte Carlo approximation of 𝜙 𝑣 ( 𝑖, 𝑐 ) is in use, for all |N| = 𝑛𝑑 embedding parameters, |N||D| = 𝑛𝑑 |D| forward passes are needed per permutation, leading to a prohibitive computational cost of O( 𝑝𝑛𝑑 |D|) . One main cause is that Eq.(8) lays emphasis on embedding parameters' values to the overall data distribution. We term this global values , whose calculation involves a sufficiently large, if not the full dataset D . With that said, the function of local values 𝑣 𝑙𝑜𝑐 (·) is defined based on single data samples:  In contrast to computing the global values, there are some efforts that draw an individual data sample at a time, pair it with one random permutation, and then derive each parameter's local value to approximate the global one [12]. This process can be described as follows:  with S 1 and S 2 defined as:  where R( x ,𝑦 ) denotes one random permutation paired with data sample ( x , 𝑦 ) . , Eq.(10) shows that, with a quality, decent-sized D , the global Shapley value can be confidently approximated as the mean of local values [12]. For all parameters, this method runs in O( 𝑛𝑑 |D|) . Despite being 𝑝 times faster than vanilla Monte Carlo and a subsampled D from the full training dataset can help further lower the complexity, the large feature size 𝑛 (normally in millionlevel) remains the efficiency bottleneck for parameter attribution. The following section shows how we further reduce those 𝑛𝑑 |D| forward passes - which is essentially iterating over the dataset for 𝑛𝑑 times - to a much smaller number..",
  "4.3 Efficient Shapley Value Computation for CRS Embeddings": "Recall that in CRSs, features are categorized into 𝑚 fields. On top of the above defined local value 𝑣 𝑙𝑜𝑐 (·) , we further define a local value function 𝑢 𝑙𝑜𝑐 (·) specific to feature fields:  Distinct from 𝑣 𝑙𝑜𝑐 (·) , in 𝑢 𝑙𝑜𝑐 (·) , a player ( 𝑗, 𝑐 ) is a group of parameters that are located at the same column 𝑐 in field 𝑗 's embedding table V 𝑗 . To avoid cluttered notations, in this section, if not specified, both 𝑣 (·) and 𝑢 (·) are the local value functions calculated with a single instance ( x , 𝑦 ) . Then, we define our notion of corresponding player as follows. Definition 4.2 (Corresponding Player). Given player ( 𝑖, 𝑐 ) in 𝑣 (·) and player ( 𝑗, 𝑐 ′ ) in 𝑢 (·) , and a data instance ( x , 𝑦 ) , we call ( 𝑖, 𝑐 ) the corresponding player of ( 𝑗, 𝑐 ′ ) if 𝑐 = 𝑐 ′ and the 𝑖 -th feature is activated, i.e., x [ 𝑖 ] = 1. With the above definition, we provide a key theorem for efficient Shapely value computation below: Theorem 4.3. Assume player ( 𝑖, 𝑐 ) in 𝑣 (·) is corresponding with player ( 𝑗, 𝑐 ) in 𝑢 (·) w.r.t. data instance ( x , 𝑦 ) . Then, denote their Shapley values based on local value functions respectively as 𝜙 x ,𝑦 𝑣 ( 𝑖, 𝑐 ) and 𝜙 x ,𝑦 𝑢 ( 𝑗, 𝑐 ) , we have 𝜙 x ,𝑦 𝑣 ( 𝑖, 𝑐 ) = 𝜙 x ,𝑦 𝑢 ( 𝑗, 𝑐 ) . Our proof is provided in Appendix A. In short, we observe that 𝜙 x ,𝑦 𝑣 ( 𝑖, 𝑐 ) is equivalent to 𝜙 x ,𝑦 𝑢 ( 𝑗, 𝑐 ) , as long as ( 𝑖, 𝑐 ) is the corresponding player of ( 𝑗, 𝑐 ) . As computing 𝜙 x ,𝑦 𝑢 ( 𝑗, 𝑐 ) will suffice, only 𝑚𝑑 rather than 𝑛𝑑 iterations over D are now required to compute 𝜙 x ,𝑦 𝑢 ( 𝑗, 𝑐 ) for all players in 𝑢 (·) . Taking our largest evaluation dataset KDD as an example (see Table 1), with 11 ( 𝑚 ) feature fields, over 6 million ( 𝑛 ) features, and an embedding dimension of 𝑑 = 16, the O( 𝑚𝑑 |D|) complexity of our Shapley value computation over O( 𝑛𝑑 |D|) given that 𝑚 ≪ 𝑛 . The algorithm description and convergence analysis are both provided in Appendix B. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. Tran et al.",
  "4.4 Replacing Zero-padding with Field-aware Codebook": "In various machine learning fields like explainable AI, the choice of placeholder value (i.e. the value representing the missing value) is crucial, and can greatly affect the final performance [6]. However, most if not all CRS pruning algorithms opt for a simple zero-padding strategy. This approach often leads to suboptimal recommendation performance, as interactions between feature embeddings are modeled using dot product and element-wise operations. Setting certain parameters to zero eliminates such interactions with all other features, compromising system effectiveness. Therefore, in this work, we introduce a approach to represent pruned features by a fieldaware codebook C ∈ R 𝑚 × 𝑑 . As it only comes with 𝑚𝑑 elements, its storage overhead is negligible. For a parameter budget, let E Q denote the sparsified embedding table that nullifies the least important parameters identified in set Q , the following equation specifies how we impute the pruned embedding table into E Q , C with the codebook:  with 𝑗 the corresponding field of feature 𝑖 . After calculating the attribution score, we will replace pruned parameters based on the above rule instead of only zeroing out. To better preserve the model performance, we will choose codebook C ∗ that minimizes the expected Euclidean distance between the original embedding table and its imputed version:  where 𝐵 ∈ ( 0 , 𝑛𝑑 ) represents any possible number of pruned parameters, and Q is uniformly sampled such that |Q| = 𝐵 . With this formulation, the optimal codebook C ∗ ∈ R 𝑚 × 𝑑 can be solved via the weighted average of all feature in each field:  where F 𝑗 is the set of features in field 𝑗 , and 𝑝 𝑖 is the occurrence frequency of feature 𝑖 . The detailed deriviation is provided in Appendix A. This closed-form solution allows for efficient computation of codebook C ∗ to compensate for the information loss from the pruning stage. When deployed on-device, both the pruned sparse embedding table E Q and the codebook C ∗ are stored. When inference is needed, only the 𝑚 embeddings { e 1 , e 2 , ..., e 𝑚 } of the feature x will be imputed as the input to 𝑓 (·) for on-device CTR prediction.",
  "5 Experiments": "In this section, we conduct experiments to study the effectiveness of Shaver. Specifically, we are interested in answering the following research questions (RQs): RQ1 : Compared with non-single-shot baselines, how does Shaver perform when compressing CRS models? RQ2 : Compared with other single-shot baselines, how does Shaver perform under different parameter budgets? RQ3 : Most on-device CRS compression methods require model retraining/finetuning post pruning. Does Shaver also benefit from such a practice? Table 1: Statistics of the preprocessed datasets. RQ4 : What is the impact of dataset size to the recommendation performance of the pruned models? RQ5 : Does Shaver exhibit any patterns when pruning parameters?",
  "5.1 Experimental Settings": "5.1.1 Datasets. Weconductourexperiments on three public datasets. In all datasets, we randomly split them into 8:1:1 as the training, validation and test set respectively. We specify the dataset preprocess procedure in Appendix C.2. Table 1 provides the core statistics of pre-processed datasets. 5.1.2 Metrics. We evaluate all models using two commonly used evaluation metric in CTR prediction community [76]: LogLoss and AUC (Area under the ROC curve). In the CTR problem, a difference of 0.001 in AUC is generally considered significant [37, 55]. The lower LogLoss suggests better performance, while the higher AUC implies more accurate recommendations. The sparsity rate 𝑡 indicates the portion of embedding parameters that have been removed compared to the original embedding table, defined as 𝑡 = |Q|/ 𝑛𝑑 . 5.1.3 Implementation Details. All methods are tested with two well-known CTR backbones: DeepFM [18] and DCN-Mix [57]. We implement two variants of Shaver, namely Shaver-Zero that uses the traditional zero-padding on pruned parameters, and ShaverCodebook described in Section 4.4. Descriptions of all baselines are provided in Appendix C.1. We adhere strictly to the experimental settings described in [55] for reproducibility, adopting the same data splits and hyperparameter search ranges. To ensure a fair comparison, we applied the same hyperparameters used in the training of the original model during our fine-tuning step. We used both the training and validation datasets to calculate the attribution scores. We select final checkpoints according to validation AUC.",
  "5.2 Comparison with Non-single-shot Pruning Methods (RQ1)": "Table 2 shows the performance of Shaver and other non-singleshot pruning methods with the DCN-Mix backbone. The baselines compared are QR [52], TTRec [66], PEP [33], OptEmb [37], and CERP [32], whose detailed descriptions are provided in Appendix C.1. As DCN-Mix is generally a more performant backbone compared with DeepFM, results with the DeepFM backbone and the inference efficiency are deferred to Appendix D due to page limits. For this comparison, we test three sparsity rate settings with 𝑡 = { 50% , 80% , 95% } as all baselines under this category needs to be retrained for every setting. Across three datasets, Shaver achieves competitive results with other non-single-shot pruning methods that require costly training steps for each parameter budget. Shaver demonstrates a strong performance compared to other methods, especially with an 80% sparsity rate. While in the low sparsity rate, the performance gap between methods is small as it is generally On-device Content-based Recommendation with Single-shot Embedding Pruning: A Cooperative Game Perspective WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. Table 2: Comparative results with non-single-shot baselines under three sparsity rates. All methods adopt DCN-Mix as the CRS backbone, where we use ↑ and ↓ to respectively mark higher-is-better and lower-is-better metrics. For OptEmb, we did not report results on some settings where it fails to converge. '#Params' indicates the parameter size of the embedding table (in millions). In each dataset, the best result is marked in bold and the second best one is underlined. Figure 2: Comparative results with single-shot baselines, where DCN-Mix is used as the backbone. 0 . 2 0 . 4 0 . 6 0 . 8 1 . 802 . 804 . 806 . 808 . 810 . 812 Sparsity Rate AUC Original MagPrune PTQ Taylor Shaver-Zero Shaver-Codebook (a) Criteo 0 . 2 0 . 4 0 . 6 0 . 8 1 . 750 . 760 . 770 . 780 Sparsity Rate Original MagPrune PTQ Taylor Shaver-Zero Shaver-Codebook (b) Avazu 0 . 2 0 . 4 0 . 6 0 . 8 1 . 500 . 600 . 700 . 800 Sparsity Rate Original MagPrune PTQ Taylor Shaver-Zero Shaver-Codebook (c) KDD easier to reduce parameters. At 95% sparsity, Shaver delivers impressive results exceeding other methods, despite a lesser extent. Higher sparsity rates pose challenges for single-shot pruning due to removing a larger portion of original model parameters, thus, leading to a more significant model degradation.",
  "5.3 Comparison with Single-shot Pruning Methods (RQ2)": "To make further comparisons, we evaluate our approach against other single-shot compression methods. The baselines compared are MagPrune [55], PTQ [42], and Taylor [40] (see Appendix C.1 for descriptions). Same with RQ1, DCN-Mix backbone is adopted by default, where results with DeepFM backbone are provided in Appendix D. Notably, different from RQ1, as all baselines tested here are all single-shot methods, we have been able to test with a wider variety of sparsity rates 𝑡 = { 0 . 2 , 0 . 4 , 0 . 5 , 0 . 6 , 0 . 8 , 0 . 9 , 0 . 95 , 0 . 99 , 0 . 999 } . Figure 2 shows the results. Shaver-Codebook is the most competitive method for KDD and Criteo, retaining the performance even with only 10% and 5% parameters of the original models, respectively. The second best is Shaver-Zero, whose performance is similar to Shaver-Codebook in the Avazu dataset. These results indicate promising potential in applying the Shapley value in compressing models for CTR problems. Taylor is the third best as they also utilize information from the training and validation datasets, especially in the Criteo dataset, where the difference is only noticeable under high-sparsity settings. PTQ provides strong performance initially but could not hold up at a higher sparsity rate, as it allocates the same memory budget for all features, limiting their performance.",
  "5.4 Effect of Post-Pruning Finetuning (RQ3)": "To answer RQ3, Table 3 shows the effect of fine-tuning pruned models. This experiment also mimics scenarios where a deployed Tran et al. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. Figure 3: Trade-off between performance and data size used to estimate Shapley value. 𝑝 is the portion of the full dataset used to calculate the Shapley value. 0 . 2 0 . 4 0 . 6 0 . 8 1 . 802 . 806 . 810 Sparsity Rate AUC 𝑝 = 10% 𝑝 = 30% 𝑝 = 64% 𝑝 = 100% (a) Criteo 0 . 2 0 . 4 0 . 6 0 . 8 1 . 700 . 720 . 740 . 760 Sparsity Rate 𝑝 = 10% 𝑝 = 30% 𝑝 = 64% 𝑝 = 100% (b) Avazu 0 . 2 0 . 4 0 . 6 0 . 8 1 . 700 . 720 . 740 . 760 . 780 Sparsity Rate 𝑝 = 10% 𝑝 = 30% 𝑝 = 64% 𝑝 = 100% (c) KDD Figure 4: Comparison of how different Shaver variants prune embeddings for features in different frequency buckets. 0 1 2 Frequency Bins 0 2 4 6 8 10 12 Embedding Size Avazu KDD Criteo Codebook Zero Table 3: Effect of post-pruning finetuning, where † indicates variants that are finetuned with the pruned embeddings. The results are reported in AUC. recommendation model has sufficient computing resources to perform further on-device updates. While there is some variation in results across different datasets, in most cases, fine-tuning models further enhance their performance or maintain it at its current level, or with minimal loss. However, occasionally, fine-tuning can lead to decreased model performance, particularly for some settings in KDD. The reason for the performance drop is that Shaver uses the validation set to estimate Shapley values, which can cause data leakage and potentially degrade the final checkpoint performance post-fine-tuning. Moreover, fine-tuning seems to have a larger effect on high sparsity rates with Shaver-Zero, where performance degradation is more pronounced.",
  "5.5 Performance and Data Size Trade-off (RQ4)": "Figure 3 depicts the trade-off between the performance and the proportion 𝑝 of the full dataset D employed. Note that D combines both training and validation samples in our setting. Under all configurations, we guarantee the algorithms have converged and seen all data. Empirically, the required runtime is linearly scaled with the dataset size employed, as Shaver generally converges after iterating through the whole dataset once. In the Criteo and KDD datasets, the amount of data employed has a negligible effect on model performance. Conversely, in the Avazu dataset, the volume of data has a markedly greater impact. One possible hypothesis for this discrepancy is that despite containing more features than the Criteo dataset, the Avazu dataset has fewer instances. Consequently, the validation dataset is insufficient to represent the actual data distribution. Nonetheless, as our methods only require forward passes, we can effortlessly parallelize the computation compared to other training-required methods. Moreover, the overhead remains minimal, even with the entire Criteo dataset - our worst case as our computation cost increases with the number of fields - Shaver takes approximately 2.5 hours to compute the Shapley value. In comparison, for any given parameter budget, training each method generally takes over 1 hour, without including the time needed for hyperparameter tuning.",
  "5.6 Qualitative Analysis (RQ5)": "Weaimto unveil the key difference between the traditional zero-out approach and our codebook-based approach in compressed models. Therefore, employing DCN-Mix with a 50% sparsity rate, we split each dataset's feature into three different bins based on frequency so that each bin had the same number of features. Figure 4 shows the average embedding size assigned per feature in each group. The codebook approach assigns less parameter budget to high frequency. One plausible explanation is that the codebook already contained parameters representing the high-frequency features. Thus, the model could have a larger budget to allocate to the lower frequency features, improving the model's performance.",
  "6 Conclusions": "In this paper, we propose Shaver, which employs Shapley value to fairly distribute each embedding parameter contribution and effectively prune the embedding table in CTR models. Shaver first provide an efficient method to calculate the Shapley value, and then implement a novel field-aware codebook quantization to represent the removed parameters. Extensive experiments show that Shaver achieves competitive performance without expensive retraining and fine-tuning steps for each parameter budget.",
  "Acknowledgments": "This work is supported by Australian Research Council under the streams of Future Fellowship (No. FT210100624), Linkage Project (No. LP230200892), Discovery Early Career Researcher Award (No. DE230101033), and Discovery Project (No. DP240101108 and No. DP240101814). On-device Content-based Recommendation with Single-shot Embedding Pruning: A Cooperative Game Perspective WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia.",
  "References": "[1] Yi Wang Aden. 2012. KDD Cup 2012, Track 2. https://kaggle.com/competitions/ kddcup2012-track2 [32] Xurong Liang, Tong Chen, Quoc Viet Hung Nguyen, Jianxin Li, and Hongzhi Yin. 2023. Learning Compact Compositional Embeddings via Regularized Pruning for Recommendation. arXiv:2309.03518 [cs.IR] [2] Marco Ancona, Cengiz Öztireli, and Markus Gross. 2020. Shapley value as principled metric for structured network pruning. arXiv preprint arXiv:2006.01795 (2020). [3] Omer Ben-Porat and Moshe Tennenholtz. 2018. A game-theoretic approach to recommendation systems with strategic content providers. NeuRIPS (2018). [4] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. 2020. Oncefor-all: Train one network and specialize it for efficient deployment. ICLR (2020). [5] Javier Castro, Daniel Gómez, and Juan Tejada. 2009. Polynomial calculation of the Shapley value based on sampling. Computers & operations research (2009). [6] Hugh Chen, Ian C Covert, Scott M Lundberg, and Su-In Lee. 2023. Algorithms to estimate Shapley value feature attributions. Nature Machine Intelligence 5, 6 (2023), 590-601. [7] Tong Chen, Hongzhi Yin, Yujia Zheng, Zi Huang, Yang Wang, and Meng Wang. 2021. Learning Elastic Embeddings for Customizing On-Device Recommenders. In SIGKDD . 138-147. [8] Yizhou Chen, Guangda Huzhang, Anxiang Zeng, Qingtao Yu, Hui Sun, Heng-Yi Li, Jingyi Li, Yabo Ni, Han Yu, and Zhiming Zhou. 2023. Clustered Embedding Learning for Recommender Systems. In WWW . [9] Benjamin Coleman, Wang-Cheng Kang, Matthew Fahrbach, Ruoxi Wang, Lichan Hong, Ed H. Chi, and Derek Zhiyuan Cheng. 2023. Unified Embedding: Battletested feature representations for web-scale ML systems. In NeuRIPS . [10] Ian Covert, Chanwoo Kim, Su-In Lee, James Zou, and Tatsunori Hashimoto. 2024. Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution. arXiv preprint arXiv:2401.15866 (2024). [11] Ian Covert and Su-In Lee. 2021. Improving kernelshap: Practical shapley value estimation via linear regression. Artificial Intelligence and Statistics (2021). [12] Ian Covert, Scott M Lundberg, and Su-In Lee. 2020. Understanding global feature contributions with additive importance measures. NeuRIPS (2020). [13] Yunfei Fang, Caihong Mu, and Yi Liu. 2023. AutoShape: Automatic Design of Click-Through Rate Prediction Models Using Shapley Value. In Pacific Rim International Conference on Artificial Intelligence . [14] Amirata Ghorbani and James Zou. 2020. Neuron Shapley: discovering the responsible neurons. In NeuRIPS . [15] Xudong Gong et al. 2022. Real-time short video recommendation on mobile devices. In CIKM . 3103-3112. [16] Hui Guan, Andrey Malevich, Jiyan Yang, Jongsoo Park, and Hector Yuen. 2019. Post-training 4-bit quantization on embedding tables. arXiv preprint arXiv:1911.02079 (2019). [17] Jiyang Guan, Zhuozhuo Tu, Ran He, and Dacheng Tao. 2022. Few-shot backdoor defense using shapley estimation. In CVPR . [18] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In IJCAI , Carles Sierra (Ed.). 1725-1731. [19] Jialiang Han, Yun Ma, Qiaozhu Mei, and Xuanzhe Liu. 2021. Deeprec: On-device deep learning for privacy-preserving sequential recommendation in mobile commerce. In WWW . 900-911. [20] Song Han, Huizi Mao, and William J. Dally. 2016. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding. In ICLR , Yoshua Bengio and Yann LeCun (Eds.). [21] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In WWW . 173-182. [22] Olivier Chapelle Jean-Baptiste Tien, joycenv. 2014. Display Advertising Challenge. https://kaggle.com/competitions/criteo-display-ad-challenge [23] Neil Jethani, Mukund Sudarshan, Ian Connick Covert, Su-In Lee, and Rajesh Ranganath. 2021. FastSHAP: Real-time shapley value estimation. In ICLR . [24] Manas R. Joglekar, Cong Li, Jay K. Adams, Pranav Khaitan, and Quoc V. Le. 2019. Neural Input Search for Large Scale Recommendation Models. SIGKDD (2019). https://api.semanticscholar.org/CorpusID:195874115 [25] Wang-Cheng Kang et al. 2021. Learning to Embed Categorical Features without Embedding Tables for Recommendation. In SIGKDD . 840-850. [26] Shuming Kong, Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2022. Autosrh: Anembedding dimensionality search framework for tabular data prediction. IEEE Transactions on Knowledge and Data Engineering 35, 7 (2022), 6673-6686. [27] Fan Lai et al. 2023. { AdaEmbed } : Adaptive Embedding for { Large-Scale } Recommendation Models. In OSDI . [28] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. 2018. SNIP: Single-shot network pruning based on connection sensitivity. In ICLR . [29] Shiwei Li et al. 2023. Adaptive Low-Precision Training for Embeddings in Clickthrough Rate Prediction. In AAAI . [30] Shiwei Li et al. 2024. Embedding Compression in Recommender Systems: A Survey. ACM Comput. Surv. (jan 2024). [31] Xurong Liang, Tong Chen, Lizhen Cui, Yang Wang, Meng Wang, and Hongzhi Yin. 2024. Lightweight Embeddings for Graph Collaborative Filtering. In SIGIR . 1296-1306. [33] Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. 2021. Learnable Embedding Sizes for Recommender Systems. In ICLR . https://openreview.net/ forum?id=vQzcqQWIS0q [34] Zirui Liu, Hailin Zhang, Boxuan Chen, Zihan Jiang, Yikai Zhao, Yangyu Tao, Tong Yang, and Bin Cui. 2025. CAFE+: Towards Compact, Adaptive, and Fast Embedding for Large-scale Online Recommendation Models. ACM Transactions on Information Systems (2025). [35] Scott M Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M Prutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. 2020. From local explanations to global understanding with explainable AI for trees. Nature machine intelligence (2020). [36] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. NeuRIPS (2017). [37] Fuyuan Lyu et al. 2022. OptEmbed: Learning Optimal Embedding Table for Click-through Rate Prediction. In CIKM . 1399-1409. [38] Matteo Marcuzzo, Alessandro Zangari, Andrea Albarelli, and Andrea Gasparetto. 2022. Recommendation systems: An insight into current development and future research challenges. IEEE Access 10 (2022), 86578-86623. [39] Lorenzo Minto and Moritz Haller. 2021. Using Federated Learning to Improve Brave's On-Device Recommendations While Protecting Your Privacy. (2021). https://brave.com/blog/federated-learning/ [40] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. 2017. Pruning Convolutional Neural Networks for Resource Efficient Inference. In ICLR . [41] Dheevatsa Mudigere et al. 2022. Software-Hardware Co-Design for Fast and Scalable Training of Deep Learning Recommendation Models. In Proceedings of the 49th Annual International Symposium on Computer Architecture . 993-1011. [42] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort. 2021. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295 (2021). [43] Niketan Pansare et al. 2022. Learning compressed embeddings for on-device inference. Proceedings of Machine Learning and Systems 4 (2022), 382-397. [44] Liang Qu, Yonghong Ye, Ningzhi Tang, Lixin Zhang, Yuhui Shi, and Hongzhi Yin. 2022. Single-shot Embedding Dimension Search in Recommender System. In SIGIR . 513-522. [45] Yunke Qu, Tong Chen, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2024. Budgeted Embedding Table For Recommender Systems. In WSDM (WSDM '24) . 557-566. [46] Yunke Qu, Tong Chen, Xiangyu Zhao, Lizhen Cui, Kai Zheng, and Hongzhi Yin. 2023. Continuous Input Embedding Size Search For Recommender Systems. In SIGIR . 708-717. [47] Yunke Qu, Liang Qu, Tong Chen, Xiangyu Zhao, Jianxin Li, and Hongzhi Yin. 2024. Sparser Training for On-Device Recommendation Systems. arXiv preprint arXiv:2411.12205 (2024). [48] Yunke Qu, Liang Qu, Tong Chen, Xiangyu Zhao, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2024. Scalable Dynamic Embedding Size Search for Streaming Recommendation. arXiv preprint arXiv:2407.15411 (2024). [49] Steffen Rendle. 2010. Factorization Machines. In ICDM . 995-1000. https://doi. org/10.1109/ICDM.2010.127 [50] Alvin E Roth. 1988. The Shapley value: essays in honor of Lloyd S. Shapley . Cambridge University Press. [51] Benedek Rozemberczki and Rik Sarkar. 2021. The Shapley Value of Classifiers in Ensemble Games. In CIKM . 1558-1567. [52] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020. Compositional Embeddings Using Complementary Partitions for MemoryEfficient Recommendation Systems. In KDD . 165-175. [53] Statista. 2024. Online advertising revenue in the United States from 2000 to 2023. (2024). https://www.statista.com/statistics/183816/us-online-advertisingrevenue-since-2000/ [54] Will Cukierski Steve Wang. 2014. The Avazu Dataset. https://kaggle.com/ competitions/avazu-ctr-prediction [55] Hung Vinh Tran, Tong Chen, Quoc Viet Hung Nguyen, Zi Huang, Lizhen Cui, and Hongzhi Yin. 2024. A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems. arXiv preprint arXiv:2406.17335 (2024). [56] Qinyong Wang, Hongzhi Yin, Tong Chen, Zi Huang, Hao Wang, Yanchang Zhao, and Nguyen Quoc Viet Hung. 2020. Next point-of-interest recommendation on resource-constrained mobile devices. In WWW . [57] Ruoxi Wang et al. 2021. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In WWW . 1785-1797. [58] Shuyao Wang, Yongduo Sui, Jiancan Wu, Zhi Zheng, and Hui Xiong. 2024. Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation. In WSDM . 740-749. [59] Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Bo Chen, Huifeng Guo, Ruiming Tang, and Zhenhua Dong. 2023. Single-shot feature selection for multi-task recommendations. In SIGIR . 341-351. WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. Tran et al. Figure 5: A graphic explanation on the notion of corresponding player. ....... Field 1 (e.g. Gender) Field 2 (e.g. Movie Genre) Field m (e.g. User ID) x 1 0 1 0 0 0 0 0 1 0 ....... [60] Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Guandong Xu, and Quoc Viet Hung Nguyen. 2022. On-device next-item recommendation with selfsupervised knowledge distillation. In SIGIR . [61] Xin Xia, Junliang Yu, Qinyong Wang, Chaoqun Yang, Nguyen Quoc Viet Hung, and Hongzhi Yin. 2023. Efficient on-device session-based recommendation. TOIS (2023). [62] Xin Xia, Junliang Yu, Guandong Xu, and Hongzhi Yin. 2023. Towards communication-efficient model updating for on-device session-based recommendation. In CIKM . 2795-2804. [63] Zhiqiang Xu, Dong Li, Weijie Zhao, Xing Shen, Tianbo Huang, Xiaoyun Li, and Ping Li. 2021. Agile and accurate CTR prediction model training for massivescale online advertising systems. In Proceedings of the International Conference on Management of Data (SIGMOD) . [64] Bencheng Yan, Pengjie Wang, Kai Zhang, Wei Lin, Kuang-Chih Lee, Jian Xu, and Bo Zheng. 2021. Learning Effective and Efficient Embedding via an AdaptivelyMasked Twins-based Layer. In CIKM . [65] Jie Amy Yang, Jianyu Huang, Jongsoo Park, Ping Tak Peter Tang, and Andrew Tulloch. 2020. Mixed-precision embedding using a cache. arXiv preprint arXiv:2010.11305 (2020). [66] Chunxing Yin, Bilge Acun, Xing Liu, and Carole-Jean Wu. 2021. TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models. ArXiv abs/2101.11714 (2021). https://api.semanticscholar.org/CorpusID:231719841 [67] Hongzhi Yin, Tong Chen, Liang Qu, and Bin Cui. 2024. On-Device Recommender Systems: A Tutorial on The New-Generation Recommendation Paradigm. In Companion Proceedings of the ACM on Web Conference 2024 . 1280-1283. [68] Hongzhi Yin, Liang Qu, Tong Chen, Wei Yuan, Ruiqi Zheng, Jing Long, Xin Xia, Yuhui Shi, and Chengqi Zhang. 2024. On-Device Recommender Systems: A Comprehensive Survey. arXiv preprint arXiv:2401.11441 (2024). [69] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. 2019. Slimmable neural networks. ICLR (2019). [70] Hailin Zhang et al. 2023. Experimental Analysis of Large-scale Learnable Vector Storage Compression. arXiv:2311.15578 [cs.LG] [71] Hailin Zhang, Zirui Liu, Boxuan Chen, Yikai Zhao, Tong Zhao, Tong Yang, and Bin Cui. 2024. CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models. Proceedings of the ACM on Management of Data (2024). [72] Xiangyu Zhao et al. 2021. AutoDim: Field-aware Embedding Dimension Searchin Recommender Systems. In WWW . 3015-3022. [73] Xiangyu Zhaok et al. 2021. AutoEmb: Automated Embedding Dimensionality Search in Streaming Recommendations. In ICDM . 896-905. [74] Ruiqi Zheng, Liang Qu, Tong Chen, Kai Zheng, Yuhui Shi, and Hongzhi Yin. 2024. Personalized Elastic Embedding Learning for On-Device Recommendation. TKDE (2024). [75] Jieming Zhu et al. 2022. BARS: Towards Open Benchmarking for Recommender Systems. In SIGIR . 2912-2923. [76] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021. Open benchmarking for click-through rate prediction. In CIKM . 2759-2769.",
  "A Proofs": "Theorem 4.3. Assume player ( 𝑖, 𝑐 ) in 𝑣 (·) is corresponding with player ( 𝑗, 𝑐 ) in 𝑢 (·) w.r.t. data instance ( x , 𝑦 ) . Then, denote their Shapley values based on local value functions respectively as 𝜙 x ,𝑦 𝑣 ( 𝑖, 𝑐 ) and 𝜙 x ,𝑦 𝑢 ( 𝑗, 𝑐 ) , we have 𝜙 x ,𝑦 𝑣 ( 𝑖, 𝑐 ) = 𝜙 x ,𝑦 𝑢 ( 𝑗, 𝑐 ) . Proof. Figure 5 shows two examples of corresponding players between game 𝑣 and 𝑢 . In the proof below, we first simplify the notations by using 𝑖 and 𝑗 to represent the player ( 𝑖, 𝑐 ) and ( 𝑗, 𝑐 ) . Let's define function 𝑔 : { 0 , 1 } | N 𝑣 | →{ 0 , 1 } | N 𝑢 | as follow:  with S 𝑣 is a subset of players in game 𝑣 . In other words, 𝑔 (S 𝑣 ) = S 𝑢 if and only if ∀ 𝑗 ∈ S 𝑢 , corresponding 𝑖 ∈ S 𝑣 and ∀ 𝑗 ∈ N 𝑢 \\ S 𝑢 , corresponding 𝑖 ∉ S 𝑣 . Intuitively, this 𝑔 function provide a method to map from a set 𝑆 𝑣 in 𝑣 to the corresponding set 𝑆 𝑢 in 𝑢 . Let's define marginal contribution Δ as:  It is easy to show that if 𝑔 ({ 𝑖 }) = { 𝑗 } and 𝑔 (S 𝑣 ) = S 𝑢 then:  Consequently, if 𝑔 (S 𝑣 ) = S 𝑢 and 𝑔 ({ 𝑖 }) = { 𝑗 } , then:  It is trivial to show that 𝜙 𝑥,𝑦 𝑣 ( 𝑖 ) = 0 if 𝑔 ({ 𝑖 }) = ∅ (feature 𝑖 is not activated). We will only consider case that ∃ 𝑗 ∈ N 𝑢 , 𝑗 = 𝑔 ({ 𝑖 }) . By definition, we have Shapley value of a player 𝑖 in game 𝑣 as:  It is easy to show that:  With known S 𝑢 and |S 𝑣 | , the number of S 𝑣 s.t. 𝑔 (S 𝑣 ) = S 𝑢 is  as we only need to find |S 𝑣 | - |S 𝑢 | from a pool of |N 𝑣 | - |N 𝑢 | elements. Thus, we can write Eq. 18 as:  With some rearrangements, we have the following equation: On-device Content-based Recommendation with Single-shot Embedding Pruning: A Cooperative Game Perspective WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. Algorithm 1: Efficient Shapley Value Approximation for Embedding Parameters Data: A trained CTR model 𝑓 (·) , dataset D , local value function 𝑢 (·) , number of feature fields 𝑚 , number of features 𝑛 , embedding dimension 𝑑 Result: Shapley value of each player { 𝜙 ( 𝑖, 𝑐 )} 𝑛,𝑑 𝑖 = 1 ,𝑐 = 1  3 R ← a random order of players {( 𝑗, 𝑐 )} 𝑚,𝑑 𝑗 = 1 ,𝑐 = 1 ; 4 S ← ∅ ; 5 𝑢 𝑝𝑟𝑒𝑣 ← 𝑢 (S| x , 𝑦 ) ; 6 for ( 𝑗, 𝑐 ) ∈ R do 7 8 9 10 11 S ← S ∪ ( 𝑗, 𝑐 𝑢 𝑛𝑒𝑥𝑡 ← 𝑢 (S| Find ( 𝜙 ( 𝑖, 𝑐 𝑢 12 𝑖, 𝑐 ) as ) ← 𝑝𝑟𝑒𝑣 end 13 end   And by Chu-Vandermonde Identity:  With Eq. (20), (21), and (22), we have:  □ In the part below, we provide proof for the codebook optimization result:  with 𝐵 is any given memory budget, F 𝑗 is the set of features that belong to field 𝑗 , 𝑝 𝑖 is the appear frequency of feature 𝑖 . To begin with, we have the following equation:  ) ; , 𝑦 x 𝑗, 𝑐 ( 𝜙 ← ( 𝑢 𝑖, 𝑐 ) ; ) 's corresponding player w.r.t. ) + 𝑛𝑒𝑥𝑡 ; 𝑢 𝑛𝑒𝑥𝑡 - 𝑢 𝑝𝑟𝑒𝑣 ; ( , 𝑦 x ) ; where 𝑃 GLYPH<0> ( 𝑖, 𝑐 ) ∈ Q GLYPH<12> GLYPH<12> |Q| = 𝐵 GLYPH<1> is the probability that parameter ( 𝑖, 𝑐 ) is in Q . Since Q is sampled from a uniform distribution with a budget 𝐵 , the probability of any embedding parameter appearing in Q is uniform across all parameters and can be treated as a constant. By removing the above constant, we have the following equation:  Byapplying Cauchy-Bunyakovsky-Schwarz inequality to the above equation, we receive the required Eq. 24.",
  "B Further Details of Shaver": "",
  "B.1 Pseudo code": "The pseudo code is shown in Algorithm 1. For each data point, we first sample a random order R for set of players in 𝑢 (·) (line 3 ). We initialize an empty set to track the selected players (line 4 ) and record the value generated by local value function 𝑢 (·) for the empty set (line 5 ). Then, for each player in the randomly ordered list, we add them to the selection set S (line 7 ) and recalculate the value with this new team S (line 8 ). With Theorem 4.3, we update the corresponding player ( 𝑖, 𝑐 ) 's contribution accordingly (lines 9-10 ), and move on to the next player in the permutation. After iterating over all data points, the algorithm normalizes accumulated Shapley values with the total number of data samples in the dataset (line 14 ). This algorithm has the same magnitude of variance as a naive Monte Carlo approximation while reducing the sampling cost (Appendix B.2). To validate the low variance of our approximation method, an algorithm convergence rate analysis is provided in Appendix B.2.",
  "B.2 Algorithm Analysis": "This part provides prove for the converge rate of Algorithm. 1, based on [12]. The central limit theorem states that if dataset D becomes large, the sample mean ˆ 𝜙 𝑣 ( 𝑖 ) converges in distribution to a Gaussian with mean:  The first equation is due to Theorem 4.3, while the second equation is due to Eq. (10). Eq. (27) shows that Algorithm 1 is unbiased. Also because of central limit theorem, the estimation variance is:  Although we do not have access to the numerator Var ( Δ ( 𝑗, 𝑐 |S 𝑢 )) , we can conclude that the variance behaves as 𝑂 ( 1 /|D|) . Thus, the algorithm would converge with large enough |D| .",
  "B.3 Shapley Values on Synthetic Data": "To further demonstrate that our approximation aligns with the standard Shapley value, we tested on a 40-sample toy dataset with 3 feature fields, each of which has 2, 2, and 3 distinct features (7 features in total), respectively. We use the DeepFM model with a hidden size of 3, leading to 7 × 3 = 21 embedding parameters. Firstly, we compute the exact Shapley value of all 21 parameters, which are listed in table 4. Then, we run our proposed method to estimate all Shapley values. Table 5 shows the estimated results. The absolute difference with the exact values is 0.0021 on average, WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. Tran et al. verifying our unbiased estimation. Besides, our estimation takes substantially less time to run compared with exact Shapley values (20s vs. 3,849s). 0.0179 0.0085 0.0062 0.0438 0.1756 0.0003 0.0292 0.0041 0.0203 -0.0074 0.0150 0.0710 0.0090 0.2519 0.0253 0.0065 0.0136 0.0861 0.0079 0.0385 -0.0003 Table 5: Estimated Shapley value",
  "C Experiment Settings": "In this section, we provide more details on our experiment settings.",
  "C.1 Baselines": "WecompareShaver with the following embedding pruning methods for on-device content-based recommendation: · QR[52] divides the original embedding table into two smaller embedding tables (commonly called meta-embedding tables). The meta-embedding vectors are aggregrated by multiplication to create the final embedding vector. · TTRec [66] tackles the challenge of excessive embedding table parameters with a sequence of matrix products, employing tensor-train decomposition. · PEP[33] adopts a Soft-Thresholding Reparametrization (STR) trick to iteratively remove redundant parameters in the first training step. · OptEmbed [37] involves three consecutive steps: Train supernet and remove excessive features, evolutionary search for optimal embedding size for each field, and finally, retrain the network from the found embedding mask. · CERP [32] incorporate STR into two equal-size embedding tables E 1 and E 2 . To compensate for the two sparse metaembedding tables, the authors integrate a regularization loss into the training loss and combine two meta-embedding vectors with a sum operation, resulting in a dense vector. · MagPrune [55] prune the trained embedding table by their magnitude. In line with our method, MagPrune also compresses the model by pruning the trained model. · Taylor [40] proposes to prune the model with the first-order Taylor approximation of the loss function. Similar to our method, this method also employs feedback from the dataset to calculate the attribution scores. · Post-Training Quantization (PTQ) [20, 42] first linearly scales the weight into [-2 𝑏 -1 , 2 𝑏 -1 -1 ] range, where 𝑏 is the bit width used. Then we round up the result, converting parameters from float to int. When performing inference, these compressed parameters would be scaled back to float32 with the stored bias and scale. For Taylor and our methods, we pruned the original checkpoints as specified. For PTQ, we compressed the models from float32 to (4, 8, 16)-bit integers, the sparsity rate is calculated as 1 -𝑏 / 32. During training, Shaver's codebook C is freezed.",
  "C.2 Datasets": "The dataset preprocess steps are described below: · Criteo [22] contains ad click data over a week. Following the winning solution from the original Criteo Challenge, we discretize each value 𝑥 in numeric feature fields to ⌈ log 2 ( 𝑥 )⌉ if 𝑥 > 2. We replace infrequent features, which appear less than min_count = 10 times, with out-of-vocabulary (OOV) tokens for each field. · Avazu [54] includes 10 days of click-through data. As a common practice, we first remove the 'id' field, which has a unique value for every record. Similar to Criteo dataset, we remove infrequent features and replace them with OOV tokens (min_count = 2). · KDD [1] consists of records gathered from search session logs. Similar to previous two datasets, we utilize OOV tokens to represent infrequent features (min_count = 10). For Criteo and Avazu dataset, we directly adopt the split and preprocessed data from [55].",
  "D Further Experimental Results": "",
  "D.1 DeepFM Backbone Experiment Results": "Table 6: Original (Uncompressed) Model Performance Table 6 provides uncompressed model performance. Table 7 and Figure 6 show the comparison results with other non-single-shot and single-shot methods of Shaver for DeepFM backbone. For DeepFM backbone, our methods achieves the best results in all settings. With the DeepFM backbone, where the original model has poor performance, our method can remove excessive parameters and improve model performance with fine-tuning. Thus, model fine-tuning has a more positive effect on the DeepFM backbone. Except for the Avazu dataset, codebook approaches outperform zero baseline slightly on Criteo, and significantly on KDD. The reason for this exception is unclear. One plausible explanation is that zero padding can act as a regularization and better prevent overfitting compared to codebook approaches. Regarding single-shot comparison, in general, the performance trend is similar to DCN, with few exceptions. First, PTQ achieves worse relative results in the Criteo dataset. Second, as mentioned above, Shaver-Zero outperforms Shaver-Codebook and other methods in Avazu settings. On-device Content-based Recommendation with Single-shot Embedding Pruning: A Cooperative Game Perspective WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. Figure 6: Comparative results with single-shot baselines for DeepFM backbone. 0 . 2 0 . 4 0 . 6 0 . 8 1 . 600 . 700 . 800 Sparsity Rate AUC Original MagPrune PTQ Taylor Shaver-Zero Shaver-Codebook (a) Criteo 0 . 2 0 . 4 0 . 6 0 . 8 1 . 500 . 600 . 700 Sparsity Rate Original MagPrune PTQ Taylor Shaver-Zero Shaver-Codebook (b) Avazu 0 . 2 0 . 4 0 . 6 0 . 8 1 . 600 . 700 . 800 Sparsity Rate Original MagPrune PTQ Taylor Shaver-Zero Shaver-Codebook (c) KDD Table 7: Comparative results with non-single-shot baselines under three sparsity rates. All methods adopt DeepFM as the CRS backbone. In each dataset, the best result is marked in bold and the second best one is underlined. Table 8: Inference efficiency benchmark results. 'Mem.' represents inference memory peak. Time is shown in seconds, while storage and memory is shown in MiB.",
  "D.2 Model Efficiency on Edge Devices": "To further demonstrate the real-world applicability of our method on edge devices, we deploy Shaver into three devices: (1) Raspberry Pi 4B, (2) a personal PC with CPU i7-13700K, (3) a Samsung Galaxy S23 with Snapdragon 8 Gen 2 CPU. We implement our algorithm using Python with PyTorch on Raspberry Pi and PC, and Java with ExecuTorch on mobile devices. To verify that the efficiency is able to support on-device deployment, on each device, we report the memory inference memory and inference time of the compressed DCN-Mix CRS backbone model. The test is performed with the largest dataset KDD. The inference memory is measured with memory profiler (for Python: https://pypi.org/project/memory-profiler/, for Java: Android Studio built-in memory profiler). Regarding the inference time, we take WWW'25, April 28-May 2, 2025, Sydney, NSW, Australia. Tran et al. the average of 10 runs with a batch size of 64. We further report the model storage requirement as it is vital for transferring bandwidth and devices with limited disk storage [19]. The sparse tensor structure is CSR. The on-device results are shown table 8. Firstly, it is worth noting that the inference time does not change dramatically between sparsity settings, it is mainly determined by the backbone CRS model's complexity. Secondly, we can observe a linear relationship between the model inference memory and the sparsity rate. Lastly, an overall conclusion is that CRS models pruned by Shaver is able to achieve a highly practical memory and inference time for when being deployed for on-device services.",
  "keywords_parsed": [],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "KDD Cup 2012, Track 2"
    },
    {
      "ref_id": "b32",
      "title": "Learning Compact Compositional Embeddings via Regularized Pruning for Recommendation"
    },
    {
      "ref_id": "b2",
      "title": "Shapley value as principled metric for structured network pruning"
    },
    {
      "ref_id": "b3",
      "title": "A game-theoretic approach to recommendation systems with strategic content providers"
    },
    {
      "ref_id": "b4",
      "title": "Once-for-all: Train one network and specialize it for efficient deployment"
    },
    {
      "ref_id": "b5",
      "title": "Polynomial calculation of the Shapley value based on sampling"
    },
    {
      "ref_id": "b6",
      "title": "Algorithms to estimate Shapley value feature attributions"
    },
    {
      "ref_id": "b7",
      "title": "Learning Elastic Embeddings for Customizing On-Device Recommenders"
    },
    {
      "ref_id": "b8",
      "title": "Clustered Embedding Learning for Recommender Systems"
    },
    {
      "ref_id": "b9",
      "title": "Unified Embedding: Battletested feature representations for web-scale ML systems"
    },
    {
      "ref_id": "b10",
      "title": "Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution"
    },
    {
      "ref_id": "b11",
      "title": "Improving kernels shap: Practical shapley value estimation via linear regression"
    },
    {
      "ref_id": "b12",
      "title": "Understanding global feature contributions with additive importance measures"
    },
    {
      "ref_id": "b13",
      "title": "AutoShape: Automatic Design of Click-Through Rate Prediction Models Using Shapley Value"
    },
    {
      "ref_id": "b14",
      "title": "Neuron Shapley: discovering the responsible neurons"
    },
    {
      "ref_id": "b15",
      "title": "Real-time short video recommendation on mobile devices"
    },
    {
      "ref_id": "b16",
      "title": "Post-training 4-bit quantization on embedding tables"
    },
    {
      "ref_id": "b17",
      "title": "Few-shot backdoor defense using shapley estimation"
    },
    {
      "ref_id": "b18",
      "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction"
    },
    {
      "ref_id": "b19",
      "title": "Deeprec: On-device deep learning for privacy-preserving sequential recommendation in mobile commerce"
    },
    {
      "ref_id": "b20",
      "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"
    },
    {
      "ref_id": "b21",
      "title": "Neural collaborative filtering"
    },
    {
      "ref_id": "b22",
      "title": "Display Advertising Challenge"
    },
    {
      "ref_id": "b23",
      "title": "FastSHAP: Real-time shapley value estimation"
    },
    {
      "ref_id": "b24",
      "title": "Neural Input Search for Large Scale Recommendation Models"
    },
    {
      "ref_id": "b25",
      "title": "Learning to Embed Categorical Features without Embedding Tables for Recommendation"
    },
    {
      "ref_id": "b26",
      "title": "Autosrh: An embedding dimensionality search framework for tabular data prediction"
    },
    {
      "ref_id": "b27",
      "title": "AdaEmbed: Adaptive Embedding for Large-Scale Recommendation Models"
    },
    {
      "ref_id": "b28",
      "title": "SNIP: Single-shot network pruning based on connection sensitivity"
    },
    {
      "ref_id": "b29",
      "title": "Adaptive Low-Precision Training for Embeddings in Clickthrough Rate Prediction"
    },
    {
      "ref_id": "b30",
      "title": "Embedding Compression in Recommender Systems: A Survey"
    },
    {
      "ref_id": "b31",
      "title": "Lightweight Embeddings for Graph Collaborative Filtering"
    },
    {
      "ref_id": "b33",
      "title": "Learnable Embedding Sizes for Recommender Systems"
    },
    {
      "ref_id": "b34",
      "title": "CAFE+: Towards Compact, Adaptive, and Fast Embedding for Large-scale Online Recommendation Models"
    },
    {
      "ref_id": "b35",
      "title": "From local explanations to global understanding with explainable AI for trees"
    },
    {
      "ref_id": "b36",
      "title": "A unified approach to interpreting model predictions"
    },
    {
      "ref_id": "b37",
      "title": "OptEmbed: Learning Optimal Embedding Table for Click-through Rate Prediction"
    },
    {
      "ref_id": "b38",
      "title": "Recommendation systems: An insight into current development and future research challenges"
    },
    {
      "ref_id": "b39",
      "title": "Using Federated Learning to Improve Brave's On-Device Recommendations While Protecting Your Privacy"
    },
    {
      "ref_id": "b40",
      "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference"
    },
    {
      "ref_id": "b41",
      "title": "Software-Hardware Co-Design for Fast and Scalable Training of Deep Learning Recommendation Models"
    },
    {
      "ref_id": "b42",
      "title": "A white paper on neural network quantization"
    },
    {
      "ref_id": "b43",
      "title": "Learning compressed embeddings for on-device inference"
    },
    {
      "ref_id": "b44",
      "title": "Single-shot Embedding Dimension Search in Recommender System"
    },
    {
      "ref_id": "b45",
      "title": "Budgeted Embedding Table For Recommender Systems"
    },
    {
      "ref_id": "b46",
      "title": "Continuous Input Embedding Size Search For Recommender Systems"
    },
    {
      "ref_id": "b47",
      "title": "Sparser Training for On-Device Recommendation Systems"
    },
    {
      "ref_id": "b48",
      "title": "Scalable Dynamic Embedding Size Search for Streaming Recommendation"
    },
    {
      "ref_id": "b49",
      "title": "Factorization Machines"
    },
    {
      "ref_id": "b50",
      "title": "The Shapley value: essays in honor of Lloyd S. Shapley"
    },
    {
      "ref_id": "b51",
      "title": "The Shapley Value of Classifiers in Ensemble Games"
    },
    {
      "ref_id": "b52",
      "title": "Compositional Embeddings Using Complementary Partitions for Memory Efficient Recommendation Systems"
    },
    {
      "ref_id": "b53",
      "title": "Online advertising revenue in the United States from 2000 to 2023"
    },
    {
      "ref_id": "b54",
      "title": "The Avazu Dataset"
    },
    {
      "ref_id": "b55",
      "title": "A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems"
    },
    {
      "ref_id": "b56",
      "title": "Next point-of-interest recommendation on resource-constrained mobile devices"
    },
    {
      "ref_id": "b57",
      "title": "DCN v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems"
    },
    {
      "ref_id": "b58",
      "title": "Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation"
    },
    {
      "ref_id": "b59",
      "title": "Single-shot feature selection for multi-task recommendations"
    },
    {
      "ref_id": "b60",
      "title": "On-device next-item recommendation with self-supervised knowledge distillation"
    },
    {
      "ref_id": "b61",
      "title": "Efficient on-device session-based recommendation"
    },
    {
      "ref_id": "b62",
      "title": "Towards communication-efficient model updating for on-device session-based recommendation"
    },
    {
      "ref_id": "b63",
      "title": "Agile and accurate CTR prediction model training for massive-scale online advertising systems"
    },
    {
      "ref_id": "b64",
      "title": "Learning Effective and Efficient Embedding via an Adaptively-Masked Twins-based Layer"
    },
    {
      "ref_id": "b65",
      "title": "Mixed-precision embedding using a cache"
    },
    {
      "ref_id": "b66",
      "title": "TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models"
    },
    {
      "ref_id": "b67",
      "title": "On-Device Recommender Systems: A Tutorial on The New-Generation Recommendation Paradigm"
    },
    {
      "ref_id": "b68",
      "title": "On-Device Recommender Systems: A Comprehensive Survey"
    },
    {
      "ref_id": "b69",
      "title": "Slimmable neural networks"
    },
    {
      "ref_id": "b70",
      "title": "Experimental Analysis of Large-scale Learnable Vector Storage Compression"
    },
    {
      "ref_id": "b71",
      "title": "CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models"
    },
    {
      "ref_id": "b72",
      "title": "AutoDim: Field-aware Embedding Dimension Search in Recommender Systems"
    },
    {
      "ref_id": "b73",
      "title": "AutoEmb: Automated Embedding Dimensionality Search in Streaming Recommendations"
    },
    {
      "ref_id": "b74",
      "title": "Personalized Elastic Embedding Learning for On-Device Recommendation"
    },
    {
      "ref_id": "b75",
      "title": "BARS: Towards Open Benchmarking for Recommender Systems"
    },
    {
      "ref_id": "b76",
      "title": "Open benchmarking for click-through rate prediction"
    }
  ]
}