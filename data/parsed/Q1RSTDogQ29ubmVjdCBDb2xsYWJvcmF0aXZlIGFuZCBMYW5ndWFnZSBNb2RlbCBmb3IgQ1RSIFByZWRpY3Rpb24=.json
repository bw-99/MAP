{
  "CTRL: Connect Collaborative and Language Model for CTR Prediction": "Xiangyang Li âˆ— lixiangyang34@huawei.com China Huawei Noah's Ark Lab Bo Chen âˆ— chenbo116@huawei.com China Huawei Noah's Ark Lab Lu Hou houlu3@huawei.com China Huawei Noah's Ark Lab",
  "ABSTRACT": "Traditional click-through rate (CTR) prediction models convert the tabular data into one-hot vectors and leverage the collaborative relations among features for inferring the user's preference over items. This modeling paradigm discards essential semantic information. Though some works like P5 and CTR-BERT have explored the potential of using Pre-trained Language Models (PLMs) to extract semantic signals for CTR prediction, they are computationally expensive and suffer from low efficiency. Besides, the beneficial collaborative relations are not considered, hindering the recommendation performance. To solve these problems, in this paper, we propose a novel framework CTRL , which is industrial-friendly and model-agnostic with superior inference efficiency. Specifically, the original tabular data is first converted into textual data. Both tabular data and converted textual data are regarded as two different modalities and are separately fed into the collaborative CTR model and pre-trained language model. A cross-modal knowledge alignment procedure is performed to fine-grained align and integrate the collaborative and semantic signals, and the lightweight collaborative model can be deployed online for efficient serving after fine-tuned with supervised signals. Experimental results on three public datasets show that CTRL outperforms the state-of-theart (SOTA) CTR models significantly. Moreover, we further verify its effectiveness on a large-scale industrial recommender system. 1",
  "1 INTRODUCTION": "Click-through rate (CTR) prediction is an important task for recommender systems and online advertising [15, 40], where users' willingness to click on items is predicted based on historical behavior data. The estimated CTR is leveraged to determine whether an item can be displayed to the user. Consequently, accurate CTR prediction service is critical to improving user experience, product sales, and advertising platform revenue [63]. For the CTR prediction task, historical data is organized in the form of tabular data. During the evolution of recommendation models, from the early Matrix Factorization (MF) [29], to shallow machine learning era models like Logistic Regression (LR) [7] and Factorization Machine (FM) [48], and continuing to the deep neural models such as DeepFM [17] and DIN [65], collaborative signals have always been the core of recommendation modeling, which leverages the feature co-occurrences and label signals for inferring user preferences. After encoding the tabular features into onehot features [20], the co-occurrence relations (i.e., interactions) âˆ— Co-first authors with equal contributions. Ruiming Tang tangruiming@huawei.com China Huawei Noah's Ark Lab of the features are captured by various human-designed operations (e.g., inner product [17, 45], outer product [34, 55], non-linear layer [6, 62], etc.). By modeling these collaborative signals explicitly or implicitly, the relevance between users and items can be inferred. Feature 1 Feature 2 Feature m â€¦ Collaborative Models 0 1 0 0 0 1 0 â€¦ 0 0 1 0 0 1 Trm Trm Trm Trm Trm Trm Language Models External Corpus John World War â…¢ (cold-start or low-frequency movie) â€¦ External World Knowledge â€¢ Name: World War â…¢ â€¢ Director: Hooman Seyedi â€¢ Actor: Mohsen Tanabandeh â€¢ Genre: War, Adventure â€¢ Language: German â€¢ The plot: â€¦ â€¢ Reviews: â€¦ Contain Label Collaborative Signals One-hot Features Knowledge Reasoning â€¢ Like this movie may also like Les MisÃ©rables â€¦ â€¢ Like war genre may also like politics genre â€¦ â€¢ The plot of World War â…¢ is similar to that of â€¦",
  "Huawei Proprietary - Restricted Distribution Figure 1: The external world knowledge and reasoning capabilities of pre-trained language models facilitate recommendations.": "However, the collaborative-based modeling paradigm discards the semantic information among the original features due to the one-hot feature encoding process. Therefore, for cold-start scenarios or low-frequency long-tailed features, the recommendation performance is unsatisfactory, limited by the inadequate collaborative relations [39]. For example, in Figure 7, when inferring the click probability of user John over a cold start movie World War III , the inadequate collaborative signals in historical data may impede accuracy recommendation. Recently, some works are proposed to address this drawback by involving Pre-trained Language Models (PLMs) to model semantic signals , such as P5 [14], M6Rec [8], CTR-BERT [42], TALLRec [1], PALR [5]. These works feed the original textual features directly into the language models for recommendation, rather than using one-hot encoded features. On the one hand, the linguistic and semantic knowledge in PLMs helps to extract the semantic information within the original textual features [35]. On the other hand, the external world knowledge such as the director, actors, even story plot and reviews for the movie World War III , as well as knowledge reasoning capability in Large Language Models (LLMs) provide general knowledge beyond training data and scenarios [64], thus enlightening a new technological path for recommender systems. Although remarkable progress has been achieved, the existing semantic-based solutions suffer from several shortcomings: 1) Making predictions based on semantics merely without traditional collaborative modeling can be suboptimal [14] because the feature co-occurrence patterns and user-item interactions are indispensable Conference'17, July 2017, Washington, DC, USA Xiangyang Li and Bo Chen et al. indicators for personalized recommendation [17], which are not yet well equipped for PLMs [36, 64]. 2) Online inferences of language models are computationally expensive due to their complex structures. To adhere to low-latency constraints, massive computational resources, and engineering optimizations are involved, hindering large-scale industrial applications [8, 14]. Therefore, incorporating PLMs into recommendation systems to capture semantic signals confronts two major challenges: Â· How to combine the collaborative signals with semantic signals to boost the performance of recommendation? Â· How to ensure efficient online inference without involving extensive engineering optimizations? To solve these two challenges above, inspired by the recent works in contrastive learning, we propose a novel framework to C onnec t Collabo r ative and L anguage Model ( CTRL ) for CTR prediction, which consists of two stages: Cross-modal Knowledge Alignment stage, and Supervised Fine-tuning stage. Specifically, the raw tabular data is first converted into textual data by human-designed prompts, which can be understood by language models. Then, the original tabular data and generative textual data are regarded as different modalities and fed into the collaborative CTR model and pre-trained language model, respectively. We execute a cross-modal knowledge alignment procedure, meticulously aligning and integrating collaborative signals with semantic signals. Finally, the collaborative CTR model is fine-tuned on the downstream task with supervised signals. During the online inference, only the lightweight fine-tuned CTR model is pushed for serving without the language model, thus ensuring efficient inference. Our main contributions are summarized as follows: Â· We first propose a novel training framework CTRL, which is capable of aligning signals from collaborative and language models, introducing semantic knowledge into the collaborative models. Â· Through extensive experiments, we demonstrate that the incorporation of semantic knowledge significantly enhances the performance of collaborative models on CTR task. Â· CTRL is industrial-friendly, model-agnostic, and can adapt with any collaborative models and PLMs, including LLMs. Moreover, the high inference efficiency is also retained, facilitating its application in industrial scenarios. Â· In experiments conducted on three publicly available datasets from real-world industrial scenarios, CTRL achieved SOTA performance. Moreover, we further verify its effectiveness on largescale industry recommender systems.",
  "2 RELATED WORK": "",
  "2.1 Collaborative Models for Recommendation": "During the evolution of recommendation models, from the early matrix factorization (MF) [29], to shallow machine learning era models like Logistic Regression (LR) [7] and Factorization Machine (FM) [48], to the deep neural models [17, 65], collaborative signals have always been the core of recommendation modeling. These collaborative-based models convert the tabular features into onehot features and leverage various interaction functions to extract feature co-occurrence relations (a.k.a. feature interactions). Different human-designed interaction functions are proposed to improve the modeling ability of collaborative signals. Wide&Deep [6] uses the non-linear layers to extract implicit high-order interactions. DeepFM [17] leverages the inner product to capture pairwise interactions with stacked and parallel structures. DCN [54] and EDCN[3] deploy cross layers to model bit-wise feature interactions. Though collaborative-based models have achieved significant progress, they cannot capture the semantic information of the original features, thereby hindering the prediction effect in some scenarios such as cold-start or low-frequency long-tailed features.",
  "2.2 Semantic Models for Recommendation": "Transformer-based language models, such as BERT [9], GPT-3 [2], and T5 [47], have emerged as foundational architectures in the realm of Natural Language Processing (NLP). Their dominance across various NLP subdomains, such as text classification [32, 41], sentiment analysis [21, 56], intelligent dialogue [14, 44], and style transfer [23, 31], is primarily attributed to their robust capabilities for knowledge reasoning and transfer. Nevertheless, since recommender systems mainly employ tabular data, which is heterogeneous with text data, it is difficult to apply the language model straightforwardly to the recommendation task. In recent times, innovative research trends have surfaced, exploring the viability of language models in recommendation tasks. P5 [14], serves as a generative model tailored for recommendations, underpinning all downstream recommendation tasks into a text generation task and utilizing the T5 [47] model for training and prediction. P-Tab [35] introduces a recommendation methodology based on discriminative language models, translating tabular data into prompts, pre-training these prompts with a Masked Language Model objective, and finally fine-tuning on downstream tasks. Concurrently, Amazon's CTR-BERT [42], a two-tower structure comprising two BERT models, encodes user and item text information respectively. More recently, a considerable upsurge in scholarly works has been observed, leveraging Large Language Models (LLMs) for recommendation systems [1, 22, 51, 60, 61]. For instance, a study by Baidu [51] investigates the possibility of using LLM for re-ranking within a search context. Similarly, RecLLM [60] addresses the issue of fairness in the application of LLMs within recommendation systems. However, although the above semanticbased recommendation models have exposed the possibility of application in recommender systems, they have two fatal drawbacks: 1) Discarding the superior experience accumulation in collaborative modeling presented in Section 2.1 and making prediction with semantics only may be suboptimal [14] and hinder the performance for cold-start scenarios or low-frequency long-tailed features. 2) Due to the huge number of parameters of the language models, it is quite arduous for language models to meet the low latency requirements of recommender systems, making online deployment much more challenging. Instead, our proposed CTRL overcomes these two shortcomings by combining both collaborative and semantic signals via two-stage training paradigm.",
  "3 PRELIMINARY": "In this section, we present the collaborative-based deep CTR model and reveal the deficiencies in modeling semantic information. The CTR prediction is a supervised binary classification task, whose dataset consists of several instances ( x , ğ‘¦ ) . Label ğ‘¦ âˆˆ { 0 , 1 } indicates CTRL: Connect Collaborative and Language Model for CTR Prediction Conference'17, July 2017, Washington, DC, USA user's actual click action. Feature x is multi-fields that contains important information about the relations between users and items, including user profiles (e.g., gender, occupation), item features (e.g., category, price) as well as contextual information (e.g., time, location) [16]. Based on the instances, the traditional deep CTR models leverage the collaborative signals to estimate the probability ğ‘ƒ ( ğ‘¦ = 1 | x ) for each instance. The existing collaborative-based CTR models first encode the tabular features into one-hot features and then model the feature co-occurrence relations by various human-designed operations. Specifically, the multi-field tabular features are transformed into the high-dimensional sparse features via field-wise one-hot encoding [20]. For example, the feature (Gender= Female , Occupation= Doctor , Genre= Sci-Fi , . . . , City= Hong Kong ) of an instance can be represented as a one-hot vector:  Generally, deep CTR models follow an 'Embedding & Feature interaction' paradigm [3, 16]. The high-dimensional sparse onehot vector is mapped into a low-dimensional dense space via an embedding layer with an embedding look-up operation. Specifically, for the ğ‘– -th feature, the corresponding feature embedding e ğ‘– can be obtained via e ğ‘– = E ğ‘– x ğ‘– , where E ğ‘– is the embedding matrix. Following, feature interaction layers are proposed to capture the explicit or implicit feature co-occurrence relations. Massive effort has been made in designing specific interaction functions, such as product [17, 45], cross layer [3, 33, 54], non-linear layer [6, 62], and attention layer [65]. Finally, the predictive CTR score Ë† ğ‘¦ is obtained via an output layer and optimized with the ground-truth label ğ‘¦ through the widely-used Binary Cross Entropy (BCE). As we can observe, collaborative-based CTR models leverage the one-hot encoding to convert the original tabular data into onehot vectors as E.q.(1), discarding the semantic information among the feature fields and values 1 . By doing this, the feature semantics is lost and the only signals that can be used for prediction are the feature co-occurrence relations, which is suboptimal when the relations are weak in some scenarios such as cold-start or lowfrequency long-tailed features. Therefore, introducing the language model to capture the essential semantic information is conducive to compensating for the information gaps and improving performance.",
  "4 METHOD": "As depicted in Figure 3, the proposed CTRL is a two-stage training paradigm. The first stage is Cross-modal Knowledge Alignment , which feeds paired tabular data and textual data from two modalities into the collaborative model and the language model respectively, and then aligns them with the contrastive learning objective. The second stage is the Supervised Fine-tuning stage, where the collaborative model is fine-tuned on the downstream task with supervised signals. Gender is Female , User Features Item Features age occupation user history Female 18 doctor Titanic ï¼Œ Avatar title genres director The Terminator Sci-Fi Cameron This is a user , gender is female , age is 18 , occupation is doctor , who has recently watched Titanic | Avatar . This is a movie , title is The Terminator , genre is Sci-FI , director is Camelon . Feature Field Conjunction Feature Value Feature Seperator User-Item Seperator Gender is Female , Gender is Female , Gender is Female , Prompt Construction Tabular Data Textual Data gender age occupation user history Female 18 doctor Titanic ï¼Œ Avatar title genres director The Terminator Sci-Fi Cameron gender age occupation user history female 18 doctor Titanic ï¼Œ Avatar title genre director The Terminator Sci-Fi Cameron gender is , female .",
  "4.1": "",
  "Figure 2: The overall process of prompt construction. Prompt Construction": "Before introducing the two-stage training paradigm, we first present the prompt construction process. As illustrated in Figure 2, to obtain textual prompt data, we design prompt templates to transform the tabular data into textual data for each training instance. As mentioned in previous work [8, 14], a proper prompt should contain sufficient semantic information about the user and the item. For example, user's profiles such as age, identity, interests, and behaviors can be summarized in a single sentence. Besides, item's description sentence can be organized with the features such as color, quality, and shape. For this purpose, we design the following template to construct the prompts: This is a user, gender is female, age is 18, occupation is doctor, who has recently watched Titanic|Avatar. This is a movie, title is The Terminator, genre is Sci-FI, director is Camelon. In this prompt, the first sentence ' This is a user, gender is female, age is 18, occupation is doctor, who has recently watched Titanic|Avatar. ' describes the user-side features, including his/her profiles such as age, gender, occupation, and historical behaviors, etc. The following sentence ' This is a movie, title is The Terminator, genre is Sci-FI, director is Camelon. ' describes the item-side features such as title, category, director, etc. In the practical implementation, we use the period '. ' to separate the user-side and item-side descriptions, the comma ', ' to separate each feature, and vertical bar '|' to separate each user's historical behavior 2 . We also explore the effect of different prompts, of which results are presented in Section 5.6.2.",
  "4.2 Cross-modal Knowledge Alignment": "As mentioned before, existing collaborative-based recommendation models [49, 54] leverage the feature co-occurrence relations to infer users' preferences over items, facilitating the evolution of recommendations. Besides, the pre-trained language models [9] specializes in capturing the semantic signals of recommendation 1 We use 'feature field' to represent a class of features following [16] and 'feature value' to represent a certain value in a specific field. For example, occupation is a 'feature field' and doctor is one of the 'feature value'. 2 Note that this step is performed in the data process pipeline, and generating millions of textual prompts only takes a few seconds with parallel computing. For datasets with hundreds of features, a subset of significant features is selected to generate prompts. Conference'17, July 2017, Washington, DC, USA Xiangyang Li and Bo Chen et al. Figure 3: An intuitive illustration of the CTRL, which is a two-stage framework, in the first stage, cross-modal contrastive learning is used to fine-grained align knowledge of the two modalities. In the second stage, the lightweight collaborative model is fine-tuned on downstream tasks. The red square represents a positive pair in the batch, while the green square represents a negative pair. Multi-Head Attention Add & Norm Feed Forward Add & Norm Textual Data Positive Pair Negative Pair First Stage: Cross-modal Knowledge Alignment 0 0 1 0 1 0 1 0 0 f f f User Item Context History Field 1 Field 2 Field m â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ This is a user,  gender is Female, age is 30,occupation is doctor,  who has recently watched Titanic|Avatar.This is a Movie ï¼Œ title is The Terminator ï¼Œ genres is Sci-FI,director is Cameron. gender age occupation user history Female 18 doctor Titanic ï¼Œ AVATAR title genres director The Terminator Sci-Fi Cameron Tabular Data Cross-modal Contrastive  Learning Semantic Model Collaborative Model This is a user,  gender is Female, age is 30,occupation is doctor,  who has recently watched Titanic|Avatar.This is a Movie ï¼Œ title is The Terminator ï¼Œ genres is Sci-FI,director is Cameron. This is a user,  gender is Female, age is 18,occupation is doctor,  who has recently watched Titanic|Avatar.This  s a Movie ï¼Œ title is The Terminator ï¼Œ genres is Sci-FI,director is Cameron. gender age occupation user history Female 18 doctor Titanic ï¼Œ AVATAR title genres director The Terminator Sci-Fi Cameron gender age occupation user history Female 18 doctor Titanic ï¼Œ AVATAR title genres director The Terminator Sci-Fi Cameron gender age occupation user history Female 18 doctor Titanic ï¼Œ AVATAR title genres director The Terminator Sci-Fi Cameron MaxSim MaxSim MaxSim sum Fine-grained Alignment This is a user,  gender is Female, age is 18,occupation is doctor,  who has recently watched Titanic|Avatar.This  s a Movie ï¼Œ title is The Terminator ï¼Œ genres is Sci-FI,director is Cameron. This is a user , gender is female , age is 18 , occupation is doctor , who has recently watched Titanic | Avatar . This is a movie , title is The Terminator , genre is Sci-FI , director is Camelon . title genres director The Terminator Sci-Fi Cameron gender age occupation user history Female 18 doctor Titanic ï¼Œ AVATAR title genre director The Terminator Sci-Fi Cameron gender age occupation user history female 18 doctor Titanic ï¼Œ AVATAR Batch Samples Batch Samples Second Stage: Supervised Finetuning FC layer 0 0 1 0 1 0 1 0 0 f f f User Item Context History Field 1 Field 2 Field m â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ gender age occupation user history Female 18 doctor Titanic ï¼Œ AVATAR title genres director The Terminator Sci-Fi Cameron Tabular Data Collaborative Model gender age occupation user history Female 18 doctor Titanic ï¼Œ AVATAR title genres director The Terminator Sci-Fi Cameron gender age occupation user history Female 18 doctor Titanic ï¼Œ AVATAR title genres director The Terminator Sci-Fi Cameron gender age occupation user history Female 18 doctor Titanic ï¼Œ AVATAR title genres director The Terminator Sci-Fi Cameron title genres director The Terminator Sci-Fi Cameron gender age occupation user history Female 18 doctor Titanic ï¼Œ AVATAR title genre director The Terminator Sci-Fi Cameron gender age occupation user history female 18 doctor Titanic ï¼Œ AVATAR Deployment Online scenarios with the linguistic and external world knowledge [14]. In order to combine the modeling capabilities of both collaborativebased models and pre-trained language models, as well as ensure efficient online inference, CTRL proposes an implicit information integration method via contrastive learning [4, 13], where crossmodal knowledge (i.e., tabular and textual information) between collaborative and semantic space is aligned. 4.2.1 Cross-modal Contrastive Learning. Thecross-modal contrastive procedure is presented in Figure 3. First, the collaborative model and semantic model (a.k.a., pre-trained language model) are utilized to encode the tabular and textual data for obtaining the corresponding representations, respectively. Specifically, let M ğ‘ğ‘œğ‘™ denotes collaborative model, and M ğ‘ ğ‘’ğ‘š denotes semantic model, for an instance x , x ğ‘¡ğ‘ğ‘ denotes the tabular form, and x ğ‘¡ğ‘’ğ‘¥ğ‘¡ denotes the textual form of the same instance that is obtained after the prompt construction process. The instance representations under collaborative and semantic space can be presented as M ğ‘ğ‘œğ‘™ ( x ğ‘¡ğ‘ğ‘ ) and M ğ‘ ğ‘’ğ‘š ( x ğ‘¡ğ‘’ğ‘¥ğ‘¡ ) , respectively. To convert the unequal-length representations into the same dimension, a linear projection layer is designed, and the transformed instance representations can be obtained as follows: Then, the contrastive learning is used to align the instance representations under different latent spaces, which is proved effective in both unimodal [4, 13] and cross-modal [46] representation learning. The assumption behind this is that, under a distance metric, the correlated representations should be constrained to be close, and vice versa should be far away. We employ InfoNCE [18] to align two representations under collaborative and semantic space for each instance. As shown in Figure 3, two different modalities (textual, tabular) of the same sample form a positive pair. Conversely, data from two different modalities (textual and tabular) belonging to diverse samples form a negative pair. Negative pairs are obtained through in-batch sampling. Denote h ğ‘¡ğ‘’ğ‘¥ğ‘¡ ğ‘˜ , h ğ‘¡ğ‘ğ‘ ğ‘˜ are the representations of two modals for the ğ‘˜ -th instance, the textual-to-tabular contrastive loss can be formulated as:   where h ğ‘¡ğ‘ğ‘ and h ğ‘¡ğ‘’ğ‘¥ğ‘¡ are the transformed collaborative and semantic representations for the same instance x , W ğ‘¡ğ‘ğ‘ , W ğ‘¡ğ‘’ğ‘¥ğ‘¡ and b ğ‘¡ğ‘ğ‘ , b ğ‘¡ğ‘’ğ‘¥ğ‘¡ are the transform matrices and biases of the linear projection layers.  where ğœ is a temperature coefficient and ğ‘ is the number of instances in a batch. Besides, function ğ‘ ğ‘–ğ‘š (Â· , Â·) measures the similarity between two vectors. Typically, cosine similarity is employed for this purpose. In order to avoid spatial bias towards collaborative modal, motivated by the Jensen-Shannon (J-S) divergence [11], we also design a tabular-to-textual contrastive loss for uniformly CTRL: Connect Collaborative and Language Model for CTR Prediction Conference'17, July 2017, Washington, DC, USA aligning into a multimodal space, which is shown as:  Finally, the cross-modal contrastive learning loss L ğ‘ğ‘ğ‘™ is defined as the average of L ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘ğ‘™ 2 ğ‘¡ğ‘ğ‘ğ‘¢ğ‘™ğ‘ğ‘Ÿ and L ğ‘¡ğ‘ğ‘ğ‘¢ğ‘™ğ‘ğ‘Ÿ 2 ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘ğ‘™ , and all the parameters including collaborative model M ğ‘ğ‘œğ‘™ and semantic model M ğ‘ ğ‘’ğ‘š are trained.  4.2.2 Fine-grained Alignment. As mentioned above, CTRL leverages the cross-modal contrastive learning to perform knowledge alignment, where the quality of alignment is measured by the cosine similarity function. However, this approach models the global similarities merely and ignores fine-grained information alignment between the two modalities h ğ‘¡ğ‘ğ‘ and h ğ‘¡ğ‘’ğ‘¥ğ‘¡ . To address this issue, CTRL adopts a fine-grained cross-modal alignment method. Specifically, both collaborative and semantic representations h ğ‘¡ğ‘ğ‘ and h ğ‘¡ğ‘’ğ‘¥ğ‘¡ are first transformed into ğ‘€ sub-spaces to extract informative knowledge from different aspects. Taking the collaborative representation h ğ‘¡ğ‘ğ‘ as example, the ğ‘š -th sub-representation h ğ‘¡ğ‘ğ‘ ğ‘š is denoted as:  where W ğ‘¡ğ‘ğ‘ ğ‘š and b ğ‘¡ğ‘ğ‘ ğ‘š are the transform matrix and bias vector for the ğ‘š -th sub-space, respectively. Similarly, the ğ‘š -th sub-representation for semantic representation is denoted as h ğ‘¡ğ‘’ğ‘¥ğ‘¡ ğ‘š . Then, the fine-grained alignment is performed by calculating the similarity score, which is conducted as a sum of maximum similarity over all sub-representations, shown as:  where h ğ‘–,ğ‘š is the ğ‘š -th sub-representation for representation h ğ‘– . By modeling fine-grained similarity over the cross-modal spaces, CTRL allows for more detailed alignment within instance representations to better integrate knowledge. In this stage, both the language model and collaborative model parameters are updated to better align the representations.",
  "4.3 Supervised Fine-tuning": "After the cross-modal knowledge alignment stage, the collaborative knowledge and semantic knowledge are aligned and aggregated in a hybrid representation space, where the relations between features are mutually strengthened. In this stage, CTRL further fine-tunes the collaborative models on different downstream tasks (CTR prediction task in this paper) with supervised signals. At the top of the collaborative model, we add an extra linear layer with random initialization, acting as the output layer for final prediction Ë† ğ‘¦ . The widely-used Binary Cross Entropy (BCE) loss is deployed to measure the classification accuracy between the prediction score Ë† ğ‘¦ and the ground-truth label ğ‘¦ , which is defined Table 1: Basic statistics of datasets. as follows:  where ğ‘¦ ğ‘˜ and Ë† ğ‘¦ ğ‘˜ are the ground-truth label and the model prediction score of the ğ‘˜ -th instance. After the supervised fine-tuning stage, only the lightweight collaborative model will be deployed online for serving, thus ensuring efficient online inference.",
  "5 EXPERIMENTS": "",
  "5.1 Experimental Setting": "5.1.1 Datasets and Evaluation Metrics. In the experiment, we deploy three large-scale public datasets, which are MovieLens, Amazon (Fashion), and Taobao, whose statistics are summarized in Table 1. Following previous work [25, 49, 65], we use two popular metrics to evaluate the performance, i.e., AUC and Logloss . As acknowledged by many studies [24, 49, 65], an improvement of 0.001 in AUC ( â†‘ ) or Logloss ( â†“ ) can be regarded as significant because it will bring a large increase in the online revenue. RelaImpr metric [65] measures the relative improvement with respect to base model, which is defined as follows:  Besides, the two-tailed unpaired ğ‘¡ -test is performed to detect a significant difference between CTRL and the best baseline. The detailed description of datasets and metrics can be referred to Appendix A. 5.1.2 Competing Models. We compare CTRL with the following models, which are classified into two classes, i.e., 1) Collaborative Models: Wide&Deep [6], DeepFM [17], DCN [54], PNN [45], AutoInt [49], FiBiNet [24], and xDeepFM [33]; and 2) Semantic Models: P5 [14], CTR-BERT [42], and P-Tab [35]. The detailed description of these models can be referred to Appendix A.2. 5.1.3 Implementation Details. For the prompt construction process, only one type of prompt is used and the comparisons are presented in Section 5.6.2. In the first stage, we utilize AutoInt [49] as the collaborative model and RoBERTa [37] as the semantic model by default, as discriminative language models are more efficient at text representation extraction than generative models like GPT under the same parameter scale [53]. Additionally, we also evaluated the performance of the LLM model like ChatGLM, with the results summarized in Table 4. The mean pooling results of the last hidden states are used as the semantic information representation. For the projection layer, we compress the collaborative representation and the semantic representation to 128 dimensions. Besides, the batch size of the cross-modal knowledge alignment stage is set to 6400 and the temperature coefficient is set to 0.7. The AdamW [38] optimizer is used and the initial learning rate is set to 1 Ã— 10 -5 , which is accompanied by a warm-up mechanism [19] to 5 Ã— 10 -4 . In Conference'17, July 2017, Washington, DC, USA Xiangyang Li and Bo Chen et al. Table 2: Performance comparison of different models. The boldface denotes the highest score and the underline indicates the best result of all baselines. â˜… represents significance level ğ‘ -value < 0 . 05 of comparing CTRL with the best baselines. RelaImpr denotes the relative AUC improvement rate of CTRL against each baseline. * It is worth noting that an AUC increase of 0.001 can be considered a significant improvement in CTR prediction [24, 30, 49, 65]. the second stage, the learning rate of the downstream fine-tuning task is set to 0.001 with Adam [28] optimizer, and batch size is set to 2048. Batch Normalization [26] and Dropout [50] are also applied to avoid overfitting. The feature embedding dimension ğ‘‘ for all models is set to 32 empirically. Besides, for all collaborative models, we set the number of hidden layers ğ¿ as 3 and the number of hidden units as [ 256 , 128 , 64 ] . To ensure a fair comparison, other hyperparameters such as training epochs are adjusted individually for all models to obtain the best results. Table 3: Inference efficiency comparison of different models in terms of Model Inference Parameters and Inference Time over testing set with single V100 GPU. As for CTRL, only the collaborative model is needed for online serving, so the number of model parameters is the same as the backbone AutoInt.",
  "5.2 Performance Comparison": "We compare the overall performance with some SOTA collaborative and semantic models, whose results are summarized in Table 2. From this, we obtain the following observations: 1) CTRL outperforms all the SOTA baselines including semantic and collaborative models over three datasets by a significant margin, showing superior prediction capabilities and proving the effectiveness of the paradigm of combining collaborative and semantic signals. 2) In comparison to the best collaborative model, our proposed CTRL achieves an improvement in AUC of 1.90% , 3.08% , and 4.45% on the three datasets respectively, which effectively demonstrates that integrating semantic knowledge into collaborative models contributes to boost performance. We attribute the significant improvements to the external world knowledge and knowledge reasoning capability in PLMs [64]. 3) The performance of existing semantic models is lower than that of collaborative models, indicating that collaborative signals and co-occurrence relations are crucial for recommender systems, and relying solely on semantic modeling is difficult to surpass the existing collaborative-based modeling scheme[14, 35, 42]. Instead, our proposed CTRL integrates the advantages of both by combining collaborative signals with semantic signals for recommendation. This approach is likely to be a key path for the future development of recommender systems.",
  "5.3 Serving Efficiency": "In industrial recommender systems, online model serving has a strict limit, e.g., 10 âˆ¼ 20 milliseconds. Therefore, high service efficiency is essential for CTR models. In this section, we compare the model parameters and inference time of different CTR models over the Alibaba and Amazon datasets, shown in Table 3. We can observe that existing collaborative-based CTR models have fewer model parameters and higher inference efficiency in comparison with semantic-based models. Moreover, the majority of parameters for the collaborative-based models are concentrated in the embedding layer while the hidden network has very few parameters, thus benefiting the online serving. On the contrary, the semantic-based models (e.g., P5 and CTR-BERT), have a larger number of parameters and lower inference efficiency due to the complex Transformer-based structures, hindering the industrial applications. Instead, for the CTRL with AutoInt as skeleton models, both model parameters and inference time are the same as the original AutoInt model, which is thanks to the decoupled training framework (semantic model is not required for online inference) and ensures the high online serving efficiency.",
  "5.4 Visualization of Modal Alignment": "To study in depth the distribution of tabular representations and textual representations in the latent space before and after the crossmodal knowledge alignment, we visualize the representations in the MovieLens dataset by projecting them into a two-dimensional space using t-SNE [52], shown in Figure 4. The two colored points represent the tabular and textual representations, respectively. We CTRL: Connect Collaborative and Language Model for CTR Prediction Conference'17, July 2017, Washington, DC, USA can observe that, before the cross-modal knowledge alignment, the representations of the two modalities are distributed in two separate spaces and are essentially unrelated, while mapped into a unified multimodal space after the alignment. This phenomenon substantiates that CTRL aligns the space of two modalities (i.e., tabular and textual), thus injecting the semantic information and external general knowledge into the collaborative model. modality tabular 720 310 (a) Before Alignment Figure 4: Visualization of the tabular and textual representations before and after the cross-modal knowledge alignment. (b) After Alignment modality Lbular",
  "5.5 Compatibility Study": "5.5.1 Compatibility for semantic models. Specifically, for semantic models, we compare four pre-trained language models with different sizes: TinyBERT [27] with 14.5M parameters (CTRLTinyBERT ), BERT-Base [9] with 110M parameters (CTRLBERT), RoBERTa [37] with 110M parameters (CTRLRoBERTa), and BERT-Large with 336M parameters (CTRLLarge). Moreover, we have introduced a novel LLM model, ChatGLM [10], with 6B parameters (CTRL ChatGLM ). For CTRL ChatGLM , during the training process, we freeze the majority of the parameters and only retain the parameters of the last layer. The experimental results are summarized in Table 4, from which we obtain some observations: 1) In comparison with the backbone model AutoInt, CTRL with different pre-trained language models achieves consistent and significant improvement, where AUC increases by 3.22% and 3.63% for CTRL ChatGLM , demonstrating the effectiveness of semantics modeling and model compatibility. 2) Among the four CTRL variants (CTRLTinyBERT, CTRLBERT, and CTRLBERTLarge, CTRL ChatGLM ), despite a substantial number of parameters being frozen in ChatGLM, CTRL ChatGLM achieves optimal performance. This phenomenon indicates that enlarging the size of the language model can imbue the collaborative model with a wealth of worldly knowledge. Furthermore, even when the parameter scale of the language model is elevated to the billion level, it continues to make a positive contribution to the collaborative model. 3) It can be observed that while the parameter size of ChatGLM is several times that of BERTLarge, the gains are mild. Therefore, when conducting modality alignment, it is only necessary to select language models of moderate scale, such as RoBERTa. 4) Using only TinyBert can lead to a 0.005 increase in AUC, indicating that we can use lightweight pre-trained language models to accelerate model training. 4) CTRLRoBERTa has a better performance in the case of an equal number of parameters compared to CTRLBERT . We hypothesize that this improvement is due to RoBERTa possessing a broader range of world knowledge and a more robust capability for semantic modeling compared to BERT. This indirectly underscores the advantages of increased knowledge in facilitating the knowledge alignment process in collaborative models. Table 4: Model compatibility study with different semantic models. Table 5: Model compatibility study with different collaborative models. The semantic model is set to RoBERTa. 5.5.2 Compatibility for collaborative models. Besides, we apply CTRL to different collaborative models, including Wide&Deep, DeepFM, DCN, and AutoInt. From Table 5, we can observe that CTRL achieves remarkable improvements with different collaborative models consistently. The average improvements over RelaImpr metric are 1.31% for Wide&Deep, 1.13% for DeepFM, 1.57% for DCN, and 2.61% for AutoInt respectively, which demonstrates the effectiveness and model compatibility.",
  "5.6 Ablation Study": "5.6.1 Ablation Study Analysis. In this section, we conduct ablation experiments to better understand the importance of different components. 1)We replace the maxsim similarity with cosine similarity; 2) we remove the pre-trained language model weights. 3) we investigate the impact of end-to-end training, which combines the two-stage process into a single stage(i.e., cross-modal knowledge alignment and CTR prediction tasks are trained together). From Figure 5, we observe the following results: 1) When we remove the weights of the pre-trained language model, the loss in model performance is quite significant. This demonstrates that the primary source of improvement in the collaborative model's performance is attributed to the world knowledge and semantic modeling capabilities of the language model, rather than solely due to contrastive learning. 2) After replacing cosine similarity with maxsim similarity, there is a degradation in the model performance. This indicates that fine-grained alignment facilitates the collaborative model in learning semantic representations. 3) We observe that the performance of end-to-end training is inferior to the pre-training and Conference'17, July 2017, Washington, DC, USA Xiangyang Li and Bo Chen et al. fine-tuning paradigm of CTRL. We conjecture that this may be due to the multi-objective setting in end to end training paradigm, which hampers the performance of the collaborative model on the CTR prediction task. 0.828 0.83 0.832 0.834 0.836 0.838 0.84 AUC CTRL w/o maxsim w/o langauge model pretrained weigits w/ end to end training 0.7 0.702 0.704 0.706 0.708 AUC CTRL w/o maxsim w/o langauge model pretrained weigits w/ end to end training (a) Movielens (b) Amazon Figure 5: The results of the ablation study. 5.6.2 Prompt Analysis. In this subsection, we explore the impact of different prompt construction methods on training CTRL. We believe that this exploration will inspire future work on how to better construct prompts. Below are several rules for constructing prompts: 1) Transform user and item features into natural language text that can be easily understood; 2) Remove auxiliary text descriptions and connect feature fields and values with ' -\" directly; 3) Remove the feature fields and transform all the feature values into a single phrase; 4) Mask the feature fields with a meaningless unified word 'Field'; 5) Replace the separator ' -\" with separator ' : \". We pre-train CTRL on these prompts and then fine-tune the CTR prediction task with the collaborative model, whose results are shown in Figure 6. From Figure 6, we can obtain the following observations: 1) Prompt-1 performs significantly better than all prompts, which indicates that constructing prompts in the form of natural language is beneficial for modeling. 2) The performance of Prompt-3 is weaker than Prompt-2, which confirms the importance of semantic information of feature fields, the lack of which will degrade the performance of the model remarkably. Meanwhile, the performance of Prompt-3 is weaker than Prompt-4, indicating that prompt with rules is stronger than prompt without rules. 3) The performance of Prompt-2 and Prompt-5 are similar, suggesting that the difference of connectives between feature field and feature value has little effect on the performance. Based on these findings, we can identify the following characteristics of designing a good prompt: 1) including feature fields such as age, gender, etc.; 2) having fluent and grammatically correct sentences and containing as much semantic information as possible. 0.828 0.83 0.832 0.834 0.836 0.838 0.698 0.7 0.702 0.704 0.706 0.708 AUC AUC Amazon Movielens 0.39 0.395 0.4 0.405 0.41 0.415 0.42 0.425 0.458 0.459 0.46 0.461 0.462 0.463 0.464 0.465 0.466 0.467 Logloss Logloss Amazon Movielens (a) AUC of different prompt (b) Logloss of different prompt Figure 6: Performance in terms of different prompts.",
  "6 APPLICATION IN INDUSTRY SYSTEM 6.1 Deploying Details of CTRL Online": "In this section, we deploy CTRL in a Huawei large-scale industrial system to verify its effectiveness. During the training, we collected Figure 7: Online workflow of CTRL. Recall User Request Ranking Item Candidate Pool Present Item List Pre-Ranking Online Serving User  Item Feature Scoring CTRL Offline Daily Training Deploy User Behavior Logs Processed Data Inference Node Collaborative Model and sampled seven days of user behavior data from Huawei largescale recommendation platform, where millions of user logs are generated daily. More than 30 distinct features are used, including user profile features (e.g., department), user behavior features (e.g., list of items clicked by the user), item original features (e.g., item title), and statistical features (e.g., the number of clicks on the item), as well as contextual features (e.g., time). In the first stage of the training, we only train for one epoch. In the second stage, we train for five epochs. Together, this totals to approximately five hours. This relatively short training time ensures that we are able to update the model on a daily basis. In the end, we deploy the collaborative model in CTRL at the ranking stage.",
  "6.2 Offline and Online Performance": "We compare the CTRL model (backbone AutoInt and RoBERTa) with the SOTA models. The offline performance results are presented in Table 6. It is evident that CTRL outperforms the baseline models significantly in terms of AUC and Logloss, thereby demonstrating its superior performance. By incorporating the modeling capabilities of both the semantic and collaborative models, CTRL achieves a significant performance improvement over both collaborative models and semantic models. Moreover, according to the results in Table 3, CTRL would not increase any serving latency compared to the backbone collaborative model, which is an industrial-friendly framework with high accuracy and low inference latency. During the online A/B testing for seven days, we obtained a 5% gain of CTR compared with the base model. CTRL has now been deployed in online services, catering to tens of millions of HuaWei users. Table 6: Huawei recommender system performance comparison. CTRL: Connect Collaborative and Language Model for CTR Prediction Conference'17, July 2017, Washington, DC, USA",
  "7 CONCLUSION": "In this paper, we reveal the importance of both collaborative and semantic signals for CTR prediction and present CTRL, an industrialfriendly and model-agnostic framework with high inference efficiency. CTRL treats the tabular data and converted textual data as two modalities and leverages contrastive learning for fine-grained knowledge alignment and integration. Finally, the lightweight collaborative model can be deployed online for efficient serving after fine-tuned with supervised signals. Our experiments demonstrate that CTRL outperforms state-of-the-art collaborative and semantic models while maintaining good inference efficiency. Future work includes exploring the application on other downstream tasks, such as sequence recommendation and explainable recommendation.",
  "REFERENCES": "[1] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. arXiv preprint arXiv:2305.00447 (2023). [2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. https://doi.org/10.48550/ARXIV.2005.14165 [3] Bo Chen, Yichao Wang, Zhirong Liu, Ruiming Tang, Wei Guo, Hongkun Zheng, Weiwei Yao, Muyu Zhang, and Xiuqiang He. 2021. Enhancing Explicit and Implicit Feature Interactions via Information Sharing for Parallel Deep CTR Models. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 3757-3766. [4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In Proceedings of ICML. PMLR, 1597-1607. [5] Zheng Chen. 2023. PALR: Personalization Aware LLMs for Recommendation. arXiv preprint arXiv:2305.07622 (2023). [6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems. 7-10. [7] David R Cox. 1958. The regression analysis of binary sequences. Journal of the Royal Statistical Society: Series B (Methodological) 20, 2 (1958), 215-232. [8] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems. https://doi.org/10.48550/ARXIV.2205.08084 [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [10] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 320-335. [11] Bent Fuglede and Flemming Topsoe. 2004. Jensen-Shannon divergence and Hilbert space embedding. In International symposium on Information theory, 2004. ISIT 2004. Proceedings. IEEE, 31. [12] Kun Gai, Xiaoqiang Zhu, Han Li, Kai Liu, and Zhe Wang. 2017. Learning piecewise linear models from large scale data for ad click prediction. arXiv preprint arXiv:1704.05194 (2017). [13] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of EMNLP. 6894-6910. [14] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems. 299-315. [15] Thore Graepel, Joaquin Quinonero Candela, Thomas Borchert, and Ralf Herbrich. 2010. Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft's bing search engine. Omnipress. [16] Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and Xiuqiang He. 2021. An embedding learning framework for numerical features in ctr prediction. In SIGKDD. 2910-2918. [17] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [18] Michael Gutmann and Aapo HyvÃ¤rinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of AISTATS. JMLR Workshop and Conference Proceedings, 297-304. [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR. 770-778. [20] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising. 1-9. [21] Mickel Hoang, Oskar Alija Bihorac, and Jacobo Rouces. 2019. Aspect-based sentiment analysis using bert. In Proceedings of the 22nd nordic conference on computational linguistics. 187-196. [22] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large Language Models are Zero-Shot Rankers for Recommender Systems. arXiv preprint arXiv:2305.08845 (2023). [23] Zhiqiang Hu, Roy Ka-Wei Lee, Charu C Aggarwal, and Aston Zhang. 2022. Text style transfer: A review and experimental evaluation. ACM SIGKDD Explorations Newsletter 24, 1 (2022), 14-45. [24] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET. In Proceedings of the 13th ACM Conference on Recommender Systems. ACM. https://doi.org/ 10.1145/3298689.3347043 [25] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems. 169-177. [26] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML. PMLR, 448-456. [27] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351 (2019). [28] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. https://doi.org/10.48550/ARXIV.1412.6980 [29] Daniel D Lee and H Sebastian Seung. 1999. Learning the parts of objects by non-negative matrix factorization. Nature 401, 6755 (1999), 788-791. [30] Xiangyang Li, Bo Chen, HuiFeng Guo, Jingjie Li, Chenxu Zhu, Xiang Long, Sujian Li, Yichao Wang, Wei Guo, Longxia Mao, et al. 2022. IntTower: the Next Generation of Two-Tower Model for Pre-Ranking System. In CIKM. 3292-3301. [31] Xiangyang Li, Xiang Long, Yu Xia, and Sujian Li. 2022. Low Resource Style Transfer via Domain Adaptive Meta Learning. arXiv preprint arXiv:2205.12475 (2022). [32] Xiangyang Li, Yu Xia, Xiang Long, Zheng Li, and Sujian Li. 2021. Exploring texttransformers in aaai 2021 shared task: Covid-19 fake news detection in english. In Combating Online Hostile Posts in Regional Languages during Emergency Situation: First International Workshop, CONSTRAINT 2021, Collocated with AAAI 2021, Virtual Event, February 8, 2021, Revised Selected Papers 1. Springer, 106-115. [33] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In SIGKDD. 1754-1763. [34] Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang. 2019. Feature generation by convolutional neural network for click-through rate prediction. In The World Wide Web Conference. 1119-1129. [35] Guang Liu, Jie Yang, and Ledell Wu. 2022. PTab: Using the Pre-trained Language Model for Modeling Tabular Data. arXiv preprint arXiv:2209.08060 (2022). [36] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is ChatGPT a Good Recommender? A Preliminary Study. arXiv preprint arXiv:2304.10149 (2023). [37] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). [38] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). [39] Yuanfu Lu, Yuan Fang, and Chuan Shi. 2020. Meta-learning on heterogeneous information networks for cold-start recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1563-1573. [40] H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. 2013. Ad click prediction: a view from the trenches. In SIGKDD. 1222-1230. [41] Marcin MichaÅ‚ MiroÅ„czuk and JarosÅ‚aw Protasiewicz. 2018. A recent overview of the state-of-the-art elements of text classification. Expert Systems with Applications 106 (2018), 36-54. [42] Aashiq Muhamed, Iman Keivanloo, Sujan Perera, James Mracek, Yi Xu, Qingjun Cui, Santosh Rajagopalan, Belinda Zeng, and Trishul Chilimbi. 2021. CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models. In NeurIPS Efficient Natural Language and Speech Processing Workshop. Conference'17, July 2017, Washington, DC, USA Xiangyang Li and Bo Chen et al. [43] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In EMNLP-IJCNLP. 188-197. [44] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 (2022). [45] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In ICDM. IEEE, 1149-1154. [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748-8763. [47] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. https: //doi.org/10.48550/ARXIV.1910.10683 [48] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference on data mining. IEEE, 995-1000. [49] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In CIKM. 1161-1170. [50] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research 15, 1 (2014), 1929-1958. [51] Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. arXiv preprint arXiv:2304.09542 (2023). [52] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (2008). [53] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2020. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. arXiv:1905.00537 [cs.CL] [54] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In ADKDD. 1-7. [55] Xin Xin, Bo Chen, Xiangnan He, Dong Wang, Yue Ding, and Joemon M Jose. 2019. CFM: Convolutional Factorization Machines for Context-Aware Recommendation.. In IJCAI, Vol. 19. 3926-3932. [56] Hu Xu, Bing Liu, Lei Shu, and Philip S Yu. 2019. BERT post-training for review reading comprehension and aspect-based sentiment analysis. arXiv preprint arXiv:1904.02232 (2019). [57] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2021. FILIP: fine-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783 (2021). [58] Yantao Yu, Weipeng Wang, Zhoutian Feng, and Daiyue Xue. 2021. A Dual Augmented Two-tower Model for Online Large-scale Recommendation. (2021). [59] Zeping Yu, Jianxun Lian, Ahmad Mahmoody, Gongshen Liu, and Xing Xie. 2019. Adaptive User Modeling with Long and Short-Term Preferences for Personalized Recommendation.. In IJCAI. 4213-4219. [60] Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. arXiv preprint arXiv:2305.07609 (2023). [61] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: A large language model empowered recommendation approach. arXiv preprint arXiv:2305.07001 (2023). [62] Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep learning over multi-field categorical data. In ECIR. Springer, 45-57. [63] Weinan Zhang, Shuai Yuan, and Jun Wang. 2014. Optimal real-time bidding for display advertising. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 1077-1086. [64] Yuhui Zhang, Hao Ding, Zeren Shui, Yifei Ma, James Zou, Anoop Deoras, and Hao Wang. 2021. Language models as recommender systems: Evaluations and limitations. (2021). [65] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In SIGKDD. 1059-1068.",
  "A EXPERIMENTAL SETTING": "",
  "A.1 Datasets and Evaluation Metrics": "MovieLens Dataset 3 is a movie recommendation dataset and following previous work [49], we consider samples with ratings less 3 https://grouplens.org/datasets/MovieLens/1m/ than 3 as negative, samples with scores greater than 3 as posi- tive, and remove neutral samples, i.e., rating equal to 3. Amazon Dataset 4 [43] is a widely-used benchmark dataset [45, 58, 59, 65] and our experiment uses a subset Fashion following [65]. We take the items with a rating of greater than 3 as positive and the rest as negative. Alibaba Dataset 5 [12] is a Taobao ad click dataset. For the MovieLens and Amazon datasets, following previous work [30], we divide the train, validation, and test sets by user interaction time in the ratio of 8:1:1. For the Alibaba dataset, we divide the datasets according to the official implementation [65], and the data from the previous seven days are used as the training and validation samples with 9:1 ratio, and the data from the eighth day are used for test. The area under the ROC curve ( AUC ) measures the probability that the model will assign a higher score to a randomly selected positive item than to a randomly selected negative item. Logloss is a widely used metric in binary classification to measure the distance between two distributions.",
  "A.2 Competing Models": "Collaborative Models : Wide&Deep combines linear feature interactions (wide) with nonlinear feature learning (deep). DeepFM integrates a Factorization Machine with Wide&Deep, minimizing feature engineering. DCN enhances Wide&Deep with a cross-network to capture higher-order interactions. AutoInt uses Multi-head SelfAttention for feature interaction. PNN , xDeepFM , and FiBiNET all serve as strong baselines. Semantic Models : P5 transforms recommendation into text generation with a T5 base, while CTR-BERT , an Amazon model, leverages BERT towers for semantic prediction. P-Tab employs pre-training with Masked Language Modeling (MLM) on recommendation datasets, then fine-tunes for prediction.",
  "B HYPERPARAMETER ANALYSIS": "",
  "B.1 The Impact of Contrastive Learning Temperature Coefficient": "To explore the effect of different temperature parameters in the cross-modal knowledge alignment contrastive learning, we implement experiments on MovieLens and Amazon datasets, and the results are in Figure 8(a). From the results we can get the following observations: 1) The temperature coefficient in contrastive learning has an obvious impact on the performance. As the temperature coefficient increases, the performance will have a tendency to improve first and then decrease, indicating that increasing the coefficient within a certain range is beneficial to improve the performance. 2) For both MovieLens and Amazon datasets, the optimal temperature coefficient is below 1 in our experiments, which has also been verified in previous work [46, 57].",
  "B.2 The Impact of First Stage Batch Size": "We also explore the impact of different batch sizes, and the results are shown in Figure 8(b). We can observe that as the batch size increases, the performance is also improved on both datasets, which indicates that increasing the batch size during the contrastive 4 https://jmcauley.ucsd.edu/data/amazon/ 5 https://tianchi.aliyun.com/dataset/dataDetail?dataId=56 Conference'17, July 2017, Washington, DC, USA CTRL: Connect Collaborative and Language Model for CTR Prediction learning pre-training is conducive to achieving better cross-modal knowledge alignment effect and improving the prediction accuracy. 0.815 0.82 0.825 0.83 0.835 0.84 0.69 0.692 0.694 0.696 0.698 0.7 0.702 0.704 0.706 0.708 0.1 0.5 0.7 1 1.2 1.5 AUC AUC Amazon Movielens 0.815 0.82 0.825 0.83 0.835 0.84 0.69 0.692 0.694 0.696 0.698 0.7 0.702 0.704 0.706 0.708 200 400 800 1600 3200 6400 AUC AUC Amazon Movielens (a) Temperature Coefficient (b) Batch sizes Figure 8: Influence of different contrastive learning temperature coefficient and batch sizes.",
  "keywords_parsed": [],
  "references_parsed": [
    {
      "ref_id": "b1",
      "title": "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation"
    },
    {
      "ref_id": "b2",
      "title": "Language Models are Few-Shot Learners"
    },
    {
      "ref_id": "b3",
      "title": "Enhancing Explicit and Implicit Feature Interactions via Information Sharing for Parallel Deep CTR Models"
    },
    {
      "ref_id": "b4",
      "title": "A simple framework for contrastive learning of visual representations"
    },
    {
      "ref_id": "b5",
      "title": "PALR: Personalization Aware LLMs for Recommendation"
    },
    {
      "ref_id": "b6",
      "title": "Wide & deep learning for recommender systems"
    },
    {
      "ref_id": "b7",
      "title": "The regression analysis of binary sequences"
    },
    {
      "ref_id": "b8",
      "title": "M6Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems"
    },
    {
      "ref_id": "b9",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "ref_id": "b10",
      "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"
    },
    {
      "ref_id": "b11",
      "title": "Jensen-Shannon divergence and Hilbert space embedding"
    },
    {
      "ref_id": "b12",
      "title": "Learning piecewise linear models from large scale data for ad click prediction"
    },
    {
      "ref_id": "b13",
      "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
    },
    {
      "ref_id": "b14",
      "title": "Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5)"
    },
    {
      "ref_id": "b15",
      "title": "Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft's bing search engine"
    },
    {
      "ref_id": "b16",
      "title": "An embedding learning framework for numerical features in ctr prediction"
    },
    {
      "ref_id": "b17",
      "title": "DeepFM: a factorization-machine based neural network for CTR prediction"
    },
    {
      "ref_id": "b18",
      "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"
    },
    {
      "ref_id": "b19",
      "title": "Deep residual learning for image recognition"
    },
    {
      "ref_id": "b20",
      "title": "Practical lessons from predicting clicks on ads at facebook"
    },
    {
      "ref_id": "b21",
      "title": "Aspect-based sentiment analysis using bert"
    },
    {
      "ref_id": "b22",
      "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems"
    },
    {
      "ref_id": "b23",
      "title": "Text style transfer: A review and experimental evaluation"
    },
    {
      "ref_id": "b24",
      "title": "FiBiNET"
    },
    {
      "ref_id": "b25",
      "title": "FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction"
    },
    {
      "ref_id": "b26",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift"
    },
    {
      "ref_id": "b27",
      "title": "Tinybert: Distilling bert for natural language understanding"
    },
    {
      "ref_id": "b28",
      "title": "Adam: A Method for Stochastic Optimization"
    },
    {
      "ref_id": "b29",
      "title": "Learning the parts of objects by non-negative matrix factorization"
    },
    {
      "ref_id": "b30",
      "title": "IntTower: the Next Generation of Two-Tower Model for Pre-Ranking System"
    },
    {
      "ref_id": "b31",
      "title": "Low Resource Style Transfer via Domain Adaptive Meta Learning"
    },
    {
      "ref_id": "b32",
      "title": "Exploring texttransformers in aaai 2021 shared task: Covid-19 fake news detection in english"
    },
    {
      "ref_id": "b33",
      "title": "xdeepfm: Combining explicit and implicit feature interactions for recommender systems"
    },
    {
      "ref_id": "b34",
      "title": "Feature generation by convolutional neural network for click-through rate prediction"
    },
    {
      "ref_id": "b35",
      "title": "PTab: Using the Pre-trained Language Model for Modeling Tabular Data"
    },
    {
      "ref_id": "b36",
      "title": "Is ChatGPT a Good Recommender? A Preliminary Study"
    },
    {
      "ref_id": "b37",
      "title": "Roberta: A robustly optimized bert pretraining approach"
    },
    {
      "ref_id": "b38",
      "title": "Decoupled weight decay regularization"
    },
    {
      "ref_id": "b39",
      "title": "Meta-learning on heterogeneous information networks for cold-start recommendation"
    },
    {
      "ref_id": "b40",
      "title": "Ad click prediction: a view from the trenches"
    },
    {
      "ref_id": "b41",
      "title": "A recent overview of the state-of-the-art elements of text classification"
    },
    {
      "ref_id": "b42",
      "title": "CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models"
    },
    {
      "ref_id": "b43",
      "title": "Justifying recommendations using distantly-labeled reviews and fine-grained aspects"
    },
    {
      "ref_id": "b44",
      "title": "Training language models to follow instructions with human feedback"
    },
    {
      "ref_id": "b45",
      "title": "Product-based neural networks for user response prediction"
    },
    {
      "ref_id": "b46",
      "title": "Learning transferable visual models from natural language supervision"
    },
    {
      "ref_id": "b47",
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
    },
    {
      "ref_id": "b48",
      "title": "Factorization machines"
    },
    {
      "ref_id": "b49",
      "title": "Autoint: Automatic feature interaction learning via selfattentive neural networks"
    },
    {
      "ref_id": "b50",
      "title": "Dropout: a simple way to prevent neural networks from overfitting"
    },
    {
      "ref_id": "b51",
      "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent"
    },
    {
      "ref_id": "b52",
      "title": "Visualizing data using t-SNE"
    },
    {
      "ref_id": "b53",
      "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
    },
    {
      "ref_id": "b54",
      "title": "Deep & cross network for ad click predictions"
    },
    {
      "ref_id": "b55",
      "title": "CFM: Convolutional Factorization Machines for Context-Aware Recommendation"
    },
    {
      "ref_id": "b56",
      "title": "BERT post-training for review reading comprehension and aspect-based sentiment analysis"
    },
    {
      "ref_id": "b57",
      "title": "FILIP: fine-grained interactive language-image pre-training"
    },
    {
      "ref_id": "b58",
      "title": "A Dual Augmented Two-tower Model for Online Large-scale Recommendation"
    },
    {
      "ref_id": "b59",
      "title": "Adaptive User Modeling with Long and Short-Term Preferences for Personalized Recommendation"
    },
    {
      "ref_id": "b60",
      "title": "Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation"
    },
    {
      "ref_id": "b61",
      "title": "Recommendation as instruction following: A large language model empowered recommendation approach"
    },
    {
      "ref_id": "b62",
      "title": "Deep learning over multi-field categorical data"
    },
    {
      "ref_id": "b63",
      "title": "Optimal real-time bidding for display advertising"
    },
    {
      "ref_id": "b64",
      "title": "Language models as recommender systems: Evaluations and limitations"
    },
    {
      "ref_id": "b65",
      "title": "Deep interest network for click-through rate prediction"
    }
  ]
}