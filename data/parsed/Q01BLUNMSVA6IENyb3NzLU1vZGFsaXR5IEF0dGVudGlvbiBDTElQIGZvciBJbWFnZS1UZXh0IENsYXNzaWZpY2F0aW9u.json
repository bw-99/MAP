{"title": "CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification", "authors": "Huidong Liu; Shaoyuan Xu; Jinmiao Fu; Yang Liu; Ning Xie; Chien-Chih Wang; Bryan Wang; Yi Sun", "pub_date": "", "abstract": "Modern Web systems such as social media and e-commerce contain rich contents expressed in images and text. Leveraging information from multi-modalities can improve the performance of machine learning tasks such as classification and recommendation. In this paper, we propose the Cross-Modality Attention Contrastive Language-Image Pre-training (CMA-CLIP), a new framework which unifies two types of cross-modality attentions, sequence-wise attention and modality-wise attention, to effectively fuse information from image and text pairs. The sequence-wise attention enables the framework to capture the fine-grained relationship between image patches and text tokens, while the modality-wise attention weighs each modality by its relevance to the downstream tasks. In addition, by adding task specific modality-wise attentions and multilayer perceptrons, our proposed framework is capable of performing multi-task classification with multi-modalities. We conduct experiments on a Major Retail Website Product Attribute (MRWPA) dataset and two public datasets, Food101 and Fashion-Gen. The results show that CMA-CLIP outperforms the pre-trained and fine-tuned CLIP by an average of 11.9% in recall at the same level of precision on the MRWPA dataset for multitask classification. It also surpasses the state-of-the-art method on Fashion-Gen Dataset by 5.5% in accuracy and achieves competitive performance on Food101 Dataset. Through detailed ablation studies, we further demonstrate the effectiveness of both cross-modality attention modules and our method's robustness against noise in image and text inputs, which is a common challenge in practice.", "sections": [{"heading": "INTRODUCTION", "text": "Inspired by the recent rise of the pre-trained NLP models such as BERT [11], learning to classify image-text pairs for vision-language (\ud835\udc49 \ud835\udc3f) tasks using Transformer [37] based encoders has received much attention as both modalities can be informative and beneficial to each other. Current methods can be classified into two main categories: one-stream methods and two-stream methods. One-stream methods capture the cross-modality attention across * All three authors contributed equally to this research.\nimage and text by concatenating them at early stage and input the concatenated feature into one unified transformer encoder. Twostream methods first extract the image and text features using two separate encoders and then learn their cross-modal relationship through various methods such as contrastive learning [3], etc.\nAmong those one-stream and two-stream methods, the Contrastive Language-Image Pre-Training (CLIP) [30] has achieved great success recently. CLIP is trained on the WebImageText (WIT) Dataset which consists of 400 million image-text pairs collected from a variety of publicly available sources on the Web and achieves many state-of-the-art results in zero-shot learning tasks, pre-training tasks, and supervised classification tasks when a linear probe is added on top of it.\nDespite of CLIP's [30] strength, it is mainly designed for zeroshot image classification, resulting in the limitation of its ability to leverage both image and text input when available. Since the training of CLIP only involves global image and text features, thus the fine-grained relationship between image patches and text tokens are not modeled. Such relationship is useful in fine-trained classification tasks, especially in the situations where only a small proportion of image patches or text tokens are related to the classification tasks. Moreover, since it chooses the user-defined textual description called \"prompts\" with class value that matches the most with the image as the classification result, significant efforts to engineer the prompts for optimizing downstream tasks are required. Last but not least, in practice, it is quite common for the image-text pairs to contain noise. For instance, on E-commerce websites, some images or text could be irrelevant to the product due to catalog errors. In social media apps, users might enter irrelevant textual comments or upload unrelated images. Treating input from both modalities equally in such situation may lead to poor classification performances as one of the modalities could be pure noise. To address the aforementioned issues, in this paper we propose the Cross-Modality Attention CLIP (CMA-CLIP). Our contributions include:\n\u2022 We combine CLIP with a sequence-wise attention module, which refines the CLIP-generated image and text embeddings  \ud835\udc38.\ud835\udc54., the embedding of the black image patches are more correlated with the '\ud835\udc4f\ud835\udc59\ud835\udc4e\ud835\udc50\ud835\udc58' token in text. We experimentally prove that such refinement can improve the performance of classification tasks. \u2022 We adopt a modality-wise attention module to assign learnable weights to each modality that measures its relevance to the classification task. The impact of the irrelevant modality will be dampened, and therefore our network is robust against noisy image or text inputs, which is a common challenge in practice. \u2022 We add task specific modality-wise attentions and MLP heads on top of the sequence-wise attention module, so that the same network can be leveraged for multi-task classification. Moreover, compared with CLIP, this architecture enables the network to leverage both image and text inputs in both training and inference stage. \u2022 On the MRWPA, CMA-CLIP outperforms the raw CLIP and the fine-tuned CLIP (fine-tuned using image-text pairs from a major retail website) on the classification of three product attributes by 10.9% and 12.9% in recall at the same level of precision. It also improves the state-of-the-art performance [44] on Fashion-Gen Dataset from 88.1% to 93.6% in accuracy and achieves competitive performance on Food101 Dataset against [21].", "publication_ref": ["b10", "b36", "b2", "b29", "b29", "b44", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "Current multi-modality learning methods are mainly one-stream and two-steam where one-stream methods use a single Transformer encoder to process the concatenated image and text embedding, while two-stream methods use both image encoder and text encoder to extract image and text embeddings at early stage and then learn their cross-modal relationship.\nThe one-stream methods, such as ViLBERT [28], VisualBERT [26], VL-BERT [35], Unicoder-VL [25], ImageBERT [29] and Unified VLP [43], concatenate the image's Region-Of-Interest (ROI) patches and text tokens as the input tokens for BERT [11]. These models are typically pre-trained using tasks including Masked Language Modeling (MLM), Masked Region Modeling (MRM), Multi-Model Alignment Prediction (MMAP). The UNITER [8] and OSCAR [27] incorporate additional pre-training tasks. The UNITER uses the Optimal Transport (OT) [38] to model the relationship between the image patches and the text tokens. The OSCAR uses the object categories detected by the Faster-RCNN [31] and encodes the category text as additional input tokens to BERT. Instead of using the Faster-RCNN to detect ROIs, methods such as ICMLM [33], Pixel-BERT [18], and SOHO [17] use a CNN to extract the feature maps of an image and use the depth vectors in feature maps as image tokens. Such configuration is able to capture the semantic connection between image pixels and text tokens, which is overlooked by region based image features extracted by Faster-RCNN. Similar to ICMLM, the VirTex [10] also uses the depth vectors in feature maps as image tokens and input the image and text tokens into a forward transformer decoder and a backward transformer decoder. Instead of feeding the whole image or ROI into a CNN, methods like FashionBERT [13], KaleidoBERT [44], and ViLT [22] cut an image into patches and treat each patch as an \"image token\". For FashionBERT, it uses a pre-trained image model such as InceptionV3 [36] or ResNeXt-101 [40] to extract image features. Different from FashionBERT, KaleidoBERT adopts the SAT [41] network to generate description of salient image patches aiming to find an approximate correlation between image patches and text tokens to serve their pre-training tasks, \ud835\udc56.\ud835\udc52., Aligned Masked Language Modeling (AMLM), Image and Text Matching (ITM), and Aligned Kaleido Patch Modeling (AKPM). ViLT differs from all above-mentioned methods by simply applying linear projection on flattened image patches which greatly reduce the model size, thus leading to significant runtime and parameter efficiency.\nThe two-stream methods are mainly motivated by self-supervised learning methods [2-7, 14, 15]. In self-supervised learning, two views (\ud835\udc52.\ud835\udc54., two augmentations), of a single image are forwarded into one network respectively. Their outputs are compared using the contrastive loss [3], so that the two views of the same image are much similar than the two views from two different images. The ConVIRT [42] adopts this idea on the self-supervised learning of image-text pairs. Two networks are used to extract the image and text features respectively. The image and text features from the paired image-text input is trained to be much similar than the unpaired ones. CLIP [30] is a simplified version of ConVIRT, where the text in each image-text pair is a single sentence instead of a pool of sentences as in ConVIRT. Similar as CLIP, BriVL [19] uses MoCo [15] which is a more advanced cross-modal contrastive learning algorithm to help train the network with limited GPU memory by leveraging more negative samples. The ALIGN [20] collects 1.8 billion image-text pairs, and adopts a similar network architecture as CLIP. The performance of ALIGN is comparable to CLIP on the Im-ageNet dataset for the classification task, Flickr30K and MSCOCO datasets for image-text retrieval task.\nIn order to perform image-text classification tasks, a classification layer needs to be added on top of the image-text embeddings of the pre-trained models such as VL-BERT [35], UNITER [8], et al.\nThe MMBT [21] is specifically designed for image-text classification tasks. Unlike VL-BERT or UNITER, MMBT directly loads weights from BERT which does not require pre-training. In MMBT, ResNet [16] is used to extract image features. The image features are projected into the same space as the text tokens, used as image tokens and feed into a BERT together with text tokens. A linear layer is added on the classification embedding to perform supervised tasks.\nHowever, both one-stream and two-stream methods have their own inadequacies. One-stream methods heavily rely on pre-trained Faster-RCNN [31] or ResNet [16] to extract image feature which does not support end-to-end training of the whole network, and therefore, the extracted image and text features are not optimized to model the image-text relationship. Two-stream methods only focus on learning global image and text features, and cannot capture the fine-grained relationship between image patches and text tokens.\nIn order to overcome the aforementioned disadvantages, our proposed method fuses both one-stream and two-stream architectures which complements each other's inadequacies. It leverages the pre-trained CLIP, a two-stream architecture, to capture the overall alignment between image and text. Subsequently, we add a sequence-wise attention module, which is a transformer based cross-modality attention module used in most one-stream architectures, to capture the fine-grained relationship between image patches and text tokens. SemVLP [24] applies similar fusion logic. The difference between our method and SemVLP is that, SemVLP leverages the same cross-modality attention module to capture both high-level and fine-grained relationship between image and text. It was pre-trained on tasks such as MLM and MRM, whereas CLIP was directly pre-trained to maximize the overall image-text alignment through contrastive learning. More importantly, SemVLP does not consider the situation where one of the modalities is irrelevant to the downstream classification tasks due to input noises, which is a common challenge in practice. To handle such situation, we add a modality-wise attention module, which learns the importance of both modalities so that the irrelevant modality can be dampened for the classification tasks. At last, by adding task specific modalitywise attentions and MLPs, our model is able to perform multi-task classifications.", "publication_ref": ["b27", "b25", "b34", "b24", "b28", "b43", "b10", "b7", "b26", "b38", "b30", "b32", "b17", "b16", "b9", "b12", "b44", "b21", "b35", "b40", "b41", "b2", "b42", "b29", "b18", "b14", "b19", "b34", "b7", "b20", "b15", "b30", "b15", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "METHOD", "text": "The rest of the paper is arranged as follows: In Section 3, we first give a brief review of CLIP, and then we introduce our proposed CMA-CLIP with detailed explanation of each component. In Section 4, we introduce the datasets that we use, the corresponding experimental results, the visualization of the sequence-wise attention module, and the ablation study to prove the effectiveness of modality-and sequence-wise attention modules. In Section 5, we conclude this paper and elaborate our future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Contrastive Language-Image Pre-Training (CLIP)", "text": "The Contrastive Language-Image Pre-Training (CLIP) consists of an image encoder and a text encoder. For each image-text pair, the image and text encoders project the pair into an image and text embedding in the same multi-modal space. Given \ud835\udc41 imagetext pairs, the training objective of CLIP is to maximize the cosine similarity of the paired image and text embedding while minimize the cosine similarity of the unpaired ones.\nDuring inference, for a classification task with \ud835\udc3e classes, it first uses the \ud835\udc3e class values to construct \ud835\udc3e prompts such as 'A photo of {class value}'. These \ud835\udc3e prompts are then projected to \ud835\udc3e text embeddings by the text encoder. For any given image, it is projected to an image embedding by the image encoder, then CLIP computes the cosine similarities between the image embedding and those \ud835\udc3e text embeddings. The class value with the largest similarity is then considered as the class prediction.\nCLIP is trained using WIT Dataset which contains 400 million image-text pairs collected from the Web. According to the results reported in [30], its zero-shot classification performance surpasses the supervised linear classifier fitted on ResNet50 [16] features on datasets such as StanfordCars [23], Country211 [30], Food101 [1], and UCF101 [34] etc.", "publication_ref": ["b29", "b15", "b22", "b29", "b0", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "The Cross-Modality Attention CLIP (CMA-CLIP)", "text": "CLIP focuses on the learning of the global image and text features.\nIn CMA-CLIP, we build a sequence-wise attention module to capture the fine-grained relationship between the image patches and the text tokens such as the black image patches and the 'black' tokens in text. This module leverages the transformer architecture [37]. It takes the sequence of embeddings corresponding to all the image patches and text tokens generated by CLIP as input. The  \ud835\udc38.\ud835\udc54., a retailer might upload wrong product images to E-commerce website, or a user might enter random textual comments on social media apps. To handle such situations, we leverage the similar architecture as in [39] to learn the importance of each modality to the classification tasks. The sum of the two embeddings weighted by their importances is followed by a MLP head for the classification. To leverage the network for multiple classification tasks, we configure task specific modality-wise attention modules and MLP heads. The complete architecture of CMA-CLIP is shown in Fig. 1.", "publication_ref": ["b36", "b39"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "3.2.1", "text": "Sequence-wise Attention. In our implementation, the sequencewise attention module is a transformer encoder [37]. Let \ud835\udc4b \u2208 R \ud835\udc60\u00d7\ud835\udc51 be the matrix of the sequence of embedding of all the image patches and text tokens generated by CLIP, where \ud835\udc60 is the length of the sequence and \ud835\udc51 is the dimension of the embedding. Let \ud835\udc4a \ud835\udc3e \u2208 R \ud835\udc60\u00d7\ud835\udc51 , \ud835\udc4a \ud835\udc44 \u2208 R \ud835\udc60\u00d7\ud835\udc51 and \ud835\udc4a \ud835\udc49 \u2208 R \ud835\udc60\u00d7\ud835\udc51 be the projection matrices which project each embedding in \ud835\udc4b to key space, query space and value space respectively:\n\ud835\udc3e = \ud835\udc4b\ud835\udc4a \ud835\udc3e , \ud835\udc44 = \ud835\udc4b\ud835\udc4a \ud835\udc44 , \ud835\udc49 = \ud835\udc4b\ud835\udc4a \ud835\udc49(1)\nThe embedding matrix \ud835\udc4b is updated as\nAttention(\ud835\udc3e, \ud835\udc44, \ud835\udc49 ) = softmax( \ud835\udc44\ud835\udc3e \ud835\udc47 \u221a\ufe01 \ud835\udc51 \ud835\udc58 )\ud835\udc49(2)\nThe self-attention block learns a similarity matrix \ud835\udc44\ud835\udc3e \ud835\udc47 between each pair of embeddings in \ud835\udc4b . Each embedding in the sequence is then updated as the average of the projected embedding across all the embeddings in the value space weighted by their similarities. This sequence-wise attention module captures the fine-grained relationship between each image patch and text token.", "publication_ref": ["b36"], "figure_ref": [], "table_ref": []}, {"heading": "Modality-wise Attention.", "text": "After the image feature and the text feature are generated, they need to be aggregated to form the final feature for the classification tasks. In order to dampen the irrelevant modality, we leverage the keyless attention module proposed in [39].\nGiven an image-text pair, the sequence-wise attention module will output \ud835\udc3c \u2032 [\ud835\udc36\ud835\udc3f\ud835\udc46 ] and \ud835\udc47 \u2032 [\ud835\udc36\ud835\udc3f\ud835\udc46 ] as the global image and text embedding, respectively. The aggregated embedding is the weighted average of \ud835\udc3c \u2032\n[\ud835\udc36\ud835\udc3f\ud835\udc46 ] and \ud835\udc47 \u2032 [\ud835\udc36\ud835\udc3f\ud835\udc46 ] :\n\ud835\udc50 = \ud835\udf06\ud835\udc3c \u2032 [\ud835\udc36\ud835\udc3f\ud835\udc46 ] + (1 -\ud835\udf06)\ud835\udc47 \u2032 [\ud835\udc36\ud835\udc3f\ud835\udc46 ](3)\nThe weight \ud835\udf06 is computed by:\n\ud835\udc52 \ud835\udc3c = \ud835\udc64 \ud835\udc47 \ud835\udc3c \u2032 [\ud835\udc36\ud835\udc3f\ud835\udc46 ](4)\n\ud835\udc52 \ud835\udc47 = \ud835\udc64 \ud835\udc47 \ud835\udc47 \u2032 [\ud835\udc36\ud835\udc3f\ud835\udc46 ]\n(5)\n\ud835\udf06 = exp(\ud835\udc52 \ud835\udc3c ) exp(\ud835\udc52 \ud835\udc3c ) + exp(\ud835\udc52 \ud835\udc47 )(6)\nwhere \ud835\udc64 is a learnable parameter vector that is of the same dimension as \ud835\udc3c \u2032\n[\ud835\udc36\ud835\udc3f\ud835\udc46 ] and \ud835\udc47 \u2032 \u22b2 since the CLIP module is frozen, we set \ud835\udc59\ud835\udc5f \ud835\udc36\ud835\udc3f\ud835\udc3c \ud835\udc43 to be 0 5: while \ud835\udc56 < \ud835\udc40\ud835\udc4e\ud835\udc65\ud835\udc38\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e1 do 6:\n( X\ud835\udc36\ud835\udc3f\ud835\udc3c\ud835\udc43 , \u0176\ud835\udc36\ud835\udc3f\ud835\udc3c\ud835\udc43 ) = \ud835\udc36\ud835\udc3f\ud835\udc3c\ud835\udc43 (X, Y)\n\u22b2 apply CLIP's image and text encoders on X and Y 7:\nX \u0176\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61 = \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61 ( X\ud835\udc36\ud835\udc3f\ud835\udc3c\ud835\udc43 , \u0176\ud835\udc36\ud835\udc3f\ud835\udc3c\ud835\udc43 ) \u22b2 concatenate the image feature X\ud835\udc36\ud835\udc3f\ud835\udc3c\ud835\udc43 and the text feature \u0176\ud835\udc36\ud835\udc3f\ud835\udc3c\ud835\udc43 8:\n( X\ud835\udc46\ud835\udc34 , \u0176\ud835\udc46\ud835\udc34 ) = \ud835\udc36\ud835\udc40\ud835\udc34 \ud835\udc46\ud835\udc34 ( X \u0176\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61 ) \u22b2 apply the sequence-wise attention module to the concatenated feature X \u0176\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61 9:\nfor k = 1 to K do 10: is used to compute the loss for this classification task. For multitask classification, we add task-specific modality-wise attention and MLP for each task separately. This is because the relevance of modality is dependent on the task, hence we need task-specific modality-wise attentions and multiple MLPs as the classification heads. Table 6: Ablation study of CMA-CLIP. Recall (%) at 90% precision on the MRWPA with Color, Pattern and Style attributes. \ud835\udc40\ud835\udc34 denotes modality-wise attention and \ud835\udc46\ud835\udc34 denotes to sequence-wise attention.\nU \ud835\udc58 \ud835\udc40\ud835\udc34 = \ud835\udc36\ud835\udc40\ud835\udc34 \ud835\udc58 \ud835\udc40\ud835\udc34 ( X\ud835\udc46\ud835\udc34 ,", "publication_ref": ["b39"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS 4.1 Datasets", "text": "We perform experiments on three datasets, the MRWPA Dataset, the Food101 Dataset [1] and the Fashion-Gen [32] Dataset. All three datasets consist of image-text pairs. Data samples from the three datasets are shown in Table 1.", "publication_ref": ["b0", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "4.1.1", "text": "The MRWPA Dataset. This dataset includes the product image and title pairs of dress products from a major retail website. The goal is to classify three dress related product attributes, color, pattern, and style. Color attribute has 17 classes such as black and white, pattern has 12 classes such as graphic and plain, and style has 21 classes such as pencil and a-line. The training data consists of 5.8 Million product image-title pairs. We also prepare 310 and 132 image-title pairs as the validation and test set, which are used for hyper-parameter tuning and performance evaluation respectively. 4.1.3 The Fashion-Gen Dataset. This dataset contains 293,008 fashion images. Each image is paired with a text describing the image. This dataset contains 48 main categories, such as \"DRESSES\", \"JEANS\", \"SKIRTS\", \"SHIRTS\", etc., and 121 sub-categories, such as \"SHORT DRESSES\", \"LEATHER JACKETS\", \"MID LENGTH SKIRTS\", \"T-SHIRTS\" and so on. In our experiments, we perform 121 subcategory classification. We use the same data as used in [44] for training and testing. The number of training data is 260480, and the number of testing data is 32528.", "publication_ref": ["b44"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation and Settings", "text": "4.2.1 Experiment Settings. Same as CLIP, the image encoder of CMA-CLIP is a 12-layer 768-width ViT-B/32 [12] with 12 attention heads, and the text encoder of CMA-CLIP is a 12-layer 512-width Transformer with 8 heads used in [37]. The sequence-wise attention transformer is also a 12-layer 512-width model with 8 attention heads. In all the experiments, the batch size is set to 1024, weight decay of Adam is set to 1\ud835\udc52 -4, and the learning rate is set to 1\ud835\udc52 -5.  ", "publication_ref": ["b11", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.3.1", "text": "The MRWPA Dataset. We compare CMA-CLIP with the zeroshot performance of raw CLIP and fine-tuned CLIP (fine-tuned using image-title pairs in MRWPA dataset) in terms of the recall at 90% precision for the color, pattern, and style attributes. The results are included in Table 2. We observe that CMA-CLIP consistently outperforms both raw CLIP and fine-tuned CLIP by a large margin across all three attributes.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "The Food101", "text": "Dataset. On the Food101 dateset, we compare CMA-CLIP with two single-modality baseline methods including BERT [11] and ViT [12], and two multi-modality baseline methods including raw CLIP [30] with same ViT-B/32 and MMBT [21]. Results are included in Table 3. CMA-CLIP achieves the best accuracy of 93.1%, which improves 1% over the a current strong baseline method MMBT. Using only image features achieves 81.8% by ViT, and using only text features achieves 87.2% by BERT.", "publication_ref": ["b10", "b11", "b29", "b20"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "4.3.3", "text": "The Fashion-Gen Dataset. On the Fashion-Gen dataset, we compare CMA-CLIP with multiple SOTA methods including Fash-ionBERT [13], ImageBERT [29], OSCAR [27] and KaleidoBERT [44]. CMA-CLIP achieves the highest accuracy of 93.6%, which improves over KaleidoBERT [44], the previous SOTA method, by 5.5%.", "publication_ref": ["b12", "b28", "b26", "b44", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Ablation Study", "text": "We conduct systematic ablation study to validate the effectiveness of modality-wise attention module and sequence-wise attention module by removing them sequentially and comparing the performance with CMA-CLIP. Detailed results are shown in Table 6.\nOn MRWPA, the average recall at 90% precision across the 3 attributes drops from 53.4% to 47.5% after removing the modalitywise attention module. This is because the proportions of titles that contain tokens related to color, pattern, and style are 67%, 25% and 15% respectively. When a title does not contain any tokens related to an attribute, it becomes irrelevant for the classification of that attribute. The performance drop indicates that the modality-wise attention module significantly improves CMA-CLIP's robustness against noisy inputs.\nTo illustrate our model's robustness to input noise, in Table 5 we randomly pick some product image-title examples that CMA-CLIP is able to give correct classification whereas CMA-CLIP without modality-wise attention module cannot. We can clearly observe that in those examples, the product titles do not contain any tokens related to the attribute labels. Furthermore, for these examples, we complete the titles by adding the label related keywords and re-test them. This time, both methods can provide correct classification results which further proves that the modality-wise attention has the ability of filtering out irrelevant information (text without label information is considered as noise). We are not able to select similar examples in the Food101 and Fashion-Gen datasets, because in these two public datasets, there are no images or text that are irrelevant to the classification task.\nThe average recall drops from 47.5% to 45.8% after further removing the sequence-wise attention module. The sequence-wise attention module enhances the context-awareness of the image and text embedding by capturing the fine-grained correlation among image patches and text tokens, and the resulting embedding is expected to yield better results for classifications. The performance drop supports this conclusion.\nWe also visualize the result of sequence-wise attention for MR-WPA, Food101 and Fashion-Gen datasets in Figure 2. For each text input, we locate the token that is related to the classification task, and visualize the image patches that are most correlated to it by checking the inner product between the query embedding of the text token and the key embeddings of the image patches. In Figure 2, red regions are where the correlation is high. We observe that the sequence-wise attention is able to identify the highly correlated image patches and text tokens across all three datasets.", "publication_ref": [], "figure_ref": ["fig_7", "fig_7"], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, we propose the CMA-CLIP, which unifies two types of cross-modality attentions: sequence-wise attention, a transformer based attention module that captures the fine-grained relationship between image patches and text tokens, and modality-wise attention, which learns the importance of image and text modalities in order to filter out the irrelevant modality for the classification task. We also design task specific modality-wise attentions and MLPs so that we can leverage a unified network for multi-task classifications. We evaluate our method on the MRWPA Dataset, the Food101 dataset and the Fashion-Gen dataset. CMA-CLIP outperforms the pre-trained and fine-tuned CLIP by an average of 11.9% in recall at the same level of precision on the MRWPA Dataset for the classifications for color, pattern, and style attributes. It also surpasses the state-of-the-art method on the Fashion-Gen Dataset by 5.5% in accuracy and achieves competitive performance on the Food101 Dataset. For the future work, we are interested in training CMA-CLIP with other datasets to enable the contrastive loss, improving CMA-CLIP's robustness against noisy labels, and also, exploring semi-supervised learning methods so that unlabeled image-text pairs can be leveraged in the training process to improve model generalizability.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Food-101 -Mining Discriminative Components with Random Forests", "journal": "", "year": "2014", "authors": "Lukas Bossard; Matthieu Guillaumin; Luc Van Gool"}, {"ref_id": "b1", "title": "Unsupervised learning of visual features by contrasting cluster assignments", "journal": "", "year": "2020", "authors": "Mathilde Caron; Ishan Misra; Julien Mairal; Priya Goyal; Piotr Bojanowski; Armand Joulin"}, {"ref_id": "b2", "title": "A simple framework for contrastive learning of visual representations", "journal": "PMLR", "year": "2020", "authors": "Ting Chen; Simon Kornblith; Mohammad Norouzi; Geoffrey Hinton"}, {"ref_id": "b3", "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Ting Chen; Simon Kornblith; Kevin Swersky; Mohammad Norouzi; Geoffrey E Hinton"}, {"ref_id": "b4", "title": "Improved baselines with momentum contrastive learning", "journal": "", "year": "2020", "authors": "Xinlei Chen; Haoqi Fan; Ross Girshick; Kaiming He"}, {"ref_id": "b5", "title": "Exploring simple siamese representation learning", "journal": "", "year": "2021", "authors": "Xinlei Chen; Kaiming He"}, {"ref_id": "b6", "title": "An empirical study of training self-supervised vision transformers", "journal": "", "year": "2021", "authors": "Xinlei Chen; Saining Xie; Kaiming He"}, {"ref_id": "b7", "title": "Uniter: Universal image-text representation learning", "journal": "Springer", "year": "2020", "authors": "Yen-Chun Chen; Linjie Li; Licheng Yu; Ahmed El Kholy; Faisal Ahmed; Zhe Gan; Yu Cheng; Jingjing Liu"}, {"ref_id": "b8", "title": "Xception: Deep learning with depthwise separable convolutions", "journal": "", "year": "2017", "authors": "Fran\u00e7ois Chollet"}, {"ref_id": "b9", "title": "Virtex: Learning visual representations from textual annotations", "journal": "", "year": "2021", "authors": "Karan Desai; Justin Johnson"}, {"ref_id": "b10", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b11", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "journal": "", "year": "2021", "authors": "Alexey Dosovitskiy; Lucas Beyer; Alexander Kolesnikov; Dirk Weissenborn; Xiaohua Zhai; Thomas Unterthiner; Mostafa Dehghani; Matthias Minderer; Georg Heigold; Sylvain Gelly; Jakob Uszkoreit; Neil Houlsby"}, {"ref_id": "b12", "title": "Fashionbert: Text and image matching with adaptive loss for crossmodal retrieval", "journal": "", "year": "2020", "authors": "Dehong Gao; Linbo Jin; Ben Chen; Minghui Qiu; Peng Li; Yi Wei; Yi Hu; Hao Wang"}, {"ref_id": "b13", "title": "Bootstrap Your Own Latent -A New Approach to Self-Supervised Learning", "journal": "", "year": "2020", "authors": "Jean-Bastien Grill; Florian Strub; Florent Altch\u00e9; Corentin Tallec; Pierre Richemond; Elena Buchatskaya; Carl Doersch; Bernardo Avila Pires; Zhaohan Guo; Mohammad Gheshlaghi Azar; Bilal Piot; Remi Munos; Michal Valko"}, {"ref_id": "b14", "title": "Momentum contrast for unsupervised visual representation learning", "journal": "", "year": "2020", "authors": "Kaiming He; Haoqi Fan; Yuxin Wu; Saining Xie; Ross Girshick"}, {"ref_id": "b15", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b16", "title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning", "journal": "", "year": "2021", "authors": "Zhicheng Huang; Zhaoyang Zeng; Yupan Huang; Bei Liu; Dongmei Fu; Jianlong Fu"}, {"ref_id": "b17", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers", "journal": "", "year": "2020", "authors": "Zhicheng Huang; Zhaoyang Zeng; Bei Liu; Dongmei Fu; Jianlong Fu"}, {"ref_id": "b18", "title": "WenLan: Bridging vision and language by large-scale multi-modal pre-training", "journal": "", "year": "2021", "authors": "Yuqi Huo; Manli Zhang; Guangzhen Liu; Haoyu Lu; Yizhao Gao; Guoxing Yang; Jingyuan Wen; Heng Zhang; Baogui Xu; Weihao Zheng"}, {"ref_id": "b19", "title": "Scaling up visual and vision-language representation learning with noisy text supervision", "journal": "", "year": "2021", "authors": "Chao Jia; Yinfei Yang; Ye Xia; Yi-Ting Chen; Zarana Parekh; Hieu Pham; V Quoc; Yunhsuan Le; Zhen Sung; Tom Li;  Duerig"}, {"ref_id": "b20", "title": "Supervised multimodal bitransformers for classifying images and text", "journal": "", "year": "2019", "authors": "Douwe Kiela; Suvrat Bhooshan; Hamed Firooz; Ethan Perez; Davide Testuggine"}, {"ref_id": "b21", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision", "journal": "", "year": "2021", "authors": "Wonjae Kim; Bokyung Son; Ildoo Kim"}, {"ref_id": "b22", "title": "3d object representations for fine-grained categorization", "journal": "", "year": "2013", "authors": "Jonathan Krause; Michael Stark; Jia Deng; Li Fei-Fei"}, {"ref_id": "b23", "title": "SemVLP: Vision-Language Pre-training by Aligning Semantics at Multiple Levels", "journal": "", "year": "2021", "authors": "Chenliang Li; Ming Yan; Haiyang Xu; Fuli Luo; Wei Wang; Bin Bi; Songfang Huang"}, {"ref_id": "b24", "title": "Unicodervl: A universal encoder for vision and language by cross-modal pre-training", "journal": "", "year": "2020", "authors": "Gen Li; Nan Duan; Yuejian Fang; Ming Gong; Daxin Jiang"}, {"ref_id": "b25", "title": "Visualbert: A simple and performant baseline for vision and language", "journal": "", "year": "2019", "authors": "Liunian Harold; Li ; Mark Yatskar; Cho-Jui Da Yin; Kai-Wei Hsieh;  Chang"}, {"ref_id": "b26", "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks", "journal": "Springer", "year": "2020", "authors": "Xiujun Li; Xi Yin; Chunyuan Li; Pengchuan Zhang; Xiaowei Hu; Lei Zhang; Lijuan Wang; Houdong Hu; Li Dong; Furu Wei"}, {"ref_id": "b27", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Jiasen Lu; Dhruv Batra; Devi Parikh; Stefan Lee"}, {"ref_id": "b28", "title": "Imagebert: Cross-modal pre-training with large-scale weak-supervised imagetext data", "journal": "", "year": "2020", "authors": "Di Qi; Lin Su; Jia Song; Edward Cui; Taroon Bharti; Arun Sacheti"}, {"ref_id": "b29", "title": "Learning Transferable Visual Models From Natural Language Supervision", "journal": "", "year": "2021-07", "authors": "Alec Radford; Jong Wook Kim; Chris Hallacy; Aditya Ramesh; Gabriel Goh; Sandhini Agarwal; Girish Sastry; Amanda Askell; Pamela Mishkin; Jack Clark"}, {"ref_id": "b30", "title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "journal": "Advances in neural information processing systems", "year": "2015", "authors": "Kaiming Shaoqing Ren; Ross He; Jian Girshick;  Sun"}, {"ref_id": "b31", "title": "Fashion-gen: The generative fashion dataset and challenge", "journal": "", "year": "2018", "authors": "Negar Rostamzadeh; Seyedarian Hosseini; Thomas Boquet; Wojciech Stokowiec; Ying Zhang; Christian Jauvin; Chris Pal"}, {"ref_id": "b32", "title": "Learning visual representations with caption annotations", "journal": "Springer", "year": "2020-08-23", "authors": "Mert Bulent Sariyildiz; Julien Perez; Diane Larlus"}, {"ref_id": "b33", "title": "UCF101: A dataset of 101 human actions classes from videos in the wild", "journal": "", "year": "2012", "authors": "Khurram Soomro; Mubarak Amir Roshan Zamir;  Shah"}, {"ref_id": "b34", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "journal": "", "year": "2020", "authors": "Weijie Su; Xizhou Zhu; Yue Cao; Bin Li; Lewei Lu; Furu Wei; Jifeng Dai"}, {"ref_id": "b35", "title": "Rethinking the Inception Architecture for Computer Vision", "journal": "IEEE Computer Society", "year": "2016", "authors": "C Szegedy; V Vanhoucke; S Ioffe; J Shlens; Z Wojna"}, {"ref_id": "b36", "title": "Attention is All you Need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141 Ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b37", "title": "", "journal": "Curran Associates, Inc", "year": "", "authors": "U V Guyon; S Luxburg; H Bengio"}, {"ref_id": "b38", "title": "Optimal transport: old and new", "journal": "Springer", "year": "2009", "authors": "C\u00e9dric Villani"}, {"ref_id": "b39", "title": "Multimodal Keyless Attention Fusion for Video Classification", "journal": "", "year": "2018", "authors": "Long Xiang; Gan Chuang; Melo Gerard D; Liu Xiao; Li Yandong; Li Fu; Wen Shilei"}, {"ref_id": "b40", "title": "Aggregated Residual Transformations for Deep Neural Networks", "journal": "", "year": "2017", "authors": "Saining Xie; Ross Girshick; Piotr Doll\u00e1r; Zhuowen Tu; Kaiming He"}, {"ref_id": "b41", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "journal": "", "year": "2015", "authors": "Kelvin Xu; Jimmy Ba; Ryan Kiros; Kyunghyun Cho; Aaron Courville; Ruslan Salakhudinov; Rich Zemel; Yoshua Bengio"}, {"ref_id": "b42", "title": "Contrastive learning of medical visual representations from paired images and text", "journal": "", "year": "2020", "authors": "Yuhao Zhang; Hang Jiang; Yasuhide Miura; Christopher D Manning; Curtis P Langlotz"}, {"ref_id": "b43", "title": "Unified vision-language pre-training for image captioning and vqa", "journal": "", "year": "2020", "authors": "Luowei Zhou; Hamid Palangi; Lei Zhang; Houdong Hu; Jason Corso; Jianfeng Gao"}, {"ref_id": "b44", "title": "Kaleido-BERT: Vision-Language Pre-training on Fashion Domain", "journal": "", "year": "2021", "authors": "Mingchen Zhuge; Dehong Gao; Deng-Ping Fan; Linbo Jin; Ben Chen; Haoming Zhou; Minghui Qiu; Ling Shao"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The pipeline of our proposed CMA-CLIP.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "with Cinnamon Cream Cheese Syrup -Cooking Classy Fashion-Gen Dataset Jeans Slim-fit jeans in dark blue. Distressing throughout. Fading at front. Textured black leather logo patch at back waist. Silver-tone metal logo plaque at back pocket. Contrast stitching in tan. Red logo tab at button-fly.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Table 1 :1Data example for each of the datasets module outputs two embeddings incorporating the aggregated image and text information. Instead of directly leveraging these two embeddings for classification, we add a modality-wise attention module to handle the situation where a certain modality (image or text) is irrelevant to the classification task. This is because, in practice, it is common for the image-text pairs to contain noise.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "[\ud835\udc36\ud835\udc3f\ud835\udc46 ] . Algorithm 1 1 :11Training Process of CMA-CLIP. Line 4-16: Warm-up Stage, Line 18-21: End-to-End Training Stage, Line 23-26: Tuning Stage Input: Image-text pairs and their labels of K tasks (X, Y, L 1 , L 2 , ..., L \ud835\udc3e ). \ud835\udc36\ud835\udc3f\ud835\udc3c\ud835\udc43 denotes the CLIP model including the image and text encoders. \ud835\udc36\ud835\udc40\ud835\udc34 \ud835\udc46\ud835\udc34 denotes the sequence-wise attention model. \ud835\udc36\ud835\udc40\ud835\udc34 \ud835\udc40\ud835\udc34 denotes the modality-wise attention model. \ud835\udc40\ud835\udc3f\ud835\udc43 denotes the multi-layer perception head for classification. 2: Training: 3: Use the image encoder and the text encoder from CLIP, and freeze their weights. 4: Set \ud835\udc59\ud835\udc5f \ud835\udc36\ud835\udc3f\ud835\udc3c \ud835\udc43 = 0, \ud835\udc59\ud835\udc5f \ud835\udc36\ud835\udc40\ud835\udc34 = 1\ud835\udc52 -5, \ud835\udc59\ud835\udc5f \ud835\udc40\ud835\udc3f\ud835\udc43 = 1\ud835\udc52 -5", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Sexy V Neck Crisscross Backless Cocktail Party Bodycon Peplum [Plain] Dress, White, L Pattern Plain Women's [Plain] Mini DungareeTable5: Examples where CMA-CLIP is able to give the correct attribute classification while CMA-CLIP w/o the modality-wise attention cannot. Noting that, text tokens which are shown in red are the attribute label keywords that do not exist in the original titles. We add them in and re-do the prediction with both methods to further validate that, without the modality-wise attention, the model is not able to manage noise properly.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "4. 1 . 212The Food101 Dataset. This dataset contains 101 food categories. The goal is to classify each image-text pair to a food category. We download the preprocessed images and texts from the Kaggle competition 1 . In the processed data, 67971 images are in the training set, and 22715 images are in the testing set. During training, we randomly split 80% of the data in the training set for training and the rest 20% data for validation.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "(a) MRWPA: Fashion Women Graphic Print Round Neck Ringer T-Dress Yellow (b) MRWPA: Women's Summer Long Sleeve Casual Loose T-Shirt Dress (c) MRWPA: Fashion Women's Oversized Short Sleeves Floral Print Mid-Long Dress Yellow Size UK 16 (d) Food101: The Brewer & The Baker: Lobster Tail Ale & Lobster Rolls (e) Food101: Scallop Recipe for Beginners | Pop Sugar Food (f) Food101: Shrimp and Grits -Picture-Perfect Meals \\xc2 \\xae Picture-Perfect Meals (g) Fashion-Gen: Grained calfskin shoulder bag in 'plaid' red. Curb chain shoulder strap. Logo stamp gold-tone... (h) Fashion-Gen: kinny-fit jeans in mid blue wash. White paint at leg. Low waist. Fading and distressing... (i) Fashion-Gen: Ankle-high grained leather boots in black. Pointed toe. Zip closure at heel. Tonal leather...", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 2 :2Figure 2: Visualization examples. Each image is highlighted using the attention map between the image embedding and the embedding of the most relevant text token.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "\u0176\ud835\udc46\ud835\udc34 ) \u22b2 apply the modality-wise attention module to get the weighted average feature U \ud835\udc58 \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60 \ud835\udc58 = \ud835\udc46\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 (L \ud835\udc58 , L\ud835\udc58 ) \u22b2 compute Softmax Loss between ground truth L \ud835\udc58 and prediction L\ud835\udc58 \ud835\udf03 \ud835\udc56+1 = \ud835\udc34\ud835\udc51\ud835\udc4e\ud835\udc5a(\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60, \ud835\udf03 \ud835\udc56 ) \u22b2 update model parameters using Adam 16: end while 17: Release all modules. Fine-tune on the best check-points from the previous stage. 18: Set \ud835\udc59\ud835\udc5f \ud835\udc36\ud835\udc3f\ud835\udc3c \ud835\udc43 = 1\ud835\udc52 -5, \ud835\udc59\ud835\udc5f \ud835\udc36\ud835\udc40\ud835\udc34 = 1\ud835\udc52 -5, \ud835\udc59\ud835\udc5f \ud835\udc40\ud835\udc3f\ud835\udc43 = 1\ud835\udc52 -5\u22b2 since all modules are released, all learning rates are set to 1\ud835\udc52 -5 19: while \ud835\udc56 < \ud835\udc40\ud835\udc4e\ud835\udc65\ud835\udc38\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e2 do Freeze the image encoder, the text encoder, the sequence-wise attention transformer. Fine-tune on the best check-points from the previous stage. 23: Set \ud835\udc59\ud835\udc5f \ud835\udc36\ud835\udc3f\ud835\udc3c \ud835\udc43 = 0, \ud835\udc59\ud835\udc5f \ud835\udc36\ud835\udc40\ud835\udc34 = 0, \ud835\udc59\ud835\udc5f \ud835\udc40\ud835\udc3f\ud835\udc43 = 1\ud835\udc52 -5\u22b2 since both CLIP and CMA modules are frozen, we set \ud835\udc59\ud835\udc5f \ud835\udc36\ud835\udc3f\ud835\udc3c \ud835\udc43 and \ud835\udc59\ud835\udc5f \ud835\udc36\ud835\udc40\ud835\udc34 to be 0 24: while \ud835\udc56 < \ud835\udc40\ud835\udc4e\ud835\udc65\ud835\udc38\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e3 do \ud835\udc36\ud835\udc3f\ud835\udc3c\ud835\udc43 model, \ud835\udc36\ud835\udc40\ud835\udc34 \ud835\udc46\ud835\udc34 model, \ud835\udc36\ud835\udc40\ud835\udc34 \ud835\udc40\ud835\udc34 model, and \ud835\udc40\ud835\udc3f\ud835\udc43 model. Recall (%) at 90% precision on the MRWPA dataset.", "figure_data": "\ud835\udc40\ud835\udc34"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Accuracies (%) on the Food101 dataset.", "figure_data": "MethodFashion-GenFashionBERT85.3ImageBERT80.1OSCAR84.2KaleidoBERT88.1CMA-CLIP93.6"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Accuracies (%) on the Fashion-Gen dataset.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\ud835\udc3e = \ud835\udc4b\ud835\udc4a \ud835\udc3e , \ud835\udc44 = \ud835\udc4b\ud835\udc4a \ud835\udc44 , \ud835\udc49 = \ud835\udc4b\ud835\udc4a \ud835\udc49(1)", "formula_coordinates": [4.0, 107.65, 629.97, 186.39, 8.97]}, {"formula_id": "formula_1", "formula_text": "Attention(\ud835\udc3e, \ud835\udc44, \ud835\udc49 ) = softmax( \ud835\udc44\ud835\udc3e \ud835\udc47 \u221a\ufe01 \ud835\udc51 \ud835\udc58 )\ud835\udc49(2)", "formula_coordinates": [4.0, 103.31, 660.18, 190.74, 22.63]}, {"formula_id": "formula_2", "formula_text": "\ud835\udc50 = \ud835\udf06\ud835\udc3c \u2032 [\ud835\udc36\ud835\udc3f\ud835\udc46 ] + (1 -\ud835\udf06)\ud835\udc47 \u2032 [\ud835\udc36\ud835\udc3f\ud835\udc46 ](3)", "formula_coordinates": [4.0, 387.17, 521.14, 171.03, 13.4]}, {"formula_id": "formula_3", "formula_text": "\ud835\udc52 \ud835\udc3c = \ud835\udc64 \ud835\udc47 \ud835\udc3c \u2032 [\ud835\udc36\ud835\udc3f\ud835\udc46 ](4)", "formula_coordinates": [4.0, 411.7, 559.78, 146.51, 13.4]}, {"formula_id": "formula_4", "formula_text": "\ud835\udc52 \ud835\udc47 = \ud835\udc64 \ud835\udc47 \ud835\udc47 \u2032 [\ud835\udc36\ud835\udc3f\ud835\udc46 ]", "formula_coordinates": [4.0, 410.2, 587.06, 54.51, 13.4]}, {"formula_id": "formula_5", "formula_text": "\ud835\udf06 = exp(\ud835\udc52 \ud835\udc3c ) exp(\ud835\udc52 \ud835\udc3c ) + exp(\ud835\udc52 \ud835\udc47 )(6)", "formula_coordinates": [4.0, 395.54, 612.8, 162.66, 20.3]}, {"formula_id": "formula_6", "formula_text": "U \ud835\udc58 \ud835\udc40\ud835\udc34 = \ud835\udc36\ud835\udc40\ud835\udc34 \ud835\udc58 \ud835\udc40\ud835\udc34 ( X\ud835\udc46\ud835\udc34 ,", "formula_coordinates": [5.0, 95.94, 222.33, 81.94, 11.96]}], "doi": ""}
