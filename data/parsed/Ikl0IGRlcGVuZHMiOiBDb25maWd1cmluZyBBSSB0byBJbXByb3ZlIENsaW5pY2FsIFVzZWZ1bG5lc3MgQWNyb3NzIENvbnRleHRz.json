{"title": "\"It depends\": Configuring AI to Improve Clinical Usefulness Across Contexts", "authors": "Hubert D Zaj\u0105c; Jorge M N Ribeiro; Silvia Ingala; Ruth Wanjohi; Jonathan F Carlsen; Tariq O Andersen; Simona Gentile", "pub_date": "2024-05-27", "abstract": "Figure 1: Design interventions in different radiology settings in Kenya and Denmark. Three versions of an AI-based prototype were used to explore configuration opportunities to achieve clinical usefulness across clinical sites (from left: version I, II, II).", "sections": [{"heading": "INTRODUCTION", "text": "Artificial Intelligence (AI) models repeatedly match or outright outperform radiologists in narrowly defined detection tasks [4,53,60,63]. There are multiple studies claiming that AI-based systems enhance radiologists' work, either by increasing accuracy or reducing time spent on each examination [48]. These claims, however, are based on retrospective evaluations conducted in laboratory settings. When looking closer into the state of the art of clinician-facing AI, the claims of utility weaken [93]. For example, Roberts et al. [61] found that out of the 62 AI models detecting and predicting COVID-19 on chest X-rays and CT scans that were described in the literature, none were deemed to be useful for clinical purposes. Furthermore, evaluations of the handful of systems approved by the authorities in the United States and European Union [1] revealed that their clinical impact when integrated into practice remains mostly unclear [79,84]. A similar study by Lehman et al. [47] showed no improvement in patient outcomes after the successful integration of an AI-based support tool for mammography screenings. Strohm et al. claimed that one of the primary causes of AI's lack of success in radiology until now is due to \"uncertain added value for clinical practice of AI applications\" [73]. What these studies show is that the clinical usefulness of hitherto AI-based support systems is limited.\nResearchers with diverse backgrounds (AI, Health, and Human-Computer Interaction (HCI)) investigated what makes AI-based support systems clinically useful. Based on the previous work, we define clinical usefulness as the overarching quality of AI-based support systems emerging from the interplay of their real-world performance, clinical efficacy, local applicability, and end-user acceptance in a situated clinical context for concrete end-users. First, robust performance in real-world settings is essential, as subpar performance has been found to increase workload and disrupt clinical routines [80,82,93]. Second, the evaluations, primarily assessing technical performance metrics through randomised clinical trials (RCTs), must encompass tangible clinical outcomes and patient benefits. Health researchers have been advocating for more flexible assessment methodologies aligned with the iterative nature of AI deployment [11,42,49]. Third, end-user acceptance, supported by qualities like trust and usability, emerges as pivotal for successful use in clinical practice [18,19,39]. Altogether, for an AI-based system to be clinically useful, it must perform well, benefit patients, and be accepted by clinical end-users working in different clinical contexts.\nIn this paper, we investigate how to design AI for clinical usefulness in different clinical contexts. This study was conducted as a part of a larger research and development project focused on innovating an AI-based system to assist examinations of chest X-rays in Denmark and Kenya. Here, we define innovation as the entirety of work conducted to create an AI-based system, from creating the datasets the AI is trained on through design and development to its integration and use in practice. We conducted 19 design sessions and design interventions (online and collocated) with 13 radiologists from 7 clinical sites in Denmark and Kenya. Throughout the design study, we explored a range of user interface mock-ups and three versions of a web-based prototype of an AI-based support system with prioritisation and decision-support functionalities.\nWe conceptualised four technical dimensions of radiological AI support that need to be configured to maximise its clinical usefulness. The technical dimensions uncovered through the design interventions span AI functionality, AI medical focus, AI decision threshold, and AI Explainability. These decisions constitute the critical aspects of radiological AI-based support systems and must be configured in relation to the local social dimensions of clinical AI.\nMoreover, to support configuration during innovation, we deconstructed social dimensions, conditioning how each of the technical dimensions supports final clinical usefulness. Namely, how medical knowledge, clinic type, user expertise level, patient context, and user situation affect the clinical usefulness of the technical dimensions.\nFinally, we discuss how these dependencies should be accounted for throughout the innovation processes to successfully configure future systems before-use and enable meaningful configuration in-use. Based on the design interventions, we offer four concrete design recommendations addressing the configuration needs of each of the conceptualised technical dimensions of clinical AI.", "publication_ref": ["b3", "b52", "b59", "b62", "b47", "b92", "b60", "b0", "b78", "b83", "b46", "b72", "b79", "b81", "b92", "b10", "b41", "b48", "b17", "b18", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK 2.1 Clinical Usefulness of AI Systems in Healthcare", "text": "The hitherto evidence of AI's positive influence on clinical practice is limited [51,75,80]. Research on the real-world effect of AI in healthcare tends to be discrete and focusing on confined goals [93]. However, to provide clinical value AI-based systems have to dovetail contributions from Human-Computer Interaction, AI, and Health into a cohesive vision [30,82,93]. First, clinical usefulness necessitates robust performance [93]. This primarily has to be true in real-world settings, retrospective evaluations in lab environments do not speak to the final performance of a system. For example, in a real-world evaluation of an acclaimed ML model for detecting diabetic retinopathy, 21% of all cases were deemed ungradable [8]. Poor performance also leads to increased workload [57,64,82], additional time spent on discerning false positive predictions [67,68], or breakages to work routines [31]. Van Leeuwen et al. [80] reported that out of 100 CE-approved radiological AI-based systems, 64 showed no peerreviewed evidence of clinical efficacy. Most evidence for the remaining 36 systems focused on diagnostic accuracy, not real-world clinical outcomes.\nSecond, clinical usefulness necessitates clinical efficacy [42]. However, randomised clinical trial (RCT) -a focused, systematic, rigorous, and insulated method commonly used to evaluate the validity of clinical interventions independent of external confounders -is often following the traditional sequential paradigm of work characteristic for drug development [13]. In this tradition, the intervention is evaluated only when deemed complete [21]. When translating this mentality to AI-based systems, not only does it hinder innovation, but it also results in the evaluation of AI through the measure of technical performance [49]. While technical performance is the backbone of useful AI, clinical efficacy is not its immediate consequence [11,69]. For example, Lehman et al. [47] conducted a prospective evaluation of a computer-aided detection system supporting mammography reporting. Researchers concluded that the use of AI had no \"established benefit to women.\" Instead, healthcare researchers are opening up towards more flexible evaluation approaches that align with the iterative and situated nature of AI innovation and \"go beyond measures of technical accuracy to include quality of care and patient outcomes\" [23,42]. Achieving high performance but in metrics that are clinically relevant is the next step towards clinically useful AI-based systems.\nThird, clinical usefulness necessitates clinical organisational acceptance. HCI community's claim to fame is understanding that regardless of a system's performance, it will not have any impact if no one wants to use it. Thus, many facets of making clinical AI an appealing solution were explored. Trust has been hallmarked as a critical quality of clinical AI. HCI researchers investigated its origin [5,18] and dependencies [59], as well as issued recommendations for design [39]. Explainable AI (XAI) has been the most promising answer to enhance trust, support oversight, and increase the perceived usefulness of clinical AI [19,34,46,86]. AI as a new source of information and agency prompted the exploration of new ways of reasoning and human-AI collaboration [10,16,20,25]. Researchers also investigated AI's position in a clinical decision-making process [41] and the rationale behind integration opportunities into clinical practice [39,66,68,90]. They argued that the workflows, current work practices, and the broader sociotechnical context should also be taken into account when implementing clinical AI-based systems [22,39,56,62,76,93]. Addressing these concerns is crucial for AI to have a chance at benefiting patients and being accepted by healthcare professionals.\nAltogether, for an AI-based system to be clinically useful it must perform well, benefit patients, and be accepted by clinical end-users. However, oftentimes the innovation of clinical AI is conducted in silos and the work is not guided by the ultimate goal of clinical usefulness [13,93]. We need to investigate how AI-based systems can be configured to support these three goals and ultimately result in clinically useful AI.", "publication_ref": ["b50", "b74", "b79", "b92", "b29", "b81", "b92", "b92", "b7", "b56", "b63", "b81", "b66", "b67", "b30", "b79", "b41", "b12", "b20", "b48", "b10", "b68", "b46", "b22", "b41", "b4", "b17", "b58", "b38", "b18", "b33", "b45", "b85", "b9", "b15", "b19", "b24", "b40", "b38", "b65", "b67", "b89", "b21", "b38", "b55", "b61", "b75", "b92", "b12", "b92"], "figure_ref": [], "table_ref": []}, {"heading": "System Configurability", "text": "Configurability has been long considered crucial to the appropriation of IT systems [26,27,45]. There are two types of configurability that should be explored in the context of this study: before-use and in-use [36].\nBefore-use configurability typically involves the active participation of end-users in the design processes, aiming to tailor systems to their specific needs and preferences [36]. Various methods and approaches have emerged to facilitate meaningful engagement with end-users, such as participatory design techniques [45]. Acquiring an understanding of work practices and work environment, but also technology aspects of a future system and changes it may introduce, is critical for developing systems that effectively respond to user needs [43]. This understanding enables developers and designers to implement systems that are not only technically sound but also contextually appropriate.\nHowever, according to Stewart and Williams [72], the paradigm of user-centred design does not properly answer the challenges of implementing useful systems. Rather, the final usefulness of a system is created iteratively through the acts of in-use configuration.\nThis stance echoes Suchman who recognised the need for design activities to continue after a system's deployment [74].\nThe in-use configuration may cover functionalities, user interface, or other settings that let the end-users adjust the system to their preference and work environment [85]. However, the system is not the only configurable arena. The environment also undergoes a process of configuration to the new system. The in-use configuration processes encompass changes to the \"technical environment, organisational relations, space technology relations, as well as people's connections to other people, to other places, and work materials\" [6]. Dourish highlights how the appropriation of IT systems in practice is an act of both adapting the technology and adapting the practices to fit into the new reality [27].\nAs usual with AI, the matter of configuration is burdened by the immutability of certain aspects of the system in-use and the dependency of early design decisions on the use context [93]. HCI researchers investigating the design of AI-based systems learned that it is impossible to envision all aspects of clinical AI-based systems before deployment. As a result, the final capabilities of such systems only take shape after they have been deployed. [33,88]. On the opposite end of AI innovation, i.e., prior to data labelling, Zaj\u0105c & Avlona [93] established that very concrete choices and assumptions about the final context of AI use form the data used for AI training and, by extension, shape the space of capabilities of future AI-based systems. This vicious cycle of dependencies prompted researchers into new ways of thinking about AI innovation. Edwards et al. [28] proposed the concept of \"growing\" to foreground the need for almost organic adoption and adaptation of new IT systems in an existing environment. Elish and Watkins presented a similar argument [29] who emphasise that early realisation of clinical AI and acknowledgement and support of the necessary \"repair work\" are crucial to counter the risk of a system remaining \"a potential solution\", i.e., a solution that is not viable when actually implemented.\nWe see the problem of configuration of clinical AI, as a problem of obtaining reliable information related to design decisions made during the innovation process. The emergence and propagation of dependencies (or \"sociotechnical interdependencies\", see [93]) at the point of deployment hamper the ability to configure clinical AI-based systems in-use. At this point, the assumptions about the context of use are already ingrained in the AI model. We want to support the configuration of radiological AI-based systems for clinical usefulness by uncovering the dependencies anchored in clinical contexts and linking them with specific design decisions. This extended understanding of contextual factors will allow developers and designers to implement radiological AI support configurable and useful across clinical contexts.", "publication_ref": ["b25", "b26", "b44", "b35", "b35", "b44", "b42", "b71", "b73", "b84", "b5", "b26", "b92", "b32", "b87", "b92", "b27", "b28", "b92"], "figure_ref": [], "table_ref": []}, {"heading": "METHODOLOGY", "text": "In this paper, we explored how to design radiological AI-based systems for clinical usefulness across contexts. This study was part of a larger project set to design and develop an AI-based support tool for radiologists examining chest X-rays, funded by the Innovation Fund Denmark (0176-00013B). The project is a multidisciplinary collaboration between the Department of Computer Science at the \nspecialised hospitals -that provide tertiary and quaternary care, handling the most complex medical procedures in their respective countries. The participants were recruited through email and the professional networks of the project members. Nine senior (consultant) radiologists and four junior (in-training) radiologists joined the study (Table 2). Junior radiologists' reports must be most often approved by a senior colleague before sharing with clinicians. The senior radiologist's assessment is final. Participants were not compensated, and we collected written consent from all participants. According to the authors' institutions' institutional review boards (IRBs), our study was considered non-interventional and thus exempt from a formal ethical review.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Research Through Design: Design Interventions with Working Prototypes", "text": "To explore the clinical usefulness of AI in different radiology contexts, we undertook a research through design approach [96]. We conducted three iterations based on a series of design sessions and design interventions using mock-ups of user interfaces (Prototype I) and working prototypes (II and III) (Fig. 1, Fig. 2, and Fig. 3). The three iterations were determined by decisions to deploy major changes in the web-based prototypes, i.e. version 1-3, followed by gathering feedback from the participants. The design sessions were carried out both online and collocated with radiologists in hospital offices. During these sessions, we obtained medical domain knowledge, typically by clarifying questions about radiology work and X-rays, but we also collectively explored the design space through a range of mock-ups and prototypes. The design interventions were carried in-situ with the performative purpose of exploring how  the proposed solutions would be enacted close to real-world radiology practices. A design intervention, as defined by Halse and Boffi [35], is a method that integrates design and ethnography and \"enables new forms of experience, dialogue, and awareness about the problem to emerge\" (see also [14,15]). It is an experimental form of inquiry that enables a positioning \"in-between what is already there and what is emerging as a possible future\" [3].\nIn our case, this meant that we intervened in the radiologists' everyday work settings with design artefacts as a vehicle for exploring the dependencies of AI usefulness in situated contexts. During observations of the radiologists' work practices, we brought in the prototypes and mock-ups as a way to enact while experimenting with new forms of AI support in radiology. The benefit of this approach was the possibility to engage radiologists in moving between considering the proposed solutions and envisioning alternatives while constrained by the requirements of the local context. This mode of research was important for this study because it provided more grounded and realistic visions of how AI could become clinically useful across hospital contexts.\nIn total, we conducted thirteen design interventions and six design sessions (Table 2) with thirteen radiologists in Denmark and Kenya, lasting between 30 and 120 min (avg. 60 minutes). In between sessions and interventions, we designed a range of user interface mock-ups using Figma, consisting of different AI functionalities and alternatives to interactive features. A total of three versions of a web-based prototype, which included an AI model developed in the greater part of the project. This meant that the participants in this study interacted with real data and real output from the AI model during design interventions. Importantly, the data was completely anonymised, and no other medical information about the patients was available. The mock-ups, prototypes and feedback from the participants became input for multiple design meetings within a group of three of the authors (HDZ, JMNR, TOA) and 3.1.1 Prototypes. As part of the greater project, a deep learningbased model was developed by machine learning engineers at Unumed ApS to detect selected radiological findings [92]. The AI model was developed using a convolutional neural network. The first prototype was merely a proof of concept, not designed to collect feedback from external domain experts. It was developed to guide future work in terms of model development and data labelling. However, inspired by earlier research [92,93], we considered it an opportunity to engage in more concrete discussions on the merit of clinical usefulness with medical professionals at an early stage of the innovation work.\nThe second and third iteration of the prototype consisted of an interactive web application designed to emulate a DICOM viewer. The web application integrated with the AI model developed within the bigger project. This connection enabled us to work with real data and, thus, explore with fidelity the interactions of the radiologists with the system. For the design interventions, radiologists were given access to the prototype, either in-person or remotely. They were requested to choose the next examination to report, following their usual practice and using information displayed in the prototype. Then, they were asked to interpret the selected examination without the use of AI and with AI decision support. Moreover, they were asked to configure the AI tool using available options to fit their practice. Finally, they were encouraged to explore the prototype independently and interact with any element of the user interface.", "publication_ref": ["b95", "b34", "b13", "b14", "b2", "b91", "b91", "b92"], "figure_ref": ["fig_0", "fig_1"], "table_ref": ["tab_2"]}, {"heading": "Analysis Positionality", "text": "The data analysis was conducted by the first and last authors (HDZ and TOA) with backgrounds in Health Informatics, HCI, and AI (5+ & 15+ years of experience). Moreover, before the analysis of the data from the design sessions and design interventions, the two co-authors concluded extensive ethnographic investigations into the work practices of radiologists from the visited sites with a particular outlook on opportunities for AI support (described in a manuscript prepared for publication). First-hand experience with the work practices and similarities and differences across clinical settings informed the initial analysis of this data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Analysis", "text": "We used reflective thematic analysis [17] to analyse collected data (transcriptions of the design interventions). The analysis took place in Dovetail -a web application for qualitative data analysis. Except for the transcription software, no AI-based analysis support was used in this study. The two authors familiarised themselves with the collected data after every iteration of the design sessions and design interventions when deciding on the next focus. Moreover, the two authors, prior to coding, based on their fieldwork experience (60+ hours) and a literature review [93], devised three bucket themes to support the later organisation of codes: type of clinical site, domain expertise of medical professionals, and patient and situational context. Additionally, a fourth residual category was added not to limit coding. Next, to test the bucket themes, the two authors coded one transcript each for any references to challenges, preferences, dependencies, and configurations in relation to AI and their clinical practice. After this test, the fourth bucket theme was renamed to technical dependencies. The first author coded the remaining transcripts following the same directions. The two authors met weekly to discuss the coverage of the coding and future conceptualisation of themes. The themes were created within their respective bucket themes based on their grounding in the clinical context. Importantly, the division of codes between the bucket themes was never final and was used only to support analysis of the significant amount of codes (n=260). Through discussion, reflection on data across the interventions, and fieldwork experience, the authors iteratively clarified themes and reorganised data, moving away from the original bucket themes (while maintaining their initial assignment known). This interpretative work was conducted twice, creating ten reflective themes. The ten themes were framed as dependencies conditioning four specific design decisions that formed an AI-based support design space.", "publication_ref": ["b16", "b92"], "figure_ref": [], "table_ref": []}, {"heading": "CONFIGURING FOUR TECHNICAL DIMENSIONS OF CLINICALLY USEFUL RADIOLOGICAL AI", "text": "We identified ten dependencies that emerge from the social dimensions of clinical AI and condition the configuration of four technical dimensions of clinical AI for radiology (see Fig. 4). Each of the technical dimensions needs to be configured in relation to the local clinical context to achieve clinical usefulness. In this section, we will briefly explain the social dimensions of clinical AI to then explore in-depth the conceptualised dependencies.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Social Dimensions of Clinical AI", "text": "Medical knowledge. This dimension includes concepts and definitions relevant to the medical domain addressed by the innovated AI-based system, for example, the meaning of radiological findings detected by our AI-based system. Familiarity with them supports meaningful collaboration between designers, developers, and medical professionals and reduces the risk of incorrect assumptions throughout the innovation process.\nClinic type. This social dimension addresses types of clinical sites. Imaging clinics, general hospitals, and specialised hospitals provide unique healthcare services and, thus, cater to the needs of patients with different conditions. Moreover, the type of clinical site determines the available resources, the speciality of medical professionals working there, their workflows, and their goals.\nUser expertise level. All medical professionals have different domain expertise. This is evident when comparing junior to senior medical professionals. However, it was also observed between board-certified radiologists. The level of expertise also determines the workload and clinical responsibilities.\nPatient context. This context encompasses the current location of a patient (in or out of a hospital) and their medical history. Patients are the centre of medical work. Their health and well-being are the priority. Thus, by extension, any system supporting healthcare professionals should support patients and depend on their context.\nUser situation. This dimension pertains to the workload, available time, and resources of medical professionals. While the other four dependencies describe relatively stable medical practice, situational context introduces a temporal factor to the work done and may affect the priorities of medical professionals.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "AI Functionality", "text": "Which AI functionality should the system provide? Answering this question defines this technical dimension. The functionalities explored during design interventions (prioritisation and decision support, see, Fig. 5) were linked to the AI model developed for the project this study was a part of. We explored the conditions for these functionalities to provide clinical value and propose a third functionality: quality assurance, which originated during the design interventions.\nDependency 1: AI functionality depends on clinic type. Each clinical site has different (1) positions within the healthcare system, (2) amounts of resources available, and (3) workloads related to the size of a clinic. This is why it is important to ensure that AI functionality is implemented in a way that makes sense for the clinical site in which it will be deployed.\nFirst, while every radiologist puts the well-being of their patients first, the healthcare systems that they are a part of operate under different incentives. Public and private clinics face different challenges and may require adjusted AI functionalities, for example, The number of cases in court, medical and legal cases, is way more than what you would get in the public sector. So from the medical director's office [point of view], they would want .... any small thing to be flagged so that we don't get into problems later... it would be different in K5, compared to the public sector, where even if you missed this, people are rarely taken to court but in a private setting... if they  [K5 administration] were to purchase this software, they would insist that it's set... to catch it all but you know of course this would irritate some radiologists [S18, Senior, Big general hospital, Kenya]. The difference in the prevalence of legal litigation against medical professionals in public and private healthcare centres highlighted by P05 may run along different axes in other countries. However, it is imperative for the creators of AI-based support systems to envision alternative motivations for the use of their systems and allow appropriate configuration.\nSecond, different clinical sites have different financial resources available. This factor, which has rarely been considered during the design of clinical AI-based systems, is a very real limitation for which functionalities will be considered worth the investment. What is the harm in having a second opinion for each and every case? ... What is the cost? Is it a cost implication that we have to choose which images to prioritise or what? [S18, Senior, Big general hospital, Kenya]. The different business models implemented may be detrimental to the usefulness of a system in practice. Providing decision support on all the examinations and detecting all the radiological findings may be too costly for clinics that could use such support the most, e.g., rural hospitals suffering from the lack of qualified radiologists.\nThird, the clinical usefulness of AI functionalities may vary depending on the size of a clinical site, as recounted by a senior radiologist from a busy specialised hospital, For me, the most relevant aspect of it is triage [prioritisation], but if I have five X-rays to report, then I'm not too worried because I'll get to the 5 \ud835\udc61\u210e X-ray in 20 minutes. But if I have 100 X-rays to go through, I don't want to get to the 100 \ud835\udc61\u210e X-ray and see that it was the one with critical findings. So in a setup where you're not very busy, I don't think it would be very useful [S19, Senior, Specialised hospital, Kenya]. Conversely, in smaller clinics that serve mostly outpatients, implementing AI that provides quality assurance functionality would provide more value to both the radiologists and the patients. For example, If I look at it [an examination] and [my colleague] looks at it, no one looks at it until the patient comes back four weeks later, two years later... and then \"Oh, look! That's the damn thing. \" [e.g., a missed tumour] It could be very nice to have this second opinion [I15, Senior, Imaging clinic, Denmark]. In this imaging clinic, radiologists, rather than being afraid of not reaching a critical patient in time, are worried about missing a critical but subtle finding, e.g., a small nodule, which may signify cancer. This means that the same AI functionality may provide useful support depending on the size of a clinic.\nDependency 2: AI functionality depends on user expertise level. The value of support in detecting findings on a medical examination decreases with increasing experience. Instead, the assigned workload increases with seniority. Thus, prioritisation and quality assurance functionalities gain importance.\nRadiological AI-based decision support typically presents a list of findings detected on an examination accompanied by an XAI visualisation, as also explored in our prototypes. While this mode of support seems straightforward, it misses the reality of clinical practice. Senior radiologists spend a very short time interpreting chest X-rays. To ask them to revisit every examination to discern the validity of AI predictions is wishful. However, when discussing the potential value of AI-based decision support, they focused on quality assurance. Thus, AI should be treated not as an all-knowing peer who is going to point out every finding on an examination but as a safety net that activates only in time of need. For example, It could read the text we write and say: \"Oh, you missed that.\" That could be good [I11, Senior, Imaging clinic, Denmark]. This way, the envisioned system would not require the mental effort and time to discern AI output but would inform a radiologist about potentially missed findings based on the report they were writing.\nOn the other hand, junior radiologists in clinical settings usually take significantly more time to report every examination. Moreover, all of their reports have to be confirmed by a senior colleague. For them, reporting serves as a primary learning exercise. In this context, they envisioned using AI support not as a quality assurance but as a new source of information used to draw their own conclusions. I would take a look at a chest X-ray, formulate my opinion, and then see what the AI says... If it agrees... good, if it disagrees or finds something that I hadn't, I'll examine it critically... I like getting almost overwhelmed by data, and I sort it out afterwards... [S14, Junior, Specialised hospital, Denmark]. These two perspectives highlight how workflow, workload, and the act of detecting findings on a medical examination changes with expertise. The educational value created for junior radiologists by verbose explanations of AI's predictions may become a burden for senior radiologists who expect minimal disruption to their existing workflows. #1 Recommendation: Enable users to select preferred AI functionality.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "AI Medical Focus", "text": "Which radiological findings should the AI detect? This is where our participants, for the first time, responded, starting with \"It depends...\" (see Fig. 6). Let's explore how to ensure the detected findings are clinically useful in the real world. Dependency 3: AI medical focus depends on clinic type. Different clinics take care of different types of patients suffering from different conditions. Types of patients seen in different clinical settings result in a local prevalence of observed radiological findings. As a result, a single fit-them-all system that detects an arbitrarily selected set of findings is not going to provide a similar quality of support across the different clinical contexts. Imaging clinics and general hospitals usually examine patients referred by general practitioners. Of such patients the majority of the examinations are deemed \"normal\" or with findings related to infections. Hospitals with emergency departments may observe an increased prevalence of trauma-related findings, whereas specialised hospitals of post-operative, oncological, and chronic nature, as exemplified in these quotes: That depends on the setting. If you're in a private clinic, most of the X-rays are normal... [I11] If there's something wrong, that could be pneumonia or a tumour, but usually, it's pneumonia when you go out to a private clinic [I15, Senior, Specialised Hospital / Imaging Clinic, Denmark]. However, detecting pneumonia would not bring value in other types of settings as highlighted by P05: If you're working in a trauma centre, the number of critical findings [e.g. pneumothorax or hemothorax] would definitely be more than in [K5], where most of the time it's just coughs and fever [S18, Senior, Big General Hospital, Kenya]. We argue that to deliver a clinically useful AI-based system for radiologists, it is imperative to understand the local population served by the clinical site where the system is implemented. Otherwise, the developers may risk deploying a system detecting findings that may be objectively relevant to patient management yet not prevalent at the deployment site.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Dependency 4: AI medical focus depends on user expertise level.", "text": "Junior radiologists may interpret a single X-ray for up to tens of minutes. Whereas, according to our senior participants, interpreting a chest X-ray takes around 1 to 2 minutes. This means that with experience, many findings become \"obvious\" and are no feat to detect. When discussing the decision support functionality of our prototypes and previous systems that our participants had piloted, the common complaint related to the detection of \"obvious\" radiological findings, which took additional time to discern.\nIf it's an obvious finding, we'll see that one quickly, and we all agree on it. The problem comes when it's something more subtle [I06, Senior, Big General Hospital, Kenya]. Detecting the difficult or \"subtle\" radiological findings is where the value lies for senior radiologists. However, the less experienced, the more support a radiologist may accept. This was captured by P01: Maybe it'll help the resident radiologist in the first or second year, but I don't think it will help a specialised radiologist with experience because once we can have a look, we can't miss something like this [I01, Senior, Small General Hospital, Kenya]. This means that in order to support different radiologists in practice, AI-based systems may need to allow users to select findings to receive support with. Without such configuration, discerning AI predictions regarding \"obvious\" findings, even when true, would result in more time spent and annoyance.\nDependency 5: AI medical focus depends on patient context. Radiologists are not interpreting medical imaging to find every possible finding. Rather, they are interpreting them to help the ordering clinicians take action in patient management. Such actions usually occur when a new condition is being diagnosed, or a patient's health may be at risk. However, the clinical meaning of certain radiological findings depends on the location of a patient. This means that a finding may be expected when observed in an examination of a patient who is admitted to a hospital. Whereas the same finding observed in an examination of a patient who is not admitted to a hospital may warrant immediate action.\nOur participants stressed that useful prioritisation should consider patients' medical history to filter out already-known findings, which our prototype could not do. But how urgent is it? We know that pneumothorax has decreased. It's a big heart, but it's much smaller than it was a week ago. It has [pleural] effusion, but much, much less than it was a week ago. That's the thing we miss with this [I11, Senior, Specialised Hospital / Imaging Clinic, Denmark]. In this quote, P11 explains that the examination they looked at may not be urgent at all despite the fact that the AI correctly detected three findings, one of them (pneumothorax) being life-threatening. These findings would not be urgent if they were already known to the ordering clinician. In such a case, the patient would have already been undergoing treatment, and this examination's sole purpose was to control its progress. In specialised hospitals and bigger general hospitals, patients often have taken several X-rays to monitor the progress of treatment. This means that the same findings, but of different severity, will be visible on their examinations. The ability to assess the detected findings in the light of patient history is crucial to correctly prioritise findings that warrant clinical action.\nWhen looking at radiologists' work from the perspective of contributing to the broader clinical work, it is counterproductive to prioritise findings that clinicians taking care of a patient are already aware of. In other words, a radiological finding may be relevant to detect on examinations from patients who are not admitted to a hospital, but not so much for patients currently admitted. A senior radiologist explained, It depends on the findings, and it depends on the patient... some findings in the out-patients would be more important to be prioritised than if they're in-house. Because if they're in-house, then I would suspect that someone not from the radiology department would have looked at them. If it's out-patient, then nobody has looked at them... [I10, Senior, Specialised Hospital, Denmark]. Whereas, as explained by P10, patients referred from outside of a hospital are more likely to have conditions that their doctors are unaware of. Thus, the location of the patient is crucial to selecting which findings are relevant to receiving support from an AI-based system. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "AI Decision Threshold", "text": "At what certainty level should the AI inform a user about detected findings? Specifying when a radiological AI-based system should inform a user about a finding is usually done by specifying a decision threshold (see Fig. 7). Selecting a specific threshold value determines the measured performance of an AI model captured by evaluation metrics like specificity, sensitivity, or positive and negative predictive values. Arguably, in practice, a decontextualised performance value is less important than the practical consequences of selecting a specific threshold level. Every time an AI model detects a finding (based on a selected threshold), a radiologist may have to take action to assess it. The balance between clinical value and additional burden is thus closely tied to how well the threshold is configured to match the local clinical context. We conceptualised four dependencies that influence the configuration of the AI decision threshold.\nDependency 6: AI decision threshold depends on medical knowledge. While some of the radiological findings are well understood across the contexts, some definitions are more subjective and their meanings change across countries. Infiltration or consolidation are two examples of radiological findings which have been found to be used differently in clinical practice in Denmark and Kenya. Moreover, some of the findings were too vague for the radiologists to decide how to assess them, for example, I think vascular changes can mean one of two things if it's the big vessels -I think it's important to have it, e.g., if the computer can say the aorta is big... It could also be about... the small vessels, and then it's more like stasis. Then it's quite different [I17, Junior, Specialised hospital, Denmark]. The underlying definition of a finding, in this described case, affected how P13 understood the condition and what threshold level they deemed appropriate. Within radiology, chest X-rays are a particularly subjective modality. Due to their visual complexity, radiologists rely on their expertise to interpret the observed findings. Precise definitions used to label data the AI model was trained on are crucial to assess the predictions. Dependency 7: AI decision threshold depends on user expertise level. Often, junior and senior radiologists are juxtaposed as two groups of AI support end-users with different needs. This is also visible in the strategy for selecting thresholds.\nWhen used by junior radiologists, both junior and senior radiologists (who supervised them) leaned towards accepting AI predictions only with a high degree of certainty. As explained before, the interpretation of chest X-rays is uniquely subjective. It takes experience to report them with a high degree of certainty. In this context, an uncertain AI prediction would jeopardise the learning process and introduce more confusion, resulting in more work for the junior students and their supervisors.\nOn the other hand, allowing senior radiologists to set the threshold for different findings according to their personal preferences could entice them to utilise the system in their own way. P05, who also had senior administrative experience, explained that senior radiologists do not always have the same level of expertise and may need different levels of support. This would be amazing. I wouldn't want to do it [adjust threshold] at the administrative level because ... not all the radiologists in the department have the same capabilities. So I'd rather let people set it for themselves [S18, Senior, Big general hospital, Kenya]. By enabling users to select the AI decision threshold on their own, they could build trust by incrementally including AI in their own practice. Dependency 8: AI decision threshold depends on patient context. Diverging from a fixed threshold level defined at a system level towards finding-level threshold specification may boost the clinical usefulness of AI-based systems for radiology. A finding-level threshold specification would allow radiologists to stratify which findings in a given context are more relevant.\nThey could do it by lowering the threshold. A lower threshold would be associated with a higher rate of false positive prediction for that particular radiological finding. Thus, more work for radiologists. However, for a subset of findings, our participants were willing to accept more false positive detections if it would benefit their patients. For pneumothorax, I would probably lower the threshold because you would want to find every pneumothorax there is. But for some other stuff, like fibrosis, I would probably have a higher threshold because that's not critical [S13, Senior, Specialised hospital, Denmark]. Based on design interventions with the third prototype, our participants saw a utility in such fine-grained configuration. I think the relevance of certainty [threshold] is the clinical implication of the diagnosis. So, something like a pneumothorax needs some form of intervention... whereas on a suspected infection, a clinician may go ahead and treat it even if the X-ray is normal. So that's why it may not be such a big deal whether I call a pneumonia or not. Whereas a pneumothorax might need a chest tube insertion. It's a do-or-die call [S19, Senior, Specialised hospital, Kenya]. As shown in this quote, the clinical implications for a patient made radiologists more accepting of false positives. Meanwhile, findings that were less severe or that could be discerned using other indicators, e.g., clinical indicators (cough, fever) to decide on pneumonia diagnosis, were less preferred to lower the threshold. This suggests that configuring AI decision threshold on a finding level could reduce the workload associated with false positive predictions and help focus AI support. Dependency 9: AI decision threshold depends on user situation. Radiologists' approach to AI support changes with time. In this paper, we uncovered two temporal aspects that affected how radiologists thought of configuring AI decision thresholds: the time spent using the system and the rhythm of clinical work.\nOne of the common comments when discussing the threshold with our participants was about its arbitrary nature. Radiologists wondered what the real-life consequences of changing the threshold would be. Based on these concerns, our final prototype included an estimate of false positive predictions. These values, while more relatable, were still considered difficult to imagine in real practice both for senior and junior radiologists. I mean, it's a bit arbitrary at this moment because you don't have any idea what the effect is [I17, Junior, Specialised hospital, Denmark]. It would be nice to be able to adjust this... try all this out and see in real life how many cases it's missing or over-calling [S19, Senior, Specialised hospital, Kenya]. These quotes highlight that such essential development tasks as selecting a threshold have little to no basis in clinical practice. They uncover a need for a better translation between the domains of AI and Health to support meaningful configuration. Currently, this translation has to be conducted through real-world experimentation in the final context of use. This way, medical professionals may gain a practical understanding of what the changes to the threshold mean and further purposefully and consciously adjust it to fit their work.\nThe second temporal aspect of selecting an appropriate threshold relates to the routine of end users. Radiologists saw an advantage in adjusting the threshold depending on their workload. For example, a specialised radiologist from a busy specialised hospital mentioned, on Fridays, we tend to be more active because if you leave a long list on Friday, the turnaround time will be way longer -there is very low coverage over the weekend [few on-call doctors]... and then Monday tends to be very busy [I04, Senior, Specialised hospital, Kenya]. During this conversation, the radiologist concluded that lowering the threshold could help them ensure that no examinations with critical findings were left to be reported after the weekend. These two aspects highlight that what radiologists consider a useful level of detection (including false positive predictions) may vary throughout the use. #3 Recommendation: Enable users to adjust AI thresholds.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "AI Explainability", "text": "How should the AI explain its decisions? Understanding AI predictions supports building trust towards AI-based systems. In this study, we explored three visual ways of explaining AI predictions: heat maps, bounding boxes, and arrows (see Fig. 8). We discovered that no single method can support the explainability of all the radiological findings. The visual appearance of radiological findings dictates the best way to highlight them for radiologists for inspection. Radiologists discern between different radiological findings based on their visual appearance. Their presentation ranges from barely visible nodules to diffused opacities (areas of less transparency) present across both lungs. The breadth of visual impressions suggests the need for flexibility, I think that both ways of displaying the findings are fine, but for different pathologies. I mean, the heat map makes sense in this case for pneumothorax because it's a very extensive finding. And for the fracture, it makes sense to see it with a box, whereas the heat map doesn't make that much sense. It becomes too blurry... [S13, Senior, Specialised hospital, Denmark]. Radiologists preferred bounding boxes for more contained findings, whereas the more diffused, the more inclined they were towards the heat map. An important factor when designing XAI for chest X-rays is allowing for inspection of the underlying examination. The main purpose of XAI is to direct radiologists' attention to the detected findings. To assess the validity of a prediction, radiologists have to inspect the examination itself without additional overlays. #4 Recommendation: Enable users to choose the most suitable XAI method.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "DISCUSSION", "text": "In this paper, we investigated how to design AI for clinical usefulness in different clinical contexts of radiology practice. Based on extended research through design study, we provided four practical recommendations on addressing ten dependencies emerging from the social dimensions of clinical practice (Fig. 9). By engaging radiologists in two different countries from the Global North and Global South, respectively, we found that the radiologists' practices in Kenya and Denmark were rather similar, possibly due to resemblances in medical education, scientific models, and ethics.\nThe social dimensions derived from this study therefore orient towards similarities that cut across country specifics. However, the types of healthcare systems and present IT infrastructures differed quite substantially and need to be accounted for during AI innovation, which goes beyond this study. In this section, we will discuss how these recommendations may be enacted during the innovation process of clinical AI for different clinical contexts.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Enable users to select preferred AI functionality", "text": "Configuring clinical AI-based support systems to suit local environments is essential, as one-size-fits-all approaches often fail to address their unique needs [34,39,58,89]. In this study, we discovered how social dimensions of clinical practice condition what kind of AI functionality is considered useful. We argue that for AI to match local requirements, it needs to be configured throughout the innovation process with the intended context of use in mind, i.e., the expertise of the end-users and the work performed in their clinical site. This is especially relevant, as clinical AI is often afflicted by the problem of late realisation [33,88]. When addressing expertise-related needs for support, previous research in radiology showed that AI-based systems have different effects on junior and senior radiologists [95]. Even more, Tong et al. [78] investigated two strategies, what they called \"optimised\" and \"all-AI\", for AI support of junior and senior radiologists in thyroid nodule management. They reported that the best results were obtained when the type of support was configured to the expertise of the radiologist. However, our study showed that personal preferences play a deciding factor only if the AI functionality is appropriate in the context of the local clinic. Selecting the best way to prioritise findings will not make sense if there are only a few examinations to prioritise to begin with. AI-based systems should be designed to respond to fit the utility gap in a clinic and then be configured to the varying needs and preferences of different end-users, depending on their level of experience, knowledge, and confidence. The personal configuration of functionality also captures the integration -a famously difficult task when innovating clinical AI [58,77,90]. Many AI-based systems fail in practice due to providing support at the wrong time [9,19,32,37]. Some of the AI integrations introduce a new step in the practice. A step that sometimes cannot be skipped [9]. This study shows, seconding previous research, that the integration of AI into work practices has to be flexible [16]. Clinical work is always changing, and so are the needs for AI support. Thus, we recommend that clinical end-users should be in control of which AI functionalities are a part of their current routine.", "publication_ref": ["b33", "b38", "b57", "b88", "b32", "b87", "b94", "b77", "b57", "b76", "b89", "b8", "b18", "b31", "b36", "b8", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Enable users to select radiological findings", "text": "The innovation of AI-based systems is often initiated and defined by technical opportunities, e.g., access to medical data [70,71,87,94]. As such, the medical and social aspects of the systems are sometimes addressed only after the technology has gone through several rounds of development [2]. This inadvertently means that certain assumptions about the medical focus are made [92]. Present radiological AI models tend to detect findings relevant to the local radiologists involved in the data creation process [38,54,83]. However, we showed that the prevalence and clinical meaning of radiological findings varies based on the clinic type and patient context. This affects the usefulness of clinical systems in different settings and their transferability [55,92]. Thus, it is critical to investigate the intended clinical context of use prior to deciding on the medical focus of the AI-based system and to allow medical professionals to set the scope of support relevant to them and their practice.\nMoreover, the clinical meaning of radiological findings is tied to the patient context and not only the type of medical condition, i.e., a radiological finding expected in an in-patient examination can be life-threatening when found in an out-patient one. This discovery deepens our understanding of how medical professionals make decisions and in what situations they may need AI support. This finding contrast with systems where certain radiological findings are consistently considered urgent in patient care [86]. We suggest that linking clinical information about a patient with detected findings may better reflect radiologists' actual decision-making practices and result in improved usefulness of the AI-based system. This is why we recommend including clinical information in conjunction with AI predictions to better respond to the real-world needs of medical professionals.", "publication_ref": ["b69", "b70", "b86", "b93", "b1", "b91", "b37", "b53", "b82", "b54", "b91", "b85"], "figure_ref": [], "table_ref": []}, {"heading": "Enable users to adjust AI thresholds", "text": "Selecting AI decision threshold has significant ethical [12], performance [65], and clinical [81] consequences for AI-based systems, and it has been a notable research topic in the AI and Health communities. Recently, it gained footing in the HCI design community. Kocielnik et al. [44] explored how the decision threshold affects the number of false positive and false negative predictions, significantly altering users system perception. While from the technical point of view, the accuracy may be the same, the distribution of false positives and false negatives may have severe clinical consequences. Our participants warned that false positive predictions require additional time and resources to discern and that the potential benefits of AI often do not justify this additional cost, resulting in the failure of the AI-based systems in clinical practice [5,7,50,76].\nHowever, until AI reaches 100% accuracy, false positive predictions are the reality of AI-based systems. Improving performance is only one way of addressing them. In this paper, we offer another outlook, namely, addressing the cost-benefit ratio of AI predictions. This ratio is not static. Just like clinical practice, it fluctuates and depends on time, workload, known critical cases, and available resources. In certain situations, medical professionals may accept more false positive predictions, e.g. when making sure that there are no critical findings in a queue of examinations that will not be looked at over the weekend. This means that regardless of how well an AI decision threshold is preset, AI will not provide the same value throughout its use in clinical practice. Supporting end-users in configuring the AI decision threshold depending on their local needs can improve the clinical usefulness of AI-based systems. Thus, designers and developers should enable end-user configuration of decision thresholds in clinician-facing AI systems.", "publication_ref": ["b11", "b64", "b80", "b43", "b4", "b6", "b49", "b75"], "figure_ref": [], "table_ref": []}, {"heading": "Enable users to choose the most suitable XAI method", "text": "It has been long established that explainable AI fosters trust and increases the usefulness of the predictions [24,39]. Especially in the healthcare domain, the reasoning and explanations are sometimes more valuable to end users than the predictions themselves [19,50] or can lead to envisioning new ways of using an AI-based system altogether [40]. However, simply revealing the decision-making process of machines to humans is not enough to provide useful explanations [52]. Instead, our study suggests that for XAI methods to be effective in explaining medical conditions, they must be configured to how medical professionals assess those conditions. This means that even proven methods used in medical imaging, like heat maps or bonding boxes, when used to highlight incompatible conditions, may cause confusion and require additional work to discern. To this end, we recommend that to ensure the clinical usefulness of XAI methods, they should be configurable in accordance with medical knowledge.", "publication_ref": ["b23", "b38", "b18", "b49", "b39", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "LIMITATIONS AND FUTURE WORK", "text": "This work is not without its limitations. As explored in this paper, when interacting with the prototype, radiologists envisioned support functionalities like quality assurance through the assessment of written reports against AI's interpretation of findings on a chest X-ray. This functionality was outside of the prototyped prioritisation and decision support. This choice was dictated by the capabilities of the underlying AI model and the innovation direction of the greater project this study was a part of. We believe that this mismatch perfectly exemplifies the difficulty of innovating clinically useful AI-based systems and motivates further research into a meaningful configuration of AI-based systems, especially at the defining early stages of work. In addition, the study did not take into consideration the differences in clinicians' attitudes and expectations towards AI as well as the collaborative aspects of reporting, which may be worth considering in studies that follow up on designing for clinical usefulness. We also acknowledge the limited variability of clinical sites in Denmark compared to the visited sites in Kenya due to difficulties gaining access. While it is a strength of the study that medical professionals from very different countries participated, future studies need to further explore how geographic and cultural differences play out in regard to successfully designing for and transferring AI-based systems to entirely different healthcare systems and IT infrastructures. Moreover, this project commenced before large language models experienced a performance leap. We believe that their ability to parse and produce text may be an opportune, although challenging avenue for AI support to explore [91].", "publication_ref": ["b90"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "Innovating clinical AI-based systems is a challenging task. By investigating design interventions conducted with radiologists across diverse clinical contexts in Denmark and Kenya, we identified four key technical dimensions that require careful configuration: AI functionality, AI medical focus, AI decision threshold, and AI explainability. To support the innovation of clinically useful AI-based systems, we derived four concrete recommendations of what we propose to call \"configurable AI\" pertaining to the four key technical dimensions.\nMoreover, we explored how dependencies originating from the social dimensions of local clinical practice condition the clinical usefulness of the uncovered technical dimensions. AI functionalities (e.g., prioritisation or decision support) should be configured to provide value in the intended type of clinical site and to match the level of medical expertise of end users. AI medical focus (the detected findings in radiology-focused systems) should be configured in relation to the patient's context, the level of medical expertise of the end-users, and the type of clinical site. The AI decision threshold should be configured according to the medical knowledge (e.g., the clinical meaning of radiological findings), the patient's context, the level of medical expertise of the end users, and the user situation (e.g. time of day). Finally, the explainable AI should be configurable in accordance with medical knowledge to provide maximum value to the end-users.\nOur findings highlight the need for designers and developers to consider these dependencies throughout the innovation process, both before-use and in-use, to ensure that AI-based systems are effectively configured to meet the needs and requirements of their intended clinical contexts. By adhering to these recommendations and considering the dependencies uncovered in our study, designers and developers can contribute to the successful innovation of clinically useful AI-based systems in radiology, ultimately improving patient care and clinical outcomes.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Artificial Intelligence Solutions for Analysis of X-ray Images", "journal": "Canadian Association of Radiologists Journal", "year": "2021", "authors": "Scott J Adams; D E Robert; Xin Henderson; Paul Yi;  Babyn"}, {"ref_id": "b1", "title": "Software Engineering for Machine Learning: A Case Study", "journal": "", "year": "2019", "authors": "Saleema Amershi; Andrew Begel; Christian Bird; Robert Deline; Harald Gall; Ece Kamar; Nachiappan Nagappan; Besmira Nushi; Thomas Zimmermann"}, {"ref_id": "b2", "title": "Design interventions as multiple becomings of healthcare", "journal": "", "year": "2011-05-29", "authors": "Joachim Tariq Andersen; Jonas Halse;  Moll"}, {"ref_id": "b3", "title": "End-to-end lung cancer screening with threedimensional deep learning on low-dose chest computed tomography", "journal": "Nature Medicine", "year": "2019", "authors": "Diego Ardila; P Atilla; Sujeeth Kiraly; Bokyung Bharadwaj; Joshua J Choi; Lily Reicher; Daniel Peng; Mozziyar Tse; Wenxing Etemadi; Greg Ye; David P Corrado; Shravya Naidich;  Shetty"}, {"ref_id": "b4", "title": "If I Had All the Time in the World\": Ophthalmologists' Perceptions of Anchoring Bias Mitigation in Clinical AI Support", "journal": "ACM", "year": "2023", "authors": "Anne Kathrine; Petersen Bach; Trine Munch N\u00f8rgaard; Jens Christian Brok; Niels Van Berkel"}, {"ref_id": "b5", "title": "Making things work: dimensions of configurability as appropriation work", "journal": "Association for Computing Machinery", "year": "2006", "authors": "Ellen Balka; Ina Wagner"}, {"ref_id": "b6", "title": "Barriers to Implementing an Artificial Intelligence Model for Unplanned Readmissions", "journal": "ACI Open", "year": "2020", "authors": "Sally L Baxter; Jeremy S Bass; Amy M Sitapati"}, {"ref_id": "b7", "title": "A Human-Centered Evaluation of a Deep Learning System Deployed in Clinics for the Detection of Diabetic Retinopathy", "journal": "ACM", "year": "2020", "authors": "Emma Beede; Elizabeth Baylor; Fred Hersch; Anna Iurchenko; Lauren Wilcox; Paisan Ruamviboonsuk; Laura M Vardoulakis"}, {"ref_id": "b8", "title": "how did you get to this number?\" Stakeholder needs for implementing predictive analytics: A pre-implementation qualitative study", "journal": "Journal of the American Medical Informatics Association", "year": "2020", "authors": "Natalie C Benda; Lala Tanmoy Das; Erika L Abramson; Katherine Blackburn; Amy Thoman; Rainu Kaushal; Yongkang Zhang; Jessica S Ancker"}, {"ref_id": "b9", "title": "Designing for Control in Nurse-AI Collaboration During Emergency Medical Calls", "journal": "ACM", "year": "2023", "authors": "Arngeir Berge; Frode Guribye; Siri-Linn Schmidt Fotland; Gro Fonnes; Ingrid H Johansen; Christoph Trattner"}, {"ref_id": "b10", "title": "Moving beyond algorithmic accuracy to improving user interaction with clinical AI", "journal": "PLOS Digital Health", "year": "2023", "authors": "Shlomo Berkovsky; Enrico Coiera"}, {"ref_id": "b11", "title": "Clinical decisions using AI must consider patient values", "journal": "Nature Medicine", "year": "2022", "authors": "Jonathan Birch; Kathleen A Creel; Abhinav K Jha; Anya Plutynski"}, {"ref_id": "b12", "title": "Seven lessons for interdisciplinary research on interactive digital health interventions", "journal": "DIGITAL HEALTH", "year": "2018", "authors": "Ann Blandford; Jo Gibbs; Nikki Newhouse; Olga Perski; Aneesha Singh; Elizabeth Murray"}, {"ref_id": "b13", "title": "Reflections on 25 years of ethnography in CSCW", "journal": "Computer supported cooperative work (CSCW)", "year": "2013", "authors": "Jeanette Blomberg; Helena Karasti"}, {"ref_id": "b14", "title": "Reflections on a work-oriented design project", "journal": "Hum.-Comput. Interact", "year": "1996-09", "authors": "Jeanette Blomberg; Lucy Suchman; Randall H Trigg"}, {"ref_id": "b15", "title": "Batman and Robin in Healthcare Knowledge Work: Human-AI Collaboration by Clinical Documentation Integrity Specialists", "journal": "ACM Transactions on Computer-Human Interaction", "year": "2023", "authors": "Claus Bossen; Kathleen H Pine"}, {"ref_id": "b16", "title": "Using thematic analysis in psychology", "journal": "Qualitative Research in Psychology", "year": "2006", "authors": "Virginia Braun; Victoria Clarke"}, {"ref_id": "b17", "title": "Healthcare AI Treatment Decision Support: Design Principles to Enhance Clinician Adoption and Trust", "journal": "ACM", "year": "2023", "authors": "Eleanor R Burgess; Ivana Jankovic; Melissa Austin; Nancy Cai; Adela Kapu\u015bci\u0144ska; Suzanne Currie; J Marc Overhage; Erika S Poole; Jofish Kaye"}, {"ref_id": "b18", "title": "Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making", "journal": "ACM", "year": "2019", "authors": "Carrie J Cai; Emily Reif; Narayan Hegde; Jason Hipp; Been Kim; Daniel Smilkov; Martin Wattenberg; Fernanda Viegas; Greg S Corrado; Martin C Stumpe; Michael Terry"}, {"ref_id": "b19", "title": "Hello AI\": Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making", "journal": "CSCW", "year": "2019-11", "authors": "Carrie J Cai; Samantha Winter; David Steiner; Lauren Wilcox; Michael Terry"}, {"ref_id": "b20", "title": "Framework for design and evaluation of complex interventions to improve health", "journal": "BMJ", "year": "2000", "authors": "Michelle Campbell; R Fitzpatrick; A L Haines; P Kinmonth; D Sandercock;  Spiegelhalter;  Tyrer"}, {"ref_id": "b21", "title": "The last mile: Where artificial intelligence meets reality", "journal": "", "year": "2019", "authors": "Enrico Coiera"}, {"ref_id": "b22", "title": "Drawing on human factors engineering to evaluate the effectiveness of health information technology", "journal": "", "year": "2017", "authors": "Kathrin M Cresswell; Ann Blandford; Aziz Sheikh"}, {"ref_id": "b23", "title": "Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle", "journal": "ACM", "year": "2021", "authors": "Shipi Dhanorkar; Christine T Wolf; Anbang Kun Qian; Lucian Xu; Yunyao Popa;  Li"}, {"ref_id": "b24", "title": "Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness", "journal": "ACM", "year": "2022", "authors": "Kate Donahue; Alexandra Chouldechova; Krishnaram Kenthapadi"}, {"ref_id": "b25", "title": "Accounting for system behavior: representation, reflection, and resourceful action", "journal": "", "year": "1997", "authors": "Paul Dourish"}, {"ref_id": "b26", "title": "The Appropriation of Interactive Technologies: Some Lessons from Placeless Documents", "journal": "Computer Supported Cooperative Work (CSCW)", "year": "2003", "authors": "Paul Dourish"}, {"ref_id": "b27", "title": "Understanding Infrastructure: Dynamics, Tensions, and Design", "journal": "", "year": "2007", "authors": "N Paul; Steven J Edwards; Geoffrey C Jackson; Cory P Bowker;  Knobel"}, {"ref_id": "b28", "title": "Repairing Innovation: A Study of Integrating AI in Clinical Care", "journal": "", "year": "2020", "authors": "Madeleine ; Clare Elish; Elizabeth Anne Watkins"}, {"ref_id": "b29", "title": "Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging", "journal": "ACM", "year": "2022", "authors": "Riccardo Fogliato; Shreya Chappidi; Matthew Lungren; Paul Fisher; Diane Wilson; Michael Fitzke; Mark Parkinson; Eric Horvitz; Kori Inkpen; Besmira Nushi"}, {"ref_id": "b30", "title": "CAROTID -A web-based platform for optimal personalized management of atherosclerotic patients", "journal": "Computer Methods and Programs in Biomedicine", "year": "2014", "authors": "Aimilia Gastounioti; Vasileios Kolias; Spyretta Golemati; Nikolaos N Tsiaparas; Aikaterini Matsakou; John S Stoitsis; P E Nikolaos; Christos Kadoglou; John D Gkekas; Christos D Kakisis; Petros Liapis; Ioannis Karakitsos; Pantelis Sarafis; Konstantina S Angelidis;  Nikita"}, {"ref_id": "b31", "title": "Clinician Perception of a Machine Learning-Based Early Warning System Designed to Predict Severe Sepsis and Septic Shock", "journal": "Critical care medicine", "year": "2019", "authors": "Jennifer C Ginestra; Heather M Giannini; William D Schweickert; Laurie Meadows; Michael J Lynch; Kimberly Pavan; Corey J Chivers; Michael Draugelis; Patrick J Donnelly; Barry D Fuchs; Craig A Umscheid"}, {"ref_id": "b32", "title": "When user experience designers partner with data scientists", "journal": "", "year": "2017", "authors": "Fabien Girardin; Neal Lathia"}, {"ref_id": "b33", "title": "Lessons Learned from Designing an AI-Enabled Diagnosis Tool for Pathologists", "journal": "", "year": "2021", "authors": "Hongyan Gu; Jingbin Huang; Lauren Hung; Xiang ' Anthony' Chen"}, {"ref_id": "b34", "title": "Design interventions as a form of inquiry", "journal": "", "year": "2016", "authors": "Joachim Halse; Laura Boffi"}, {"ref_id": "b35", "title": "Configuring information systems and work practices for each other: what competences are needed locally?", "journal": "International Journal of Human-Computer Studies", "year": "2019", "authors": "Morten Hertzum; Jesper Simonsen"}, {"ref_id": "b36", "title": "Effects of neural network feedback to physicians on admit/discharge decision for emergency department patients with chest pain", "journal": "Annals of Emergency Medicine", "year": "2004", "authors": "Judd E Hollander; Keara L Sease; Dina M Sparano; Frank D Sites; Frances S Shofer; William G Baxt"}, {"ref_id": "b37", "title": "CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison", "journal": "", "year": "2019-01", "authors": "Jeremy Irvin; Pranav Rajpurkar; Michael Ko; Yifan Yu; Silviana Ciurea-Ilcus; Chris Chute; Henrik Marklund; Behzad Haghgoo; Robyn Ball; Katie Shpanskaya; Jayne Seekins; David A Mong; Safwan S Halabi; Jesse K Sandberg; Ricky Jones; David B Larson; Curtis P Langlotz; N Bhavik; Matthew P Patel; Andrew Y Lungren;  Ng"}, {"ref_id": "b38", "title": "Designing AI for Trust and Collaboration in Time-Constrained Medical Decisions: A Sociotechnical Lens", "journal": "ACM", "year": "2021", "authors": "Maia Jacobs; Jeffrey He; Melanie F Pradier; Barbara Lam; Andrew C Ahn; Thomas H Mccoy; Roy H Perlis; Finale Doshi-Velez; Krzysztof Z Gajos"}, {"ref_id": "b39", "title": "CarePre: An Intelligent Clinical Decision Assistance System", "journal": "ACM Trans. Comput. Healthcare", "year": "2020", "authors": "Zhuochen Jin; Shuyuan Cui; Shunan Guo; David Gotz; Jimeng Sun; Nan Cao"}, {"ref_id": "b40", "title": "Because AI is 100% right and safe\": User Attitudes and Sources of AI Authority in India", "journal": "ACM", "year": "2022", "authors": "Shivani Kapania; Oliver Siy; Gabe Clapper; Azhagu Meena; S P ; Nithya Sambasivan"}, {"ref_id": "b41", "title": "Key challenges for delivering clinical impact with artificial intelligence", "journal": "BMC Medicine", "year": "2019", "authors": "Christopher J Kelly; Alan Karthikesalingam; Mustafa Suleyman; Greg Corrado; Dominic King"}, {"ref_id": "b42", "title": "PD: structure in the toolbox", "journal": "Commun. ACM", "year": "1993", "authors": "Finn Kensing; Andreas Munk-Madsen"}, {"ref_id": "b43", "title": "Will You Accept an Imperfect AI?", "journal": "ACM", "year": "2019", "authors": "Rafal Kocielnik; Saleema Amershi; Paul N Bennett"}, {"ref_id": "b44", "title": "Computers and design in context", "journal": "MIT Press", "year": "1997", "authors": "Morten Kyng; Lars Mathiassen"}, {"ref_id": "b45", "title": "Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making", "journal": "Proceedings of the ACM on Human-Computer Interaction", "year": "2023", "authors": "Min Hun; Lee ; Chong Jun Chew"}, {"ref_id": "b46", "title": "Diagnostic Accuracy of Digital Screening Mammography With and Without Computer-Aided Detection", "journal": "JAMA Internal Medicine", "year": "2015", "authors": "Constance D Lehman; Robert D Wellman; Diana S M Buist; Karla Kerlikowske; Anna N A Tosteson; Diana L Miglioretti"}, {"ref_id": "b47", "title": "The Added Effect of Artificial Intelligence on Physicians' Performance in Detecting Thoracic Pathologies on CT and Chest X-ray: A Systematic Review", "journal": "Diagnostics", "year": "2021", "authors": "Dana Li; Lea Marie Pehrson; Carsten Ammitzb\u00f8l Lauridsen; Lea T\u00f8ttrup; Marco Fraccaro; Desmond Elliott; Hubert Dariusz Zaja \u00b8c; Sune Darkner; Jonathan Frederik Carlsen; Michael Bachmann Nielsen"}, {"ref_id": "b48", "title": "Reporting guidelines for clinical trials evaluating artificial intelligence interventions are needed", "journal": "Nature Medicine", "year": "2019", "authors": "Xiaoxuan Liu; Samantha Cruz Rivera; Livia Faes; Lavinia Ferrante; Di Ruffano; Christopher Yau; Hutan Pearse A Keane; Ara Ashrafian; Sebastian J Darzi; Jonathan Vollmer; Lucas Deeks; Christopher Bachmann; An Wen Holmes; David Chan; Melanie J Moher; Alastair K Calvert;  Denniston"}, {"ref_id": "b49", "title": "Clinician Preimplementation Perspectives of a Decision-Support Tool for the Prediction of Cardiac Arrhythmia Based on Machine Learning: Near-Live Feasibility and Qualitative Study", "journal": "JMIR Human Factors", "year": "2021", "authors": "S\u00f8ren Stina Matthiesen; Mikkel Z\u00f6ga Diederichsen;  Klitzing Hartmann; Christina Hansen; Mats Villumsen;  Christian H\u00f8jbjerg Lassen; Karl Peter; Niels Jacobsen; Bo Gregers Risum;  Winkel; T Berit; Jesper Philbert; Tariq Hastrup Svendsen;  Osman Andersen"}, {"ref_id": "b50", "title": "How do providers of artificial intelligence (AI) solutions propose and legitimize the values of their solutions for supporting diagnostic radiology workflow? A technography study in 2021", "journal": "European radiology", "year": "2023", "authors": "Mohammad H Rezazade Mehrizi;  Simon H Gerritsen; M Wouter; Chantal De Klerk;  Houtschild; M H Silke; Luna Dinnessen; Rik Zhao; Abby Van Sommeren;  Zerfu"}, {"ref_id": "b51", "title": "Visualizing Ubiquitously Sensed Measures of Motor Ability in Multiple Sclerosis", "journal": "ACM Transactions on Interactive Intelligent Systems", "year": "2018", "authors": "Cecily Morrison; Kit Huckvale; Bob Corish; Richard Banks; Martin Grayson; Jonas Dorn; Abigail Sellen; S\u00e2n Lindley"}, {"ref_id": "b52", "title": "Development and Validation of Deep Learning-based Automatic Detection Algorithm for Malignant Pulmonary Nodules on Chest Radiographs", "journal": "Radiology", "year": "2019", "authors": "Ju Gang; Nam ; Sunggyun Park; Jin Eui; Jong Hwang; Kwang-Nam Hyuk Lee;  Jin; Young Kun; Thienkai Huy Lim; Jae Vu; Sangheum Ho Sohn; Jin Mo Hwang; Chang Min Goo;  Park"}, {"ref_id": "b53", "title": "VinDr-CXR: An open dataset of chest X-rays with radiologist's annotations", "journal": "", "year": "2020", "authors": "Q Ha; Khanh Nguyen; Linh T Lam;  Le; H Hieu;  Pham; Dung B Dat Q Tran; Dung D Nguyen; Chi M Le; Hang Pham; T T Tong; Cuong D Diep H Dinh;  Do; Cuong N Luu T Doan;  Nguyen; T Binh; Que V Nguyen;  Nguyen; D Au; Hien N Hoang; Anh T Phan; Phuong H Nguyen; Dat T Ho;  Ngo; T Nghia;  Nguyen; Minh Nhan T Nguyen; Van Dao;  Vu"}, {"ref_id": "b54", "title": "Factors influencing the transferability of medical decision support systems", "journal": "International Journal of Bio-Medical Computing", "year": "1991", "authors": "John Nolan; Peter Mcnair; Jytte Brender"}, {"ref_id": "b55", "title": "Realizing AI in Healthcare: Challenges Appearing in the Wild", "journal": "ACM", "year": "2021", "authors": "Francisco Tariq Osman Andersen; Lauren Nunes; Elizabeth Wilcox; Stina Kaziunas; Farah Matthiesen;  Magrabi"}, {"ref_id": "b56", "title": "Investigating the barriers to physician adoption of an artificial intelligence-based decision support system in emergency care: An interpretative qualitative study", "journal": "Studies in Health Technology and Informatics", "year": "2020", "authors": "C\u00e9cile Petitgand; Aude Motulsky; Jean Louis Denis; Catherine R\u00e9gis"}, {"ref_id": "b57", "title": "Right Information, Right Time, Right Place: Physical Alignment and Misalignment in Healthcare Practice", "journal": "ACM", "year": "2020", "authors": "Kathleen H Pine; Yunan Chen"}, {"ref_id": "b58", "title": "Holding AI to Account: Challenges for the Delivery of Trustworthy AI in Healthcare", "journal": "ACM Transactions on Computer-Human Interaction", "year": "2023", "authors": "Rob Procter; Peter Tolmie; Mark Rouncefield"}, {"ref_id": "b59", "title": "Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists", "journal": "PLOS Medicine", "year": "2018", "authors": "Pranav Rajpurkar; Jeremy Irvin; Robyn L Ball; Kaylie Zhu; Brandon Yang; Hershel Mehta; Tony Duan; Daisy Ding; Aarti Bagul; Curtis P Langlotz; N Bhavik; Kristen W Patel; Katie Yeom; Francis G Shpanskaya; Jayne Blankenberg; Timothy J Seekins; David A Amrhein; Safwan S Mong; Evan J Halabi; Andrew Y Zucker; Matthew P Ng;  Lungren"}, {"ref_id": "b60", "title": "Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans", "journal": "Nature Machine Intelligence", "year": "2021", "authors": "Michael Roberts; Derek Driggs; Matthew Thorpe; Julian Gilbey; Michael Yeung; Stephan Ursprung; Angelica I Aviles-Rivero; Christian Etmann; Cathal Mccague; Lucian Beer; Jonathan R Weir-Mccall; Zhongzhao Teng; Effrossyni Gkrania-Klotsas; Alessandro Ruggiero; Anna Korhonen; Emily Jefferson; Emmanuel Ako; Georg Langs; Ghassem Gozaliasl; Guang Yang; Helmut Prosch; Jacobus Preller; Jan Stanczuk; Jing Tang; Johannes Hofmanninger; Judith Babar; Lorena Escudero S\u00e1nchez; Muhunthan Thillai; Paula Martin Gonzalez; Philip Teare; Xiaoxiang Zhu; Mishal Patel; Conor Cafolla; Hojjat Azadbakht; Joseph Jacob; Josh Lowe; Kang Zhang; Kyle Bradley; Marcel Wassin; Markus Holzer; Kangyu Ji; Maria Delgado Ortet; Tao Ai; Nicholas Walton; Pietro Lio; Samuel Stranks; Tolou Shadbahr; Weizhe Lin; Yunfei Zha; Zhangming Niu; H F James; Evis Rudd; Carola-Bibiane Sala;  Sch\u00f6nlieb"}, {"ref_id": "b61", "title": "Modeling Assumptions Clash with the Real World: Transparency, Equity, and Community Challenges for Student Assignment Algorithms", "journal": "ACM", "year": "2021", "authors": "Samantha Robertson; Tonya Nguyen; Niloufar Salehi"}, {"ref_id": "b62", "title": "Stand-Alone Artificial Intelligence for Breast Cancer Detection in Mammography: Comparison With 101 Radiologists", "journal": "JNCI: Journal of the National Cancer Institute", "year": "2019", "authors": "Alejandro Rodriguez-Ruiz; Kristina L\u00e5ng; Albert Gubern-Merida; Mireille Broeders; Gisella Gennaro; Paola Clauser; H Thomas; Margarita Helbich; Tao Chevalier; Thomas Tan;  Mertelmeier; G Matthew; Ingvar Wallis; Sophia Andersson; Ritse M Zackrisson; Ioannis Mann;  Sechopoulos"}, {"ref_id": "b63", "title": "A lesson in implementation: A pre-post study of providers' experience with artificial intelligence-based clinical decision support", "journal": "International Journal of Medical Informatics", "year": "2019", "authors": "Santiago Romero-Brufau; Kirk D Wyatt; Patricia Boyum; Mindy Mickelson; Matthew Moore; Cheristi Cognetta-Rieke"}, {"ref_id": "b64", "title": "Reliable Decisions with Threshold Calibration", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Roshni Sahoo; Shengjia Zhao; Alyssa Chen; Stefano Ermon"}, {"ref_id": "b65", "title": "Integrating a machine learning system into clinical workflows: Qualitative study", "journal": "Journal of Medical Internet Research", "year": "2020", "authors": "Sahil Sandhu; Anthony L Lin; Nathan Brajer; Jessica Sperling; William Ratliff; Armando D Bedoya; Suresh Balu; O' Cara; Mark P Brien;  Sendak"}, {"ref_id": "b66", "title": "The human body is a black box\": Supporting clinical decision-making with deep learning", "journal": "", "year": "2020", "authors": "Mark Sendak; Madeleine ; Clare Elish; Michael Gao; Joseph Futoma; William Ratliff; Marshall Nichols; Armando Bedoya; Suresh Balu; Cara O' Brien"}, {"ref_id": "b67", "title": "Real-world integration of a sepsis deep learning technology into routine clinical care: Implementation study", "journal": "JMIR Medical Informatics", "year": "2020", "authors": "Mark Sendak; William Ratliff; Dina Sarro; Elizabeth Alderton; Joseph Futoma; Michael Gao; Marshall Nichols; Mike Revoir; Faraz Yashar; Corinne Miller; Kelly Kester; Sahil Sandhu; Kristin Corey; Nathan Brajer; Christelle Tan; Anthony Lin; Tres Brown; Susan Engelbosch; Kevin Anstrom; Madeleine ; Clare Elish; Katherine Heller; Rebecca Donohoe; Jason Theiling; Eric Poon; Suresh Balu; Armando Bedoya; Cara O' Brien"}, {"ref_id": "b68", "title": "Making Machine Learning Models Clinically Useful", "journal": "JAMA", "year": "2019", "authors": "H Nigam; Arnold Shah; Steven C Milstein; Phd Bagley"}, {"ref_id": "b69", "title": "Human-Centered Artificial Intelligence: Reliable, Safe & Trustworthy", "journal": "International Journal of Human-Computer Interaction", "year": "2020", "authors": "Ben Shneiderman"}, {"ref_id": "b70", "title": "Human-Centered Artificial Intelligence: Three Fresh Ideas", "journal": "AIS Transactions on Human-Computer Interaction", "year": "2020", "authors": "Ben Shneiderman"}, {"ref_id": "b71", "title": "The Wrong Trousers? Beyond the Design Fallacy: Social Learning and the User", "journal": "", "year": "2005", "authors": "James Stewart; Robin Williams"}, {"ref_id": "b72", "title": "Implementation of artificial intelligence (AI) applications in radiology: hindering and facilitating factors", "journal": "European radiology", "year": "2020", "authors": "Lea Strohm; Charisma Hehakaya;  Erik R Ranschaert; P C Wouter; Ellen H M Boon;  Moors"}, {"ref_id": "b73", "title": "Working artefacts: ethnomethods of the prototype", "journal": "The British Journal of Sociology", "year": "2002", "authors": "Lucy Suchman; Randall Trigg; Jeanette Blomberg"}, {"ref_id": "b74", "title": "Current Clinical Applications of Artificial Intelligence in Radiology and Their Best Supporting Evidence", "journal": "Journal of the American College of Radiology", "year": "2020", "authors": "Amara Tariq; Saptarshi Purkayastha; Geetha Priya Padmanaban; Elizabeth Krupinski; Hari Trivedi; Imon Banerjee; Judy Wawira Gichoya"}, {"ref_id": "b75", "title": "Machine Learning in Mental Health", "journal": "ACM Transactions on Computer-Human Interaction", "year": "2020", "authors": "Anja Thieme; Danielle Belgrave; Gavin Doherty"}, {"ref_id": "b76", "title": "Designing Human-centered AI for Mental Health: Developing Clinically Relevant Applications for Online CBT Treatment", "journal": "ACM Transactions on Computer-Human Interaction", "year": "2023", "authors": "Anja Thieme; Maryann Hanratty; Maria Lyons; Jorge Palacios; Rita Faia Marques; Cecily Morrison; Gavin Doherty"}, {"ref_id": "b77", "title": "Integration of Artificial Intelligence Decision Aids to Reduce Workload and Enhance Efficiency in Thyroid Nodule Management", "journal": "JAMA Network Open", "year": "2023", "authors": "Wen-Juan Tong; Shao-Hong Wu; Mei-Qing Cheng; Hui Huang; Jin-Yu Liang; Chao-Qun Li; Huan-Ling Guo; Dan-Ni He; Yi-Hao Liu; Han Xiao; Hang-Tong Si-Min Ruan; Ming-De Li; Ming-De Lu; Wei Wang"}, {"ref_id": "b78", "title": "How does artificial intelligence in radiology improve efficiency and health outcomes?", "journal": "Pediatric radiology", "year": "2022", "authors": "Maarten Kicky G Van Leeuwen; Steven De Rooij; Bram Schalekamp; Matthieu J C M Van Ginneken;  Rutten"}, {"ref_id": "b79", "title": "Artificial intelligence in radiology: 100 commercially available products and their scientific evidence", "journal": "European radiology", "year": "2021", "authors": "Steven Kicky G Van Leeuwen;  Schalekamp; J Matthieu; Bram C M Rutten; Maarten Van Ginneken; Rooij De"}, {"ref_id": "b80", "title": "Artificial intelligence in the clinical setting", "journal": "European Journal of Anaesthesiology", "year": "2022", "authors": "Simon Tilma Vistisen; Tom Joseph Pollard; Steve Harris; Simon Meyer Lauritsen"}, {"ref_id": "b81", "title": "Brilliant ai doctor in rural clinics: Challenges in ai-powered clinical decision support system deployment", "journal": "", "year": "2021", "authors": "Dakuo Wang; Liuping Wang; Zhan Zhang"}, {"ref_id": "b82", "title": "ChestX-ray: Hospital-Scale Chest X-ray Database and Benchmarks on Weakly Supervised Classification and Localization of Common Thorax Diseases", "journal": "Advances in Computer Vision and Pattern Recognition", "year": "2019", "authors": "Xiaosong Wang; Yifan Peng; Le Lu; Zhiyong Lu; Mohammadhadi Bagheri; Ronald M Summers"}, {"ref_id": "b83", "title": "A Practical Guide to Artificial Intelligence-Based Image Analysis in Radiology", "journal": "Investigative radiology", "year": "2020", "authors": "Thomas Weikert; Joshy Cyriac; Shan Yang; Ivan Nesic; Victor Parmar; Bram Stieltjes"}, {"ref_id": "b84", "title": "Direct activation: A concept to encourage tailoring activities", "journal": "Behaviour & Information Technology", "year": "2001", "authors": "Volker Wulf; Bj\u00f6rn Golombek"}, {"ref_id": "b85", "title": "CheXplain: Enabling Physicians to Explore and Understand Data-Driven, AI-Enabled Medical Imaging Analysis", "journal": "ACM", "year": "2020", "authors": "Yao Xie; Melody Chen; David Kao; Ge Gao; Xiang ' Anthony' Chen"}, {"ref_id": "b86", "title": "Toward human-centered AI: A Perspective from Human-Computer Interaction", "journal": "Interactions", "year": "2019", "authors": "Wei Xu"}, {"ref_id": "b87", "title": "Investigating how experienced UX designers effectively work with machine learning", "journal": "", "year": "2018", "authors": "Qian Yang; Alex Scuito; John Zimmerman; Jodi Forlizzi; Aaron Steinfeld"}, {"ref_id": "b88", "title": "Unremarkable AI: Fiting intelligent decision support into critical, clinical decision-making processes", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Qian Yang; Aaron Steinfeld; John Zimmerman"}, {"ref_id": "b89", "title": "Investigating the Heart Pump Implant Decision Process: Opportunities for Decision Support Tools to Help", "journal": "ACM", "year": "2016", "authors": "Qian Yang; John Zimmerman; Aaron Steinfeld; Lisa Carey; James F Antaki"}, {"ref_id": "b90", "title": "Multimodal Healthcare AI: Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology", "journal": "", "year": "2024", "authors": "Nur Yildirim; Hannah Richardson; Maria T Wetscherek; Junaid Bajwa; Joseph Jacob; Mark A Pinnock; Stephen Harris; Daniel Coelho De Castro; Shruthi Bannur; Stephanie L Hyland"}, {"ref_id": "b91", "title": "Ground Truth Or Dare: Factors Affecting The Creation Of Medical Datasets For Training AI", "journal": "ACM", "year": "2023", "authors": "D Hubert; Natalia R Zaja \u00b8c; Finn Avlona;  Kensing; O Tariq; Irina Andersen;  Shklovski"}, {"ref_id": "b92", "title": "Clinician-Facing AI in the Wild: Taking Stock of the Sociotechnical Challenges and Opportunities for HCI", "journal": "ACM Transactions on Computer-Human Interaction", "year": "2023", "authors": "D Hubert; Dana Zaja \u00b8c; Xiang Li; Jonathan F Dai; Finn Carlsen; Tariq O Kensing;  Andersen"}, {"ref_id": "b93", "title": "Hybrid-augmented intelligence: collaboration and cognition", "journal": "Frontiers of Information Technology & Electronic Engineering", "year": "2017", "authors": "Nan-Ning Zheng; Zi-Yi Liu; Peng-Ju Ren; Yong-Qiang Ma; Shi-Tao Chen; Si-Yu Yu; Jian-Ru Xue; Ba-Dong Chen; Fei-Yue Wang"}, {"ref_id": "b94", "title": "Interpretable artificial intelligencebased app assists inexperienced radiologists in diagnosing biliary atresia from sonographic gallbladder images", "journal": "BMC Medicine", "year": "2024", "authors": "Wenying Zhou; Zejun Ye; Guangliang Huang; Xiaoer Zhang; Ming Xu; Baoxian Liu; Bowen Zhuang; Zijian Tang; Shan Wang; Dan Chen; Yunxiang Pan; Xiaoyan Xie; Ruixuan Wang; Luyao Zhou"}, {"ref_id": "b95", "title": "Research through design as a method for interaction design research in HCI", "journal": "Association for Computing Machinery", "year": "2007", "authors": "John Zimmerman; Jodi Forlizzi; Shelley Evenson"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Online and co-located design sessions with user interface mock-ups and functional prototypes (from left: iteration I, II, and III)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: The collage showcases three iterations of the AI prototype, including two screens: the X-ray viewer and the X-ray worklist.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Configuration matrix for achieving clinical usefulness of AI. Showing how the technical AI dimensions need to be configured against the social dimensions, which they depend on.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Two UI elements allowing the user to toggle prioritisation and decision support functionalities and to select findings on the X-ray work list and X-ray viewer.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: A configuration panel allowing users to select radiological findings detected by the AI model.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: A configuration panel allowing users to select AI decision threshold globally and per finding. In this version, we introduced two levels for quick access depending on the user situation: high confidence and medium confidence.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: Different XAI methods available in the prototype. From left to right: gradient overlay, bounding box, arrow.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 :9Figure 9: Four design recommendations on how to achieve clinically useful AI-based systems. Accompanied by more in-depth considerations.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Radiology participants that took part in the study.", "figure_data": ""}], "formulas": [], "doi": "10.1145/3643834.3660707"}
