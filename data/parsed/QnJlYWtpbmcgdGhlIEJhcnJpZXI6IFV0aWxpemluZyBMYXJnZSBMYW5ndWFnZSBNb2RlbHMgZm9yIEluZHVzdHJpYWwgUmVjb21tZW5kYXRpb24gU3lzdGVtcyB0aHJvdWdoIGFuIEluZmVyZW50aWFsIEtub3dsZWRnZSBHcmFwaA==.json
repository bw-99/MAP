{
  "Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph": "Qian Zhao Ant Group Shanghai, China zq317110@antgroup.com Hao Qian Ant Group Shanghai, China qianhao.qh@antgroup.com Ziqi Liu Ant Group Hangzhou, China ziqiliu@antgroup.com Gong-Duo Zhang Ant Group Hangzhou, China gongduo.zgd@antgroup.com Lihong Gu Ant Group Hangzhou, China lihong.glh@antgroup.com",
  "ABSTRACT": "Recommendation systems are widely used in e-commerce websites and online platforms to address information overload. However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions. Recently, Knowledge Base (KB)-based models are proposed to incorporate expert knowledge, but it struggles to adapt to new items and the evolving e-commerce environment. To address these challenges, we propose a novel L arge L anguage M odel based Complementary K nowledge E nhanced Rec ommendation System (LLM-KERec). It introduces an entity extractor that extracts unified concept terms from items and user information. To provide cost-effective and reliable prior knowledge, entity pairs are generated based on entity popularity and specific strategies. The large language model determines complementary relationships in each entity pair, and constructs a complementary knowledge graph. Furthermore, a new complementary recall module and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of the ranking model by using real complementary exposure-click samples. We conduct extensive experiments on three industry datasets. The results demonstrate significant performance improvement of our model compared to existing approaches. Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm for consumption by recommending complementary items. In summary, LLM-KERec addresses the limitations of traditional recommendation systems by incorporating complementary knowledge and utilizing a large language model to capture user intent transitions, adapt to new items, and enhance recommendation efficiency in the evolving e-commerce landscape. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym 'XX, August 25-29, 2024, Barcelona, Spain © 2024 ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX",
  "CCS CONCEPTS": "· Information systems → Recommender systems ; Learning to rank ; Data mining.",
  "KEYWORDS": "Recommendation system, Large language model, Knowledge graph",
  "ACMReference Format:": "Qian Zhao, Hao Qian, Ziqi Liu, Gong-Duo Zhang, and Lihong Gu. 2024. Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph. In Proceedings of 30TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATAMINING(Conference acronym 'XX). ACM,NewYork, NY, USA, 9 pages. https://doi.org/XXXXXXX.XXXXXXX",
  "1 INTRODUCTION": "The Recommendation System (RS) has been widely used in online service platforms (e.g., Amazon and Taobao) as an effective tool for alleviating information overload. The primary objective of RS is to infer user preferences from their past behaviors, recommend the most suitable items that align with their interest. Hence, existing recommendation systems are mostly trained based on historical exposure and click logs. Hereby, we summarize the existing recommendation tasks as the combination of following sub-tasks: 1) Recommend substitutive items based on the exposure or click feedback from users. 2) Recommend complementary items based on the conversion feedback from users. 3) Conduct traffic exploration or business intervention to explore users' other potential interests. Traditional deep ClickThrough Rate (CTR) prediction models[8, 14, 20, 21] equipped with well-designed feature interaction techniques through deep neural networks have been widely applied to tackle these sub-tasks in major e-commerce systems. These methods provide personality in RS via extracting user preference from historical exposure-click samples. Despite achieving notable performance improvements in RS, we argue they still suffer from the following two major challenges in real-world scenarios. 1) These models rely heavily on exposed samples and user feedback, which limits the performance of RS in cold-start scenarios and makes it difficult to cope with the continuous emergence of new items. 2) The sparsity of user interaction samples results in existing CTR models being more Conference acronym 'XX, August 25-29, 2024, Barcelona, Spain Qian Zhao and Hao Qian, et al. effective in recommending substitutes (sub-task 1) than complementary items (sub-task 2). While models based on expert-crafted complementary rules or knowledge graphs can aid in recommending complementary items, they are not a sustainable solution in the ever-evolving landscape of e-commerce due to efficiency and expenditure challenges. Therefore, it's indispensable to incorporate efficient knowledge and Large Language Model (LLM) as the carrier of human reasoning and logic to improve the performance of RS[1, 4, 5, 19]. However, in RS, due to the difficulty of large-scale deployment and long inference time of the large language model, it has only been used as a tool for text embedding in previous work[2, 9], making it difficult to fully utilize its powerful reasoning ability. In the light of the above limitations and challenges, we propose a novel LLM-KERec for recommendation. Our method combines the efficient collaborative signal processing capability of traditional models with large language models and complementary graph to help users quickly find their preferred items. This method not only reduces the homogeneity of traditional model recommendation results, but also improves overall click-through and conversion rates. Specifically, we first use our designed entity extractor to extract unified concept terms (referred to as entities) from the information of all items and user billing information. Next, we generate entity pairs based on the popularity of entities and carefully designed strategies. Then we construct a complementary graph based on a large language model, where each edge in the graph represents a complementary purchasing relationship between corresponding entities. Finally, we launch a new complementary recall module and train the E-E-I weight decision model through real exposure click samples. This model will apply the edge weights of the graph corrected by real feedback to the fine-ranking layer model to achieve recommendation of complementary items. It is worth mentioning that both the entity extractor and complementary graph are periodically updated to adapt to new items and the changing ecommerce environment. The main contributions of this paper can be summarized as follows: · For the first time, we utilize the inference ability of large language models as a medium to improve the scenario preference when recommending items to each user, achieving large-scale application of large language model in industrial scenarios. · Our method continuously adjusts the weights of graph edges based on real exposure samples of complementary item pairs, addressing the language model's weakness in determining user preference strength. · Extensive experiments are conducted on three industry scenarios, demonstrating our approach is consistently better than a number of competitive baselines.",
  "2 SYSTEM OVERVIEW": "In this section, we present the overview of the LLM-KERec System, including Traditional Recommendation Module and LLM-based Complementary Knowledge Enhancement , shown in Fig. 1. Figure 1: Overall framework of our proposed LLM-KERec System. Entity Extractor Recall Module Coarse-Ranking Model Fine-Ranking Model Re-Ranking Model Items User Complementary Graph E-E-I weight Decision model Recent Bills Request Item Infomation Traditional Recommendation Module LLM-based Complementary Knowledge Enhancement Large Language Model e.g. ChatGPT, ChatGLM, Claude via World Knowledge, Commonsense Reasoning +U2E2I  Complementary Recall inject knowledge Display Order Historical Feedbacks logs …",
  "2.1 Traditional Recommendation Module": "In the traditional recommendation architecture, when a user opens an application, the application will automatically send a request to server. This process follows these steps: 1) The server triggers the recall module based on the user's request information, including popular item recall, LBS recall, personalized recall, etc. The recall module returns a large number of candidate items. 2) These candidate items are then input into the coarse-ranking model for filtering. The coarse-ranking model produces a smaller set of candidate items. 3) Finally, the fine-ranking model and re-ranking model are used to make the final decision on the display order of these items. Additionally, manual intervention may occur at each step, such as assigning weights to the items for publication. Thefine-ranking model and re-ranking model are typically trained using historical exposure and click logs. As a result, existing recommendation models often prioritize recommending similar items based on positive user feedback. This poses a challenge when it comes to providing reliable recommendations for supplementary items that have potential reasoning behind them, such as suggesting complementary item B after a user has purchased item A.",
  "2.2 LLM-based Complementary Knowledge Enhancement": "In this paper, the LLM-KERec system maintains the ability to efficiently process a large number of collaborative signals of the existing recommendation system. It also overcomes above challenge through the LLM-based Complementary Knowledge Enhancement Module. To establish connections between different content in Alipay, LLM-KERec creates a unified entity (category) system for users' billing behaviors and all items. Each item or bill is classified into a unique entity, which serves as a bridge between various contents. Utilizing world knowledge and commonsense knowledge, we employ a large language model to determine the existence of a complementary relationship between two entities and construct a complementary graph. The nodes of this graph are all entities, while the edges indicate the complementary relationship between the corresponding entities. Subsequently, using the real exposure and click feedback of complementary items, we train an entity-entity-item (E-E-I) weight decision model. This model is then used to inject knowledge into the ranking model. By adopting this approach, we can provide personalized recommendations for both favorite items and complementary items. This solution has been successfully implemented in Alipay marketing scenarios, and experimental results have demonstrated its effectiveness. Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph Conference acronym 'XX, August 25-29, 2024, Barcelona, Spain Figure 2: Extracting entites from item information and user bills. BERT-CRF BERT-CRF BERT-CRF CRF Layer BERT Preprocessing 舒客  小苏打  牙膏  120克 (Shuke baking soda toothpaste 120g) 一次性  免撕  保鲜膜罩  100支 (100 disposable tear free plastic wrap covers) 硫磺除螨    液体    ⾹皂    400ml Intent / Product / [香皂; soap] [保鲜膜罩; plastic wrap cover] [牙膏; toothpaste] 硫磺除螨  液体  香皂  400ml (Sulfur Mite Removal Liquid Soap 400ml) extract results item information & user bills",
  "3 DIVING INTO THE LLM-KEREC SYSTEM": "In this section, we will zoom into each module in LLM-KERec System.",
  "3.1 Entity Extractor": "3.1.1 Entity Dict. In real-world applications, like Alipay, users' behaviors span across various scenarios, each with diverse content. To align information and knowledge from these diverse sources, it is crucial to establish a unified association pattern. This is where our Entity Dict comes into play, serving as a bridge for different content types. In the Entity Dict, each entity represents a specific concept, such as 'phone' or 'cola'. Our dedicated group of experts meticulously designed the Entity Dict, incorporating tens of thousands of entities. Importantly, the Entity Dict is regularly updated every week to ensure its adaptability to new items and content. 3.1.2 Extracting Entities. Building upon the Entity Dict, our focus shifts to extracting entities from various user behaviors within Alipay, including bills, visit logs, and the entity information of items in marketing scenarios. This extraction process can be viewed as a Named Entity Recognition (NER) task, which has been extensively studied in the field of Natural Language Processing (NLP) [13, 15, 23]. To perform entity extraction, we utilize the BERT-CRF model. This model combines the transfer capabilities of BERT[4] with the structured predictions of CRF[11]. The BERT-CRF model enables us to accurately extract entities from user behaviors in Alipay. In the LLM-based Complementary Knowledge Enhancement, our primary objective is to establish connections between user purchase behaviors and the items to be recommended. To achieve this, we extract entities from each user's recent bills, forming their recent entity transaction sequence. Furthermore, we extract entities from item information and assign a unique entity as the item's category. The detailed procedure is illustrated in Fig. 2.",
  "3.2 Complementary Graph Construction": "We utilize the results of the entity extractor to construct a complementary graph, which helps us gain insights into users' purchasing patterns. Specifically, we aim to understand which item B ( e.g., paper towels) users typically buy after purchasing item A ( e.g., utensils) by leveraging natural language understanding and commonsense reasoning. The construction of the complementary graph involves two main steps: 1) We generate candidate entity pairs from the entity dict, ensuring both execution efficiency and comprehensive item coverage. 2) Through a combination of carefully designed Figure 3: Long-tail distribution in Entity Dict. Extremely Popular Unpopular Chicken wings Roll paper pencil glove tablet computer bracket Popular sauce Popular Popular Extremely Popular Unpopular Large Language Model e.g. ChatGPT, ChatGLM, Claude via world Knowledge, commonsense Reasoning (a) rank entities (b) construct entity pairs (c) construct entity graph prompt engineering and the utilization of a large language model, we perform reasoning tasks to extract meaningful insights from the data. 3.2.1 Entity Pair Construction. Firstly, it is important to recognize that certain items have complementary relationships with specific concepts, and these concepts often encompass more specific items. In industrial e-commerce scenarios, where the number of items can reach millions or even more, there are only a few thousand concept categories. By using concepts as entities instead of individual items, computational resources can be significantly conserved. In Section 3.1, we have already assigned unique entities to all items using the entity extractor. To construct entity pairs, a straightforward approach would involve taking elements from a set containing 𝑛 entities and combining them pairwise, resulting in 𝑛 ( 𝑛 -1 ) 2 candidate entity pairs. However, this method is not cost-effective due to the slower inference speed of downstream large language models. Additionally, in real-world scenarios, there is often a long-tail distribution where a few entities are frequently purchased while the majority of entities are rarely consumed (as depicted in Fig. 3). Focusing solely on tail entity combinations makes it challenging to improve the overall performance of the recommendation system. To tackle this challenge, we have devised a cost-effective segmented combination strategy as follows: 1) Initially, we sort entities in descending order based on metrics like total conversions and clicks. This allows us to classify them into three categories: extremely popular, popular, and unpopular entities. 2) We focus on constructing entity pairs exclusively within the popular entities. This approach enhances the performance and coverage specifically Conference acronym 'XX, August 25-29, 2024, Barcelona, Spain Qian Zhao and Hao Qian, et al. for popular items. 3) Additionally, we construct entity pairs that include both extremely popular and unpopular entities. This ensures comprehensive coverage in the complementary graph for unpopular items. By merging and eliminating duplicates from all entity pairs, we obtain the final output. This segmented combination strategy ensures reliable support for downstream modules while minimizing resource waste. 3.2.2 Large Language Model. Large language models have garnered significant attention from researchers due to their remarkable understanding and reasoning abilities in natural language processing. A specific research direction, exemplified by methods like Prompt-Turing[12] and LoRA[7], explores prompt engineering techniques based on these large language models. In these approaches, researchers can obtain desired answers from the large language model by providing a simple task description and a small number of examples. By fine-tuning the model efficiently using techniques like LoRA[7] based on annotated samples, researchers can enhance its support for the current task. In this study, we also leverage the capabilities of large language models to determine the existence of a complementary relationship in an entity pair. Specifically, we utilize Claude 2 1 as the underlying language model and thoughtfully design reliable prompts to guide the model in conducting a step-by-step analysis and providing dependable reasoning evidence. The ultimate goal is to enhance the interpretability of the reasoning results. Upon completing the reasoning process, we sample thousands of examples for manual annotation and continuously refine the prompts to attain an acceptable level of accuracy in the reasoning outcomes. The prompts we have designed encompass various aspects, including: 1) Description of the input data format, where each line consists of two entities representing real-world concepts. 2) Task description, which involves determining whether there is a likelihood of a person purchasing entity B shortly after purchasing entity A . 3) Provide multiple data examples and their corresponding reasons. For instance, we provide examples like the complementary relationship between bread and milk, as they form a popular breakfast combination. Conversely, we highlight that there is no complementary relationship between a phone and milk, as they are unrelated. 4) Explanation of the output format, which includes a concise description of the purposes of the two entities, whether a complementary relationship exists between them, and a detailed explanation. Ultimately, the answer is denoted as either Y or N. Moreover, we have explored methods such as ChatGPT 3.5 2 and ChatGLM 2[5, 22]. A comprehensive comparison between these methods can be found in Section 4.5. 3.2.3 Automatic Update Strategy. In a real e-commerce environment, users and merchants continually rely on each other's cognitive updates and mutually promote one another. This means that the popularity of entities is not static. For instance, certain merchants may employ marketing strategies to rapidly gain public attention for their products, and over time, older products may be phased out. To address this dynamic nature of popularity, we have implemented an automatic daily schedule for constructing 1 https://www.anthropic.com/index/claude-2 2 https://openai.com/blog/chatgpt the incremental complementary graph. By promptly recognizing such changes and updating our complementary graph accordingly, we can ensure the effective and sustained operation of the entire system. This proactive approach is crucial for maintaining optimal system performance in the long run.",
  "3.3 E-E-I weight decision model": "At present, we have successfully linked each user's recent bills and each item to entities in the complementary graph. Our objective is to recommend complementary items (entity2) based on user bill (entity1), where relation entity1 -entity2 exist in the complementary graph. However, due to the limited ability of LLM to accurately assess user preferences, we require an E-E-I ( entity1-entity2-item ) weight decision model to effectively accomplish this task. 3.3.1 Model Overview. Intuitively, the success of the LLM-KERec System relies heavily on the construction of a high-quality E-E-I weight decision model. Therefore, we propose a Two-stage Complementary Knowledge Enhancement Procedure, which consists of the Ranking Stage and the Integration Stage , as shown in Fig. 4. In the following sections, we will take a closer look at each well-designed stage. 3.3.2 Ranking Stage. As shown in Fig. 4(a.0), our model adopts a dual-tower architecture, where the outputs of the two towers represent the representations of the complementary item and bill entity, respectively. The dot product of these outputs serves as the preference level indicator. For the representation of item, we can extract a rich set of features from the database, including basic features, statistical features, and interaction features, etc. However, for the entity representation, we face a challenge as we lack specific information to describe them, aside from a pre-assigned ID. To overcome this limitation, we employ Graph Neural Network[10] and Contrastive Learning to representative entity from two distinct perspectives: the first-order substitutable view and the second-order complementary view. The Ranking Stage can be further subdivided into the following modules: Graph Construction. Graph Neural Networks (GNNs) has demonstrated promising results for recommender systems, as they can effectively leverage high-order relationship. These methods represent interaction data as graphs, such as the user-item interaction graph, and iteratively propagate neighborhood information to learn effective node representations. Similarly, as show in Fig. 4(a.1), we have designed the following edge relationships around how to better represent entity: 1) Establish edges for click behaviors between user nodes and item nodes. 2) Establish edges for dependency relationships between item nodes and entity nodes. 3) Establish edges for complementary relationships between entity nodes and entity nodes. Given the user set U = { 𝑢 } , the item set I = { 𝑖 } and the entity set E = { 𝑒 } . The number of nodes is 𝑛 = |U| + |I| + |E| . Our method formulate the available data as a user-item-entity graph G = (V , A ) , where V = U∪I∪E and A ∈ R 𝑛 × 𝑛 is the adjacent matrix. Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph Conference acronym 'XX, August 25-29, 2024, Barcelona, Spain item features bill entity features DNN DNN score dot product basic infomation interactions statistical value entity id Attention Attention MP 1 MP 2 meta-path L cl graph feature e 2 e 2 z f z s (a.0) Overview i 1 i 2 i 3 i 4 i 5 e 1 u 2 e 1 i 4 i 5 i 1 i 3 u 1 u 2 u 1 u 2 i 2 u 2 i 3 (a.3) Second-order Complementary View (a.2) First-order Substitutable View (a.4) Contrasive Learning (a.1) Graph Construction user item entity e 1 e 1 e 1 e 2 U2E2I Recall Basic Recall Popular item / LBS / Personalization user requests … duplicate recall recalled items user score entity & item entity & item embeddings user & entity embeddings E-E-I weight decision user & entity coarse/fine ranking … … 0.1 0.4 0.8 0.6 0.3 0.7 scores (a) Ranking Stage (b) Integration Stage Figure 4: Overall framework of Two-stage Complementary Knowledge Enhancement Procedure. First-order Substitutable View. In order to model substitutable relationships, we consider two different sources of information for each entity: (1) From an item sub-perspective, we need to explore the common features of items that have a dependency relationship on the current entity. (1) From the complementary graph, we design a meta path MP 1 : item (database) -> entity (graph) -> entity (bill) , represents the collection of item features complementary to the current entity from the perspective of semantic reasoning. (2) Similarly, from an user sub-perspective, we need to explore the common features of the users who frequently click on the current entity. Specifically, we aggregate information using the Graph Attention Network (GAT), denoted by h ′ 𝑖 = 𝑓 𝑡 ( h , 𝑖, N 𝑖 ; 𝜃 𝑡 ) . Here, h represents the embeddings of all nodes, 𝑖 denotes current node index, N 𝑖 is neighbors of node 𝑖 , 𝜃 𝑡 is the network parameters, and function 𝑓 (· , · , · ; ·) is defined as:  where 𝛼 𝑖 𝑗 is defined as:  where h 𝑖 represents the embeddings of node 𝑖 , W 1 ∈ R 𝑑 × 𝑑 and W 2 ∈ R 2 𝑑 are trainable parameters, 𝜎 (·) is a non-linearity activate function, LeakyReLU (·) is the LeakyReLU activate function and [·||·] is concatenate operation. Then we can fuse information from different sub-perspectives including user and item side, based on attention mechanism to obtain entity node 𝑖 embedding:  As shown in the Fig. 4(a.2), where 𝑖 1 and 𝑖 3 are aggregated to 𝑒 1 on the item side, and 𝑢 1 and 𝑢 2 are aggregated to 𝑒 1 on the user side. Second-order Complementary View. In the modeling of complementary relationships, we also consider two different sources of information for each entity: (2) From the user' daily behaviors, we also design a meta path MP 2 : item1 (bill) -> user -> item2 (bill) -> entity (bill) , which indicates what items have been recently consumed by users who have consumed item2 in the short term. Similarly, we obtain the representation of entity node 𝑖 through Eq. 4:  where N 𝑖 -MP 1 and N 𝑖 -MP 2 are the target node sets explored throught meta-path MP 1 and MP 2 , respectively, starting from entity node 𝑖 . As shown in the Fig. 4(a.3), where 𝑖 4 and 𝑖 5 are aggregated to 𝑒 1 , and 𝑖 3 is aggregated to 𝑒 1 . Contrasive Learning. z 𝑓 𝑖 and z 𝑠 𝑖 are aggregated through information from first-order substitutable view and second-order complementary view , respectively, representing the characterization of entity 𝑖 from two independent and complementary perspectives. z 𝑓 𝑖 and z 𝑠 𝑖 are interrelated and complementary, as they can supervise each other in training process. Therefore, we utilize the contrastive loss, InfoNCE [17], to maximize the agreement of positive pairs and minimize that of negative pairs:  where 𝑠 (·) measures the similarity between two vectors, which is set as cosine similarity function; 𝜏 is the hyper-parameter, known as the temperature in softmax. Finally, the representation of the node 𝑖 is the weighted sum of z 𝑓 𝑖 and z 𝑠 𝑖 , which will be used for downstream recommended tasks. Training Process. We leverage a multi-task training strategy to optimize the main E-E-I weight decision task and the auxiliary tasks including contrastive learning task and L2 normalization task < l a t e x i s h 1 _ b 6 4 = \" z / G 7 E A p 3 D 8 P v f g O Z S q k > B n c V N J U r 9 L 0 K Y F 2 o Q + I m u j w y d W X 5 C M R H T < l a t e x i s h 1 _ b 6 4 = \" F u J 3 M + g n 0 d y Y N G O z L > A B c V S 8 E U r q / 9 m k 7 Q o W 2 p Z I P C j X f H D w T 5 K R v < l a t e x i s h 1 _ b 6 4 = \" z / G 7 E A p 3 D 8 P v f g O Z S q k > B n c V N J U r 9 L 0 K Y F 2 o Q + I m u j w y d W X 5 C M R H T < l a t e x i s h 1 _ b 6 4 = \" Z O S R z 0 k u 9 B w g 2 5 H E J 8 > A n c V N 3 U r q / L m I F o Q + D 7 G j Y y f d v X M W C P K p T < l a t e x i s h 1 _ b 6 4 = \" E q L A + f M O S w N C 5 Z o > B n c V 8 J 3 U r / 9 0 K Y F 2 p Q z D 7 I m G u j y d g W X v k P R H T < l a t e x i s h 1 _ b 6 4 = \" I A 0 3 N v + 7 q y D / Z U B w V W p f > n c S 8 E J r 9 L m M F Q o 2 k P C j z X G H u T K R g 5 O d Y < l a t e x i s h 1 _ b 6 4 = \" g Q 7 3 + w O Z 8 F c A X n m 2 p E o v > B V N S J U r q / 9 L 0 k M W y I P C j z Y u d f H G D T 5 K R < l a t e x i s h 1 _ b 6 4 = \" q d A E 9 r R Q J W D X 7 S w > B n c V L g N O y f U Y o P K 2 8 I 5 H m v Z z F p 3 k G j T / u 0 + M C < l a t e x i s h 1 _ b 6 4 = \" W 7 o f 0 Y 3 R r 8 5 y j + / G > A B n c Z D L S g M F I P K V u v T q z C k m U Q 9 N H 2 O p J X E d w < l a t e x i s h 1 _ b 6 4 = \" w 3 9 j 2 g A G + U m T / B K y J k > n c Z D L S M F I P u q X o V 7 C 5 v 0 E H Q O 8 W f z d p N R Y r < l a t e x i s h 1 _ b 6 4 = \" p K P n W R O E m / + I r g C V A > B c N S 8 J 3 U q j y 5 o L X Q Z T 0 Y 9 d 2 z H w v 7 G M k F f u D < l a t e x i s h 1 _ b 6 4 = \" g Q 7 3 + w O Z 8 F c A X n m 2 p E o v > B V N S J U r q / 9 L 0 k M W y I P C j z Y u d f H G D T 5 K R < l a t e x i s h 1 _ b 6 4 = \" q d A E 9 r R Q J W D X 7 S w > B n c V L g N O y f U Y o P K 2 8 I 5 H m v Z z F p 3 k G j T / u 0 + M C < l a t e x i s h 1 _ b 6 4 = \" Z O S R z 0 k u 9 B w g 2 5 H E J 8 > A n c V N 3 U r q / L m I F o Q + D 7 G j Y y f d v X M W C P K p T < l a t e x i s h 1 _ b 6 4 = \" I A 0 3 N v + 7 q y D / Z U B w V W p f > n c S 8 E J r 9 L m M F Q o 2 k P C j z X G H u T K R g 5 O d Y < l a t e x i s h 1 _ b 6 4 = \" I A 0 3 N v + 7 q y D / Z U B w V W p f > n c S 8 E J r 9 L m M F Q o 2 k P C j z X G H u T K R g 5 O d Y < l a t e x i s h 1 _ b 6 4 = \" F u J 3 M + g n 0 d y Y N G O z L > A B c V S 8 E U r q / 9 m k 7 Q o W 2 p Z I P C j X f H D w T 5 K R v < l a t e x i s h 1 _ b 6 4 = \" z / G 7 E A p 3 D 8 P v f g O Z S q k > B n c V N J U r 9 L 0 K Y F 2 o Q + I m u j w y d W X 5 C M R H T < l a t e x i s h 1 _ b 6 4 = \" z / G 7 E A p 3 D 8 P v f g O Z S q k > B n c V N J U r 9 L 0 K Y F 2 o Q + I m u j w y d W X 5 C M R H T < l a t e x i s h 1 _ b 6 4 = \" P + A c M p S w E y C g B r 7 9 j 8 G > X V D L F T W q o I N u z U R 3 Q v 2 Y m k K H / n 0 5 J O d Z f <latexi sh1_b64=\"w39j2gAG+UmT/BKyJk>ncZDLSMFIPuqXoV7C5v0EHQO8WfzdpNRYr < l a t e x i s h 1 _ b 6 4 = \" W 7 o f 0 Y 3 R r 8 5 y j + / G > A B n c Z D L S g M F I P K V u v T q z C k m U Q 9 N H 2 O p J X E d w < l a t e x i s h 1 _ b 6 4 = \" W 7 o f 0 Y 3 R r 8 5 y j + / G > A B n c Z D L S g M F I P K V u v T q z C k m U Q 9 N H 2 O p J X E d w < l a t e x i s h 1 _ b 6 4 = \" p K P n W R O E m / + I r g C V A > B c N S 8 J 3 U q j y 5 o L X Q Z T 0 Y 9 d 2 z H w v 7 G M k F f u D < l a t e x i s h 1 _ b 6 4 = \" r F 2 N W M 0 v Z f k X T 8 V U S + H > A B 9 c D L g 7 q o I E u C z R d 3 Q Y m j K / n y 5 J O P p G w < l a t e x i s h 1 _ b 6 4 = \" E q L A + f M O S w N C 5 Z o > B n c V 8 J 3 U r / 9 0 K Y F 2 p Q z D 7 I m G u j y d g W X v k P R H T < l a t e x i s h 1 _ b 6 4 = \" W 7 o f 0 Y 3 R r 8 5 y j + / G > A B n c Z D L S g M F I P K V u v T q z C k m U Q 9 N H 2 O p J X E d w < l a t e x i s h 1 _ b 6 4 = \" W 7 o f 0 Y 3 R r 8 5 y j + / G > A B n c Z D L S g M F I P K V u v T q z C k m U Q 9 N H 2 O p J X E d w < l a t e x i s h 1 _ b 6 4 = \" c Q q y U L 3 o R P r p n j V k > A B 9 X D J S g N E O 2 W v H w F G Z I Y z d T 8 / f 7 M m K 0 C u + 5 < l a t e x i s h 1 _ b 6 4 = \" Q g F o V J + L K P X 0 W O H c > A B 9 D S N E 2 y 3 q v j U w G Z I Y k z d n R T 8 / f 7 r M m C u p 5 < l a t e x i s h 1 _ b 6 4 = \" r y g I Z N 3 R 8 A w q S B 0 K d > + X c V D L F 2 p f U G m J E W u n H o 7 T M k Q / z v Y O j C P 5 9 Conference acronym 'XX, August 25-29, 2024, Barcelona, Spain Qian Zhao and Hao Qian, et al. jointly:  where Θ is the set of model parameters, 𝜆 1 , 𝜆 2 are hyperparameters to control the strengths of the diversity preserving loss. L 𝑚𝑎𝑖𝑛 is the Cross Entropy Loss of the main E-E-I weight decision task. 3.3.3 Integration Stage. To effectively and efficiently recommend items complemented by recent user bills to those with higher demand, we optimize both the recall module and the fine-ranking model, as shown in Fig. 4(b). Specifically, for the recall module, we added a new complementary recall route. To avoid excessive recall, we prepared a set of up to top-k newly recalled complementary items based on the scores from the E-E-I weight decision model and the recent bill entities retrieved from real-time requests by user ID. As for the fine-ranking model, during the training phase, we also introduce the E-E-I weight decision model to provide scores, entity embeddings, and item embeddings for current samples. The new recall module enables the downstream fine-ranking model to pay attention to complementary items, overcoming the limited input of complementary items caused by exposure bias in previous recommendation systems. The fine-ranking model combines the features of current complementary items and user profile behaviors to comprehensively and personalized sort candidate items.",
  "4 EXPERIMENTS": "To verify the effectiveness of the proposed LLM-KERec, we conduct extensive offline experiments utilizing the real industrial dataset procured from the Alipay online environment and report detailed analysis results. Moreover, we conduct online A/B tests in realworld marketing recommendation scenario to evaluate the performance of LLM-KERec in real industrial applications. This section encompasses a series of experiments designed to answer the following key questions: · Q1 : How does LLM-KERec perform when compared with other state-of-the-art (SOTA) baseline methods? (see Subsection 4.2) · Q2 : How does LLM-KERec perform in real-world industrial applications? (see Subsection 4.3) · Q3 : How do the distinct modules of LLM-KERec contribute to performance improvements? (see Subsection 4.4) · Q4 : How do the different large language models impact the performance of LLM-KERec? (see Subsection 4.5)",
  "4.1 Experimental Setups": "4.1.1 Datasets. This paper mainly focuses on recommendation in digital marketing scenarios, we utilize real-world industrial datasets 3 from Alipay. It includes three major marketing and recommendation scenarios within Alipay: Super 567 (Dataset A), Consumer Channel (Dataset B), and Payment Result Page (Dataset C). The Alipay application (APP) facilitates the presentation of numerous coupons to users through Super 567 and the Payment Result Page. The intention is to encourage user engagement by prompting them to click and collect these coupons, subsequently redeeming 3 The data set does not contain any Personal Identifiable Information (PII). The data set is desensitized and encrypted. Adequate data protection was carried out during the experiment to prevent the risk of data copy leakage, and the data set was destroyed after the experiment. Table 1: The statistics of three datasets. them through purchases made on Alipay. Moreover, within the Consumer Channel, the APP directly showcases goods that align with users' potential interests, aiming to stimulate clicks and subsequent purchases. Each day, a substantial user base, amounting to tens of millions, is exposed to the assortment of coupons and goods available on Alipay. To conduct our study, we randomly selected some instances spanning various dates over a one-month duration. The primary objective underlying data optimization efforts is to augment user conversions. These scenarios exhibit significant differences in terms of user population distribution, as well as user intentions and behaviors. They are further randomly divided into the disjoint training set, validation set, and test set. The statistics of these datasets are presented in Table 1. 4.1.2 Evaluation Metrics. In order to assess the overall system performance, we employ AUC (Area Under Curve) as the evaluation metric for offline experiments. Despite the actual industrial scenario being a ranking scenario, we simplify the offline experiments by treating them as a binary classification problem during modeling. In this approach, the model produces a score indicating whether the user likes (clicks or converts) the recommended item. Therefore, AUC is utilized for offline evaluation purposes. For online experiments, we directly measure the quality of different models by counting the number of clicks and conversions made by real users in various experimental groups. Consequently, the experimental group exhibiting a higher number of recommended items clicked and converted by users signifies better model performance. 4.1.3 Baselines. We choose the state-of-the-art recommendation system models as baselines for efficiency comparison. The baselines include DNN [6], Wide&Deep [3], DCN [20], ESMM [16], PLE [18] and Masknet [21].",
  "4.2 Offline Performance Comparison": "Table 2 presents the AUC results of the offline performance comparison for all methods. The Click and Conv. columns indicate the click AUCandconversion AUC values for the three datasets, respectively. In order to facilitate a more comprehensive comparison, we have incorporated the i-i graph into the baseline models, denoted as \"+ ii graph\". This adjustment has been implemented as our methodology capitalizes on graph-based techniques. The superior results are emphasized in bold, while the second-best results are denoted in underline. We utilize the symbol ' † ' to indicate that LLM-KERec exhibits a significant difference from the top-performing baseline, as determined by paired t-tests at a significance level of 0 . 01. Upon meticulous examination of the table, it is evident that LLM-KERec surpasses other methods in terms of AUC across the three datasets, Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph Conference acronym 'XX, August 25-29, 2024, Barcelona, Spain Table 2: Offline performance comparison, with evaluation metrics including click AUC and conversion AUC(conv.). The best results are bolded and the second best results are underlined. exhibiting superior performance across the majority of experimental outcomes.",
  "4.3 Online Performance Comparison": "To assess the effectiveness of LLM-KERec in real-world industrial scenarios, online A/B Tests were conducted across the three recommendation scenarios in Alipay: Super 567, Consumer Channel, and Payment Result Page. Evaluation metrics differed based on the dataset. For Dataset A (Super 567) and Dataset C (Payment Result Page), both representing coupon issuance scenarios, we employed #Click and #Conv as evaluation metrics. #Click denotes the number of coupons clicked by users, while #Conv signifies the number of converted items. On the other hand, for Dataset B (Consumer Channel), which represents a selling goods scenario, we used #Click and GMV as evaluation metrics. #Click represents the number of goods clicked by users, while GMV (Gross Merchandise Volume) indicates the total monetary value spent by users on purchasing goods. Our objective was to increase both coupon conversion and goods GMV. To conduct the A/B Tests, 10 percent of the actual online traffic was allocated, with the testing traffic candidates assigned randomly and evenly to two experimental groups. LLM-KERec was compared against the online baseline approach, which represents the existing model version serving all online users. Over a period of one month, data on #Click, #Conv, and GMV were collected for the different experimental groups. The results of the online experiments are summarized in Table 3. Due to commercial confidentiality, specific figures are withheld and represented with the symbol ' ∗ ∗ '. The percentage of relative improvement achieved by our method compared to the baseline is presented in the last row. The results demonstrate that our proposed LLM-KERec approach achieved a 6.24% increase and a 10.07% increase in #Conv for Dataset A and Dataset C, respectively. Additionally, a 6.45% increase in GMV was observed for Dataset B. The results of the A/B Test demonstrate the Table 3: The overall online performance comparison, where #conv. is number of coupon conversion and GMV is Gross Merchandise Volume. Note that the improvements achieved by LLM-KERec are statistically significant ( 𝑝 -value ≪ 0.05). significant improvements achieved by our method in real-world industrial recommendation scenarios.",
  "4.4 Ablation Study": "In order to comprehensively evaluate the impact of the U2E2I recall module and the E-E-I model ranking module on LLM-KERec, we conducted deeper ablation studies on Dataset A by selectively removing either the recall or ranking modules. The annotation w/o indicates the absence of the U2E2I recall module or the E-E-I model ranking module, while w/ signifies the inclusion of these modules. The results are show in Table 4 and final row represents the improvements achieved by retaining each respective module compared to removing it. The experimental findings presented in Table 4 demonstrate that both U2E2I recall and E-E-I model ranking modules contribute to an increase in clicks and conversions, thus affirming the effectiveness of our U2E2I recall module and E-E-I model. Table 4: The online ablation performance comparsion for Dataset C, where w/o and w/ represent without and with, respectively.",
  "4.5 Difference LLMs Comparison": "In this subsection, we perform a comparative analysis of different large language models, namely ChatGPT, ChatGLM, and Claude. To assess their performance, we randomly selected 1,000 complementary entity pairs from the generated complementary graphs of these models. These entity pairs were manually evaluated and assigned scores based on their relevance. The scoring scale consists of five levels: 1-Completely unrelated, 2-Somewhat unrelated, 3-Uncertain, 4-Somewhat related, and 5-Completely related. The numbers of entity pairs falling into each of these five levels are reported in Table 5. We then calculate the weighted average of these entity pairs using the following formula: Conference acronym 'XX, August 25-29, 2024, Barcelona, Spain Qian Zhao and Hao Qian, et al.",
  "1 × Completely Unrelated Num + . . . + 5 × Completely Related Num": "",
  "1000": "This calculation yields the final manual judgment score, which is presented as the last row in Table 5. Based on the manual judgment scores reported in Table 5, it is evident that the complementary entity pairs recommended by Claude exhibit a higher level of correlation. Table 5: Comparing the performance of complementary graphgenerated by different LLMs using five levels of manual annotation (randomly sampled 1000 entity pairs), a higher Mean Score indicates that the model's predictions are closer to human judgments. To provide a more comprehensive understanding, we also extract and present the instances of misjudgment made by ChatGPT and ChatGLM, where the models considered certain entity pairs as relevant, but they were manually determined as irrelevant. These instances are listed in Table 6. An analysis of the table reveals that ChatGPT associates 'Presbyopic Glasses\" with 'Makeup Remover\" based on the reasoning that Makeup Remover needs to be carefully applied by hand, and using a Presbyopic Glass after makeup removal can provide enhanced observation of the facial skin condition. ChatGLM, on the other hand, links 'Cake\" with 'Pajamas\" by suggesting that people may wear pajamas while eating cakes at night. We consider these explanations provided by the language models to be excessively imaginative, as they forcefully establish connections between these entity pairs. Table 6: The instances of problematic complementary entity pairs generated by the large language models (LLMs) from their respective complementary graphs. . Figure 5: The relative improvement of conversion rate (CVR) for randomly sampled complementary pairs in LLM-KERec compared to the baseline. 1.00 0.75 0.50 0.25 0.00 -0.25 -0.50 -0.75 -1.00 piggy bank chicken nuggets supermarket tea milk tea grape shared bike alcoholic beverages snack food delivery recharge call fees chicken leg fort movie tickets electric vehicle hotels card coupon noodles medicine box online ride hailing train tickets convenience stores local cuisine barbecue fruits VIP cards roll paper piggy bank chicken nuggets supermarket tea milk tea grape shared bike alcoholic beverages snack food delivery recharge call fees chicken leg fort movie tickets electric vehicle hotels card coupon noodles medicine box online ride hailing train tickets convenience stores local cuisine barbecue fruits VIP cards roll paper",
  "4.6 Case Study": "In this subsection, we present an additional case study focusing on the online experiment conducted on Dataset A. Specifically, we calculate and compare the Conversion Rate (CVR) of a sample set of complementary entity pairs recommended by LLM-KERec and the baseline model. The comparison results are depicted in Figure 5. In the figure, blank squares indicate a non-associative relationship between the two entity words, while colored squares indicate the improvement in CVR of the experimental group compared to the baseline group. Red squares represent a higher CVR in the experimental group compared to the baseline group, while blue squares indicate a lower CVR in the experimental group compared to the baseline group. As observed from Figure 5, the complementary pairs recommended by the experimental group generally exhibit a higher CVR than those recommended by the baseline group.",
  "5 CONCLUSION": "In this paper, we propose a novel LLM based Complementary Knowledge Enhanced Recommendation (LLM-KERec) System. It involves utilizing an entity extractor to extract unified concept terms from the information available for all items and user bills. To construct a complementary graph, we initially generate entity pairs on their popularity and designed strategies. Next, we leverage a large language model to uncover existing complementary purchasing relationship between each entity pairs. Furthermore, we incorporate a new complementary recall module and train the E-E-I weight decision model to enhance the ranking model's knowledge and facilitate the recommendation of complementary items. Comprehensive experiments demonstrate the effectiveness of our proposed LLM-KERec system. Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph Conference acronym 'XX, August 25-29, 2024, Barcelona, Spain",
  "REFERENCES": "[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models Are Few-Shot Learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS'20) . Curran Associates Inc., Red Hook, NY, USA, Article 159, 25 pages. [2] Ting Chen, Liangjie Hong, Yue Shi, and Yizhou Sun. 2017. Joint text embedding for personalized content-based recommendation. arXiv preprint arXiv:1706.01084 (2017). [3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [5] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 320-335. [6] Simon Haykin. 1998. Neural Networks: A Comprehensive Foundation (2nd ed.). Prentice Hall PTR, USA. [7] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations . https: //openreview.net/forum?id=nZeVKeeFYf9 [8] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . 169-177. [9] Bei Hui, Lizong Zhang, Xue Zhou, Xiao Wen, and Yuhui Nian. 2022. Personalized recommendation system based on knowledge embedding and historical behavior. Applied Intelligence (2022), 1-13. [10] Thomas N. Kipf and Max Welling. 2019. Semi-supervised classification with graph convolutional networks. 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings (2019), 1-14. arXiv: 1609.02907. [11] John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning (ICML '01) . Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 282-289. [12] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 3045-3059. https: //doi.org/10.18653/v1/2021.emnlp-main.243 [13] Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. 2022. A Survey on Deep Learning for Named Entity Recognition. IEEE Trans. on Knowl. and Data Eng. 34, 1 (jan 2022), 50-70. https://doi.org/10.1109/TKDE.2020.2981314 [14] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1754-1763. [15] Wei Liu, Tongge Xu, Qinghua Xu, Jiayu Song, and Yueran Zu. 2019. An Encoding Strategy Based Word-Character LSTM for Chinese NER. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for Computational Linguistics, Minneapolis, Minnesota, 2379-2389. https://doi.org/10.18653/v1/N19-1247 [16] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 1137-1140. [17] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [18] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations. In RecSys 2020: Fourteenth ACM Conference on Recommender Systems, Virtual Event, Brazil, September 22-26, 2020 , Rodrygo L. T. Santos, Leandro Balby Marinho, Elizabeth M. Daly, Li Chen, Kim Falk, Noam Koenigstein, and Edleno Silva de Moura (Eds.). ACM, 269-278. https://doi.org/10.1145/3383313.3412236 [19] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. http://arxiv.org/abs/2302.13971 cite arxiv:2302.13971. [20] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 . 1-7. [21] Zhiqiang Wang, Qingyun She, and Junlin Zhang. 2021. MaskNet: Introducing feature-wise multiplication to CTR ranking models by instance-guided mask. (2021). [22] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022). [23] Yue Zhang and Jie Yang. 2018. Chinese NER Using Lattice LSTM. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Melbourne, Australia, 1554-1564. https://doi.org/10.18653/v1/P18-1144 Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009",
  "keywords_parsed": [
    "Recommendation system",
    " Large language model",
    " Knowledge graph"
  ]
}