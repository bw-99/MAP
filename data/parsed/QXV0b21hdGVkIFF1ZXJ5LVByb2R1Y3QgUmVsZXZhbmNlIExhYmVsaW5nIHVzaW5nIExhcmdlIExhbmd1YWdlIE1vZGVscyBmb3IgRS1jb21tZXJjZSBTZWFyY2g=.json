{"Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search": "Jayant Sachdev \u2020 jayant.sachdev@walmart.com Walmart Global Tech Sunnyvale, CA, USA Sean D Rosario \u2217\u2020 sean.drosario@walmart.com Walmart Global Tech Sunnyvale, CA, USA", "He Wen": "vicky.wen@walmart.com Walmart Global Tech Sunnyvale, CA, USA", "Swati Kirti": "swati.kirti@walmart.com Walmart Global Tech Sunnyvale, CA, USA", "ABSTRACT": "", "Abhijeet Phatak \u2020": "abhijeet.phatak@walmart.com Walmart Global Tech Sunnyvale, CA, USA Chittaranjan Tripathy ctripathy@walmart.com Walmart Global Tech Sunnyvale, CA, USA", "1 INTRODUCTION": "Accurate query-product relevance labeling is indispensable to generate ground truth dataset for search ranking in e-commerce. Traditional approaches for annotating query-product pairs rely on human-based labeling services, which is expensive, time-consuming and prone to errors. In this work, we explore the application of Large Language Models (LLMs) to automate query-product relevance labeling for large-scale e-commerce search. We use several publicly available and proprietary LLMs for this task, and conducted experiments on two open-source datasets and an in-house e-commerce search dataset. Using prompt engineering techniques such as Chain-of-Thought (CoT) prompting, In-context Learning (ICL), and Retrieval Augmented Generation (RAG) with Maximum Marginal Relevance (MMR), we show that LLM's performance has the potential to approach human-level accuracy on this task in a fraction of the time and cost required by human-labelers, thereby suggesting that our approach is more efficient than the conventional methods. We have generated query-product relevance labels using LLMs at scale, and are using them for evaluating improvements to our search algorithms. Our work demonstrates the potential of LLMs to improve query-product relevance thus enhancing ecommerce search user experience. More importantly, this scalable alternative to human-annotation has significant implications for information retrieval domains including search and recommendation systems, where relevance scoring is crucial for optimizing the ranking of products and content to improve customer engagement and other conversion metrics.", "CCS CONCEPTS": "\u00b7 Computing methodologies \u2192 Information extraction ; Natural language generation ; Language resources; \u00b7 Information systems \u2192 Relevance assessment ; Search engine indexing; Novelty in information retrieval .", "KEYWORDS": "Large Language Models, Search Relevance, Information Retrieval, Data Annotation, Prompt Engineering, Retrieval Augmented Generation, Maximum Marginal Relevance. \u2217 Corresponding author \u2020 These authors contributed equally to this research. When a customer performs a search on an e-commerce platform like Walmart, it is essential that the search results show the exact products or relevant products they are looking for, saving the customer time and effort while providing a positive shopping experience. The product search experience on e-commerce platforms is powered by a collection of machine learning models and algorithms across the stages of pre-retrieval, retrieval, and post-retrieval ranking and filtering [20]. Many of these models rely on accurate query-product (Q-P) relevance label data for training and testing. Specifically, these labels indicate how relevant a given product is to a customer search query. For example, the product ' Ergonomic PU leather high back office chair with flip-up armrest ' is highly relevant to the search query ' leather chair ', while the product ' hard floor beveled edge chair mat ' is not relevant to the same query. We have worked with an external data labeling service which employs human labelers. The labelers were given Q-P pairs with detailed instructions and examples to guide labelers. They were not given any information on customer engagement data for Q-P pairs. Typically, three to five independent assessments are gathered per Q-P pair. Despite the guidelines, human labelers do not always reach consensus on an annotation, hence labeled data is often noisy. Furthermore, external human labeling is expensive and time-consuming, and hence only Q-P pairs corresponding to a small fraction of all queries can be labeled. Despite these challenges, external human labelers are considered as the most reliable method of acquiring data annotations, and is widely used across e-commerce and internet industries. Previously, Q-P relevance labels were obtained from clickstream data and human annotation as shown in Figure 1(a). Higher engagement of a product given a search query would be considered a positive signal for relevance. However, cold-start products (such as newly introduced products and tail products that are sold infrequently) may lack sufficient impressions, click-through-rate (CTR), or add-to-cart (ATC) rate information due to their limited exposure to the customers. Furthermore, some products might get a lot of impressions and clicks even if they are not very relevant to the search query. For example, the product ' fresh lime ' gets a relatively high number of impressions and clicks for the query ' lemon ', but it is not a very relevant product. Identifying the strong need for an automated framework Jayant Sachdev, Sean D Rosario, Abhijeet Phatak, He Wen, Swati Kirti, and Chittaranjan Tripathy (a) Previous approaches of acquiring Q-P relevance QP Pairs Relevance Labels Customer Engagement Instructions with Examples Relevance Labels Pairs Third-Party Human Annotators (b) Our approach to annotating Q-P relevance Pairs Retrieval Augmentation Shot Examples Examples Few-Shot Exanples Examples Large Language Diversely Models Relevance Labels Sanpled Figure 1: Comparison of pipeline architectures for Q-P relevance annotation for Q-P relevance labeling, in this work, we demonstrate that Large Language Models (LLMs) can be used successfully to obtain accurate relevance labels in a fully automated manner and at scale. A schematic of our approach is shown in Figure 1(b), which we will detail in the subsequent sections. We have experimented with several LLMs for this task. We have explored different e-commerce search datasets, as well as ablation studies on the effectiveness of prompt engineering techniques including Chain-of-Thought (CoT) prompting, In-context Learning (ICL), and Retrieval Augmented Generation (RAG) with Maximum Marginal Relevance (MMR). Figure 2 shows Retrieval-Augmented Generation (RAG) architecture, designed and used in our work, which contains a retrieval component that retrieves relevant labeled query-product pairs for a given query-product pair. These relevant query-product pairs, providing the contextual information, are then used to create prompts which are provided as input to the LLMs.", "2 BACKGROUND": "This work is an application of data-centric Artificial Intelligence (AI), which is systematic engineering of the data used to build an AI system, including but not limited to automated data labeling, collection, augmentation, and cleaning [29]. One approach to automated data labeling is called programmatic labeling which includes using human-defined labeling functions (LFs) that capture labeling logic to auto-generate large training sets [21]. Programmatic labeling significantly increases efficiency over manual labeling through automating the labeling process. However, labeling decisions still require human domain expertise on what heuristics should be applied [9].", "2.1 Large Language Models": "Large Language Models can generate human-like text and demonstrate state-of-the-art logical reasoning ability across a broad range of reasoning tasks and benchmarks [7]. With advancements in the field, open-source models have also proven to be an effective alternative to GPT models in multiple reasoning tasks [12, 13, 18, 25]. Prior work has shown that LLMs can be used to label data at par with human labeling on a variety of NLP tasks, with up to 96% saving in costs [26]. LLMs are proven to be a viable substitute to crowd-sourced annotators and can match or surpass crowd-sourced annotators [10]. Recent work has also explored the idea of using LLMs to annotate shopping relevance [8, 24].", "2.2 Mathematical Formulation": "Let ( \ud835\udc44, \ud835\udc43 ) denote the set of all possible Q-P pairs. We define the task of relevance labeling as a classification problem with the candidate classes set Y = { \ud835\udc66 1 , \ud835\udc66 2 , . . . , \ud835\udc66 \ud835\udc5b } . We want to find a map \u03a6 \u210e : ( \ud835\udc44, \ud835\udc43 ) \u2192 R Y that is able to ideally score the relevance for the Q-P pairs against the candidate classes. We assume \ud835\udc44 and \ud835\udc43 are infinite sets [1]. Given an input query \ud835\udc5e \u2208 \ud835\udc44 and product \ud835\udc5d \u2208 \ud835\udc43 , we want to maximize the probability  where  \u03a6 \u210e can be expressed as a function of the choice of the LLM and associated sampling parameters ( L ) along with the baseline input prompt or context ( P ) to the LLM. With advanced prompting techniques, we modify the context to include additional instructions \ud835\udc3c and set of \ud835\udc58 few shot examples or demonstrations that are added to the input context provided to the LLM. This can be represented as P \ud835\udc3c,\ud835\udc58 , and we can express  where \u03a8 (\u00b7 , \u00b7) is a function of L and P \ud835\udc3c,\ud835\udc58 . In case of Chain-ofThought (CoT) prompting, we include additional instructions to step by step reason through the generation process that is I \ud835\udc36\ud835\udc5c\ud835\udc47 . With crowd-sourced data annotation, we first need to provide the annotators with a thorough description of the labeling task, with meanings of each category and specific examples with explanations. Therefore, using the same approach, we guide LLMs to annotate data by providing the task description and some labeled examples. Weobserve that prompting the LLM to explain the reasoning behind the provided annotation can improve the labeling accuracy. We use RAG to retrieve most similar and hence most relevant demonstrations or examples that are represented as \ud835\udc58 \ud835\udc45\ud835\udc34\ud835\udc3a . Since we use the retriever to provide additional context simply by concatenating the context with these examples. RAG exhibits significant potential in enhancing the quality of prompts by providing external knowledge as a preliminary step before generating the response, thereby ensuring contextual precision and a higher degree of detail. We use Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search Figure 2: System Architecture for Automated Q-P relevance with LLM and RAG Generation Retrieval 3) Prompt Creation Indexing QP pairs Final Task Prorpt 1 Input WANDS Vector Prompt DataSet Similar Pairs with labels Large Language Model Search LLM Factory Labelled Labelled QP pairs Retrieval Output pairs labels Maximum Marginal Relevance (MMR) [3] retrieval to iteratively find documents that are dissimilar to previous results. MMR has been shown to improve retrievals in LLMs [28]. Consider the set \ud835\udc37 consisting of all candidate documents and \ud835\udc45 consisting of the previously chosen (retrieved) documents, and \ud835\udc5e representing a query. We also consider two similarity functions \ud835\udc46\ud835\udc56\ud835\udc5a 1 that compares a document with the query, and \ud835\udc46\ud835\udc56\ud835\udc5a 2 that assesses the similarity between two documents. Let \ud835\udc51 \ud835\udc56 and \ud835\udc51 \ud835\udc57 denote individual documents in \ud835\udc37 and \ud835\udc45 . Then MMR is given by the following formula: a certain answer and helping identify where the reasoning may have gone wrong. Using CoT, we provided the reasoning as to why certain label was selected for each of the examples used in few-shot prompting. There are multiple ways to invoke a CoT response. In our experiments we use the words 'Let's think step by step' in the few-shot examples to invoke CoT responses [14]. MMR = arg max \ud835\udc51\ud835\udc56 \u2208 \ud835\udc37 \\ \ud835\udc45 [ \ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 \u00b7 \ud835\udc46\ud835\udc56\ud835\udc5a 1 ( \ud835\udc51 \ud835\udc56 , \ud835\udc5e ) - ( 1 -\ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 ) \u00b7 max \ud835\udc51 \ud835\udc57 \u2208 \ud835\udc45 \ud835\udc46\ud835\udc56\ud835\udc5a 2 ( \ud835\udc51 \ud835\udc56 , \ud835\udc51 \ud835\udc57 ) ] , where the first term and the second term respectively represent relevance and diversity. The adjustable parameter \ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 balances the importance of relevance and diversity. \ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 \u2192 1 prioritizes relevance whereas \ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 \u2192 0 prioritizes diversity. Finally, we select the optimal map \u02c6 \u03a6 \u210e through experiments on ground truth data and ablation studies.", "3 METHODS": "", "3.1 Prompt Engineering": "Prompt engineering is the systematic design and optimization of input prompts to guide the LLM responses, ensuring accuracy, relevance, and coherence in the generated output [4]. 3.1.1 In-Context Learning (Few-Shot Prompting). In-context learning refers to a setting in which the model is given a few demonstrations of the task at inference time, but no weight and gradient updates are allowed [2, 6]. The major advantage of this technique is reduced reliance on task-specific data and does not require finetuning on domain specific datasets. 3.1.2 Chain-of-Thought (CoT) Prompting. CoT prompting can enhance reasoning in language models by decomposing problems into intermediate steps [27]. It provides an interpretable insight into the model's behavior, revealing how it might have reached 3.1.3 Retrieval Augmented Generation (RAG). RAG technique utilizes input to identify relevant information for the task at hand, which is then used to offer additional context in the formation of input prompt to the LLMs. The RAG architecture contains two main components: a retriever that identifies relevant information (documents) in response to input query, and a generator that generates the ouput, taking into account the context of retrieved information, the initial query, and required task information. It has been noted that extensive pre-trained language models have the capacity to encapsulate factual knowledge within their parameters and can attain superior outcomes when they are fine-tuned for distinct NLP tasks. Despite this, their proficiency in accessing and manipulating knowledge precisely is still lacking, leading to less than satisfactory performance in tasks that are heavily reliant on knowledge[16]. As shown in Figure 2, given a query-product pair, using the RAG technique, we provide the relevant context information about the query-product pair to the LLMs, which proves to be beneficial in generating the correct relevance labels. 3.1.4 Maximum Marginal Relevance Retrieval (MMR). Retrieving similar Q-P pairs as few shot examples with RAG often leads to a set of examples with a high degree of overlap. Hence the amount of new information that each example adds to the prompt is limited. In order to overcome this, we added a component of diversity into the few-shot examples retrieved, as described in the previous section. MMR ensures that examples retrieved are both relevant to the question Q-P pair, while being sufficiently diverse. We experiment Jayant Sachdev, Sean D Rosario, Abhijeet Phatak, He Wen, Swati Kirti, and Chittaranjan Tripathy with different values of \ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 to find the optimal balance between similarity and diversity.", "3.2 Details of our Prompt": "Our prompt consists of three parts. First, we have a context prompt which specifies the instruction \ud835\udc3c to the LLM including the classes relevance labels with explanations of each label and the expected output structure. Second, we do in-context learning by providing sample inputs (search query and product title) and the outputs (corresponding relevance labels). For CoT, we included more instructions in the instruction prompt \ud835\udc3c and provided annotated few-shot examples. Lastly, we provide the Q-P actual pairs we want labeled by the LLM (see Figure 2 for a schematic).", "3.3 Datasets": "We perform experiments on two publicly available datasets and also on our proprietary dataset to validate the effectiveness of our methods. ESCI Dataset: ESCI dataset [22] is an e-commerce dataset that has Q-P pairs that are human-annotated with corresponding relevance labels. We assume that the annotations provided are accurate, and serve as ground truth labels. There are 4 classes for relevance labels: Exact , Substitute , Complement , and Irrelevant . We sampled 5000 Q-P pairs from the test split of the ESCI dataset, with an even split across the four class labels to create a test set for experimentation and evaluation purposes. WANDS Dataset: WANDS [5] is another e-commerce dataset that contains Q-P pairs that are human-annotated with the corresponding relevance labels. We assume that the annotations provided are accurate, and serve as ground truth labels. There are 3 classes for relevance labels: Exact , Partial , and Irrelevant . Since there is no train/test split in the data, we sampled 5000 Q-P pairs evenly across the three class labels to create a test set for experimentation and evaluation purposes. Walmart Mexico Search Dataset: Our primary data source is Walmart Mexico search session data, which contains customer search queries and product impressions. This data is proprietary and acquired from Walmart's internal databases. We sampled 100 queries uniformly across search traffic segments. For each of the 100 sampled queries, we sample 10 products retrieved before reranking. Thus, we have 1000 distinct query product pairs in our ground truth data, mostly in Spanish. Domain experts have manually annotated relevance across the 1000 Q-P pairs. The ground truth data has 5 relevance classes: Excellent , Good , Okay , Bad , and Embarrassing . The data is heavily skewed towards Excellent because the products per query are sampled from search retrieval data, which is likely to be more relevant than not. Hence, for this dataset, weighted F1 score is a more reliable metrics than average F1 score.", "3.4 Experimental Setup": "Across all three datasets, we experimented with 8 Few-Shot examples and 16 Few-Shot examples for in-context learning. These examples could be randomly sampled from the training set (referred to as FS in the tables and figures), or retrieved based on embedding distance from the test question (referred to as FS_RAG in the tables and figures), and diversity could be introduced to the RAG examples (referred to as FS_RAG_MMR in tables and figures). For experiments with Walmart Mexico data, we sampled some Q-P pairs outside the test set from historical search relevance data to be used as Few-Shot (FS) examples. Explanations for few shot example were also manually annotated for CoT examples. Third-party human annotations serve as the baseline for experiments on this data. For the ESCI dataset, FS examples were randomly sampled from the train split of the ESCI dataset. For the prompt, we made use of descriptions of relevance labels as defined in the [5, 22] for the experiments on WANDS and ESCI respectively. We used an 80GB GPU for running inference of open source LLMs: LLM1 is an 8 billion parameter LLM [18], and LLM2 is a 70 billion parameter version of the same model. LLM3 is another open-source model with 7 billion parameters [12], and LLM4 [13] is a Mixture-of-Experts model based on 8 models of LLM3. We used vLLM [15] that uses PagedAttention to speed up the inference to make our approach more scalable and resource friendly. We used Activation-aware Weight Quantized (AWQ) models [17] that could not be fit in a single 80GB GPU like LLM2 and LLM4. LLM5 [23] is not open-sourced, so we used a paid API provided to perform the LLM calls. To implement RAG as shown in Figure 2, we used ChromaDB [11] as our vector store for retrieving similar query-product pairs from training set. We then use this retrieved query-product pairs along with our task prompt, and input query-product pair to create an input prompt for the LLM. We used scikit-learn [19] implementations for computing evaluation metrics that are described in the subsequent section in detail.", "3.5 Evaluation Metrics": "We compute the following metrics across LLM predictions on the aforementioned datasets. Accuracy: Accuracy ( \ud835\udc66, \u02c6 \ud835\udc66 ) = 1 \ud835\udc5b samples \u02dd \ud835\udc5b samples \ud835\udc56 = 1 1 ( \u02c6 \ud835\udc66 \ud835\udc56 = \ud835\udc66 \ud835\udc56 ) ,where \u02c6 \ud835\udc66 \ud835\udc56 is the predicted value of the \ud835\udc56 -th sample and \ud835\udc66 \ud835\udc56 is the corresponding true value, then the fraction of correct predictions over all \ud835\udc5b \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc60 gives us the accuracy score. 1 ( \ud835\udc65 ) is the indicator function. Average \ud835\udc39 1 Score: \ud835\udc39 1 = 2 \ud835\udc43\ud835\udc45 \ud835\udc43 + \ud835\udc45 , where \ud835\udc43 is Precision and \ud835\udc45 is Recall. This measures the prediction accuracy across all relevance classes without accounting for class imbalance. Higher Average \ud835\udc39 1 score means better classification performance. Weighted \ud835\udc39 1 Score: \ud835\udc39 \ud835\udf14 1 = \u02dd \ud835\udc5b \ud835\udc56 \ud835\udc57 = 1 \u02c6 \ud835\udc64 \ud835\udc56 \ud835\udc39 1 ( \ud835\udc56 ) To compute the weighted \ud835\udc39 1 score the \ud835\udc39 1 score for each class is calculated individually, and averaged using normalized support weights per label. This can result in a better measure than average \ud835\udc39 1 score, as it accounts for class imbalance. Higher weighted \ud835\udc39 1 score means better classification performance.", "4 RESULTS AND DISCUSSION": "", "4.1 Experiment Metrics": "Across experiments on all three datasets, we observe LLM2 and LLM5 outperform all other models (Please refer to Fig 3, and for the numeric outputs in Table 3). Our results show that providing FS examples improves accuracy over zero-shot in most cases, and RAG FS offers further improvements in accuracy metrics in some Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search instances (see Table 3). However, we observe that the best accuracy is obtained when RAG is used in combination with diversification by MMR, which we believe is due to how the contextual information exploited by the different LLMs. Further, for experiments with 8 and 16 few shot examples using RAG to select examples, we tried various values of \ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 \u2208 [ 0 . 75 , 0 . 5 , 0 . 25 , 0 . 0 ] . Setting \ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 = 1 is the same as the FS_RAG experiments because retrieval is solely based on relevance thus inducing zero diversity. We observe that for different datasets, and different values of FS, the optimal value of \ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 up to the resolution of our grid (0.25), can be different. Overall, we observe that diversity helps in improving accuracy. We observe that increasing diversification i.e. lower values of \ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 improves accuracy metrics, as shown in Fig 4. This is because the retrieved examples may have some degree of semantic overlap, hence diversification helps add novel information within the prompt. We also include sample prompts from our experiments with WANDS data in the Appendix A1 and A2. A sample set of Q-P pairs on the ESCI data is shown in Table 1 and a sample set of Q-P pairs on the WANDS data is shown in Table 2. For some predictions that do not match the ground truth, such as Example #2 in Table 1 and Example #4 in Table 2, the prediction labels are arguably more accurate than the ground truth labels. We can potentially use this approach to improve these datasets by identifying errors in the existing humanannotated ground truth labels. For Walmart Mexico data, LLM5 with 16 FS_RAG_MMR with \ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 = 0 . 75 approaches human-level weighted F1 Score (see Table 3). Table 1: Example results on ESCI data", "4.2 Time and Cost Analysis": "External human-annotations had a turnaround time of about 2-3 weeks. Labelers often take about a week to familiarize themselves with the task requirements, and then another 2 weeks to manually label the 1000 records (Q-P pairs) they are provided. Therefore, turnaround for third party human-annotations is estimated to be about 30 mins/record. In contrast, our experiments show that LLMs do the task in a fraction of the time. Our best performing configuration for LLM5 takes a little over 3 hours to complete, so the upper bound on LLMs for this annotation task is about 8.3 seconds/record. However, with open-source models most experiments ran in under 5 minutes on a GPU machine with an inference time of about 0.3 seconds/record. These models demonstrate a significantly efficient run time for large datasets such as those from Walmart's e-commerce Table 2: Example results on WANDS data ecosystem, thereby offering a viable alternative. Moreover, the opensource nature of these models confers an added advantage in terms of flexibility, allowing for more customized adjustments according to specific requirements through fine-tuning. The costs of using LLMs are also significantly less than thirdparty human labelers. According to our estimates, labeling each Q-P pair is about 500 times cheaper with LLMs than human labelers, even when considering compute resources and API costs. LLMs have the potential to be even more cost-effective with techniques like batching of inputs/outputs, and conciseness of prompts. Opensource models may be a more suitable choice over proprietary LLMs in many situations since they can be fine-tuned on custom data to achieve better performance and mitigate any potential data privacy/leakage concerns.", "5 CONCLUSION": "Our work demonstrates the potential of LLMs in providing accurate and reliable query-product relevance data at scale. By leveraging LLMs with the state-of-the-art prompt engineering techniques, we are able to achieve accuracy comparable to human-labelled query-product pairs, while significantly reducing the time and cost required. Our experiments demonstrated the novel use of MMR in RAGfor query-product relevance labeling has shown good improvements and strong promise for future work. We also showed how techniques like Chain-of-Thought and Few Shot prompting can help improve the accuracy of LLMs for this task. This scalable alternative to traditional human-annotation methods has far-reaching implications for information retrieval domains such as search and recommendation. One could consider using product descriptions and other attributes that are often available in the product catalogs and taxonomies, however, they are often noisy and would increase the number of input tokens to the LLMs which would have additional computational cost. Our productionized approach within Walmart Global Tech has already yielded a large number of query-product relevance labels, which are currently being used to evaluate improvements to various search algorithms. This not only improves the efficiency of our search systems but also enables us to provide better user experiences through more relevant search Jayant Sachdev, Sean D Rosario, Abhijeet Phatak, He Wen, Swati Kirti, and Chittaranjan Tripathy (a) Accuracy of experiments on ESCI data ESCI 0.85 0.25 0.20 0.15 0.10 4 WANDS (b) Accuracy of experiments on WANDS data 0.65 1 0 15 0.15 0.05 0.00 1 Walmart (c) Accuracy of experiments on Walmart data 0.80 0,35 0.15 005 Figure 3: Color Code: \u25a0 LLM1, \u25a0 LLM2, \u25a0 LLM3, \u25a0 LLM4, \u25a0 LLM5 results. Furthermore, our method's broad applicability across various information retrieval domains makes it a valuable tool for optimizing products and content for greater user interactions and revenue generation.", "REFERENCES": "[1] Alireza Bagheri Garakani, Fan Yang, Wen-Yu Hua, Yetian Chen, Michinari Momma, Jingyuan Deng, Yan Gao, and Yi Sun. 2022. Improving Relevance Quality in Product Search using High-Precision Query-Product Semantic Similarity. In Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5) , Shervin Malmasi, Oleg Rokhlenko, Nicola Ueffing, Ido Guy, Eugene Agichtein, and Surya Kallumadi (Eds.). Association for Computational Linguistics, Dublin, Ireland, 44-48. https://doi.org/10.18653/v1/2022.ecnlp-1.6 [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901. [3] Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval . 335-336. [4] Banghao Chen, Zhaofeng Zhang, Nicolas Langren\u00e9, and Shengxin Zhu. 2023. Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. arXiv:2310.14735 [cs.CL] [5] Yan Chen, Shujian Liu, Zheng Liu, Weiyi Sun, Linas Baltrunas, and Benjamin Schroeder. 2022. WANDS: Dataset for Product Search Relevance Assessment. In Proceedings of the 44th European Conference on Information Retrieval . 12 pages. [6] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A Survey on In-context Learning. arXiv:2301.00234 [cs.CL] [7] OpenAI: Achiam et al. 2023. GPT-4 Technical Report. https://api.semanticscholar. org/CorpusID:257532815 [8] Guglielmo Faggioli, Laura Dietz, Charles LA Clarke, Gianluca Demartini, Matthias Hagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, Benno Stein, et al. 2023. Perspectives on large language models for relevance judgment. In Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval . 39-50. [9] Naiqing Guan, Kaiwen Chen, and Nick Koudas. 2023. Can Large Language Models Design Accurate Label Functions? arXiv:2311.00739 [cs.CL] [10] Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, and Weizhu Chen. 2023. AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators. arXiv:2303.16854 [cs.CL] [11] Jeff Huber. 2024. Chroma DB . https://pypi.org/project/chromadb/ [12] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). [13] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2024. Mixtral of Experts. arXiv:2401.04088 [cs.LG] https://arxiv.org/abs/2401.04088 [14] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large Language Models are Zero-Shot Reasoners. arXiv:2205.11916 [cs.CL] [15] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles . [16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459-9474. [17] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. Awq: Activationaware weight quantization for llm compression and acceleration. arXiv preprint arXiv.2306.00978 (2023). [18] AI @ Meta Llama Team. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783 [19] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search Learning in Python. Journal of Machine Learning Research 12 (2011), 2825-2830. [20] Yiming Qiu, Chenyu Zhao, Han Zhang, Jingwei Zhuo, Tianhao Li, Xiaowei Zhang, Songlin Wang, Sulong Xu, Bo Long, and Wen-Yun Yang. 2022. Pre-training Tasks for User Intent Detection and Embedding Retrieval in E-commerce Search. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM '22) . Association for Computing Machinery, New York, NY, USA, 4424-4428. https://doi.org/10.1145/3511808.3557670 [21] Alexander J. Ratner, Stephen H. Bach, Henry R. Ehrenberg, Jason Alan Fries, Sen Wu, and Christopher R\u00e9. 2017. Snorkel: Rapid Training Data Creation with Weak Supervision. Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases 11 3 (2017), 269-282. https://api.semanticscholar.org/ CorpusID:6730236 [22] Chandan K. Reddy, Llu\u00eds M\u00e0rquez, Fran Valero, Nikhil Rao, Hugo Zaragoza, Sambaran Bandyopadhyay, Arnab Biswas, Anlu Xing, and Karthik Subbian. 2022. Shopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving Product Search. (2022). arXiv:2206.06588 [23] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024). [24] Beatriz Soviero, Daniel Kuhn, Alexandre Salle, and Viviane Pereira Moreira. 2024. ChatGPT Goes Shopping: LLMs Can Predict Relevance in eCommerce Search. In European Conference on Information Retrieval . Springer, 3-11. [25] Tyler Vergho, Jean-Francois Godbout, Reihaneh Rabbany, and Kellin Pelrine. 2024. Comparing GPT-4 and Open-Source Language Models in Misinformation Mitigation. arXiv:2401.06920 [cs.CL] [26] Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. 2021. Want To Reduce Labeling Cost? GPT-3 Can Help. In Findings of the Association for Computational Linguistics: EMNLP 2021 , Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Punta Cana, Dominican Republic, 4195-4205. https: //doi.org/10.18653/v1/2021.findings-emnlp.354 [27] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL] [28] Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoyanov, Greg Durrett, and Ramakanth Pasunuru. 2022. Complementary explanations for effective in-context learning. arXiv preprint arXiv:2211.13892 (2022). [29] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. 2023. Data-centric Artificial Intelligence: A Survey. arXiv:2303.10158 [cs.LG]", "A APPENDIX": "", "A.1 Sample prompt : WANDS Dataset - 8 FS with RAG": "You are a s e a r c h engine i n an eCommerce website . For a given customer query and a product t i t l e , p Exact : t h i s l a b e l r e p r e s e n t s t h e s u r f a c e d product f u l l y matches t h e s e a r c h query . P a r t i a l : t h i s l a b e l r e p r e s e n t s t h e s u r f a c e d product t h a t does not f u l l y match t h e s e a r c h query . I I r r e l e v a n t : t h i s l a b e l i n d i c a t e s t h e product i s not r e l e v a n t t o t h e query . The r e s p o n s e should be i n a python di c t i o n a r y f o r m a t { \" r a t i n g \" : l a b e l } , where l a b e l which i s e i t h e r #### Here are some examples : query : wood c o f f e e t a b l e s e t by s t o r a g e , product t i t l e : c o f f e e { ' r a t i n g ' : ' Partial ' } t a b l e query : wood c o f f e e t a b l e s e t by s t o r a g e , product t i t l e : c o f f e e t a b l e with s t o r a g e { ' r a t i n g ' : ' Partial ' } query : wood c o f f e e t a b l e s e t by s t o r a g e , product t i t l e : onshuntay c o f f e e t a b l e { ' r a t i n g ' : ' Partial ' } query : wood c o f f e e t a b l e s e t by s t o r a g e , product t i t l e : wooden c o f f e e t a b l e { ' r a t i n g ' : ' Partial ' } query : wood c o f f e e t a b l e s e t by s t o r a g e , product t i t l e : wood c o f f e e t a b l e { ' r a t i n g ' : ' Partial ' } query : wood c o f f e e t a b l e s e t by s t o r a g e , product t i t l e : ahern c o f f e e t a b l e { ' r a t i n g ' : ' Partial ' } query : wood c o f f e e t a b l e s e t by s t o r a g e , product t i t l e : fromm wood t a b l e { ' r a t i n g ' : ' Partial ' } query : wood c o f f e e t a b l e s e t by s t o r a g e , product t i t l e : bahareh c o f f e e t a b l e { ' r a t i n g ' : ' Partial ' } Now r a t e t h e r e l e v a n c e of t h i s pair : query : wood c o f f e e t a b l e s e t by s t o r a g e , product t i t l e : mikell 2 piece c o f f e e t a b l e s e t In the prompt above, there are some examples from RAG that have a high degree of overlap like the product titles \"wooden coffee table\" and \"wood coffee table\". This issue is overcome by MMR-based diversity, as seen in the next prompt Jayant Sachdev, Sean D Rosario, Abhijeet Phatak, He Wen, Swati Kirti, and Chittaranjan Tripathy", "A.2 Sample prompt : WANDS Dataset - 8 FS RAG MMR \ud835\udf06 \ud835\udc40\ud835\udc40\ud835\udc45 = 0": "", "A.3 Experimental results": "You are a s e a r c h engine i n an eCommerce website . For a given customer query and a product t i t l e , please annotate each product t i t l e i n t h e l i s t as one of t h e s e options : ' Exact ' , ' Partial ' , ' I r r e l e v a n t ' Exact : t h i s l a b e l r e p r e s e n t s t h e s u r f a c e d product f u l l y matches t h e s e a r c h query . Table 3: Experimental results Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search", "A.4 Plots for MMR Experiments": "ESCI 0,530 0.325 9 0,520 0.515 0.510 1 4 1", "(a) ESCI MMR experiments": "WANDS [ 0,75 0.70 0.65 11 4", "(b) WANDS MMR experiments": "Figure 4: Color Code: \u25a0 LLM2, \u25a0 LLM5 (c) Walmart MMR experiments Walmart 0.54 0.52 0.50 9 0.48 0.416 I"}
