{
  "RUEL: Retrieval-Augmented User Representation with Edge Browser Logs for Sequential Recommendation": "Ning Wu Microsoft STCA China wuning@microsoft.com Jian Pei Duke University USA j.pei@duke.edu Ming Gong ∗ Microsoft STCA China Linjun Shou Microsoft STCA China migon@microsoft.com lisho@microsoft.com Daxin Jiang Microsoft STCA China djiang@microsoft.com",
  "ABSTRACT": "Online recommender systems (RS) aim to match user needs with the vast amount of resources available on various platforms. A key challenge is to model user preferences accurately under the condition of data sparsity. To address this challenge, some methods have leveraged external user behavior data from multiple platforms to enrich user representation. However, all of these methods require a consistent user ID across platforms and ignore the information from similar users. In this study, we propose RUEL, a novel retrievalbased sequential recommender that can effectively incorporate external anonymous user behavior data from Edge browser logs to enhance recommendation. We first collect and preprocess a large volume of Edge browser logs over a one-year period and link them to target entities that correspond to candidate items in recommendation datasets. We then design a contrastive learning framework with a momentum encoder and a memory bank to retrieve the most relevant and diverse browsing sequences from the full browsing log based on the semantic similarity between user representations. After retrieval, we apply an item-level attentive selector to filter out noisy items and generate refined sequence embeddings for the final predictor. RUEL is the first method that connects user browsing data with typical recommendation datasets and can be generalized to various recommendation scenarios and datasets. We conduct extensive experiments on four real datasets for sequential recommendation tasks and demonstrate that RUEL significantly outperforms state-ofthe-art baselines. We also conduct ablation studies and qualitative analysis to validate the effectiveness of each component of RUEL and provide additional insights into our method.",
  "CCS CONCEPTS": "",
  "· Information systems → Personalization .": "∗ Daxin Jiang is the corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '23, October 21-25, 2023, Birmingham, United Kingdom © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0124-5/23/10...$15.00 https://doi.org/10.1145/3583780.3615498",
  "KEYWORDS": "User Browsing Log, Sequential Recommendation, Dense Retrieval, Contrastive Learning",
  "ACMReference Format:": "Ning Wu, Ming Gong, Linjun Shou, Jian Pei, and Daxin Jiang. 2023. RUEL: Retrieval-Augmented User Representation with Edge Browser Logs for Sequential Recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23), October 21-25, 2023, Birmingham, United Kingdom. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3583780.3615498",
  "1 INTRODUCTION": "Recommender systems (RS) play a vital role in various online applications by matching user needs with a large number of candidates (called items). A key challenge for RS is to accurately characterize and understand users' interests and preferences, which are often sparse and noisy in user-item interactions. To enhance the recommendation performance, recent studies have leveraged various types of side information to enrich the representation of users and items, such as spatial-temporal information [22], user/item attributes [52], knowledge graph [41, 43] and cross-domain behavior [36, 45, 49]. Web browsing behavior data can reveal user interests and preferences that are useful for recommender systems. For example, many online users search for information they need using web browsers such as Chrome and Edge. The browsing data and search queries collected by these browsers can cover a large and diverse user population. However, most existing methods for leveraging browsing data in recommender systems only focus on the users who are also in the recommendation datasets [36, 45, 49]. This limits the applicability and effectiveness of these methods for two reasons. First, they require a unified user identifier across multiple domains, which is not always available or feasible. Second, they ignore the potential benefits of using the browsing data of other anonymous users who may have similar interests to the target user. Motivated by the recent advances in open-domain question answering using dense retrieval [30, 33] and some knowledge-based recommendation methods [41, 43], we aim to bridge the gap between the Edge browsing log and entity-related recommendation tasks such as movie, book and music recommendations. The overall procedure is illustrated in Figure 1. However, the browsing log is very noisy and sparse, and many webpages may not be relevant CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Ning Wu, Ming Gong, Linjun Shou, Jian Pei, and Daxin Jiang Figure 1: Illustration of the overall procedure on movie recommendation. Webpages browsed by users are first linked to entities by entity linking technology. Then these entities are mapped to items based on item name, publish years and etc. Finally, a well-trained retriever will search for useful browsing sequence and feed them into recommender. Off-world: The Blade Runner Wiki The Cast of '2001: A Space Odyssey,' Looking Back on the Original Star Wars Anonymous User 1 …… Sleepless in Seattle Movie Review Roman Holiday - Rotten Tomatoes Who was the director of Titanic? Anonymous User N …… …… … … Entity Linking … … Recommender Retrieved Sequences Mapped Browsing Sequences … Current Sequence Browsing Log … … … Entity-Item Mapping Knowledge Graph Entity-Item Mapping Entity Linking Entities Item Prediction to the candidate items. The key challenge is how to select and represent useful browsing data in the presence of noise. To address this challenge, we propose a novel retrieval-augmented sequential recommender based on Edge browsing log, named RUEL, which stands for Retrieval-Augmented User Representation with Edge Browser Logs for Sequential Recommendation. First, we apply an augmented contrastive learning framework to the encoder that takes the current user behavior sequence as input. We generate two augmented views from each user behavior sequence and feed them into a transformer encoder. The objective of contrastive learning is to maximize the agreement between the two views from the same sequence. This has two advantages: (1) it improves the embedding space and reduces anisotropy [29], and (2) it enhances the model robustness to noise by using data augmentation techniques [25, 47]. Moreover, we construct a momentum encoder and a memory bank for browsing sequences. The momentum encoder encodes browsing sequences and stores their embeddings in the memory bank. All embeddings in the memory bank are used as negative samples in contrastive learning. The encoder is trained to distinguish the augmented original user behavior sequence from a large number of browsing sequences. Finally, we convert all browsing sequences into a retrieval index using the retriever for fast retrieval. At inference time, we retrieve top k browsing sequences from the index for each user, and assign them different weights by an item-level attentive selector. The predictor aggregates multiple weighted sequence embeddings by attention mechanism, and predicts the target item. In summary, our main contributions are as follows. · We propose a novel approach to mine useful patterns from anonymous browsing data to improve recommender systems. We bridge the gap between anonymous webpage browsing data and various recommendation tasks. · Weuse a momentum contrastive learning framework on user behavior sequences and anonymous browsing sequences to train a powerful retriever, and design an attentive selector to generate fine-grained weights for each retrieved sequence. Table 1: Interaction information statistics for browsing datasets. We present the total browsing webpage numbers of each dataset after preprocessing of three stages. · Weconductextensive experiments on four real-world datasets to demonstrate the effectiveness and robustness of our proposed approach.",
  "2 RELATED WORK": "Our work is related to the following research directions. Sequential Recommendation. By modeling high-order dependency between between each items, sequential recommenders aims to recommend appropriate items to users. Previous [3, 12, 35] efforts utilize MCs to identify first-order transition relationships. Subsequently, with the rapid growth of deep learning techniques, numerous works [13-16, 21, 31] has been developed to apply deep models to SR. GRU4Rec [14] first introduces RNN to the SR task, taking into account practical features of the task and a number of modifications to standard RNNs, such as a ranking loss function. Caser [38] uses convolutional filters to learn sequential patterns as local features of the image. In addition, influenced by the success of the attention mechanism and Transformers in other domains [2, 8, 24, 32, 39, 42, 48], SASRec [17] has achieved significant performance improvements by first utilizing self-attention to represent the interplay of past interactions. Then, BERT4Rec [37] models user behavior sequences using deep bidirectional self-attention by adopting the Cloze objective to SR. ReDA[1] generates relevant and diverse augmentation by the related information from similar users. Contrastive Learning. Self-supervised learning (SSL) has attracted widespread attention in past several years, [4, 9, 11, 20, 26, 27]. MoCo [11] design a momentum mechanism to enhance memory bank mechanism and SimCLR [4] proposes a simple contrastive learning framework without memory bank and any specialized architectures. SimSiam [5] is a conclusive work on doing contrastive learning with convolutional neural networks. S3-Rec [52] is built on self-attention architecture, and proposes to use attribute information to produce self-supervision signals and augment data representations. SGL [46] generates multiple views of a subgraph, and maximizes the agreement between different views of the same node in two subgraphs CLS4Rec [47], DuoRec [29], MMInfoRec [28], CoSeRec [25] and ContraRec [40] proposes to utilize contrastive learning to empower sequential recommendation.",
  "3 PREPROCESSING": "We preprocess the browsing data in three stages: entity linking, session segmentation, and item alignment. In the first stage, we use Microsoft Satori to link webpages to entities based on their title and RUEL: Retrieval-Augmented User Representation with Edge Browser Logs for Sequential Recommendation CIKM '23, October 21-25, 2023, Birmingham, United Kingdom main text. We build a billion-level webpage-entity dictionary by retrieving and ranking candidate entities for each webpage using BM25 and a roberta-based ranking model. The best entity with a ranking score model above 0.9 is selected. With the webpage-entity dictionary, we filter raw browsing data by only keeping webpages that are linked to Movie/Book/Artist entities. In the second stage, we split the browsing log of each user into sequences with lengths greater than 4 using a 4-hour time interval. Then we use side information in the dataset to match these entities to candidate items in three datasets [50]. For Movie-lens dataset, we compare the release year and movie name. For Amazon-book dataset, we use the author name, and book name. For the Last FM dataset, we use the artist's name. We choose the top-1 candidate after verification for each item. Table 1 shows the statistics of the browsing data after each stage.",
  "4 TASK FORMULATION": "We consider a sequential recommendation task with a set of users U = { 𝑢 1 , · · · , 𝑢 | U| } and a set of items V = { 𝑣 1 , · · · , 𝑣 | V | } . The user-item interaction matrix 𝑌 = { 𝑦 𝑢𝑣 | 𝑢 ∈ U , 𝑣 ∈ V} captures the implicit feedback of users, where 𝑦 𝑢𝑣 = 1 indicates that user 𝑢 has interacted with item 𝑣 , and 𝑦 𝑢𝑣 = 0 otherwise. The interaction can be any type of behavior such as clicking, watching, browsing, etc. For each user 𝑢 , we can also obtain the interaction sequence 𝑠 𝑢 = ( 𝑣 1 , ..., 𝑣 𝑗 , ..., 𝑣 𝑙 𝑢 ) , where 𝑠 𝑢 ∈ C , 𝑣 𝑗 is the item that 𝑢 has interacted with at time step 𝑗 , and 𝑙 𝑢 is the length of the interaction history for user 𝑢 . Moreover, we assume that we have access to a large amount of webpage browsing data D from Edge browser, which consists of numerous webpage sequences 𝑠 𝑟 𝑖 , where 𝑖 denotes the 𝑖 -th browsing session of anonymous users from D . Based on these definitions, we formulate the retrieval-augmented recommendation problem as follows. Given the interaction sequence 𝑠 𝑢 = ( 𝑣 1 , ..., 𝑣 𝑡 , ..., 𝑣 𝑙 𝑢 ) of user 𝑢 , and the webpage browsing data D from Edge browser, our goal is to predict the next item 𝑣 𝑡 that user 𝑢 will interact with.",
  "5 THE RUEL MODEL": "In the section, we present the proposed RUEL: Retrieval-Augmented User Representation with Edge Browser Logs for Sequential Recommendation . Figure 2 illustrates our model framework, which consists of three main components: 1) Contrastive learning with multiple data augmentation strategies; 2) Momentum encoder and memory bank. They help the encoder learn to discriminate positive sample from enormous browsing negative samples; and 3) prediction module, which consists of an attentive selector on item-level and a predictor combines retrieved information and current item sequence to predict the next item.",
  "5.1 Transformer-based Recommender": "Following previous work [17, 29, 37], we also choose transformer encoder [39] as modeling tool 𝑓 (·) of input sequence 𝑠 𝑢 , which has been widely applied to numerous CV and NLP tasks. For a browsing sequence 𝑠 𝑖 , we use 𝒉 𝑢 to denote its representation generated by transformer encoder. Furthermore, we use 𝒉 𝑢,𝑗 to denote embedding of 𝑗 -th item in 𝑢 -th user in C , and use 𝒉 𝑟 𝑖,𝑗 to denote embedding of 𝑗 -th item in 𝑖 -th browsing session in D . Figure 2: The overall architecture of the RUEL model. 𝑓 (·) is trained to maximize agreement between positive pairs, and discriminate positive pairs from enormous negative embeddings generated by momentum encoder 𝑓 𝑘 (·) . Blue part depicts browsing sequences are sent into 𝑓 𝑘 (·) , and converted into negative embeddings in memory bank. After first stage training, all browsing sequences are encoded into embeddings to construct full retrieval index. Encoder 𝑓(∙) 𝑠 𝑢 𝑠 1 𝑟 𝑠 𝑏 𝑟 Momentum Encoder 𝑓 k (∙) 𝑠 𝑢 ′ v1 v2 v𝑙𝑢 …… … …… 𝑠 1 𝑟 𝑠 𝑛 𝑟 𝑠 2 𝑟 𝑠 𝑛-1 𝑟 …… 𝒉𝑢 ∙ 𝒉 𝑢 ′ 𝒉𝑢 ∙ 𝒉 𝑘 ′ 𝑟 Minimize Agreement Maximize Agreement Encoder 𝑓(∙) …… Retrieve Full Index Index Sample … 𝒉𝑘 𝑟 𝒉1 𝑟 Top k Item Prediction Attentive Selector … Predictor Current User Behavior Sequence Browsing Sequences Current User Behavior Sequence 𝑠𝑢 v1 v2 v𝑙𝑢 …… Memory Bank",
  "5.2 Contrastive Learning for Sequential Modeling": "Wefirst introduce the data augmentation methods used in this paper. Then, we explain the technical aspects of momentum contrastive learning on browsing sequences. 5.2.1 Sequential Augmentation. Given current items sequence 𝑠 𝑢 , wedirectly introduce some typical sequence-based augmentations [25]. We randomly select an operator from Mask , Crop , Reorder in each augmentation. · Mask (M). The item masking approach [6, 47, 52] randomly replaces each token with special token [Mask] by a probability 𝛾 ∈ ( 0 , 1 ) . This augmentation strategy can be formulated as: 𝑠 𝑀 𝑢 ′ = [ 𝑣 1 , 𝑣 [Mask] , · · · , 𝑣 𝑙 𝑢 ] . · Reorder (R). Different item orders may reflect the same user interests[6, 47]. We can shuffle a continuous sub-sequence [ 𝑣 𝑟 + 1 , · · · , 𝑣 𝑟 + 𝑙 𝑟 ] to [ 𝑣 ′ 𝑟 , 𝑣 ′ 𝑟 + 1 , · · · , 𝑣 ′ 𝑟 + 𝑙 𝑟 ] , where 𝑙 𝑟 = 𝜇 ∗ 𝑘 is the sub-sequence length and 𝜇 ∈ ( 0 , 1 ) . · Crop (C). Given a current behavior sequence 𝑠 𝑢 , we randomly select a sub-sequence 𝑠 𝐶 𝑢 ′ = [ 𝑣 𝑐 + 1 , · · · , 𝑣 𝑐 + 𝑙 𝑐 ] with length 𝑙 𝑐 = 𝜂 ∗ 𝑙 𝑢 , where 𝜂 ∈ ( 0 , 1 ) is a length hyper-parameter. 5.2.2 Momentum Contrastive Learning. Moreover, we adopt momentum contrastive learning, which uses a large memory bank with many negative examples, and maintains a consistent encoder for generating negative embeddings with a momentum-based update mechanism. The memory bank is updated in a FIFO (First-In-FirstOut) fashion. At each iteration, the representation 𝒉 𝑟 𝑖 is generated by a key encoder 𝑓 𝑘 (·) that has the same structure as 𝑓 (·) . Then 𝒉 𝑟 𝑖 is added to 𝑴 , and the oldest representation in the memory bank is removed. Notably, our memory bank 𝑴 stores the embeddings of browsing sequences as negative examples to enhance the discriminative power of the query encoder. Using a queue-based memory bank allows us to increase the size of negative samples in each loss computation, but it prevents us from updating the key encoder 𝑓 𝑘 (·) via back-propagation (the gradient is disconnected for all samples CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Ning Wu, Ming Gong, Linjun Shou, Jian Pei, and Daxin Jiang in the queue). Following [11], the parameters of the key encoder 𝜃 𝑘 are updated by following those of the query encoder 𝜃 𝑞 with a momentum coefficient 𝑚 , resulting in stable representations for similar sequences. Formally, we have:  where 𝑚 ∈ [ 0 , 1 ) is a hyper-parameter. Back-propagation only modifies 𝜃 𝑞 . The momentum update in Equation 1 makes 𝜃 𝑘 evolve more smoothly than 𝜃 𝑞 . We simply set 𝑚 to 0.999 following [11]. Based on the memory bank, we optimize the sequence representation with a momentum contrastive loss function as:  where 𝜏 is a temperature hyper-parameter, 𝐾 is the size of memory bank. This loss is equivalent to minimizing the negative log likelihood of a ( 2 𝑁 + 𝐾 ) -way softmax classifier that tries to distinguish j-th sequence from other sequences in the batch and many browsing sequences in the memory bank.  In above equation, 𝑠 𝑢 denotes each behavior sequence in dataset C , and we use 𝑠 𝑢 ′ to denote augmented view of it.",
  "5.3 Retrieval-augmented Sequential Modeling": "In this section, we present our retrieval-augmented sequential recommendation model, which uses the observed user behavior session data 𝑠 𝑢 to retrieve top-k browsing sequences { 𝑠 𝑟 1 , 𝑠 𝑟 2 , ..., 𝑠 𝑟 𝑘 } and use them as additional context for sequential recommendation. As shown on the right of Figure 2, after top-k sessions are retrieved from full index, we perform fine-grained attentive selection on item level. For each retrieved behavior sequence, its final representation is computed by a weighted sum of its item representations. Then, a predictor takes both weighted retrieved sequence embedding and current user sequence embedding as input, and predicts target item. 5.3.1 Session-level Retrieval. We use transformer encoders 𝑓 𝑘 (·) to build index of retriever. For each user in browsing data, their records are split into different sessions by time interval as stated above. The encoder 𝑓 𝑘 (·) takes these sessions as input, and produces dense embedding 𝒉 𝑟 𝑖 for each browsing sequence 𝑠 𝑟 𝑖 in D . Then a relevance score 𝑓 ( 𝑠 𝑢 , 𝑠 𝑟 𝑖 ) = 𝒉 𝑇 𝑢 𝒉 𝑟 𝑖 is calculated between dense session vector 𝒉 𝑢 of user u and browsing sequence vector 𝒉 𝑟 𝑖 by inner product. Thus, we can employ Maximum Inner Product Search (MIPS) algorithms to find the approximate top k sessions { 𝑠 𝑟 1 , 𝑠 𝑟 2 , ..., 𝑠 𝑟 𝑘 } , using running time and storage space that scale sub-linearly with the number of browsing sequences. We use faiss 1 to implement above-mentioned retrival procedures. 5.3.2 Attentive Selection. After top 𝑘 sessions are retrieved from numerous browsing data, we aim to reduce the impact of noisy items on prediction. Therefore we apply an item-level attention 1 https://github.com/facebookresearch/faiss mechanism to compute attention weight for each item in retrieved sessions.   where 𝑙 𝑖 denotes the length of 𝑠 𝑟 𝑖 , 𝒉 𝑟 𝑖,𝑗 denotes the output hidden state of 𝑗 -th item in 𝑖 -th retrieved browsing sequence 𝑠 𝑟 𝑖 , and 𝒐 𝑖 denotes the weighted embedding of sequence 𝑠 𝑟 𝑖 . 𝑾 1 and 𝑾 2 are learnable parameters. 𝛼 𝑗 𝑢 represents the attention weight between 𝑠 𝑢 and the 𝑗 -th items in 𝑖 -th retrieved browsing behavior sequence 𝑠 𝑟 𝑖 . After obtaining 𝒐 𝑖 for each retrieved sequence, we directly use the retrieve score 𝑓 ( 𝑠 𝑢 , 𝑠 𝑟 𝑖 ) as weight of 𝒐 𝑖 to compute the final context vector 𝒐 .  where 𝑓 ( 𝑠 𝑢 , 𝑠 𝑖 ) is the dot product relevance score generated by retriever. In summary, we select top-k retrieved browsing sequences from both sequence level and item level, and obtain a comprehensive context vector 𝒐 to enhance the predictor. 5.3.3 Prediction and Loss Function. Given the aggregated context vector 𝒐 , we use a two-layer multilayer perceptron (MLP) to capture the high-level interaction between 𝒐 and 𝒉 𝑢 .  where 𝒘 𝑇 𝑗 is a learnable embedding for item 𝑗 , and D denotes the full set of browsing data. The final loss function is cross entropy, which can be written as:  where C denotes the set of all user behavior sequences. 𝑣 𝑡 denotes the target item of behavior sequence 𝑠 𝑢 . 5.3.4 Optimization and Training Strategy. We adopt a two-stage training strategy. In the first stage, we train the transformer encoder 𝑓 (·) by minimizing the contrastive loss in Equation 2, and update the momentum encoder 𝑓 𝑘 (·) by Equation 1. Then we construct a fixed browsing sequence index using 𝑓 𝑘 (·) . In the second stage, we jointly optimize the contrastive loss and the cross entropy loss.  Unlike some previous work [10], which rely on a reward to guide the gradient of the retriever and constantly refresh the index with the updated retriever, we use a fixed browsing data index, which allows us to directly optimize the transformer encoder 𝑓 (·) in the item prediction task by gradient backpropagation. By sharing the encoder in both momentum contrastive learning and item prediction task, the transformer encoder 𝑓 (·) will also benefit from the contrastive learning, because a good embedding space is also conducive to item prediction [25, 29]. RUEL: Retrieval-Augmented User Representation with Edge Browser Logs for Sequential Recommendation CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Table 2: Interaction information statistics for datasets. We use Inter to denote interaction.",
  "6 EXPERIMENTS": "In this section, we first set up the experiments, and then present the performance comparison and analysis.",
  "6.1 Experimental Setup": "Edge Browser Log Mining. Wepresent the details of entity linking and entity-item mapping in Section 4. Then, we can transform the user browsing log into a sequence of entities or items. Due to the limitation of resources, we only use user browsing log of Edge browser in 2021. We collect more than 32 billion records and convert them to browsing sequences for four datasets. Table 2 shows detailed information. Evaluation Metrics. In this paper, we use Normalized Discounted Cumulative Gain (NDCG) and Hit Ratio (HR) as metrics [7, 19, 23]. We set k=5 and k=10 in experiments. Task Setting. For data preprocessing, we follow the common practice in previous work. To ensure the quality of the dataset, we only keep users with at least five interactions following previous practice. To evaluate the sequential recommendation models, we adopt the leave-one-out evaluation (i.e., next item recommendation) task. For each user, we hold out the last item of the behavior sequence as the test data and use the item just before the last as the validation set. The remaining items are used for training. We implemented our baselines based on the RecBole [51] framework. All baseline settings and training strategies refer to the original authors' implementation and further tune the parameters based on it. For fairness, the embedding size, layers number and heads number of all transformer-based models are fixed to the widely used 128, 2 and 2. The size of memory bank K is set as 8096 directly. We use the Adam optimizer [18] to optimize all models. Moreover, all models are trained for 500 epochs, and the early stopping strategy is applied, i.e ., premature stopping if ndcg@10 on the validation set does not increase for 20 successive epochs. MethodstoCompare. Weconsider the following methods for comparison: GRU4Rec [16], Caser [47], BERT4Rec [37], CL4Rec [47], CoSeRec [25]. Shallow models like BPR-MF [34], NCF [44] and FPMC [44] are omitted due to length limitation. Among these baselines, GRU4Rec, Caser and BERT4Rec belongs to typical deep learning based sequential recommender. CL4Rec and CoSeRec are stateof-the-art contrastive sequential recommender. The parameters in all the models have been optimized using the validation set. Table 3: Performance comparison using four metrics on four datasets. All the results are better with larger values. With paired 𝑡 -test, the improvement of the RUEL over the best baselines is significant at the level of 0.05.",
  "6.2 Performance Comparison": "Table 3 shows the results of the performance comparison between our RUEL method and the baseline methods on four datasets. We can observe that RUEL consistently achieves the best performance on all datasets. Among the baselines, CL4Rec and CoSeRec are the most competitive ones, especially CoSeRec, which leverages multiple data augmentation strategies to enhance its contrastive learning. We can also see that the transformer-based recommenders have an advantage over other deep learning architectures. Moreover, we notice that our method has a larger gain on ML-1M than on ML-20M, indicating that our method is more effective on relatively small datasets. Table 4: Ablation analysis on the Amazon-Book and ML-1m datasets.",
  "6.3 Ablation Analysis": "We performed ablation experiments on sequential recommendation and retrieval tasks to examine the contribution of each component of RUEL. The results are presented in Table 4. For the retrieval task, we utilized our encoder to generate embeddings for a user behavior sequence and its augmented counterpart. We then evaluated the encoder's ability to retrieve the augmented sequence from numerous browsing sequences using HR@10 and HR@20 metrics. In this CIKM '23, October 21-25, 2023, Birmingham, United Kingdom Ning Wu, Ming Gong, Linjun Shou, Jian Pei, and Daxin Jiang table, RUEL denotes our full model with all components. RUEL ¬ 𝑅𝐴 eliminates the retriever and the attentive selector components, and directly infers the target item by employing the output embedding of the transformer encoder 𝑓 (·) . RUEL ¬ 𝐷𝐴 discards all data augmentation strategies. RUEL ¬ 𝑀𝐶 removes the memory bank and the momentum mechanism, and only retains batch negatives in contrastive learning. RUEL ¬ 𝐴𝑆 removes the attentive selector, and simply adopts average pooling to obtain the sequence embedding. The results in Table 4 indicate that RUEL ¬ 𝑅𝐴 slightly outperforms CoSeRec, which implies that our momentum contrast and memory bank are also advantageous for direct recommendation. RUEL ¬ 𝐷𝐴 has a very similar performance to RUEL. It implies that our method does not depend on manually designed sequence augmentation strategies. Discarding the momentum mechanism and the memory bank also deteriorates the model performance. The model needs to be trained to discriminate the original sequence from numerous browsing sequences. The attentive selector plays a vital role in enhancing the model robustness and effectiveness by applying token-level attention. Eliminating this component will substantially impair the model performance. Table 5: A/B Test on Bing Movie.",
  "6.4 Online A/B Test": "We have deployed our model on Bing desktop search by replacing the existed transformer-based sequential ranker with RUEL. To get a stable conclusion, we observe the online experiment for two weeks. Three common metrics in desktop search systems are used to measure the online performance: WHR (Weighted Hover Rate), CTR (Click Through Rate), Short-Term DAU. As the result shown in Table 5, the present method RUEL gets overall improvements on multiple regions in our online A/B test experiment.",
  "6.5 Parameter Analysis": "We experiment with RUEL using different 𝑘 values in [3, 5, 10, 20, 30] and report the results in Figure 3(a). The metrics peak at 𝑘 =10 on the Amazon-Book dataset and decline afterwards. The attentive selector performs poorly when 𝑘 is small due to less noise in the top-ranked items. The performance gap between RUEL and RUEL ¬ 𝐴𝑆 grows as 𝑘 increases because of more irrelevant items in the retrieved sequences. We also vary the embedding size in [32, 64, 96, 128] and show the results in Figure 3(b). RUEL consistently outperforms RUEL ¬ 𝑅𝐴 on the Amazon-Book dataset, especially when the embedding size is 128. RUEL benefits more from higher embedding size than RUEL ¬ 𝑅𝐴 .",
  "6.6 Qualitative Analysis": "Weshow how RUEL works in Figure 4. The red frames are retrieved browsing sequences with popular movies in science fiction and romance genres. The first sequence has The Shawshank Redemption , \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000 \u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000 EL \u0000\u0000 EL ¬ AS \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000 EL \u0000\u0000 EL ¬ RA \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 (a) Retrieval number k w.r.t ndcg@10. (b) Embedding size w.r.t ndcg@10. Figure 3: Effect of retrieval number and embedding size. Star War: Episode 1 , and The Blade Runner . The second sequence has Roman Holiday , Titanic and Saving Private Ryan . The retriever finds the 10 most similar browsing sequences using the current sequence embedding. The attentive selector assigns a weight to each item in the retrieved sequences. Items that are irrelevant to the user interests, such as The Shawshank Redemption in sequence 1 and Saving Private Ryan in sequence 2, get lower weights. The item-level and sentence-level weights are combined to produce a context vector. Then the enhanced recommender uses the context embedding and the current sequence embedding to predict the next item. The Blade Runner and Romance Holiday are the top two candidates because they match the user interests better. Figure 4: Prediction procedure for a sample user in Bing Movie dataset. We use the red frame to denote useful sequences in browsing sequences. For item and sequence-level attention weights, we use the color darkness to indicate attention weights: darker is larger. For predicting results, we use the size to represent predicting probability. … … … … … … Browsing Log Entity Matching …… … Retrieval Similar Sequences … … … Attentive Selector 2001: A Space Odyssey Star War: Episode 1 Titantic Browsing Sequences Current User Behavior Sequence Prediction … Enhanced Recommender Name: Titantic Description: An American epic romance and disaster film based on accounts of the sinking of the RMS Titanic. Type: Romance Entity Information … Webpages Item Mapping",
  "7 CONCLUSIONS": "This paper proposes a retrieval-augmented sequential recommendation model RUEL to fully utilize Edge browsing information. For an item sequence in the recommendation dataset, it's encoded into a vector, and the vector is used to retrieve similar user behavior sequences from cross-domain behavior. Then retrieved sequences are filtered by an item attentive selector, and refined sequences are used to enhance next item prediction. In future, we will further explore more heterogeneous cross-domain user behavior data to enhance user modeling. \u0000\u0000\u0000\u0000\u0000\u0000\u0000 RUEL: Retrieval-Augmented User Representation with Edge Browser Logs for Sequential Recommendation CIKM '23, October 21-25, 2023, Birmingham, United Kingdom",
  "REFERENCES": "[1] Shuqing Bian, Wayne Xin Zhao, Jinpeng Wang, and Ji-Rong Wen. 2022. A Relevant and Diverse Retrieval-enhanced Data Augmentation Framework for Sequential Recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 2923-2932. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901. [3] Chenwei Cai, Ruining He, and Julian McAuley. 2017. SPMC: socially-aware personalized markov chains for sparse sequential recommendation. arXiv preprint arXiv:1708.04497 (2017). [4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning . PMLR, 1597-1607. [5] Xinlei Chen and Kaiming He. 2021. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 15750-15758. [6] Mingyue Cheng, Fajie Yuan, Qi Liu, Xin Xin, and Enhong Chen. 2021. Learning Transferable User Representations with Sequential Behaviors via Contrastive Pre-training. In 2021 IEEE International Conference on Data Mining (ICDM) . IEEE, 51-60. [7] Ge Cui, Jun Luo, and Xin Wang. 2018. Personalized travel route recommendation using collaborative filtering based on GPS trajectories. IJED 11, 3 (2018), 284-307. [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [9] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems 33 (2020), 21271-21284. [10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International Conference on Machine Learning . PMLR, 3929-3938. [11] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 9729-9738. [12] Ruining He and Julian McAuley. 2016. Fusing similarity models with markov chains for sparse sequential recommendation. In 2016 IEEE 16th international conference on data mining (ICDM) . IEEE, 191-200. [13] Balázs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks with top-k gains for session-based recommendations. In Proceedings of the 27th ACM international conference on information and knowledge management . 843-852. [14] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [15] Balázs Hidasi, Massimo Quadrana, Alexandros Karatzoglou, and Domonkos Tikk. 2016. Parallel recurrent neural network architectures for feature-rich session-based recommendations. In Proceedings of the 10th ACM conference on recommender systems . 241-248. [16] Dietmar Jannach and Malte Ludewig. 2017. When recurrent neural networks meet the neighborhood for session-based recommendation. In Proceedings of the eleventh ACM conference on recommender systems . 306-310. [17] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE, 197-206. [18] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR'15 . [19] Takeshi Kurashima, Tomoharu Iwata, Go Irie, and Ko Fujimura. 2010. Travel route recommendation using geotags in photo sharing sites. In CIKM . ACM, 579-588. [20] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 (2019). [21] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017. Neural attentive session-based recommendation. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management . 1419-1428. [22] Defu Lian, Yongji Wu, Yong Ge, Xing Xie, and Enhong Chen. 2020. Geographyaware sequential location recommendation. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining . 20092019. [23] Kwan Hui Lim, Jeffrey Chan, Christopher Leckie, and Shanika Karunasekera. 2015. Personalized Tour Recommendation Based on User Interests and Points of Interest Visit Durations.. In IJCAI , Vol. 15. 1778-1784. [24] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). [25] Zhiwei Liu, Yongjun Chen, Jia Li, Philip S Yu, Julian McAuley, and Caiming Xiong. 2021. Contrastive self-supervised sequential recommendation with robust augmentation. arXiv preprint arXiv:2108.06479 (2021). [26] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [27] Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. 2020. Gcc: Graph contrastive coding for graph neural network pre-training. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1150-1160. [28] Ruihong Qiu, Zi Huang, and Hongzhi Yin. 2021. Memory Augmented MultiInstance Contrastive Predictive Coding for Sequential Recommendation. In 2021 IEEE International Conference on Data Mining (ICDM) . IEEE, 519-528. [29] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2022. Contrastive learning for representation degeneration problem in sequential recommendation. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining . 813-823. [30] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. In NAACL-HLT . Association for Computational Linguistics, 5835-5847. [31] Massimo Quadrana, Alexandros Karatzoglou, Balázs Hidasi, and Paolo Cremonesi. 2017. Personalizing session-based recommendations with hierarchical recurrent neural networks. In proceedings of the Eleventh ACM Conference on Recommender Systems . 130-137. [32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 140 (2020), 1-67. [33] Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021. RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking. In EMNLP (1) . Association for Computational Linguistics, 2825-2835. [34] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [35] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized markov chains for next-basket recommendation. In Proceedings of the 19th international conference on World wide web . 811-820. [36] Zihua Si, Xueran Han, Xiao Zhang, Jun Xu, Yue Yin, Yang Song, and Ji-Rong Wen. 2022. A Model-Agnostic Causal Learning Framework for Recommendation using Search Data. In Proceedings of the ACM Web Conference 2022 . 224-233. [37] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management . 1441-1450. [38] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining . 565-573. [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [40] Chenyang Wang, Weizhi Ma, and Chong Chen. 2022. Sequential Recommendation with Multiple Contrast Signals. ACM Transactions on Information Systems (TOIS) (2022). [41] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2018. Ripplenet: Propagating user preferences on the knowledge graph for recommender systems. In Proceedings of the 27th ACM international conference on information and knowledge management . 417-426. [42] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition . 7794-7803. [43] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat: Knowledge graph attention network for recommendation. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining . 950-958. [44] Ling Yin Wei, Yu Zheng, and Wen Chih Peng. 2012. Constructing popular routes from uncertain trajectories. In SIGKDD . 195-203. [45] Chuhan Wu, Fangzhao Wu, Mingxiao An, Tao Qi, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019. Neural news recommendation with heterogeneous user behavior. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . 4874-4883. [46] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021. Self-supervised graph learning for recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval . 726-735. Ning Wu, Ming Gong, Linjun Shou, Jian Pei, and Daxin Jiang CIKM '23, October 21-25, 2023, Birmingham, United Kingdom [47] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, and Bin Cui. 2020. Contrastive learning for sequential recommendation. arXiv preprint arXiv:2010.14395 (2020). [48] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems 32 (2019). [49] Jing Yao, Zhicheng Dou, Ruobing Xie, Yanxiong Lu, Zhiping Wang, and Ji-Rong Wen. 2021. USER: A Unified Information Search and Recommendation Model based on Integrated Behavior Sequence. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 2373-2382. [50] Wayne Xin Zhao, Gaole He, Kunlin Yang, Hongjian Dou, Jin Huang, Siqi Ouyang, and Ji-Rong Wen. 2019. Kb4rec: A data set for linking knowledge bases with recommender systems. Data Intelligence 1, 2 (2019), 121-136. [51] Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, et al. 2021. Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management . 4653-4664. [52] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 1893-1902.",
  "keywords_parsed": [
    "User Browsing Log",
    " Sequential Recommendation",
    " Dense Retrieval",
    " Contrastive Learning"
  ]
}