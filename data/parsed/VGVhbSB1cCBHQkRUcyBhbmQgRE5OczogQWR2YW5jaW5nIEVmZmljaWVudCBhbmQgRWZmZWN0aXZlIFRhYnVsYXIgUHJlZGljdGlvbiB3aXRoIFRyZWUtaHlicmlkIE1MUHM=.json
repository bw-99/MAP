{"Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs": "Jiahuan Yan Zhejiang University Hangzhou, China jyansir@zju.edu.cn Jintai Chen \u2217 University of Illinois at Urbana-Champaign Urbana, IL, USA jtchen721@gmail.com Qianxing Wang Zhejiang University Hangzhou, China w.qianxing@zju.edu.cn Danny Z. Chen University of Notre Dame Notre Dame, IN, USA dchen@nd.edu Jian Wu Zhejiang University Hangzhou, China wujian2000@zju.edu.cn", "ABSTRACT": "Tabular datasets play a crucial role in various applications. Thus, developing efficient, effective, and widely compatible prediction algorithms for tabular data is important. Currently, two prominent model types, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks (DNNs), have demonstrated performance advantages on distinct tabular prediction tasks. However, selecting an effective model for a specific tabular dataset is challenging, often demanding time-consuming hyperparameter tuning. To address this model selection dilemma, this paper proposes a new framework that amalgamates the advantages of both GBDTs and DNNs, resulting in a DNN algorithm that is as efficient as GBDTs and is competitively effective regardless of dataset preferences for GBDTs or DNNs. Our idea is rooted in an observation that deep learning (DL) offers a larger parameter space that can represent a well-performing GBDT model, yet the current back-propagation optimizer struggles to efficiently discover such optimal functionality. On the other hand, during GBDT development, hard tree pruning, entropy-driven feature gate, and model ensemble have proved to be more adaptable to tabular data. By combining these key components, we present a T ree-hybrid simple MLP (T-MLP). In our framework, a tensorized, rapidly trained GBDT feature gate, a DNN architecture pruning approach, as well as a vanilla back-propagation optimizer collaboratively train a randomly initialized MLP model. Comprehensive experiments show that T-MLP is competitive with extensively tuned DNNs and GBDTs in their dominating tabular benchmarks (88 datasets) respectively, all achieved with compact model storage and significantly reduced training duration. The codes and full experiment results are available at https://github.com/jyansir/tmlp. \u2217 The corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '24, August 25-29, 2024, Barcelona, Spain \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671964", "CCS CONCEPTS": "\u00b7 Computing methodologies \u2192 Machine learning ; Supervised learning; Neural networks .", "KEYWORDS": "classification and regression, tabular data, green AI, AutoML", "ACMReference Format:": "Jiahuan Yan, Jintai Chen \u2217 , Qianxing Wang, Danny Z. Chen, and Jian Wu. 2024. Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 25-29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 14 pages. https: //doi.org/10.1145/3637528.3671964", "1 INTRODUCTION": "Tabular data are a ubiquitous and dominating data structure in various machine learning applications (e.g., click-through rate (CTR) prediction [17] and financial risk detection [3]). Current prevalent tabular prediction (i.e., classification and regression) models can be generally categorized into two main types: (1) Gradient Boosted Decision Trees (GBDTs) [16, 18, 31, 43], a kind of classical non-deeplearning approach that has been extensively verified as test-of-time solutions [7, 23, 55]; (2) Deep Neural Networks (DNNs), on which continuous endeavors apply deep learning (DL) techniques from computer vision (CV) and natural language processing (NLP) to develop tabular learning methods such as meticulous architecture engineering [2, 12, 22, 42, 62] and pre-training [48, 58, 69]. With recent developments of bespoke tabular DNNs, increasing studies reported their better comparability [13, 62] and even superiority [14, 48] to GBDTs, especially in complex data scenarios [45, 58], while classical thinking believes that GBDTs still completely surpass DNNs in typical tabular tasks [7, 23], both evaluated with different benchmarks and baselines, implying respective tabular data proficiency of these two model types. For DNNs, their inherent high-dimensional feature spaces and smooth back-propagation optimization gain tremendous success on unstructured data [10, 44] and capability of mining subtle feature interactions [46, 49, 57, 62]. Besides, leveraging DNN's transferability, recent popular tabular Transformers can be further improved by costly pre-training [48, 58, 69], like their counterparts in KDD '24, August 25-29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu NLP [10, 32, 68]. However, compared to the simple multi-layer perceptron (MLP) and GBDTs, Transformer architectures are more complicated and are prone to be over-parameterized, data-hungry, and increase processing latency , especially those recent language-modelbased architectures [8, 67]. Thus, they typically under-perform on tabular datasets that are potentially small-sized [23]. Regarding GBDTs, they thrive on greedy feature selection, tree pruning, and efficient ensemble, yielding remarkable performances and efficiency on the majority of tabular prediction applications [47, 55]. Yet, they are usually hyperparameter-sensitive [43, 63] and not well-suited in extreme tabular scenarios , such as large-scale tables with intricate feature interactions [45]. Also, their inference latency increases markedly as the data scale grows [7]. Besides, both the GBDT and DNN frameworks achieve respective state-of-the-art results with expensive training costs , since heavy hyperparameter search is required to achieve considerable performance. But, this is carbon-unfriendly and is not compatible in computation-limited or real-time applications, while not enough proactive efforts on economical tabular prediction have been made. To address the model selection dilemma, we comprehensively combine the advantages of both GBDTs and DNNs, and propose a new T ree-hybrid simple MLP (T-MLP), which is high-performing, efficient, and lightweight. Specifically, a single T-MLP is equipped with a GBDT feature gate to perform sample-specific feature selection in a greedy fashion, and GBDT-inspired pruned MLP architectures to process the selected salient features. The whole framework is optimized using back-propagation with these GBDTs' properties, and all the components make the system compact, overfit-resistant, and generalizable. Furthermore, model ensemble can be efficiently achieved by training multiple sparse MLPs (we uniformly use 3 MLPs here) in parallel with a shared gate and predicting in a bagging manner. Overall, T-MLP has the following appealing features. (1) Generalized data adaptability: Different from existing tabular prediction methods that suffer from the model selection dilemma, T-MLP is flexible enough to handle all datasets regardless of the framework preference (see Sec. 4.2 and Sec. 4.4). (2) Hyperparameter tuning free: T-MLP is able to produce competitive results with all the configurations pre-fixed , which is significantly time-saving, user-friendly, environmentally friendly and widely practical in broader applications. (3) Lightweight storage: In TMLP, the DNN part is purely composed of simple and highly sparse MLP architectures, yet is still able to be state-of-the-art competitive even with one-block MLP. Table 1 presents the economical costperformance trade-off of T-MLP compared to common DNNs; such cost-effectiveness becomes more profound as the data scale grows. In summary, our main contributions are as follows: \u00b7 We propose a new GBDT-DNN hybrid framework, T-MLP, which is a one-stop and economical solution for effective tabular data prediction regardless of framework preferences of specific datasets, offering a novel optimization paradigm for tabular model architectures. \u00b7 Multi-facet analysis on feature selection strategy, parameter sparsity, and decision boundary pattern is given for in-depth understanding of the T-MLP efficiency and superiority. \u00b7 Comprehensive experiments on 88 datasets from 4 benchmarks, covering DNN- and GBDT-favored ones, show that a Table 1: Comparison of model cost-effectiveness on small and large datasets across popular tabular DNNs. \ud835\udc39 and \ud835\udc41 denote the amounts of features and samples, \ud835\udc43 is the parameter number, and \ud835\udc47 denotes the overhead of total training time against the proposed T-MLP. We reuse performances and parameter sizes of the best model configurations in the FTTransformer benchmark. \ud835\udc47 is evaluated on an NVIDIA A100 PCIe 40GB (see Sec. 4.1). Based on the fixed architecture and training configurations, T-MLP achieves stable model size and cheap training duration cost regardless of the data scale. single T-MLP is competitive with advanced or pre-trained DNNs, and T-MLP ensemble can even consistently outperform them and is competitive with extensively tuned stateof-the-art GBDTs, all achieved with a compact model size and significantly reduced training duration. \u00b7 We develop an open-source Python package with APIs of benchmark loading, uniform baseline invocation (DNNs, GBDTs, T-MLP), DNN pruning, and other advanced functions as a developmental tool for the tabular learning community.", "2 RELATED WORK": "", "2.1 Model Frameworks for Tabular Prediction": "In the past two decades, classical non-deep-learning methods [25, 35, 65, 66] have been prevalent for tabular prediction applications, especially GBDTs [16, 18, 31, 43] due to their efficiency and robustness in typical tabular tasks [23]. Because of the universal success of DNNs on unstructured data and the development of computation devices, there is an increasing effort in applying DNNs to such tasks. The early tabular DNNs aimed to be comparable with GBDTs by emulating the ensemble tree frameworks (e.g., NODE [42], Net-DNF [30], and TabNet [2]), but they neglected the advantages of DNNs for automatic feature fusion and interaction. Hence, more recent attempts leveraged DNNs' superiority, as they transferred successful neural architectures (e.g., AutoInt [49], FT-Transformer [22]), proposed bespoke designs (e.g., T2G-Former [62]), or adopted pre-training (e.g., SAINT [48], TransTab [58]), reporting competitive or even surpassing results compared to conventionally dominating GBDTs in specific data scenarios [14, 48]. Contemporary surveys [7] demonstrated that GBDTs and DNNs are two prevailing types of frameworks in current tabular learning research.", "2.2 Lightweight DNNs": "Lightweight DNNs are an evergreen research topic in CV and NLP, which aim to maintain effective performance while promoting DNN compactness and efficiency. A recent trend is to substitute dominating backbones with pure simple MLPs, such as MLP-Mixer [53], Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs KDD '24, August 25-29, 2024, Barcelona, Spain Figure 1: Our proposed T-MLP vs. existing tabular prediction approaches: GBDTs and DNNs. (a) GBDTs are classical nondeep-learning models for tabular prediction. (b) DNNs are emerging promising methods especially for large-scale, complex, cross-table scenarios. (c) T-MLP is a hybrid framework that integrates the strengths of both GBDTs and DNNs, accomplished via GBDT feature gate tensorization, MLP framework pruning, simple block ensemble, and end-to-end back-propagation. It yields competitive results on both DNN- and GBDT-favored datasets, with a rapid development process and compact model size. (a) GBDTs (b) DNNs MLP Transformer pre-trained models ... - Incompatible with non-typical tables - Heavy hyperparameter tuning (HPT) T-MLP \u00d73 Shared GBDTs Feature Gate Simplified Sparse MLPs Features # ID \ud835\udc65 ! 101 \ud835\udc65 \" \ud835\udc65 # Ensemble Prediction - One framework with both properties for all datasets - Fixed and simple architecture, stable model size - No HPT & pre-training ... feature selection (c) T-MLP (Tree-hybrid MLPs) pre- pruning efficient ensemble smooth optimization - Complicated architecture engineering - Heavy HPT or costly pre-training - Inferior to GBDTs on typical tables large model capacity dmlc XGBoost CatBoost LightGBM gMLP [36], MAXIM [54], and other vision MLPs [11, 24, 51], achieving comparable or even superior results to their CNN or Transformer counterparts with reduced capacity or FLOPs. This pureMLP trend is also arising in NLP [19] and other real-world applications [15]. Another lightweight scheme is model compression, where pruning is a predominant approach used to trim down large language models [56, 68] from various granularity [38, 50, 61]. In the tabular prediction field, there are a few pure-MLP studies, but all focusing on regularization [29] or numerical embedding [21] rather than the DNN architecture itself. Besides, model compression of tabular DNNs has not yet been explored. We introduce related techniques to make our T-MLP more compact and effective.", "3 TREE-HYBRID SIMPLE MLP": "We first review some preliminaries of typical GBDTs' inference process and feature encoding techniques in current Transformerbased tabular DNNs. Next, we elaborate on the detailed designs of several key components of T-MLP, including the GBDT feature gate for sample-specific feature selection, the pure-MLP basic block, and GBDT-inspired fine-grained pruning for sparse MLPs. Finally, we provide a discussion of the T-MLP workflow.", "3.1 Preliminaries": "Problem Statement. Given a tabular dataset with input features \ud835\udc4b \u2208 R \ud835\udc41 \u00d7 \ud835\udc39 and targets \ud835\udc66 \u2208 R \ud835\udc41 , the tabular prediction task is to find an optimal solution \ud835\udc53 : \u2208 R \ud835\udc41 \u00d7 \ud835\udc39 \u2192 R \ud835\udc41 that minimizes the empirical difference between the predictions \u02c6 \ud835\udc66 and the targets \ud835\udc66 . Here in current practice, the common choice of \ud835\udc53 is either traditional GBDTs (e.g., XGBoost [16], CatBoost [43], LightGBM [31]) or tabular DNNs (e.g., TabNet [2], FT-Transformer [22], SAINT [48], T2G-Former [62]). A typical difference metric is accuracy or AUC score for classification tasks, and is the root of mean squared error (RMSE) for regression. Definition 3.1: GBDT Feature Frequency. Given a GBDT model with \ud835\udc47 decision trees (e.g., CART [35]), the GBDT feature frequency of a sample denotes the number of times each feature is accessed by this GBDT on the sample. Specifically, the process of GBDT inference on a sample \ud835\udc65 \u2208 R \ud835\udc39 provides \ud835\udc47 times a single decision tree prediction \u02c6 \ud835\udc66 ( \ud835\udc58 ) = CART ( \ud835\udc58 ) ( \ud835\udc65 ) , \ud835\udc58 \u2208 { 1 , 2 , . . . , \ud835\udc47 } . For each decision tree prediction, there exists a sample-specific decision path from its root to one of the leaf nodes, forming a used feature list that includes features involved in this prediction action. We denote this accessed feature list of the \ud835\udc58 -th decision tree as a binary vector \ud835\udefc ( \ud835\udc58 ) \u2208 { 0 , 1 } \ud835\udc39 , in which 0 indicates that the corresponding feature of this sample is not used by the \ud835\udc58 -th decision, and 1 indicates that it is accessed. Consequently, we can represent the GBDT feature frequency of the sample with the sum of the \ud835\udc58 decision trees' binary vectors, as:  where \ud835\udefc represents the exploitation level of each feature in the GBDT, suggesting the feature preference of the GBDT model on this sample. Feature Tokenizer. Inspired by the classical language models (e.g., BERT [32]), recent dominating Transformer-based tabular models [22, 48, 62] adopted distributed feature representation [39] by embedding tabular values into vector spaces and treating the values as 'unordered' word vectors. Such Transformer models use feature tokenizer [22] to process tabular features as follows: Each tabular scalar value is mapped to a vector \ud835\udc52 \u2208 R \ud835\udc51 with a feature-specific linear projection, where \ud835\udc51 is the feature hidden dimension. For numerical (continuous) values, the projection weights are multiplied with the value magnitudes. Given \ud835\udc39 1 numerical features and \ud835\udc39 2 categorical features, the feature tokenizer outputs feature embedding \ud835\udc38 \u2208 R ( 1 + \ud835\udc39 1 + \ud835\udc39 2 ) \u00d7 \ud835\udc51 by stacking projected features (and an extra [ CLS ] token embedding), i.e., \ud835\udc38 = stack GLYPH<16>h \ud835\udc52 CLS , \ud835\udc52 ( 1 ) num , . . . , \ud835\udc52 ( \ud835\udc39 1 ) num , \ud835\udc52 ( 1 ) cat , . . . , \ud835\udc52 ( \ud835\udc39 2 ) cat iGLYPH<17> .", "3.2 GBDT Feature Gate": "Early attempts of tabular DNNs tried to emulate behavioral patterns of GBDTs by ensembling neural networks to build differential tree models , such as representative models NODE [42] and TabNet [2]. However, even realizing decision-tree-like hard feature selection or resorting to complicated Transformer architectures, they were still KDD '24, August 25-29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu rapidly submerged in subsequent DNN studies that mainly focused on promotion from deep learning perspectives [13, 22, 62]. We seek to rethink this line of work and observe that they achieve hard feature selection with learnable continuous feature masks through DNNs' smooth back-propagation, which may be incompatible with the discrete nature of GBDTs, and hence restrict their potential. To resolve this issue, we propose GBDT Feature Gate (GFG), a GBDT-based feature selector tensorized with GBDT weights to faithfully replicate its feature selection behavior. Specifically, given a GFG initialized by a \ud835\udc47 -tree GBDT, the feature selection process on an \ud835\udc39 -feature sample \ud835\udc65 ( \u02c6 \ud835\udc38 = GFG ( \ud835\udc65 ) \u2208 R \ud835\udc39 \u00d7 \ud835\udc51 ) is formulated as:     The extra [ CLS ] embedding is omitted in this subsection for notation brevity; in implementation, it is directly concatenated to the head of the gated \u02c6 \ud835\udc38 . In Eq. (3), \u02c6 \ud835\udefc is the normalized GBDT feature frequency that represents the access probabilities of each feature in the \ud835\udc47 -tree GBDT, and \u00af \ud835\udefc is a binary feature mask sampled with the probabilities \u02c6 \ud835\udefc . To incorporate the GBDT's feature preference into the DNN framework, in Eq. (4), we use sparse feature masks from real GBDT feature access probabilities to perform hard feature selection during training, and use the soft probabilities during inference for deterministic prediction. GFG assists in filtering out unnecessary features according to the GBDT's feature preference, ensuring an oracle selection behavior compared to previous differential tree models in learning feature masks with neural networks. Since the original GBDT library (we uniformly use XGBoost in this work) has no APIs for efficiently fetching sample-specific GBDT feature frequency in Eq. (2) and the used backend is incompatible with common DL libraries (e.g., PyTorch), to integrate the GFG module into the parallel DNN framework, we tensorize the behavior of Eq. (2). Technically, we are inspired by the principle of the Microsoft Hummingbird compiling tools 1 and extract routing matrices, a series of parameter matrices that contain information of each decision tree's node adjacency and threshold values, from the XGBoost model. Based on the extracted routing matrices, feature access frequency can be simply acquired through alternating tensor multiplication and comparison on input features \ud835\udc65 , and the submodule of Eq. (2) is initialized with these parameter matrices. In the actual implementation, we just rapidly train an XGBoost with uniform default hyperparameters provided in [22] (regardless of its performance) to initialize and freeze the submodule of Eq. (2) during the T-MLP initialization step. Other trainable parameters are randomly initialized. Since there are a large number of decision trees to vote the feature preference in a GBDT model, slight hyperparameter modification will not change the overall feature preference trend, and a lightly-trained default XGBoost is always usable enough to guide greedy feature selection. To further speed up the processes in Eqs. (2)-(3), we cache the normalized feature 1 https://github.com/microsoft/hummingbird frequency \u02c6 \ud835\udefc for each sample during the first-epoch computation, and reuse the cache in the subsequent model training or inference.", "3.3 Pure MLP Basic Block": "To explore the capability of pure-MLP architecture and keep our tabular model compact, we take inspiration from vision MLPs. We observe that a key factor of their success is the attention-like interaction realized by linear projection and soft gating on features [11, 24, 53]. Thus, we employ the spatial gating unit (SGU) proposed in [36], and formulate a simplified pure-MLP block, as:   The block is similar to a single feed-forward neural network (FFN) in the Transformer with an extra SGU (Eq. (6)) for feature-level interaction. The main parameters are located in two transformations, i.e., \ud835\udc4a 1 \u2208 R \ud835\udc51 \u00d7 2 \ud835\udc51 \u2032 and \ud835\udc4a 2 \u2208 R \ud835\udc51 \u2032 \u00d7 \ud835\udc51 in Eq. (5), where \ud835\udc51 \u2032 corresponds to the FFN intermediate dimension size. In Eq. (6), \ud835\udc4b \u2208 R \ud835\udc39 \u00d7 2 \ud835\udc51 \u2032 denotes the input features of SGU, and \ud835\udc4a 3 \u2208 R \ud835\udc39 \u00d7 \ud835\udc39 is a feature-level transformation to emulate attention operation. Since \ud835\udc51 \u2248 \ud835\udc51 \u2032 \u226b \ud835\udc39 in most cases, the model size is determined by \ud835\udc4a 1 and \ud835\udc4a 2, and is comparable to the FFN size. All the bias vectors are omitted for notation brevity. Analogous to vision data, we treat tabular features and feature embeddings as image pixels and channels. But completely different from vision MLPs, T-MLP is a hybrid framework tailored for economical tabular prediction that performs competitively against tabular Transformers and GBDTs with significantly reduced runtime costs. On most uncomplicated datasets, using only one basic block in T-MLP is enough. In comparison, previous vision MLP studies emphasized architecture engineering and often demanded dozens of blocks in order to be comparable to vision Transformers.", "3.4 Sparsity with User-controllable Pruning": "Inspired by the pre-pruning of GBDTs that controls model complexity and promotes generalization with user-defined hyperparameters (e.g., maximum tree depth, minimum samples per leaf), we design a similar mechanism for T-MLP by leveraging the predominant model compression approach, i.e., DNN pruning [26, 38, 50], which is widely used in NLP research to trim down over-parameterized language models while maintaining the original reliability [61]. Specifically, we introduce two fine-grained variables \ud835\udc67 h \u2208 { 0 , 1 } \ud835\udc51 and \ud835\udc67 in \u2208 { 0 , 1 } \ud835\udc51 \u2032 to mask parameters from hidden dimension and intermediate dimension, respectively. As the previous FFN pruning in language models [59], the T-MLP pruning operation can be attained by simply applying the mask variables to the weight matrices, i.e., substituting \ud835\udc4a 1 and \ud835\udc4a 2 with diag ( \ud835\udc67 h ) \ud835\udc4a 1 and diag ( \ud835\udc67 in ) \ud835\udc4a 2 in Eq. (5). We use the classical \ud835\udc59 0 regularization reparametrized with hard concrete distributions [37], and adopt a Lagrangian multiplier objective to achieve the controllable sparsity as in [61]. Although early attempts of tabular DNNs have considered sparse structures, for example, TabNet [2] and NODE [42] built learnable sparse feature masks, and more recently TabCaps [12] and T2G-Former [62] designed sparse feature interaction, there are two essential differences: (1) existing tabular DNNs only considered sparsity on the feature dimension, while T-MLP introduces sparsity Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs KDD '24, August 25-29, 2024, Barcelona, Spain on the input features (Sec. 3.2) and the hidden dimension (this subsection), which was ignored in previous tabular DNN prediction studies and widely recognized as an over-parameterized facet in NLPpractice [59, 61]; (2) learnable sparsity in existing tabular DNNs is completely coupled and determined by prediction loss functions, while our introduced DNN pruning techniques determine the sparsity based on the user-defined sparsity rate (objective-independent), with the same controllable nature of GBDTs pre-pruning. In the main experiments (Sec. 4.2), we uniformly fix the target sparsity at 0.33 for T-MLP, i.e., only around 33% of DNN parameters are retained after training. We further explore the relationship between model sparsity and performance in Sec. 4.3, and obtain performance boost with suitable parameter pruning, even on T-MLP with one basic block, This implies pervasive over-parameterization in previous tabular DNN designs.", "3.5 Overall Workflow and Efficient Ensemble": "The overall T-MLP workflow is as follows: During the training stage, the input tabular features are embedded with the feature tokenizer and discretely selected by the sampled feature mask \u00af \ud835\udefc in Eq. (3); then, they are processed by a single pruned basic block in Eq. (5), and the pruning parameter masks \ud835\udc67 h and \ud835\udc67 in are sampled with reparameterization on the \ud835\udc59 0 regularization; the final prediction is made with the [ CLS ] token feature using a normal prediction head as in other tabular Transformers, as:  where FC denotes a fully connected layer. We use the cross entropy loss for classification and the mean squared error loss for regression as in previous tabular DNNs. The whole framework is optimized with back-propagation. After training, the parameter masks are directly applied to \ud835\udc4a 1 and \ud835\udc4a 2 by accordingly dropping the pruned hidden and intermediate dimensions. In the inference stage, the input features are softly selected by the normalized GBDT feature frequency \u02c6 \ud835\udefc in Eq. (3), and processed by the simplified basic block. Since the T-MLP architecture is compact and computation-friendly with low runtime cost, we further provide an efficient ensemble version by simultaneously training three branches with the shared GBDT feature gate from the same initialization point with three fixed learning rates. This produces three different sparse MLPs, inspired by the model soups ensemble method [60]. The final ensemble prediction is the average result of the three branches as in a bagging ensemble model. Since the ensemble learning process can be implemented by simultaneous training and inference with multi-processing programming (e.g., RandomForest [9]), the training duration is not tripled but determined by the slowest converging branch.", "4 EXPERIMENTS": "In this section, we first compare our T-MLP with advanced DNNs and classical GBDTs on their dominating benchmarks (including 88 datasets for different task types) and analyze from the perspective of cost-effectiveness. Next, we conduct ablation and comparison experiments with multi-facet analysis to evaluate the key designs that make T-MLP effective. Besides, we compare the optimized patterns of common DNNs, GBDTs, and T-MLP by visualizing their decision boundaries to further examine the superiority of T-MLP.", "4.1 Experimental Setup": "Datasets. We use four recent high-quality tabular benchmarks (FT-Transformer 2 (FT-T, 11 datasets) [22], T2G-Former 3 (T2G, 12 datasets) [62], SAINT 4 (26 datasets) [48], and Tabular Benchmark 5 (TabBen, 39 datasets) [23]), considering their elaborated results on extensive baselines and datasets. The FT-T and T2G benchmarks are representative of large-scale tabular datasets, whose sizes vary from 10K to 1,000K and include various DNN baselines. The SAINT benchmark is gathered from the OpenML repository 6 , and is dominated by the pre-trained DNN SAINT, containing balanced task types and diverse GBDTs. TabBen is based on 'typical tabular data' settings that constrain dataset properties, e.g., the data scale (a maximum data volume of 10K) and the feature number (not highdimension) [23], and the datasets are categorized into several types with combinations of task types and feature characteristics. Notably, on TabBen, GBDTs achieve overwhelming victory, surpassing commonly-used DNNs. Each benchmark represents a specific framework preference. Since several benchmarks have adjusted dataset arrangements in their current repositories (e.g., some datasets were removed and some were added), to faithfully follow and reuse the results, we only retain the datasets reported in the published original papers. We provide detailed benchmark statistical information in Table 2 and discuss benchmark characteristics in Appendix A. Figure 2: The winning rates of GBDTs and DNNs on three benchmarks, which represent the proportion of each framework achieving the best performance in the benchmarks. It exhibits varying framework preferences among the datasets used in different tabular prediction works. T2G SAINT TabBen GBDT wins DNN wins ties 7.79 0.0% 11.5% 12,8% 34.69 41.7% 58.3% 53.89 79.59 Implementation Details. We implement our T-MLP model and Python package using PyTorch on Python 3.10. Since the reported baseline training durations on the original benchmarks are estimated under different runtime environments and using different evaluation codes, and do not consider hyperparameter tuning (HPT) budgets, for uniform comparison of training costs, we encapsulate the experimental baselines with the same sklearn-style APIs as T-MLP in our built package, and conduct all the experiments on 2 https://github.com/yandex-research/rtdl-revisiting-models/tree/main 3 https://github.com/jyansir/t2g-former/tree/master 4 https://github.com/somepago/saint/tree/main 5 https://github.com/LeoGrin/tabular-benchmark/tree/main 6 https://www.openml.org KDD '24, August 25-29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu Table 2: Dataset statistics on four experimental benchmarks. '# bin., # mul., and # reg.' are the amounts of binary classification, multi-class classification, and regression datasets. '# small, # middle, # large, and # ex. large' represent the amounts of small ( \ud835\udc41 \u2264 3K), middle (3K < \ud835\udc41 \u2264 10K), large (10K < \ud835\udc41 \u2264 100K), and extremely large ( \ud835\udc41 > 100K) datasets, where \ud835\udc41 denotes the training data size. '# wide and # ex. wide' are the amounts of wide (32 < \ud835\udc39 \u2264 64) and extremely wide ( \ud835\udc39 > 64) datasets, where \ud835\udc39 is the feature amount. 'bin. metric, mul. metric, and reg. metric' represent the evaluation metrics used for each task type in the benchmarks. 'R-Squared' score is the coefficient of determination. Table 3: Cost-effectiveness comparison on the FT-T benchmark. Classification datasets and regression datasets are evaluated using the accuracy and RMSE metrics, respectively. 'Rank' denotes the average values (standard deviations) of all the methods across the datasets. ' \ud835\udc47 ' represents the average overhead of the used training time against T-MLP, and ' \ud835\udc47 \u2217 ' compares only the duration before achieving the best validation scores. All the training durations are estimated with the original hyperparameter search settings. ' \ud835\udc43 ' denotes the average parameter number of the best model configuration provided by the FT-T repository. TabNet is not compared considering its different backend (Tensorflow) in the evaluation. The top performances are marked in bold, and the second best ones are underlined (similar marks are used in the subsequent tables). Table 4: Cost-effectiveness comparison on the T2G benchmark with similar notations as in Table 3. The baseline performances and configurations are also reused from the T2G repository. According to the T2G paper, for the extremely large dataset Year, FT-T and T2G use 50-iteration hyperparameter tuning (HPT), DANet-28 follows its default hyperparameters, and the other baseline results are acquired with 100-iteration HPT. Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs KDD '24, August 25-29, 2024, Barcelona, Spain NVIDIA A100 PCIe 40GB. All the hyperparameter spaces and iteration numbers of the baselines follow the settings in the original papers to emulate the tuning process of each baseline. For T-MLP, we use fixed hyperparameters as the model is trained only once. The XGBoost used for T-MLP's GBDT Feature Gate is in default configuration as in [22]. In experiments, each single T-MLP uses one basic block for most datasets if without special specification. We uniformly use a learning rate of 1e-4 for a single T-MLP and learning rates of 1e-4, 5e-4, and 1e-3 for the three branches in the T-MLP ensemble (group 'T-MLP(3)'). We reuse the same data splits as in the original benchmarks. The baseline performances are inherited from the reported benchmark results, and the baseline capacities are calculated based on the best model configurations provided in the corresponding paper repositories. Detailed information of the runtime environment and hyperparameters is given in Appendix C. Compared Methods. On the four benchmarks, we compare our T-MLP (the single-model and 3-model-ensemble versions) with: (1) knownnon-pre-trained DNNs: MLP, ResNet, SNN [33], GrowNet [4], TabNet [2], NODE [42], AutoInt [49], DCNv2 [57], TabTransformer [28], DANets[13], FT-Transformer (FT-T) [22], and T2G-Former (T2G) [62]; (2) pre-trained DNN: SAINT [48]; (3) GBDT models: XGBoost [16], CatBoost [43], LightGBM [31], GradientBoostingTree (GBT), HistGradientBoostingTree (HistGBT), and other traditional non-deep machine learning methods like RandomForest (RF) [9]. For other unmentioned baselines, please refer to Appendix B. In the experiment tables below, 'T-MLP' denotes a single T-MLP and 'T-MLP(3)' denotes the ensemble version with three branches.", "4.2 Main Results and Analysis": "In Table 3 to Table 6, the baseline results are based on heavy HPT, and are obtained from respectively reported benchmarks. All the T-MLP results are based on default hyperparameters. Comparison with Advanced DNNs. Tables 3 and 4 report detailed performances and runtime costs on the FT-T and T2G benchmarks for comparison of our T-MLP versions and bespoke tabular DNNs [22, 62]. The baseline results in these tables are based on 50 (for complicated models on large datasets, e.g., FT-Transformer on the Year dataset) or 100 (the other cases) iterations of HPT except special models (default NODE for the datasets with large class numbers and default DANets for all datasets). An overall trend that one may observe is that the single T-MLP is able to achieve competitive results as the state-of-the-art DNNs on each benchmark, and a simple ensemble of three T-MLPs (i.e., 'T-MLP(3)') exhibits even better performances with significantly reduced training costs. Specifically, benefiting from fixed hyperparameters and simple structures, the single T-MLP achieves obvious speedup and reduces training durations by orders of magnitude compared to the powerful DNNs, and is also more training-friendly than XGBoost, a representative GBDT that highly relies on heavy HPT. Besides, we observe only about 10% training duration increase in T-MLP ensemble since we adopt multiprocessing programming to simultaneously train the three T-MLPs (see Sec. 3.5) and thus the training time depends on the slowest converging sub-model. In the implementation details (Sec. 4.1), the single T-MLP uses the smallest learning rate in the three sub-models, and hence the convergence time of T-MLP Table 5: The average values (standard deviations) of all the method ranks on the SAINT benchmark of three task types. | \ud835\udc37 | is the dataset number in each group. Notably, all the baseline results are based on HPT, and SAINT variants need further training budgets on pre-training and data augmentation. More detailed results are given in the Appendix. ensemble often approximates that of the single T-MLP. From the perspective of model storage, as expected, the size of the single T-MLP is comparable to the average level of naive MLPs across the datasets and its size variation is stable (see Table 1), since the block number, hidden dimension size, and sparsity rate are all fixed. In Sec. 4.3, we will further analyze the impact of model sparsity and theoretical complexity of the model parameters. Comparison with Pre-trained DNNs. Table 5 reports the means and standard deviations of model ranks on the SAINT benchmark [48]. Surprisingly, we find that the simple pure MLP-based T-MLP outperforms Transformer-based SAINT variants (SAINT-s and SAINT-i) and is comparable with SAINT on all the three task types. It is worth noting that SAINT and its variants adopt complicated intersample attention and self-supervised pre-training along with HPT on parameters of the training process. Moreover, T-MLP ensemble even achieves stable results that are competitive to tuned GBDTs (i.e., XGBoost, CatBoost, LightGBM) and surpasses the pre-trained SAINT on classification tasks. Since the detailed HPT conditions (i.e., iteration times, HPT methods, parameter sampling distributions) are not reported, we do not estimate specific training costs. Comparison with Extensively Tuned GBDTs. Table 6 compares T-MLP on the typically GPDTs-dominating benchmark TabBen [23], on which GBDT frameworks completely outperform various DNNs across all types of datasets. Results of each baseline on TabBen are obtained with around 400 iterations of heavy HPT, almost representing the ultimate performances with unlimited computation resources and budgets. As expected, when extensively tuned XGBoost is available, the single T-MLP is eclipsed, but it is still competitive to the other ensemble tree models (i.e., RF, GBT, HistGBT) and superior to the compared DNNs. Further, we find that T-MLP ensemble is KDD '24, August 25-29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu Table 6: The average values (standard deviations) of all the method ranks on TabBen (four dataset types). 'Num.' and 'Cat.' denote numerical datasets (all features are numerical) and categorical datasets (some features are categorical), respectively. 'Classif.' and 'Reg.' denote classification and regression tasks. 'Num. Reg.' group includes only results of regression on numerical datasets (similar notations are for the others). | \ud835\udc37 | is the dataset number in each group. Baseline test results are obtained based on the best validation results during \u223c 400 iterations of HPT (according to the TabBen paper and repository). Detailed results are given in the Appendix. able to be comparable to the ultimate XGBoost in all the four dataset types with similar rank stability, serving as a candidate for a tuned XGBoost alternative. More significantly, in the experiments, each T-MLP (or T-MLP ensemble) employs a tensorized XGBoost trained in default configuration (see implementation details in Sec. 4.1), and all the other hyperparameters are fixed; thus T-MLP and its ensemble have potential capability of a higher performance ceiling by HPT or selecting other GBDTs as the feature gate. In summary, we empirically show the strong potential of our hybrid framework to achieve flexible and generalized data adaptability with various tabular preferences (tabular data preferring advanced DNNs, pre-training, or GBDTs). Based on the impressive economical performance-cost trade-off and friendly training process, T-MLP can serve as a promising tabular model framework for real-world applications, especially under limited computation budgets.", "4.3 What Makes T-MLP Cost-effective?": "Table 7 reports ablation and comparison experimental results of TMLPonseveral classification and regression datasets (i.e., California Housing (CA) [40], Adult (AD) [34], Higgs (HI) [5], and Year (YE) [6]) in various data scales (given in parentheses). Main Ablations. The top four rows in Table 7 report the impact of two key designs in a single T-MLP. An overall observation is that both the structure sparsity and GBDT feature gate (FG) contribute to performance enhancement of T-MLP. From the perspective of data processing, GBDT FG brings local sparsity through sample-specific feature selection, and the sparse MLP structure offers global sparsity shared by all samples. Interestingly, we find that the impact of GBDT FG is more profound on the CA dataset. A possible explanation is that the feature amount of CA (8 features) is relatively small Table 7: Main ablation and comparison on classical tables in various task types and data scales. The top 4 rows: ablations on key designs in the T-MLP framework. The bottom 2 rows: results of T-MLP with neural network feature gate (NN FG). compared to the others (14, 28, and 90 features in AD, HI, and YE, respectively) and the average feature importance may be relatively large; thus, the CA results are more likely to be affected by feature selection. For the datasets with larger feature amounts, selecting effective features is likely to be more difficult. Greedy Feature Selection. Wenotice a recent attempt on samplespecific sparsity for biomedical tables using a gating network; it was originally designed for low-sample-size tabular settings and helped prediction interpretability in the biomedical domain [64]. We use its code and build a T-MLP version by substituting GBDT FG with the neural network feature gate (NN FG) for comparison. The bottom two rows of Table 7 report the results. As expected, on the smallest dataset CA, NN FG can boost performance by learning to select informative features, but such a feature gating strategy consistently hurts the performance as data scales increase. This may be due to (1) large datasets demand more complicated structures to learn the meticulous feature selection, (2) the discrete nature of the selection behavior is incompatible with smooth optimization patterns of neural networks, and (3) DNNs' confirmation bias [52] may mislead the learning process, i.e., NN FG will be ill-informed once the subsequent neural network captures wrong patterns. In contrast, GBDT FG always selects features greedily as real GBDTs, which is conservative and generally reasonable. Besides, the complicated sub-tree structures are more complete for the selection action. Sparsity Promotes Tabular DNNs. Fig. 3 plots performance variations on two classification/regression tasks with respect to T-MLP sparsity. Different from the pruning techniques in NLP that aim to trim down model sizes while maintaining the ability of the original models, we find that suitable model sparsity often promotes tabular prediction, but both excessive and insufficient sparsity cannot achieve the best results. The results empirically indicate that, compared to DNN pruning in large pre-trained models for unstructured data, in the tabular data domain, the pruning has the capability to promote non-large tabular DNNs as GBDTs' beneficial sparse structures achieved by tree pre-pruning, and the hidden dimension in tabular DNNs is commonly over-parameterized.", "4.4 Superiority Interpretability of T-MLP": "In Fig. 4, we visualize decision boundaries of FT-Transformer, XGBoost, and the single T-MLP to inspect data patterns captured by Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs KDD '24, August 25-29, 2024, Barcelona, Spain Figure 3: Performance variation plots on the Adult and Year datasets with respect to variations of T-MLP sparsity. All the best results are achieved with suitable sparsity. Adult(49K) 0.864 8.875 Year(515K) 8.850 0.862 0.860 8.825 8.800 0.858 8.775 0.2 0.4 0.6 0.8 1.0 T-MLP Sparsity Rate Figure 4: Decision boundary visualization of FT-Transformer (FT-T), XGBoost, and a single-block T-MLP on the Bioresponse and Credit-g datasets, using two most important features. Different colors represent distinct categories, while the varying shades of colors indicate the predicted probabilities. FT-T (82%) XGB (84%) T-MLP (85%) FT-T (77%) XGB (76%) T-MLP (80%) Credit-g Bioresponse these three methods. The two most important features are selected by mutual information (estimated with the Scikit Learn package). Different from common DNNs and GBDTs, T-MLP exhibits a novel intermediate pattern that combines characteristics from both DNNs and GBDTs. Compared to DNNs, T-MLP yields grid-like boundaries whose edges are often orthogonal to feature surfaces as GBDTs, and the complexity is essentially simplified with pruned sparse architectures. Besides, T-MLP is able to capture tree-model-like subpatterns (see T-MLP on Credit-g), while DNNs manage only main patterns. Hence, DNNs are overfit-sensitive due to their relatively irregular boundaries and neglecting of fine-grained sub-patterns. Compared to GBDTs with jagged boundaries and excessively split sub-patterns, T-MLP holds very smooth vertices at the intersection of boundaries (see T-MLP on Credit-g). Notably, T-MLP can decide conditional split points like GBDT feature splitting (orthogonal edges at feature surfaces) through a smooth process (see T-MLP boundary edges on Bioresponse, from top to bottom, in which the split point on the horizontal feature is conditionally changed with respect to the vertical feature in a smooth manner, while XGBoost is hard to attain such dynamical split points). Overall, T-MLP possesses both the advantages to be overfit-resistant, which helps provide its superiority on both GBDT- and DNN-favored datasets.", "5 CONCLUSIONS": "In this paper, we proposed T-MLP, a novel hybrid framework attaining the advantages of both GBDTs and DNNs to address the model selection dilemma in tabular prediction tasks. We combined a tensorized GBDT feature gate, DNN pruning techniques, and a vanilla back-propagation optimizer to develop a simple yet efficient and widely effective MLP model. Experiments on diverse benchmarks showed that, with significantly reduced runtime costs, T-MLP has the generalized adaptability to achieve considerably competitive results regardless of dataset-specific framework preferences. We expect that our T-MLP will serve as a practical method for economical tabular prediction as well as in broad applications, and help advance research on hybrid tabular models.", "ACKNOWLEDGMENTS": "This research was partially supported by National Natural Science Foundation of China under grants No. 62176231, Zhejiang Key R&D Program of China under grant No. 2023C03053 and No. 2024SSYS0026.", "REFERENCES": "[1] Naomi S Altman. 1992. An introduction to kernel and nearest-neighbor nonparametric regression. The American Statistician 46, 3 (1992), 175-185. [2] Sercan \u00d6 Arik and Tomas Pfister. 2021. TabNet: Attentive interpretable tabular learning. In AAAI . 6679-6687. [3] Saqib Aziz, Michael Dowling, Helmi Hammami, and Anke Piepenbrink. 2022. Machine learning in finance: A topic modeling approach. European Financial Management (2022). [4] Sarkhan Badirli, Xuanqing Liu, Zhengming Xing, Avradeep Bhowmik, Khoa Doan, and Sathiya S Keerthi. 2020. Gradient boosting neural networks: GrowNet. arXiv preprint arXiv:2002.07971 (2020). [5] Pierre Baldi, Peter Sadowski, et al. 2014. Searching for exotic particles in highenergy physics with deep learning. Nature Communications 5, 1 (2014), 4308. [6] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. 2011. The million song dataset. In ISMIR . [7] Vadim Borisov, Tobias Leemann, Kathrin Se\u00dfler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. 2022. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems (2022). [8] Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. 2022. Language Models are Realistic Tabular Data Generators. In ICLR . [9] Leo Breiman. 2001. Random forests. Machine learning 45 (2001), 5-32. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In NeurIPS , Vol. 33. 1877-1901. [11] Guiping Cao, Shengda Luo, Wenjian Huang, Xiangyuan Lan, Dongmei Jiang, Yaowei Wang, and Jianguo Zhang. 2023. Strip-MLP: Efficient Token Interaction for Vision MLP. In ICCV . 1494-1504. [12] Jintai Chen, KuanLun Liao, Yanwen Fang, Danny Chen, and Jian Wu. 2022. TabCaps: A Capsule Neural Network for Tabular Data Classification with BoW Routing. In ICLR . [13] Jintai Chen, Kuanlun Liao, Yao Wan, Danny Z Chen, and Jian Wu. 2022. DANets: Deep abstract networks for tabular data classification and regression. In AAAI . [14] Jintai Chen, Jiahuan Yan, Danny Ziyi Chen, and Jian Wu. 2023. ExcelFormer: A Neural Network Surpassing GBDTs on Tabular Data. arXiv preprint arXiv:2301.02819 (2023). [15] Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O Arik, and Tomas Pfister. 2023. TSMixer: An All-MLP architecture for time series forecasting. arXiv preprint arXiv:2303.06053 (2023). [16] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 785-794. [17] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for YouTube recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems . KDD '24, August 25-29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu [18] Jerome H Friedman. 2001. Greedy function approximation: A gradient boosting machine. Annals of Statistics (2001). [19] Francesco Fusco, Damian Pascual, Peter Staar, and Diego Antognini. 2023. pNLPMixer: An Efficient all-MLP Architecture for Language. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 53-60. [20] Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized trees. Machine learning 63 (2006), 3-42. [21] Yury Gorishniy, Ivan Rubachev, and Artem Babenko. 2022. On embeddings for numerical features in tabular deep learning. In NeurIPS . 24991-25004. [22] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. 2021. Revisiting deep learning models for tabular data. In NeurIPS . 18932-18943. [23] L\u00e9o Grinsztajn, Edouard Oyallon, and Ga\u00ebl Varoquaux. 2022. Why do tree-based models still outperform deep learning on typical tabular data?. In NeurIPS . [24] Jianyuan Guo, Yehui Tang, et al. 2022. Hire-MLP: Vision MLP via hierarchical rearrangement. In CVPR . 826-836. [25] Xinran He, Junfeng Pan, et al. 2014. Practical lessons from predicting clicks on ads at Facebook. In Proceedings of the International Workshop on Data Mining for Online Advertising . [26] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. 2020. DynaBERT: Dynamic BERT with adaptive width and depth. In NeurIPS , Vol. 33. 9782-9793. [27] Jeremy Howard and Sylvain Gugger. 2020. Fastai: A layered API for deep learning. Information 11, 2 (2020), 108. [28] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. 2020. TabTransformer: Tabular data modeling using contextual embeddings. arXiv preprint arXiv:2012.06678 (2020). [29] Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. 2021. Welltuned simple nets excel on tabular datasets. In NeurIPS . 23928-23941. [30] Liran Katzir, Gal Elidan, and Ran El-Yaniv. 2020. Net-DNF: Effective deep modeling of tabular data. In ICLR . [31] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A highly efficient gradient boosting decision tree. In NeurIPS . [32] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT . 4171-4186. [33] G\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. 2017. Self-normalizing neural networks. In NeurIPS , Vol. 30. [34] Ron Kohavi et al. 1996. Scaling up the accuracy of Naive-Bayes classifiers: A decision-tree hybrid. In KDD , Vol. 96. 202-207. [35] Bin Li, J Friedman, R Olshen, and C Stone. 1984. Classification and regression trees (CART). Biometrics (1984). [36] Hanxiao Liu, Zihang Dai, David So, and Quoc V Le. 2021. Pay attention to MLPs. In NeurIPS . 9204-9215. [37] Christos Louizos, Max Welling, and Diederik P Kingma. 2018. Learning Sparse Neural Networks through L_0 Regularization. In ICLR . [38] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. LLM-Pruner: On the Structural Pruning of Large Language Models. In NeurIPS . [39] Tomas Mikolov, Kai Chen, et al. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013). [40] R Kelley Pace and Ronald Barry. 1997. Sparse spatial autoregressions. Statistics & Probability Letters 33, 3 (1997), 291-297. [41] F. Pedregosa, G. Varoquaux, et al. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825-2830. [42] Sergei Popov, Stanislav Morozov, and Artem Babenko. 2019. Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data. In ICLR . [43] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. 2018. CatBoost: Unbiased boosting with categorical features. In NeurIPS . [44] Alec Radford, Jong Wook Kim, et al. 2021. Learning transferable visual models from natural language supervision. In ICML . 8748-8763. [45] Camilo Ruiz, Hongyu Ren, Kexin Huang, and Jure Leskovec. 2023. Enabling tabular deep learning when \ud835\udc51 \u226b \ud835\udc5b with an auxiliary knowledge graph. arXiv preprint arXiv:2306.04766 (2023). [46] Sungyong Seo, Jing Huang, Hao Yang, and Yan Liu. 2017. Interpretable convolutional neural networks with dual local and global attention for review rating prediction. In Proceedings of the 11th ACM Conference on Recommender Systems . 297-305. [47] Ravid Shwartz-Ziv and Amitai Armon. 2022. Tabular data: Deep learning is not all you need. Information Fusion 81 (2022), 84-90. [48] Gowthami Somepalli, Avi Schwarzschild, Micah Goldblum, C Bayan Bruss, and Tom Goldstein. 2022. SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training. In NeurIPS 2022 First Table Representation Workshop . [49] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. AutoInt: Automatic feature interaction learning via selfattentive neural networks. In CIKM . 1161-1170. [50] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023. A Simple and Effective Pruning Approach for Large Language Models. arXiv preprint arXiv:2306.11695 (2023). [51] Chuanxin Tang, Yucheng Zhao, et al. 2022. Sparse MLP for image recognition: Is self-attention really necessary?. In AAAI . 2344-2351. [52] Antti Tarvainen and Harri Valpola. 2017. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS , Vol. 30. [53] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. 2021. MLP-Mixer: An all-MLP architecture for vision. In NeurIPS . 24261-24272. [54] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. 2022. MAXIM: Multi-Axis MLP for image processing. In CVPR . 5769-5780. [55] Shahadat Uddin, Arif Khan, Md Ekramul Hossain, and Mohammad Ali Moni. 2019. Comparing different supervised machine learning algorithms for disease prediction. BMC Medical Informatics and Decision Making (2019), 1-16. [56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS . [57] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. DCN V2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In WWW . 1785-1797. [58] Zifeng Wang and Jimeng Sun. 2022. TransTab: Learning transferable tabular Transformers across tables. In NeurIPS , Vol. 35. 2902-2915. [59] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2020. Structured Pruning of Large Language Models. In EMNLP . 6151-6162. [60] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. 2022. Model soups: Averaging weights of multiple finetuned models improves accuracy without increasing inference time. In ICML . 23965-23998. [61] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured Pruning Learns Compact and Accurate Models. In ACL . [62] Jiahuan Yan, Jintai Chen, Yixuan Wu, Danny Z Chen, and Jian Wu. 2023. T2GFormer: Organizing tabular features into relation graphs promotes heterogeneous feature interaction. In AAAI . [63] Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun, Jian Wu, and Jintai Chen. 2024. Making Pre-trained Language Models Great on Tabular Prediction. In ICLR . [64] Junchen Yang, Ofir Lindenbaum, and Yuval Kluger. 2022. Locally sparse neural networks for tabular biomedical data. In ICML . PMLR, 25123-25153. [65] Jun Zhang and Vasant Honavar. 2003. Learning from attribute value taxonomies and partially specified instances. In ICML . [66] Jun Zhang, D-K Kang, et al. 2006. Learning accurate and concise Na\u00efve Bayes classifiers from attribute value taxonomies and data. Knowledge and Information Systems (2006). [67] Tianping Zhang, Shaowen Wang, Shuicheng Yan, Jian Li, and Qian Liu. 2023. Generative Table Pre-training Empowers Models for Tabular Prediction. arXiv preprint arXiv:2305.09696 (2023). [68] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [69] Bingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, and Mahsa Shoaran. 2023. XTab: Cross-table Pretraining for Tabular Transformers. In ICML .", "A BENCHMARK CHARACTERISTICS": "We provide detailed dataset statistical information of each benchmark in Table 2. These benchmarks exhibit broad data diversity in data scales and task types. From the FT-T benchmark to TabBen, the overall data volume is gradually reduced. We additionally visualize the respective winning rates of GBDT and DNN frameworks in Fig. 2, indicating varying framework preferences among the dataset collections used in different tabular prediction tasks. FT-T does not include GBDT baselines in its main benchmark, but has the most extremely large datasets. Overall, the FT-T benchmark is the extremely large-scale data collection (in both data volume and feature width), the T2G benchmark is a large one, the SAINT benchmark contains diverse data scales, and TabBen focuses on middle-size typical tables. Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs KDD '24, August 25-29, 2024, Barcelona, Spain", "B BASELINE INFORMATION": "We list all the compared baselines in this section. \u00b7 MLP: Vanilla multi-layer perception with no feature interaction. \u00b7 ResNet: A popular DNN backbone in vision applications. \u00b7 SNN [33]: An MLP-like architecture with SELU activation. \u00b7 GrowNet [4]: MLPs built in a gradient boosted manner. \u00b7 NODE [42]: Generalized oblivious decision tree ensembles. \u00b7 TabNet [2]: A Transformer-based recurrent architecture emulating tree-based learning process. \u00b7 AutoInt [49]: Attention-based feature embeddings. \u00b7 DCNv2 [57]: An MLP-based architecture with the featurecrossing module. \u00b7 TabTransformer [28]: A Transformer model concatenating numerical features and encoded categorical features. \u00b7 DANets [13]: An MLP-based architecture with neural-guided feature selection and abstraction in each block. \u00b7 FT-Transformer [22]: A popular tabular Transformer encoding both numerical and categorical features. \u00b7 T2G-Former [62]: A tabular Transformer with automatic relation graph estimation for selective feature interaction. \u00b7 SAINT [48]: A Transformer-like architecture performing row-level and column-level attention, and contrastively pretraining to minimize the differences between data points and their augmented views. \u00b7 XGBoost [16]: A predominant GBDT implementation. \u00b7 CatBoost [43]: A GBDT approach with oblivious decision trees. \u00b7 LightGBM [31]: An efficient GBDT implementation. \u00b7 RandomForest [9]: A popular bagging ensemble algorithm of decision trees. \u00b7 ExtraTrees [20]: A classical tree bagging implementation. \u00b7 k-NN [1]: Traditional supervised machine learning algorithms; two KNeighbors models are used (KNeighborsDist, KNeighborsUnif). \u00b7 NeuralNetFastAI [27]: FastAI neural network models that operate on tabular data. \u00b7 sklearn-GBDT [41]: Two traditional GBDT implementations (GradientBoostingTree and HistGradientBoostingTrees) provided in the Scikit Learn package.", "C RUNTIME ENVIRONMENT AND HYPERPARAMETERS": "", "C.1 Runtime Environment": "All the experiments are conducted with PyTorch version 1.11.0, CUDA version 11.3, and Scikit Learn version 1.1.0, with each trial using an NVIDIA A100 PCIe 40GB and an Intel Xeon Processor 40C.", "C.2 Hyperparameters of T-MLP": "In the main experiments, we uniformly set the hidden size \ud835\udc51 to 1024, the intermediate size \ud835\udc51 \u2032 to 676 (2/3 of the hidden size), the sparsity rate to 0.33, and the residual dropout rate to 0.1, with three basic blocks for multi-class classification or extremely large binary classification datasets, and one block for the others. The learning rate of the single T-MLP is 1e-4, and the learning rates of the three branches in T-MLP ensemble are 1e-4, 5e-4, and 1e-3, respectively.", "C.3 Hyperparameters of Baselines": "For all the baselines on the FT-T and T2G benchmarks, we follow the given hyperparameter spaces and iteration times from the original benchmark papers to estimate the training costs. KDD '24, August 25-29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu Table 8: AUC scores (the higher the better) of the baselines on the binary classification datasets in the SAINT benchmark. Table 9: Accuracy scores (the higher the better) of the baselines on the multi-class classification datasets in the SAINT benchmark. Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs KDD '24, August 25-29, 2024, Barcelona, Spain Table 10: RMSE scores (the lower the better) of the baselines on the regression datasets in the SAINT benchmark. Table 11: Accuracy scores (the higher the better) of the baselines for the binary classification tasks on the TabBen numerical datasets. Table 12: R-Squared scores (the higher the better) of the baselines for the regression tasks on the TabBen numerical datasets. KDD '24, August 25-29, 2024, Barcelona, Spain Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu Table 13: Accuracy scores (the higher the better) of the baselines for the binary classification tasks on the TabBen categorical datasets. Table 14: R-Squared scores (the higher the better) of the baselines for the regression tasks on the TabBen categorical datasets."}
