{"title": "COOOL: A Learning-To-Rank Approach for SQL Hint Recommendations", "authors": "Xianghong Xu; Zhibing Zhao; Tieying Zhang; Rong Kang; Jianjun Chen", "pub_date": "2023-04-10", "abstract": "Query optimization is a pivotal part of every database management system (DBMS) since it determines the efficiency of query execution. Numerous works have introduced Machine Learning (ML) techniques to cost modeling, cardinality estimation, and end-to-end learned optimizer, but few of them are proven practical due to long training time, lack of interpretability, and integration cost. A recent study provides a practical method to optimize queries by recommending per-query hints but it suffers from two inherited problems. First, it follows the regression framework to predict the absolute latency of each query plan, which is very challenging because the latencies of query plans for a certain query may span multiple orders of magnitude. Second, it requires training a model for each dataset, which restricts the application of the trained models in practice. In this paper, we propose COOOL to predict Cost Orders of query plans to cOOperate with DBMS by Learning-To-Rank. Instead of estimating absolute costs, COOOL uses ranking-based approaches to compute relative ranking scores of the costs of query plans. We show that COOOL is theoretically valid to distinguish query plans with different latencies. We implement COOOL on PostgreSQL, and extensive experiments on join-order-benchmark and TPC-H data demonstrate that COOOL outperforms PostgreSQL and state-of-the-art methods on single-dataset tasks as well as a unified model for multiple-dataset tasks. Our experiments also shed some light on why COOOL outperforms regression approaches from the representation learning perspective, which may guide future research.", "sections": [{"heading": "INTRODUCTION", "text": "Query optimization is vital to the performance of every database management system (DBMS). A SQL query typically has many candidate plans that are equivalent in terms of final output but differ a lot in execution latency. The goal of query optimization is to select the best candidate plan with the lowest latency for each query from a vast search space with sufficient accuracy.\nQuery optimization has been studied for decades [31] and is still an active research field [35]. Various ML-based research lines have been proposed: cost modeling, cardinality estimation, end-to-end query optimization, etc., among which the most practical approach is Bao [23]. Bao is a query optimization system leveraging tree convolutional neural networks [27] and Thompson sampling [33] to recommend SQL hints. Bao has made a remarkable improvement in the practicality of end-to-end query optimization, but it suffers from two problems inherited from previous models [19,25,32,41,42]. First, Bao follows the same underlying assumption as the previous works, i.e., the model needs to predict the exact cost of each plan to select the plan with the minimum cost. It first estimates the absolute cost of each candidate plan under a regression framework and then selects the one with the minimum estimate. While accurately estimating the cost is sometimes desirable, it is very challenging for existing models [1,36,37]. As a result, the model may make inaccurate predictions and select query plans with high latencies. Second, Bao trains a model for each dataset respectively and evaluates the model on the corresponding dataset. The datasets vary among DBMS instances and it is costly to train and maintain individual models for every dataset. Therefore, the generalizability of the model is desirable in real-world scenarios.\nTo address these, we propose COOOL to estimate Cost Orders of query plans to cOOperate with DBMS leveraging Learning-To-Rank (LTR) techniques. COOOL is designed on top of an existing DBMS to recommend query-specific hints to facilitate practical use.\nTo obtain a unified model that can optimize queries from different datasets, we employ a data/schema agnostic TCNN as the query plan cost ranking scorer to predict relative cost orders of candidate plans. We make similar assumptions to Bao to inherit its advantages: we assume a finite set of hint sets are predefined and all hint sets result in semantically equivalent query plans.\nIn contrast to regression methods to predict costs, LTR approaches learn relative scores for different hint sets. LTR approaches have advantages over regression approaches from previous end-to-end query optimization works [23,25,40] for the following reasons. The plan execution latency ranges from several milliseconds to thousands of seconds. And a little difference in the tree structures of two semantically equivalent plans for a query may lead to a large difference in execution latencies. These facts bring difficulties in regression model training and inference because the objective function of the regression approach is to minimize the \ud835\udc3f 2 error. The squared error formula is sensitive to anomalous large or small latencies, which may make the model performance unstable [40]. Despite normalization methods can alleviate this problem by mapping the original latency distribution to a fixed interval (e.g, [0,1]), they may lead to a latency distribution distortion because the relative latency gap among query plans cannot be fully reflected. Accurately estimating the latency of every query plan is not necessary for end-to-end query optimization since the optimizer only needs to select one of them to execute. A good prediction on the orders of the latencies of query plans is sufficient for query optimization, which is exactly the objective of LTR.\nLTR is a supervised learning framework to train models for ranking tasks. There are primarily three categories of LTR approaches: pointwise, pairwise, and listwise. Most pointwise methods are the same as regression. Pairwise methods concern the relative order between two items, and listwise methods focus on the order of items in the entire list. The three approaches can be applied to most existing models, e.g., neural networks, and the only difference lies in the loss function. In this work, we leverage the widely used tree convolutional neural networks (TCNN) [23,27,40] as the underlying model. To train an LTR model, one can use the orders of latencies as labels, which has the potential to be more robust than regression models against the large range of orders of magnitude in query plan latencies. By transforming the absolute cost estimation problem into relative cost order prediction, COOOL can utilize LTR techniques to train the TCNN so that the output tells which plan is the best. Specifically, we respectively study the performance of pairwise and listwise strategies on SQL hints recommendation for query optimization, so COOOL has two implementations: COOOLpair and COOOL-list. By this means, we may also make it possible to train a unified model to improve query plans from different datasets.\nOur experiments show that COOOL can consistently improve query plans in various settings, achieving as large as 6.73\u00d7 speedup over PostgreSQL. Moreover, we investigate the regression framework and ranking strategies from the perspective of representation learning [44]. We show that the model trained by the regression approach has a dimensional collapse [9] in the plan embedding space. Whereas COOOL does not have a dimensional collapse using the same embedding method. The dimensional collapse will restrict the ML methods to building a unified model because the collapsed dimensions may be different in different datasets.\nFurthermore, Bao requires more effort to implement because it is fully integrated into PostgreSQL, while we build on top of PostgreSQL, which makes it easy for COOOL to migrate to other DBMSs.\nTo summarize, we make the following contributions: A model \ud835\udc40 is a function that takes the candidate plan tree as input and produces a score for the plan tree.\n\ud835\udc60 \ud835\udc5e \ud835\udc56 = \ud835\udc40 (\ud835\udc5e, \ud835\udc61 \ud835\udc5e \ud835\udc56 ; \u00ec \ud835\udf03 ),(2)\nwhere \u00ec \ud835\udf03 is the parameter of the model to be trained. The query execution engine then selects the plan with the highest predicted ranking score in the scenario of LTR.\n\u0124\ud835\udc46 \ud835\udc5e = \ud835\udc3b\ud835\udc46 arg max \ud835\udc56 \ud835\udc60 \ud835\udc5e \ud835\udc56 ,(3)\nwhere \u0124\ud835\udc46 \ud835\udc5e is the hint set with the maximum score, corresponding to the minimum estimated cost.", "publication_ref": ["b30", "b34", "b22", "b26", "b32", "b18", "b24", "b31", "b40", "b41", "b0", "b35", "b36", "b22", "b24", "b39", "b39", "b22", "b26", "b39", "b43", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Learning-To-Rank (LTR)", "text": "In the context of LTR, H is the set of items, and the goal is to recommend the best item from H to any \ud835\udc5e \u2208 \ud835\udc44. More specifically, LTR is to define a loss function on \ud835\udc60 \ud835\udc5e \ud835\udc56 's so that the underlying model \ud835\udc40 can be properly trained to predict the orders of query plans. Throughout this paper, \"\ud835\udc61 Before introducing our pairwise loss function, we will first introduce a classic model, the Plackett-Luce model (PL) [22,28], based on which we selected our pairwise method. PL is one of the most popular models for discrete choices, which was later used as a listwise loss function in information retrieval [20] and softmax function in classification tasks [8]. In the context of SQL hint recommendations, we provide a definition of PL as follows. Given any \ud835\udc5e \u2208 \ud835\udc44, the probability of\n\ud835\udf0e \ud835\udc5e = \ud835\udc61 \ud835\udc5e \ud835\udc56 1 \u227b \ud835\udc61 \ud835\udc5e \ud835\udc56 2 \u227b . . . \u227b \ud835\udc61 \ud835\udc5e \ud835\udc56 \ud835\udc5b is Pr PL (\ud835\udc61 \ud835\udc5e \ud835\udc56 1 \u227b \ud835\udc61 \ud835\udc5e \ud835\udc56 2 \u227b . . . \u227b \ud835\udc61 \ud835\udc5e \ud835\udc56 \ud835\udc5b ; \u00ec \ud835\udf03 ) = \ud835\udc5b \ud835\udc57=1 exp(\ud835\udc60 \ud835\udc5e \ud835\udc56 \ud835\udc57 ) \ud835\udc5b \ud835\udc5a=\ud835\udc57 exp(\ud835\udc60 \ud835\udc5e \ud835\udc56 \ud835\udc5a ) ,(4)\nwhere \ud835\udc60 \ud835\udc5e \ud835\udc56 's are functions of the parameter \u00ec \ud835\udf03 defined in Equation ( 2). The rankings of multiple queries are assumed to be independent. Therefore, the probability of multiple rankings is simply the product of the probability of each individual ranking. The marginal probability of any pairwise comparison \ud835\udc61 .\n(5)\n2.2.1 Listwise Loss Function. Given the training data {\ud835\udf0e \ud835\udc5e |\u2200\ud835\udc5e \u2208 \ud835\udc44 }, the listwise loss function is simply the negative log likelihoood function:\nL list ( \u00ec \ud835\udf03 ) = - \u2211\ufe01 \ud835\udc5e \u2208\ud835\udc44 ln Pr PL (\ud835\udf0e \ud835\udc5e ; \u00ec \ud835\udf03 ),(6)\nwhere Pr PL (\ud835\udf0e \ud835\udc5e ; \u00ec \ud835\udf03 ) is defined in Equation ( 4). This listwise loss function also coincides the listMLE loss by [38]. [38] proved that this loss function is consistent, which means as the size of the dataset goes to infinity, the learned ranking converges to the optimal one.", "publication_ref": ["b21", "b27", "b19", "b7", "b37", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Pairwise Loss Function.", "text": "To apply a pairwise loss function, the full rankings \ud835\udf0e \ud835\udc5e for all \ud835\udc5e \u2208 \ud835\udc44 need to be converted to pairwise comparisons. This process is called rank-breaking [2]. A rank-breaking method defines how the full rankings should be converted to pairwise comparisons. Basic breakings include full breaking, adjacent breaking, and others [2]. Full breaking means extracting all pairwise comparisons from a ranking, and adjacent breaking means extracting only adjacent pairwise comparisons. For example, given a ranking \ud835\udc61 1 \u227b \ud835\udc61 2 \u227b \ud835\udc61 3 , full breaking converts it to (\ud835\udc61 1 \u227b \ud835\udc61 2 , \ud835\udc61 1 \u227b \ud835\udc61 3 , \ud835\udc61 2 \u227b \ud835\udc61 3 ) while adjacent breaking converts it to (\ud835\udc61 1 \u227b \ud835\udc61 2 , \ud835\udc61 2 \u227b \ud835\udc61 3 ). Though adjacent breaking is simple and plausible, [2] proved that adjacent breaking can lead to inconsistent parameter estimation, where means even if the model is trained using an infinite amount of data, it may not make unbiased predictions. Other breakings are more complicated and beyond the scope of this paper.\nLet \ud835\udc43 = {\ud835\udf0b 1 , \ud835\udf0b 2 , . . . , \ud835\udf0b \ud835\udc5a } be the dataset of pairwise comparisons, where for any \ud835\udc57 \u2208 {1, 2, . . . , \ud835\udc5a}, \ud835\udf0b \ud835\udc57 has the form of \ud835\udc61\n\ud835\udc5e \ud835\udc57 \ud835\udc56 1 \u227b \ud835\udc61 \ud835\udc5e \ud835\udc57 \ud835\udc56 2 .\nHere \ud835\udc5e \ud835\udc57 denotes the corresponding query for \ud835\udf0b \ud835\udc57 . For different \ud835\udc57 values, \ud835\udc5e \ud835\udc57 may refer to the same query since different pairwise comparisons can be extracted from the same query. The objective function is\nL pair ( \u00ec \ud835\udf03 ) = - \ud835\udc5a \u2211\ufe01 \ud835\udc57=1 ln Pr PL (\ud835\udf0b \ud835\udc57 ; \u00ec \ud835\udf03 ),(7)\nwhere Pr PL (\ud835\udc5d \ud835\udc57 ; \u00ec \ud835\udf03 ) is defined in Equation (5). The model parameter can be estimated my maximizing \ud835\udc3f( \u00ec \ud835\udf03 ). This is equivalent to maximizing the composite marginal log likelihood based on Equation ( 5). [14] and [47] proved that \u00ec \ud835\udf03 can be efficiently estimated by under a correct rank breaking method. And full breaking, which extracts all pairwise comparisons from the full rankings, is one of the correct breaking methods.", "publication_ref": ["b1", "b1", "b1", "b4", "b13", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "COOOL ARCHITETURE", "text": "The core component of COOOL is a neural plan ranking scorer, which estimates the relative ranking scores of candidate plans. At the training stage, we execute query plans and collect their performance metrics as training data. Then we utilize the pairwise or listwise approach to implement training the ranking scorer. At the inference stage, when a user submits a query, the traditional optimizer will generate \ud835\udc5b query plans by utilizing the corresponding hint sets. Next, the scorer will compute the relative ranking score of each plan and recommend the optimal one to the execution engine. The data flow pipeline of COOOL is shown in Figure 1.\nData Collection. Our approach is trained in a standard supervised learning paradigm. For the given queries, we generate \ud835\udc5b query plans that correspond to the hint sets H for each query by the underlying traditional optimizer. Then we sent them to the execution engine and record the observed execution performance of each query plan. The collected data is used to train the neural ranking scorer, and the training stage is separate from the DBMS.\nCost Order Estimation. To execute the optimal plan with minimum estimated latency, we need to recommend its corresponding hint set to the DBMS. The scorer takes a plan tree as input and outputs the ranking score of the plan, the estimated latency orders can be acquired by sorting the scores of all candidate plans. Specifically, we first transform the nodes of the input plan tree into vectors, then feed the vector tree into a plan embedding model constructed by a TCNN. Finally, we feed the plan embedding into a multilayer perceptron (MLP) to compute the estimated score of the plan. At the training stage, we use the collected plan and latency data to train the model. So it can estimate relative orders of plans by latency and cooperate with DBMS to improve query plans at the inference stage.\nAssumptions and Comparisons. We assume that applying each hint set to the given query will generate semantically equivalent plans. Besides, the hints are applied to the entire query rather than the partial plan. Though allowing fine-grained hints (e.g., allows nested loop joins between specific tables, others not.) are available, it will bring an exponential candidate plan search space, which significantly increases training and inference overhead. We make the same assumption as Bao due to practicality.\nBao requires more effort to implement because it is fully integrated into PostgreSQL, while we build on top of PostgreSQL, which makes it easy for COOOL to migrate to other DBMSs. Moreover, we take a step forward to maintain a unified model to optimize queries from different datasets, which has not been investigated in the previous end-to-end query optimization studies.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "COOOL FOR HINT RECOMMENDATIONS", "text": "First, we introduce our cost order estimation model that takes query plans as input and computes the relative ranking scores. Next, we take advantage of the pairwise and listwise approaches to formalize the training loop. Finally, we elaborate theoretical analysis of how ranking losses work to select the best plan.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cost Order Estimation", "text": "Each hint set corresponds to a query plan tree, so recommending the optimal hint set for given queries is to select the plan tree with maximum cost ranking score, as shown in Equation ( 3). Similar to [23,25,40], we use a TCNN to obtain plan embeddings and leverage an MLP to compute the ranking scores.\nPlan Tree Vectorization. We can use the EXPLAIN command provided by the underlying optimizer to obtain the plan tree text, as shown in Equation (1). First, we need to transform each node in the plan tree into a vector. The same as [23], we use a data/schema agnostic encoding scheme, which solely contains a one-hot encoding of operator types, and cardinality and cost provided by the underlying traditional optimizer. Then, we transform the original tree into a binary tree to facilitate tree convolution operations.\nOne-hot Node Operation Type Encoding. We summarize the types of all operations (nested loop, hash join, merge join, seq scan, index scan, index only scan, and bitmap index scan) in the plan trees and number these seven operations. Then we create a vector for each node with the number of types of bits, and we set the bit of the type corresponding to each node to the high bit. For instance, \ud835\udc38 \ud835\udc5c (\ud835\udc63) is the one-hot encoding of node \ud835\udc63 and \ud835\udc38 \ud835\udc5c (\ud835\udc63) [\ud835\udc56] = 1 indicates node \ud835\udc63 is the \ud835\udc56-th operation type and the rest elements in \ud835\udc38 \ud835\udc5c (\ud835\udc63) are 0. Though one-hot node type encoding is simple, it is capable to extract structural information in plan trees.\nTree Nodes Vectorization. Apart from the operation information, each node can contain the cost and cardinality. It is applicable to acquire cost and cardinality from multiple traditional optimizers and learned models, but we only use two values obtained from the underlying traditional optimizer for simplicity. Therefore, the node encoding is the concatenation of operation type encoding, cardinality, and cost, i.e., \ud835\udc38 (\ud835\udc63) = Concat(\ud835\udc38 \ud835\udc5c (\ud835\udc63), Cost(\ud835\udc63), Card(\ud835\udc63)), where Cost(\ud835\udc63) and Card(\ud835\udc63) are the cost and cardinality estimated by the traditional optimizer, respectively. We apply the node encoding method to the nodes in the plan tree to obtain a vectorized plan tree.\nTree Structure Binarization. Some nodes in a plan tree may have only one child, e.g., nodes for aggregation and sorting operations. To facilitate tree convolution operations, we transform the nonbinary trees into binary trees by adding a pseudo-child node Null to each node with only one child, and the costs and cardinalities of pseudo-child nodes are 0. Then the original plan tree \ud835\udc61 can be transformed into vectorized tree \ud835\udc5d.\n4.1.1 TCNN Plan Embedding. TCNN was proposed in [27] to treat tree structure data in programming language processing. TCNN was first introduced in plan representation in [25], and it was wellestablished in [23,40]. We will briefly introduce how to represent a plan using TCNN in this section, refer to [25,27] for more technical details.\nDuring the execution of the original plan tree in execution engines, the computation of one node relies on the results of its child nodes. Based on this fact, the plan embedding method should reflect the recursive properties to obtain a proper inductive bias [26]. To be consistent with plan execution, the model is naturally required to simultaneously capture the features of a node and its child nodes. Specifically, denote \ud835\udc59 (\ud835\udc63) and \ud835\udc5f (\ud835\udc63) are the left and right child nodes of node \ud835\udc63, respectively. The statistic cost/cardinality information in vector \ud835\udc38 (\ud835\udc63) is closely related to \ud835\udc38 (\ud835\udc59 (\ud835\udc63)) and \ud835\udc38 (\ud835\udc5f (\ud835\udc63)). Tree convolution can naturally address this request.\nTree convolution is similar to image convolution, it has binary tree structure filters to capture local features. We take a tree convolution filer as an example, there are three weight vectors in the filter, i.e., \ud835\udc64, \ud835\udc64 \ud835\udc59 , \ud835\udc64 \ud835\udc5f . Applying tree convolution to the current node \ud835\udc38 (\ud835\udc63) can acquire the new representation:\n\ud835\udc38 (\ud835\udc63) \u2032 = \ud835\udf0e (\ud835\udc38 (\ud835\udc63) \u2299 \ud835\udc64 + \ud835\udc38 (\ud835\udc59 (\ud835\udc63)) \u2299 \ud835\udc64 \ud835\udc59 + \ud835\udc38 (\ud835\udc5f (\ud835\udc63)) \u2299 \ud835\udc64 \ud835\udc5f ), (8\n)\nwhere \ud835\udf0e is a non-linear activation function, \u2299 is a dot product operation. The new representation of node \ud835\udc63 contains its child nodes' information. By stacking tree convolution operations, the latent representations will have a larger receptive field than just being able to interact with child nodes. By this means, the model is able to capture high-level features of a long chain of plan execution for representing one node. The output of tree convolution operations is a tree with the same structure as the input, so we employ a dynamic pooling method to aggregate the latent representations of all nodes to represent the query plan. To sum up, we can obtain plan embedding by \u00ec \ud835\udc5d = TCNN(\ud835\udc5d), where \u00ec \ud835\udc5d \u2208 R \u210e is the vector of plan representation, \u210e is the size of plan tree embedding space. 4.1.2 Ranking Score Computation. Finally, we can leverage plan embedding to compute the relative ranking score. We use a simple MLP to take the plan embedding vector as input and output a scalar as the ranking score \ud835\udc60, i.e., \ud835\udc60 = MLP( \u00ec \ud835\udc5d). Stacking fully connected layers and non-linear activation functions in the MLP can enhance the representation ability, and this practice has been widely adopted in query optimizations [25,40]. We add a hidden layer and an activation function for simplicity. The cost orders can be obtained by sorting the ranking scores of all candidate plans.", "publication_ref": ["b22", "b24", "b39", "b0", "b22", "b26", "b24", "b22", "b39", "b24", "b26", "b25", "b24", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Learning-To-Rank Training Loop", "text": "The training loop consists of three parts: data collection and deduplication, label mapping and pairwise data extraction, and model training and evaluation. After the label mapping, we get a list of query plans ordered by the reciprocals of their latencies for each query. For each list of \ud835\udc5b \ud835\udc5e query plans (\ud835\udc5b \ud835\udc5e \u2264 \ud835\udc5b after deduplication), we extract all \ud835\udc5b \ud835\udc5e 2 pairwise comparisons to get the pairwise data \ud835\udc43 = {\ud835\udf0b 1 , \ud835\udf0b 2 , . . . , \ud835\udf0b \ud835\udc5a }, where for any \ud835\udc57 \u2208 {1, 2, . . . , \ud835\udc5a}, \ud835\udf0b \ud835\udc57 = \ud835\udc61\n\ud835\udc5e \ud835\udc57 \ud835\udc56 1 \u227b \ud835\udc61 \ud835\udc5e \ud835\udc57 \ud835\udc56 2 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Training and Evaluation. We compute the model parameter \u00ec", "text": "\ud835\udf03 by maximizing the pairwise or listwise log-likelihood function. Specifically, for the listwise approach, the model parameter \u00ec \ud835\udf03 is computed by minimizing the listwise loss defined in Equation (6). For the pairwise approach, \u00ec \ud835\udf03 is computed by minimizing the pairwise loss defined in Equation (7). Refer to Section 2.2 for more details.\nDuring the inference stage, we use the learned model to compute the score for each candidate plan and sort the scores to obtain the corresponding orders, then we select the estimated best plan for each query to execute.", "publication_ref": ["b5", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical Analysis", "text": "In this section, we briefly analyze how COOOL learns from the order of latencies of different query plans. To show that, we consider the query plans {\ud835\udc61 1 , \ud835\udc61 2 , . . . , \ud835\udc61 \ud835\udc5b } with the corresponding latencies {\ud835\udc59 1 , \ud835\udc59 2 , . . . , \ud835\udc59 \ud835\udc5b } for a given query \ud835\udc5e \u2208 \ud835\udc44. And our goal is to select the best one using the model. At the training stage, they have different initial scores {\ud835\udc60 1 , \ud835\udc60 2 , . . . , \ud835\udc60 \ud835\udc5b }. Without loss of generality, we assume \ud835\udc59 1 > \ud835\udc59 2 > . . . > \ud835\udc59 \ud835\udc5b .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pairwise", "text": "Approach. We start with the pairwise approach because it is easier and more intuitive than the listwise approach. In the pairwise approach, each line of data consists of two query plans for a certain query \ud835\udc5e \u2208 \ud835\udc44, denoted by \ud835\udc61 \ud835\udc5e \ud835\udc56 1 and \ud835\udc61 \ud835\udc5e \ud835\udc56 2 , respectively. Without loss of generality, we assume their corresponding latencies \ud835\udc59 \ud835\udc56 1 > \ud835\udc59 \ud835\udc56 2 . And let \ud835\udc60 \ud835\udc56 1 and \ud835\udc60 \ud835\udc56 2 denote the model outputs for the two query plans, and define \ud835\udeff = \ud835\udc60 \ud835\udc56 2 -\ud835\udc60 \ud835\udc56 1 . For simplicity, we focus on this pair of query plans and let L \ud835\udc5e,\ud835\udc56 1 ,\ud835\udc56 2 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f denote the loss on this pair of query plans. We have\nL \ud835\udc5e,\ud835\udc56 1 ,\ud835\udc56 2 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f = -ln exp(\ud835\udc60 \ud835\udc56 2 ) exp(\ud835\udc60 \ud835\udc56 1 ) + exp(\ud835\udc60 \ud835\udc56 2 ) = -ln exp(\ud835\udc60 \ud835\udc56 1 + \ud835\udeff) exp(\ud835\udc60 \ud835\udc56 1 ) + exp(\ud835\udc60 \ud835\udc56 1 + \ud835\udeff) = -ln exp(\ud835\udeff) 1 + exp(\ud835\udeff) = -\ud835\udeff + ln(1 + exp(\ud835\udeff))\nfor \ud835\udc60 \ud835\udc56 1 = 0 because the loss only depends on \ud835\udeff.\nThe partial derivative of L \n\ud835\udf15\ud835\udeff \ud835\udc56 = -1 + exp(\ud835\udeff) 1 + exp(\ud835\udeff) < 0. (9\n)\nThis means an increase in \ud835\udeff leads to a decrease in the loss function L \ud835\udc5e,\ud835\udc56 1 ,\ud835\udc56 2 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f . \ud835\udeff tends to go up while the loss function L \ud835\udc5e,\ud835\udc56 1 ,\ud835\udc56 2 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f is being minimized during the training process, which is desired.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Listwise", "text": "Approach. We are interested in the difference between the model outputs of adjacent query plans, i.e., \ud835\udeff \ud835\udc56 = \ud835\udc60 \ud835\udc56+1 -\ud835\udc60 \ud835\udc56 for all \ud835\udc56 \u2208 {1, 2, . . . , \ud835\udc5b -1}. For convenience, we define \ud835\udeff 0 = 0. Then for all \ud835\udc56 = {1, 2, . . . , \ud835\udc5b}, we have\n\ud835\udc60 \ud835\udc56 = \ud835\udc60 1 + \ud835\udc56-1 \u2211\ufe01 \ud835\udc57=0 \ud835\udeff \ud835\udc57 .\nThe loss function of the listwise approach can be written as:\nL \ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61 = -ln \ud835\udc5b \ud835\udc57=1 exp(\ud835\udc60 \ud835\udc5b-\ud835\udc57+1 ) \ud835\udc5b-\ud835\udc57+1 \ud835\udc5a=1 exp(\ud835\udc60 \ud835\udc5a ) = - \ud835\udc5b \u2211\ufe01 \ud835\udc57=1 ln exp(\ud835\udc60 1 + \ud835\udc5b-\ud835\udc57 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) \ud835\udc5b-\ud835\udc57+1 \ud835\udc5a=1 exp(\ud835\udc60 1 + \ud835\udc5a-1 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) = - \ud835\udc5b \u2211\ufe01 \ud835\udc57=1 ln exp( \ud835\udc5b-\ud835\udc57 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) \ud835\udc5b-\ud835\udc57+1 \ud835\udc5a=1 exp( \ud835\udc5a-1 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) = - \ud835\udc5b \u2211\ufe01 \ud835\udc57=1 \ud835\udc5b-\ud835\udc57 \u2211\ufe01 \ud835\udc58=0 \ud835\udeff \ud835\udc58 -ln( \ud835\udc5b-\ud835\udc57+1 \u2211\ufe01 \ud835\udc5a=1 exp( \ud835\udc5a-1 \u2211\ufe01 \ud835\udc58=0 \ud835\udeff \ud835\udc58 )) .\nIn this equation, the first equality is obtained by substituting Equation ( 4) in Equation ( 6), and the second equality is obtained by substituting \ud835\udc60 \ud835\udc56 with \ud835\udc60 1 + \ud835\udc56-foot_0 \ud835\udc57=0 \ud835\udeff \ud835\udc56 . The third equality is obtained by dividing both the numerator and denominator by exp(\ud835\udc60 1 ) since exp(\ud835\udc60 1 ) > 0. And the last equality is due to the property of the ln() function.\nNow we compute the partial derivative of L \ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61 with respect to \ud835\udeff \ud835\udc56 for any \ud835\udc56 \u2208 {1, 2, . . . , \ud835\udc5b -1}:\n\ud835\udf15L \ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61 \ud835\udf15\ud835\udeff \ud835\udc56 = - \ud835\udc5b-\ud835\udc56 \u2211\ufe01 \ud835\udc57=1 1 - \ud835\udc5b-\ud835\udc57+1 \ud835\udc5a=\ud835\udc56+1 exp( \ud835\udc5a-1 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) \ud835\udc5b-\ud835\udc57+1 \ud835\udc5a=1 exp( \ud835\udc5a-1 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) < 0. (10\n)\nThe inequality holds because when 1 \u2264 \ud835\udc56 \u2264 \ud835\udc5b -\ud835\udc57, we have\n\ud835\udc5b-\ud835\udc57+1 \u2211\ufe01 \ud835\udc5a=\ud835\udc56+1 exp( \ud835\udc5a-1 \u2211\ufe01 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) < \ud835\udc5b-\ud835\udc57+1 \u2211\ufe01 \ud835\udc5a=1 exp( \ud835\udc5a-1 \u2211\ufe01 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ).\nThis means for each \ud835\udc56 \u2208 {1, 2, . . . , \ud835\udc5b}, an increase in \ud835\udeff \ud835\udc56 leads to a decrease in the loss function L \ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61 . \ud835\udeff \ud835\udc56 tends to go up while the loss function L \ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61 is being minimized during the training process, which is desired.\nTo summarize, by minimizing L \ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61 or L \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f , the differences between the ranking scores of different query plans tend to increase, which means that our approaches are able to distinguish the best plans from others.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "To comprehensively evaluate the performance of COOOL, we conduct experiments on three scenarios: single instance, workload transfer, and maintaining one model (a unified model). The first scenario is common in machine learning for query optimization, which refers to learning a model for each dataset. The others are rarely concerned in related studies but crucial for machine learning model deployment in DBMS, especially the last scenario implies training a model to improve query plans from different datasets.\nTherefore, we conduct extensive experiments to primarily answer the following Research Questions (RQs):\n\u2022 RQ1: can COOOL achieve the best performance compared to the baseline methods in terms of total query execution latency and improving slow queries? \u2022 RQ2: is it able to directly transfer a schema agnostic model to another dataset? \u2022 RQ3: can the proposed methods improve query plans from different datasets by maintaining a unified model? \u2022 RQ4: can the experiments provide some insight on why are COOOL methods better than Bao?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "Datasets and Workloads. We use two widely used open-source datasets (IMDB, TPC-H) and the corresponding workloads (JOB, TPC-H). To make a fair comparison and facilitate reproduction, we do not modify the original queries.\n\u2022 Join Order Benchmark (JOB). JOB [17] contains 113 analytical queries, which are designed to stress test query optimizers over the Internet Movie Data Base (IMDB) collected from the real world. These queries from 33 templates involve complex joins (ranging from 3 to 16 joins, averaging 8 joins per query). \u2022 TPC-H. TPC-H [29] is a standard analytical benchmark that data and queries are generated from uniform distributions. We use a scale factor of 10. There are 22 templates in TPC-H, we omit templates #2 and #19 because some nodes in their plan trees have over two child nodes, which makes tree convolution operation unable to handle. For each template, we generate 10 queries by the official TPC-H query generation program 1 .\nBaseline Methods. We compare our proposed model with traditional and state-of-the-art methods as follows.\n\u2022 PostgreSQL: We use the optimizer of PostgreSQL (version 12.5) itself with default settings. \u2022 Bao: We substantially optimize the Bao source codefoot_1 as follows. First, we used all 48 hint sets in Bao paper, rather than the 5 hint sets in the open-sourced code. Second, because we use the standard benchmarks without modifying queries, we train Bao on all past sufficiently explored execution experiences of the training set.\nModel Implementation. We use a three-layer TCNN and the number of channels are respectively set as {256, 128, 64}, the size of the plan embedding space \u210e is 64, and the hidden size of MLP is set as 32. The activation function is Leaky ReLU [39], the optimizer is Adam [15] with an initial learning rate of 0.001, we apply an early stopping mechanism in 10 epochs on the training loss, we save the model that performed best on the validation set and report the results on the test set.\nDevice and Configuration. We use a virtual machine with 16 GB of RAM, and an 8-core Xeon(R) Platinum 8260 2.4GHz CPU. To achieve excellent DBMS performance, we configure PostgreSQL knobs suggested by PGTunefoot_2 : 4 GB of shared buffers, 12 GB of effective cache size, etc. We implement our model in Pytorch 1.12.0 and use an NVIDIA V100 GPU.\nEvaluation Protocol. We repeat each experiment 10 times and exclude the best and the worst test results except for the unified model scenario, then we report the average performance of the rest runs. The total execution latency of the test set is defined as the sum of per-query execution latency. The validation set is 10% of the training set, except for TPC-H \"repeat\" settings, which use 20% of the training set.", "publication_ref": ["b16", "b28", "b38", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Single Instance Experiments (RQ1)", "text": "In this section, we focus on the performance of different optimization methods in the single instance scenario. According to the evaluation details in previous works [23,40], there are primarily two kinds of evaluation settings in the scenario of single instance optimization: 1) randomly split the standard benchmark into train/test sets. 2) Take the standard benchmark as the test set, and augment queries by randomly replacing predicates so that the training data can cover all the templates in the test set. These two evaluation aspects are not comprehensive, so we adapt them as follows while using the standard benchmark without adding extra queries.\nEvaluation Criteria for Model Performance. Instead of selecting the test data uniformly at random, we evaluate the model on multiple dimensions of evaluation criteria so we have several ways to test data selection. First, we consider two real-world scenarios: ad hoc and repetitive.\nIn the ad hoc scenario, the queries are from templates that are not included in the training data. More concretely, we first select several templates from the dataset and use all queries from the selected templates as the test data. This corresponds to the realworld scenario where the model needs to recommend a hint set for a query that is very different from previous queries. We use queries from seven templates in the JOB dataset and four templates in the TPC-H dataset respectively as test sets.\nIn the repetitive scenario, the queries in the test data are \"similar\" to the queries in the training data, but not the same. Here \"similar\" means the queries are from the same templates as in the training data. In practice, we take one or multiple queries from each template as the test set and keep the remaining queries in the training set. For the JOB dataset, we take one query from each template and for the TPC-H dataset, we take two queries from each template as test data. This scenario is denoted as repeat. For queries from the same template, we take their average execution latency to represent the corresponding template's latency.\nFor each of the two scenarios, we comprehensively evaluate the performance of the model under the random and the tail latency evaluation protocols, so we respectively conduct random templates/queries selection and slow templates/queries selection, abbreviated as rand and slow respectively. Overall results on single instance experiments are summarized in Table 1. The individual query performance in \"repeat\" settings is shown in Figure 3, where we depict queries with an execution latency greater than 1s on PostgreSQL to facilitate observation, and \"Optimal\" represents the lowest latency under the given H .\nObservations. In Table 1, there are respectively four settings for each of the two workloads and we observe that:\n\u2022 The listwise approach (COOOL-list) achieves the best performance in most settings, and beats Bao by large margins in almost all settings except for \"repeat-rand\" on JOB, where COOOL is slightly slower than Bao. \u2022 The pairwise approach (COOOL-pair) also outperforms Bao in almost all settings except for \"adhoc-rand\" on TPC-H. It achieves the best performance on three settings (\"repeatrand\" on JOB, \"adhoc-slow\" and \"repeat-slow\" on TPC-H).\nIn most settings, the performances of COOOL-pair and COOOL-list are similar. \u2022 Bao does not have the best performance under any of these settings. For \"adhoc-slow\" on JOB, Bao even has a total execution latency regression, which means the running speed is lower than the PostgreSQL optimizer itself.\nCombined with Figure 3, we have the following observations:\n\u2022 Bao is significantly worse than COOOL on some queries under \"slow\" settings (20c in \"repeat-slow\" on JOB and template 9 in \"repeat-slow\" on TPC-H.). \"Slow\" settings are obviously more challenging for ML models than \"rand\" settings and COOOL has clear advantages over Bao. \u2022 Both COOOL and Bao are close to optimal on \"repeat-rand\" scenarios for JOB and TPC-H data. \u2022 It happens that Bao or COOOL are not as good as PostgreSQL on a small amount of queries, which is normal for ML models. Nevertheless, performance regressions occur less frequently on COOOL models than Bao models in most settings, and COOOL-pair has the lowest number of query regressions for these settings. See Table 2 for details.\nTo summarize, we observe that COOOL has advantages and competitiveness over Bao in speeding up total query execution, alleviating individual query performance regression, and optimizing slow queries. Besides, although COOOL-list is better than COOOLpair in total query execution latency speedups, it is not as good in avoiding individual query regression.", "publication_ref": ["b22", "b39"], "figure_ref": ["fig_2", "fig_2"], "table_ref": ["tab_4", "tab_4", "tab_5"]}, {"heading": "Workload Transfer Investigation (RQ2)", "text": "It is well-known that an instance-optimized model may have a poor performance on another workload because it does not learn patterns from the unseen one. Even though most ML models are schema specific which makes establishing a learned model on another workload not applicable, little experimental research has been conducted for schema agnostic models. Here we present an intuitive view of directly transferring an instance-optimized model to another workload. Specifically, we train a model on a source workload and then test its performance on another workload, namely target workload (source\u2192target).\nWe conduct experiments on training on JOB and transfer the model to TPC-H (JOB\u2192TPC-H). And we train a model on TPC-H and investigate its performance on JOB (TPC-H\u2192JOB). In this directly transferring investigation, we also explore them in \"adhoc\" and \"repeat\" scenarios and under \"rand\" and \"slow\" settings, the same as instance-optimized experiments. To make an intuitive comparison, we train the model on the source workload's training set and show the performance on the target workload's test set.\nThe overall query execution speedups are shown in Table 4, and we have the following observations.\n\u2022 The difference of the performances of models is unstable compared with the corresponding instance-optimized models, and most settings have a performance decline. Our experiments show that directly applying a model trained on one workload to another workload cannot obtain good performances even if the model is schema agnostic. \u2022 Compared with instance-optimized models, most models learned on TPC-H perform worse on JOB. By contrast, there is a performance improvement on \"TPC-H adhoc\" settings especially on \"TPC-H adhoc-slow\". We may conclude that the data in JOB may benefit TPC-H and especially improve the performance of slow queries. We will provide a deeper analysis from a representation learning perspective in Section 5.5.2. \u2022 We show the plan tree statistics for nodes and depth of the two workloads in Table 3. On the one hand, it indicates that JOB is more complicated than TPC-H, so directly transferring a model learned on JOB may optimize queries from TPC-H. On the other hand, it suggests that data distributions in different datasets affect the performance of machine learning models in workload transfer. Because the experiment results on \"TPCH repeat\" settings demonstrate that a model learned on the templates from JOB cannot achieve the same performance as that learned on the template from TPC-H. Based on the observations, introducing JOB data has a better performance than the model trained on TPC-H \"adhoc\" settings. We can conclude that for a given workload, using the training data from the same workload may not obtain satisfactory performance. Therefore, how to utilize data from other workloads is an emerging problem for query optimization models.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7", "tab_6"]}, {"heading": "Unified Model Performance (RQ3)", "text": "This section focuses on the performance of models trained using both JOB and TPC-H datasets. Similar to the previous section, we consider four scenarios: \"adhoc-rand\", \"adhoc-slow\", \"repeat-rand\", and \"repeat-slow\" for each dataset. For each scenario, the training data of JOB and TPC-H were combined as the new training set and the model is respectively evaluated on JOB and TPC-H test sets.\nThe overall results are summarized in Table 5. We observe that:\n\u2022 COOOL-pair performs the best in all \"adhoc\" settings and COOOL-list performs the best in all \"repeat\" settings. They have similar overall performances, and outperform Bao in almost all scenarios. \u2022 COOOL-pair has the most performance boost when the model is trained using both JOB and TPC-H datasets (unified model) compared with Table 1, especially on TPC-H where the unified model beats the single-instance model in three out of four scenarios. \u2022 Does a different training dataset help? We observe that JOB training data help improve the model performance on TPC-H test data, especially under \"adhoc\" scenarios. While TPC-H training data do not help improve the model performance on JOB test data under most scenarios. We believe this is because JOB queries are more complicated (more nodes and larger depth than TPC-H queries as shown in Table 3).\nThe individual query performance in \"repeat\" settings of the unified models are shown in Figure 4, where we also depict queries with an execution latency greater than 1s on PostgreSQL. Combined with the observation in Figure 4, we can get the following conclusions:\n\u2022 The performances of both COOOL approaches are close to optimal. This echoes our observation that JOB training data help improve the model performances on TPC-H. \u2022 On the other hand, the model performances on JOB test data are hurt by the TPC-H training data. For example, both Bao and COOOL methods have poor performances on query 19d in Figure 4b. \u2022 In terms of number of regressions in excution time, we list the results in Table 6. Both COOOL-pair and COOOL-list perform better than Bao, and COOOL-list has the lowest number of query regressions for these settings. Summary on Model Performances. Different data distributions in different datasets is a challenge for a unified query optimization model, but COOOL models are able to alleviate this issue to learn a unified model better than the state-of-the-art regression approach to speed up total query execution, alleviate individual query regression, and optimize slow queries. Our experiments have shown the overwhelming advantages of our proposed models. For the single instance scenarios, COOOL-list achieves the best performance in total query execution speedups while COOOL-pair is the best to reduce the number of individual query regressions. When a unified model is trained using multiple datasets, COOOL-list performs the best in \"repeat\" settings, while COOOL-pair is the best in \"adhoc\" settings and has no total query execution performance regression. COOOL-pair is the best approach among the three models in terms of stability.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3", "fig_3"], "table_ref": ["tab_8", "tab_4", "tab_6", "tab_9"]}, {"heading": "Model Comparison and Analysis (RQ4)", "text": "We have conducted extensive experiments and obtained promising results. But it is not clear how the ranking strategies affect the model training. This section aims to provide insight on why the two COOOL models outperforms Bao, especially for the unified model training.\nIn COOOL methods, we use exactly the same plan representation model as Bao to confirm that the improvements are brought by our LTR methods. We analyze the \"adhoc-slow\" setting of the two workloads in this section. 5.5.1 Model efficiency. We compare the space and time efficiency of our approaches and state-of-the-art method.\n\u2022 Space complexity. The two COOOL models use tree convolutional neural networks with the same structure and hidden sizes as Bao, the number of parameters for all of them is 132,353, i.e., 529,412 bytes. The storage consumption of the model is about 0.5MB, which is efficient enough in practice.   Let an \u210e-dimensional vector z i denote the embedding for query plan \ud835\udc56. We compute the covariance matrix \ud835\udc36 \u2208 R \u210e\u00d7\u210e for all plan embeddings obtained by the model. Formally,\n\ud835\udc36 = 1 \ud835\udc40 \ud835\udc40 \u2211\ufe01 \ud835\udc56=1 (z i -z)(z i -z) \u22a4 ,\nwhere \ud835\udc40 is the number of plans, z = 1 \ud835\udc40 \ud835\udc40 \ud835\udc56=1 z i . Then, we apply singular value decomposition on \ud835\udc36 s.t. \ud835\udc36 = \ud835\udc48 \ud835\udc46\ud835\udc49 \u22a4 , where \ud835\udc46 = \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udf0e \ud835\udc58 ). Then we can obtain the singular value spectrum in sorted order and logarithmic scale (lg(\ud835\udf0e \ud835\udc58 )). We depict the singular value spectrum of the plan embedding space of the three models learned in different scenarios in \"adhoc-slow\" setting, as shown in Figure 5. We have the following observations.\n\u2022 In Figure 5a, we observe a significant drop in singular values on a logarithmic scale (lg(\ud835\udf0e \ud835\udc58 )) in all tasks. The curve approaches zero (less than 1e-7) in the spectrum indicating that a dimensional collapse [9] occurs in Bao's embedding space in each of Bao's experimental scenarios, and the model learned on JOB even collapsed in half of the plan embedding space. The dimensional collapses result in Bao's plan embedding vectors only spanning a lower-dimensional subspace, which may harm the representation ability of the model and therefore hurt the performance. Moreover, the models in the single instance scenario on different datasets result in a different number of collapsed dimensions. Based on the workload transfer scenario curves, we observe that the plan embedding space of the target workload spans the same number of dimensions as the source workload. So we can conclude that the embedding space is determined by the training data. \u2022 In Figures 5b and5c, singular values of the COOOL approaches does not drop abruptly, meaning that there is no collapse in their plan embedding space. The plan embedding methods of Bao and COOOL are the same, and the comparison reflects that the embedding spaces learned from the regression framework and ranking strategies are significantly different. \u2022 The spectrum of Bao's unified model is closer to the spectrum of the learned representation from TPC-H data than that learned from JOB data, whereas the spectra of COOOLpair and COOOL-list are closer to the spectrum of the representation from JOB data than that learned from TPC-H data.\nIn fact, the number of training samples of TPC-H is greater than that of JOB, and JOB is more complicated than TPC-H (see Table 3). When learning from multiple datasets, Bao's performance is affected more by the datasets with more samples. By contrast, the two COOOL methods are able to learn from sophisticated query plans. \u2022 Singular values are related to the latent information in the covariance matrix. We can get three conclusions from this perspective. First, the overall spectrums of Bao in different settings are lower than COOOL in terms of absolute singular value, which indicates the representation ability of Bao is worse than COOOL. Second, the curve of Bao's unified model is not the highest in most dimensions, while that of the two COOOL models are just the opposite. It may reveal that the ranking strategies can help models learn latent patterns from two workloads but the regression framework cannot. Third, the range and the trend of the curves of COOOL-pair and COOOL-list are different, which may result in the different performances of the two models in different scenarios. \u2022 The dimensional collapse issue may not greatly hurt single instance optimization since the embedding space is learned from one workload and the latent collapsed dimensions are fixed when the model converges. Because the collapsed dimensions may be different in different datasets, there may be orthogonal situations, which may cause the subspace representation learned in one dataset to be noise in another, resulting in unstable performance. Therefore, it may limit the scalability of machine learning query optimization models.\nTo sum up, dimensional collapse in plan tree embedding space is a challenge for machine learning query optimization models in maintaining a unified model to learn from different workloads. The two variants of COOOL have the exact same model as Bao but consistently outperform Bao in almost all scenarios and settings. We provide some insight on why COOOL methods outperform Bao from the query plan representation perspective. It may guide researchers to further improve the performance of ML models for query optimization.", "publication_ref": ["b8"], "figure_ref": ["fig_4", "fig_4", "fig_4", "fig_4"], "table_ref": ["tab_6"]}, {"heading": "RELATED WORK 6.1 Machine Learning for Query Optimization", "text": "Traditional cost-based query optimizers aim to select the candidate plan with the minimum estimated cost, where cost represents the execution latency or other user-defined resource consumption metrics. Various techniques have been proposed in cost-based optimizers [4,5,12], such as sketches, histograms, probability models, etc. Besides, they work with different assumptions (e.g., attribute value independence [3], uniformity [11], data independence [6]) and when these cannot be met, the techniques usually fall back to an educated guess [30]. Traditional optimizers have been studied for decades [31], focusing on manually-crafted and heuristic methods to predict costs, estimate cardinality, and generate plans [10]. However, the effect of heuristics depends on data distribution, cardinality estimation, and cost modeling have been proven difficult to solve [21]. Consequently, cost-based optimizers may generate poor performance plans.\nTherefore, the database community has attempted to apply Machine Learning (ML) techniques to solve these issues. For example, some efforts introduced Reinforcement Learning (RL) and Monte Carlo Tree Search (MCTS) to optimize the join order selection task [16,24,34,45], and some studies use neural networks to accomplish cost modeling and cardinality estimation [19,32,41,42]. These works depict that the elaborated ML models can improve the performance of parts of those components in query optimization. However, they may not improve the performance of optimizers since none of them demonstrate that the performance improvement of a single component can actually bring a better query plan [18].\nIn recent years, some studies have attempted to build end-toend ML models for query optimization [23,25,40]. They all use a TCNN to predict the cost/latency of query plans and leverage a deep reinforcement learning framework to train the model. Though they have made improvements compared with the previous works of replacing some components of the optimizer with ML models, the evaluation of an optimizer should be measured from multiple dimensions [35], and some of the most concerned parts are practical, data/schema agnostic, explainability, performance, scalability, etc. Neo [25] learns row vector embeddings from tables, so it needs to maintain row vector embeddings for different data. Balsa [40] can not treat advanced SQL features (e.g., sub-queries) due to its dependency, which limits its scalability. Furthermore, Neo and Balsa are less practical than Bao [23]. Bao introduced a novel approach to end-to-end query optimization that recommends per-query hints over an existing optimizer. SQL hints can limit the search space of existing optimizers and each set of hints corresponds to a plan, which makes it practical in real scenarios.", "publication_ref": ["b3", "b4", "b11", "b2", "b10", "b5", "b29", "b30", "b9", "b20", "b15", "b23", "b33", "b44", "b18", "b31", "b40", "b41", "b17", "b22", "b24", "b39", "b34", "b24", "b39", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Learning-To-Rank", "text": "Learning-To-Rank (LTR) is an active research topic [43] and has many applications in information retrieval [20], meta-search engines [7], recommendation systems [13], preference learning [46], etc. A search engine or a recommendation system usually need to rank a large and variable number of items (webpages, movies, for example). The SQL hint recommendation problem is more like the preference learning problem, where a relatively small and fixed number of items (SQL hints in our case), are to be ranked for each user (SQL query in our case). Plackett-Luce model [22,28], which was later used as a listwise loss function in information retrieval [20] and softmax function in classification tasks [8], is one of the most popular models in preference learning. It is a listwise model, but can be learned efficiently with pairwise methods [14,47], by breaking the full rankings into pairwise comparisons and minimizing a pairwise loss function. Similar to [14] and [47], we maximize the marginal pairwise likelihood for its similicity.", "publication_ref": ["b42", "b19", "b6", "b12", "b45", "b21", "b27", "b19", "b7", "b13", "b46", "b13", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, we propose COOOL that predicts cost orders of query plans to cooperate with DBMS by LTR techniques to recommend SQL hints for query optimization. COOOL has pairwise and listwise methods, each of which can improve overall query execution speed especially on slow queries and alleviate individual query regression. Moreover, we can maintain a unified model to improve query plan selections by the proposed method, and we shed some light on why both COOOL-list and COOOL-pair approaches outperform Bao from the representation learning perspective.\nCOOOL makes a step forward to maintaining a unified endto-end query optimization ML model on two datasets, and the elaborated analysis may provide preliminary for large-scale pretrained query optimization models. While COOOL methods are promising, they cannot estimate the cost of the recommended query plan, or quantitatively compare the costs of two query plans.\nFor future work, we plan to investigate the evaluation metrics for ranking candidate plans that differ by multiple orders of magnitude in execution latency for query optimization, which facilitates introducing state-of-the-art LTR techniques. LTR is still an active research field in recent years, so there are considerable future works to be explored in query optimization. Besides, developing large-scale pre-trained query optimization models and quantitatively compare the costs of different query plans accurately are also challenging future directions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning-based query performance modeling and prediction", "journal": "IEEE", "year": "2012", "authors": "Mert Akdere; Ugur \u00c7etintemel; Matteo Riondato; Eli Upfal; Stanley B Zdonik"}, {"ref_id": "b1", "title": "Generalized method-of-moments for rank aggregation", "journal": "Advances in Neural Information Processing Systems", "year": "2013", "authors": "Hossein Azari Soufiani; William Chen; David C Parkes; Lirong Xia"}, {"ref_id": "b2", "title": "Implications of certain assumptions in database performance evauation", "journal": "ACM Transactions on Database Systems (TODS)", "year": "1984", "authors": "Stavros Christodoulakis"}, {"ref_id": "b3", "title": "Histograms and wavelets on probabilistic data", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2010", "authors": "Graham Cormode; Minos Garofalakis"}, {"ref_id": "b4", "title": "Synopses for massive data: Samples, histograms, wavelets, sketches", "journal": "Foundations and Trends\u00ae in Databases", "year": "2011", "authors": "Graham Cormode; Minos Garofalakis; Peter J Haas; Chris Jermaine"}, {"ref_id": "b5", "title": "Independence is good: Dependency-based histogram synopses for high-dimensional data", "journal": "ACM SIGMOD Record", "year": "2001", "authors": "Amol Deshpande; Minos Garofalakis; Rajeev Rastogi"}, {"ref_id": "b6", "title": "Rank aggregation methods for the web", "journal": "", "year": "2001", "authors": "Cynthia Dwork; Ravi Kumar; Moni Naor; Dandapani Sivakumar"}, {"ref_id": "b7", "title": "On the properties of the softmax function with application in game theory and reinforcement learning", "journal": "", "year": "2017", "authors": "Bolin Gao; Lacra Pavel"}, {"ref_id": "b8", "title": "On Feature Decorrelation in Self-Supervised Learning", "journal": "IEEE", "year": "2021-10-10", "authors": "Tianyu Hua; Wenxiao Wang; Zihui Xue; Sucheng Ren; Yue Wang; Hang Zhao"}, {"ref_id": "b9", "title": "Query optimization", "journal": "ACM Computing Surveys (CSUR)", "year": "1996", "authors": "E Yannis;  Ioannidis"}, {"ref_id": "b10", "title": "Optimal histograms for limiting worst-case error propagation in the size of join results", "journal": "ACM Transactions on Database Systems (TODS)", "year": "1993", "authors": "E Yannis; Stavros Ioannidis;  Christodoulakis"}, {"ref_id": "b11", "title": "Balancing histogram optimality and practicality for query result size estimation", "journal": "Acm Sigmod Record", "year": "1995", "authors": "E Yannis; Viswanath Ioannidis;  Poosala"}, {"ref_id": "b12", "title": "Learning to rank for recommender systems", "journal": "", "year": "2013", "authors": "Alexandros Karatzoglou; Linas Baltrunas; Yue Shi"}, {"ref_id": "b13", "title": "Data-driven rank breaking for efficient rank aggregation", "journal": "PMLR", "year": "2016", "authors": "Ashish Khetan; Sewoong Oh"}, {"ref_id": "b14", "title": "Adam: A Method for Stochastic Optimization", "journal": "", "year": "2015-05-07", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b15", "title": "Learning to optimize join queries with deep reinforcement learning", "journal": "", "year": "2018", "authors": "Sanjay Krishnan; Zongheng Yang; Ken Goldberg; Joseph Hellerstein; Ion Stoica"}, {"ref_id": "b16", "title": "How good are query optimizers, really?", "journal": "Proceedings of the VLDB Endowment", "year": "2015", "authors": "Viktor Leis; Andrey Gubichev; Atanas Mirchev; Peter Boncz; Alfons Kemper; Thomas Neumann"}, {"ref_id": "b17", "title": "Query optimization through the looking glass, and what we found running the join order benchmark", "journal": "The VLDB Journal", "year": "2018", "authors": "Viktor Leis; Bernhard Radke; Andrey Gubichev; Atanas Mirchev; Peter Boncz; Alfons Kemper; Thomas Neumann"}, {"ref_id": "b18", "title": "Cardinality estimation using neural networks", "journal": "", "year": "2015", "authors": "Henry Liu; Mingbin Xu; Ziting Yu; Vincent Corvinelli; Calisto Zuzarte"}, {"ref_id": "b19", "title": "Learning to rank for information retrieval", "journal": "Foundations and Trends in Information Retrieval", "year": "2009", "authors": "Tie-Yan Liu"}, {"ref_id": "b20", "title": "Is query optimization a \"solved\" problem", "journal": "", "year": "2014", "authors": "Guy Lohman"}, {"ref_id": "b21", "title": "Individual Choice Behavior", "journal": "", "year": "1959", "authors": "Luce Duncan"}, {"ref_id": "b22", "title": "Bao: Making learned query optimization practical", "journal": "ACM SIGMOD Record", "year": "2022", "authors": "Ryan Marcus; Parimarjan Negi; Hongzi Mao; Nesime Tatbul; Mohammad Alizadeh; Tim Kraska"}, {"ref_id": "b23", "title": "Deep reinforcement learning for join order enumeration", "journal": "", "year": "2018", "authors": "Ryan Marcus; Olga Papaemmanouil"}, {"ref_id": "b24", "title": "Neo: A Learned Query Optimizer", "journal": "", "year": "2019", "authors": "Ryan C Marcus; Parimarjan Negi; Hongzi Mao; Chi Zhang; Mohammad Alizadeh; Tim Kraska; Olga Papaemmanouil; Nesime Tatbul"}, {"ref_id": "b25", "title": "The need for biases in learning generalizations", "journal": "Citeseer", "year": "1980", "authors": "M Tom;  Mitchell"}, {"ref_id": "b26", "title": "Convolutional neural networks over tree structures for programming language processing", "journal": "", "year": "2016", "authors": "Lili Mou; Ge Li; Lu Zhang; Tao Wang; Zhi Jin"}, {"ref_id": "b27", "title": "The analysis of permutations", "journal": "Journal of the Royal Statistical Society Series C: Applied Statistics", "year": "1975", "authors": "Robin L Plackett"}, {"ref_id": "b28", "title": "New TPC benchmarks for decision support and web commerce", "journal": "ACM Sigmod Record", "year": "2000", "authors": "Meikel Poess; Chris Floyd"}, {"ref_id": "b29", "title": "Selectivity estimation without the attribute value independence assumption", "journal": "VLDB", "year": "1997", "authors": "Viswanath Poosala; Yannis E Ioannidis"}, {"ref_id": "b30", "title": "Access path selection in a relational database management system", "journal": "", "year": "1979", "authors": " Griffiths Selinger; Donald D Morton M Astrahan; Raymond A Chamberlin; Thomas G Lorie;  Price"}, {"ref_id": "b31", "title": "An End-to-End Learning-based Cost Estimator", "journal": "Proc. VLDB Endow", "year": "2019", "authors": "Ji Sun; Guoliang Li"}, {"ref_id": "b32", "title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "journal": "Biometrika", "year": "1933", "authors": " William R Thompson"}, {"ref_id": "b33", "title": "Skinnerdb: Regret-bounded query evaluation via reinforcement learning", "journal": "ACM Transactions on Database Systems (TODS)", "year": "2021", "authors": "Immanuel Trummer; Junxiong Wang; Ziyun Wei; Deepak Maram; Samuel Moseley; Saehan Jo; Joseph Antonakakis; Ankush Rayabhari"}, {"ref_id": "b34", "title": "Database Optimizers in the Era of Learning", "journal": "IEEE", "year": "2022", "authors": "Dimitris Tsesmelis; Alkis Simitsis"}, {"ref_id": "b35", "title": "Expand your training limits! generating training data for ml-based data management", "journal": "", "year": "2021", "authors": "Francesco Ventura; Zoi Kaoudi; Jorge Arnulfo Quian\u00e9-Ruiz; Volker Markl"}, {"ref_id": "b36", "title": "Predicting query execution time: Are optimizer cost models really unusable?", "journal": "IEEE", "year": "2013", "authors": "Wentao Wu; Yun Chi; Shenghuo Zhu; Junichi Tatemura; Hakan Hacig\u00fcm\u00fcs; Jeffrey F Naughton"}, {"ref_id": "b37", "title": "Listwise approach to learning to rank: theory and algorithm", "journal": "", "year": "2008", "authors": "Fen Xia; Tie-Yan Liu; Jue Wang; Wensheng Zhang; Hang Li"}, {"ref_id": "b38", "title": "Empirical evaluation of rectified activations in convolutional network", "journal": "", "year": "2015", "authors": "Bing Xu; Naiyan Wang; Tianqi Chen; Mu Li"}, {"ref_id": "b39", "title": "Balsa: Learning a Query Optimizer Without Expert Demonstrations", "journal": "ACM", "year": "2022-06-12", "authors": "Zongheng Yang; Wei-Lin Chiang; Sifei Luan; Gautam Mittal; Michael Luo; Ion Stoica"}, {"ref_id": "b40", "title": "NeuroCard: one cardinality estimator for all tables", "journal": "", "year": "2020", "authors": "Zongheng Yang; Amog Kamsetty; Sifei Luan; Eric Liang; Yan Duan; Xi Chen; Ion Stoica"}, {"ref_id": "b41", "title": "Deep unsupervised cardinality estimation", "journal": "", "year": "2019", "authors": "Zongheng Yang; Eric Liang; Amog Kamsetty; Chenggang Wu; Yan Duan; Xi Chen; Pieter Abbeel; Sanjay Joseph M Hellerstein; Ion Krishnan;  Stoica"}, {"ref_id": "b42", "title": "An in-depth study on adversarial learning-to-rank", "journal": "Information Retrieval Journal", "year": "2023", "authors": "Hai-Tao Yu; Rajesh Piryani; Adam Jatowt; Ryo Inagaki; Hideo Joho; Kyoung-Sook Kim"}, {"ref_id": "b43", "title": "Network representation learning: A survey", "journal": "IEEE transactions on Big Data", "year": "2018", "authors": "Daokun Zhang; Jie Yin; Xingquan Zhu; Chengqi Zhang"}, {"ref_id": "b44", "title": "AlphaJoin: Join Order Selection \u00e0 la AlphaGo", "journal": "", "year": "2020", "authors": "Ji Zhang"}, {"ref_id": "b45", "title": "Learning mixtures of random utility models with features from incomplete preferences", "journal": "", "year": "2022", "authors": "Zhibing Zhao; Ao Liu; Lirong Xia"}, {"ref_id": "b46", "title": "Composite marginal likelihood methods for random utility models", "journal": "PMLR", "year": "2018", "authors": "Zhibing Zhao; Lirong Xia"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: A brief view of the COOOL pipeline.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Individual query performance of the models in the single instance scenario.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Individual query performance of the unified model.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Singular value spectrum of the plan embedding space for Bao and COOOL in \"adhoc-slow\" setting of the two workloads in the scenarios of the single instance, workload transfer, and unified model.", "figure_data": ""}, {"figure_label": "52", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "5. 5 . 2 A52representation learning perspective for plan tree embeddings. Regression-based methods usually limit the model output domain in an interval depending on the normalization methods. By contrast, ranking-based methods do not limit the absolute value of model outputs, which may help the model learn better plan representations. This section aims to provide some insight on why COOOL outperforms Bao from a representation learning perspective.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "We propose COOOL, a learned model that predicts Cost Orders of the query plans to cOOperate with DBMS by LTR techniques, to recommend better SQL hints for query optimization. To our best knowledge, COOOL is the first endto-end query optimization method that maintains a unified model to optimize queries from different datasets.\u2022 We theoretically show that COOOL can distinguish query plans with different latencies when optimizing the loss functions, and verify that COOOL is superior to regression approaches from the representation learning perspective.\u2022 Comprehensive experiments on join-order-benchmark and TPC-H show that COOOL can outperform PostgreSQL and state-of-the-art methods on multiple dimensions of evaluation criteria.", "figure_data": "2 PRELIMINARIES2.1 Task Definition and Formalization"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Total query execution latency speedups on single datasets over PostgreSQL. The best performance on each workload is in boldface.", "figure_data": "JOBTPC-Hadhoc-rand adhoc-slow repeat-rand repeat-slow adhoc-rand adhoc-slow repeat-rand repeat-slowBao1.070.913.021.375.361.175.284.73COOOL-list1.351.463.011.576.093.635.335.55COOOL-pair1.301.363.471.563.863.865.285.56"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Number of regressions for Bao and COOOL compared with PostgreSQL (single instance)", "figure_data": "SettingBao COOOL-list COOOL-pairJOB repeat-rand241713JOB repeat-slow17118TPC-H repeat-rand311TPC-H repeat-slow111"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Overall plan tree statistics of the two workloads.", "figure_data": "Workload Max Nodes Avg. Nodes Max Depth Avg. DepthJOB7223.63612.0TPC-H3514.3209.6"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Total query execution latency speedups on the target workload over PostgreSQL of direct transfer, where \u2191 indicates an increase in performance compared to the corresponding instance-optimized model.", "figure_data": "TPC-H \u2192 JOBJOB \u2192 TPC-Hadhoc-rand adhoc-slow repeat-rand repeat-slow adhoc-rand adhoc-slow repeat-rand repeat-slowBao1.071.19\u21911.070.696.35\u21911.56\u21911.641.42COOOL-list0.970.930.920.820.854.70\u21911.641.70COOOL-pair0.961.180.890.865.90\u21914.60\u21911.481.82"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Total query execution latency speedups of a unified model (trained on both JOB and TPC-H data) over PostgreSQL. The best performance on each workload is in boldface. \"\u2191\" indicates that the performance of the unified model is better than that trained on the corresponding single dataset.", "figure_data": "JOBTPC-Hadhoc-rand adhoc-slow repeat-rand repeat-slow adhoc-rand adhoc-slow repeat-rand repeat-slowBao0.800.922.791.215.77\u21911.934.834.31COOOL-list1.001.213.24\u21911.266.59\u21912.915.375.53COOOL-pair1.061.71\u21912.901.216.73\u21913.92\u21915.34\u21915.51"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Number of regressions for Bao and COOOL compared with PostgreSQL (unified model)", "figure_data": "SettingBao COOOL-list COOOL-pairJOB repeat-rand231217JOB repeat-slow20911TPC-H repeat-rand811TPC-H repeat-slow411Table 7: Comparison of training time required for conver-gence in the \"adhoc-slow\" settingJOBTPC-H UnifiedBao119.5s34.5s265.9sCOOOL-list 167.0s 159.8s317.1sCOOOL-pair 493.6s 222.8s894.6s"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "The number of training samples. Since there are three different strategies involved in this paper, it is difficult to directly derive their time complexity. So we first show the number of training samples. Let \ud835\udc41 denote the number of training queries. And there are \ud835\udc5b candidate hint sets in H for each query. The time complexity of Bao is \ud835\udc41 \ud835\udc5b. Let \ud835\udc5a \ud835\udc56 denote the number of unique query plans for each query given H , where \ud835\udc56 \u2208 {1, 2, . . . , \ud835\udc41 } and \ud835\udc5a \ud835\udc56 \u2264 \ud835\udc5b. Because we use a full rank-breaking strategy in COOOL-pair, the number of sample is \u0398( \ud835\udc41 \ud835\udc56=1 \ud835\udc5a \ud835\udc56 (\ud835\udc5a \ud835\udc56 -1) 2 ) = \ud835\udc42 (\ud835\udc41\ud835\udc5b 2 ). Since \ud835\udc5b is a given constant in practice, the complexity is acceptable. For COOOL-list, there are \ud835\udc41 training samples. \u2022 Training time consumption. To intuitively demonstrate the time efficiency of the three methods, we summarize the average training time required for convergence in the \"adhocslow\" settings, as shown in Table 7. It is obvious that COOOL indeed needs more training time for convergence than Bao. Besides, COOOL-pair requires much more time for convergence than COOOL-list. \u2022 Conclusion. Notwithstanding COOOL-pair and COOOL-list require longer convergence time than Bao, they have the same model inference efficiency and the number parameters as Bao. Therefore, we can conclude that COOOL is a practical method.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\ud835\udc60 \ud835\udc5e \ud835\udc56 = \ud835\udc40 (\ud835\udc5e, \ud835\udc61 \ud835\udc5e \ud835\udc56 ; \u00ec \ud835\udf03 ),(2)", "formula_coordinates": [2.0, 408.44, 458.29, 149.77, 12.09]}, {"formula_id": "formula_1", "formula_text": "\u0124\ud835\udc46 \ud835\udc5e = \ud835\udc3b\ud835\udc46 arg max \ud835\udc56 \ud835\udc60 \ud835\udc5e \ud835\udc56 ,(3)", "formula_coordinates": [2.0, 406.61, 527.85, 151.59, 17.96]}, {"formula_id": "formula_2", "formula_text": "\ud835\udf0e \ud835\udc5e = \ud835\udc61 \ud835\udc5e \ud835\udc56 1 \u227b \ud835\udc61 \ud835\udc5e \ud835\udc56 2 \u227b . . . \u227b \ud835\udc61 \ud835\udc5e \ud835\udc56 \ud835\udc5b is Pr PL (\ud835\udc61 \ud835\udc5e \ud835\udc56 1 \u227b \ud835\udc61 \ud835\udc5e \ud835\udc56 2 \u227b . . . \u227b \ud835\udc61 \ud835\udc5e \ud835\udc56 \ud835\udc5b ; \u00ec \ud835\udf03 ) = \ud835\udc5b \ud835\udc57=1 exp(\ud835\udc60 \ud835\udc5e \ud835\udc56 \ud835\udc57 ) \ud835\udc5b \ud835\udc5a=\ud835\udc57 exp(\ud835\udc60 \ud835\udc5e \ud835\udc56 \ud835\udc5a ) ,(4)", "formula_coordinates": [3.0, 82.36, 162.51, 211.69, 47.46]}, {"formula_id": "formula_3", "formula_text": "L list ( \u00ec \ud835\udf03 ) = - \u2211\ufe01 \ud835\udc5e \u2208\ud835\udc44 ln Pr PL (\ud835\udf0e \ud835\udc5e ; \u00ec \ud835\udf03 ),(6)", "formula_coordinates": [3.0, 117.59, 344.56, 176.46, 21.62]}, {"formula_id": "formula_4", "formula_text": "\ud835\udc5e \ud835\udc57 \ud835\udc56 1 \u227b \ud835\udc61 \ud835\udc5e \ud835\udc57 \ud835\udc56 2 .", "formula_coordinates": [3.0, 241.44, 597.59, 32.56, 14.16]}, {"formula_id": "formula_5", "formula_text": "L pair ( \u00ec \ud835\udf03 ) = - \ud835\udc5a \u2211\ufe01 \ud835\udc57=1 ln Pr PL (\ud835\udf0b \ud835\udc57 ; \u00ec \ud835\udf03 ),(7)", "formula_coordinates": [3.0, 117.51, 654.67, 176.53, 24.75]}, {"formula_id": "formula_6", "formula_text": "\ud835\udc38 (\ud835\udc63) \u2032 = \ud835\udf0e (\ud835\udc38 (\ud835\udc63) \u2299 \ud835\udc64 + \ud835\udc38 (\ud835\udc59 (\ud835\udc63)) \u2299 \ud835\udc64 \ud835\udc59 + \ud835\udc38 (\ud835\udc5f (\ud835\udc63)) \u2299 \ud835\udc64 \ud835\udc5f ), (8", "formula_coordinates": [4.0, 343.58, 401.29, 211.45, 10.93]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [4.0, 555.03, 404.28, 3.17, 7.94]}, {"formula_id": "formula_8", "formula_text": "\ud835\udc5e \ud835\udc57 \ud835\udc56 1 \u227b \ud835\udc61 \ud835\udc5e \ud835\udc57 \ud835\udc56 2 .", "formula_coordinates": [5.0, 191.49, 559.98, 32.55, 14.16]}, {"formula_id": "formula_9", "formula_text": "L \ud835\udc5e,\ud835\udc56 1 ,\ud835\udc56 2 \ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5f = -ln exp(\ud835\udc60 \ud835\udc56 2 ) exp(\ud835\udc60 \ud835\udc56 1 ) + exp(\ud835\udc60 \ud835\udc56 2 ) = -ln exp(\ud835\udc60 \ud835\udc56 1 + \ud835\udeff) exp(\ud835\udc60 \ud835\udc56 1 ) + exp(\ud835\udc60 \ud835\udc56 1 + \ud835\udeff) = -ln exp(\ud835\udeff) 1 + exp(\ud835\udeff) = -\ud835\udeff + ln(1 + exp(\ud835\udeff))", "formula_coordinates": [5.0, 370.74, 563.11, 133.21, 83.29]}, {"formula_id": "formula_10", "formula_text": "\ud835\udf15\ud835\udeff \ud835\udc56 = -1 + exp(\ud835\udeff) 1 + exp(\ud835\udeff) < 0. (9", "formula_coordinates": [5.0, 384.14, 691.09, 170.89, 20.25]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [5.0, 555.03, 694.47, 3.17, 7.94]}, {"formula_id": "formula_12", "formula_text": "\ud835\udc60 \ud835\udc56 = \ud835\udc60 1 + \ud835\udc56-1 \u2211\ufe01 \ud835\udc57=0 \ud835\udeff \ud835\udc57 .", "formula_coordinates": [6.0, 145.56, 176.32, 56.21, 24.75]}, {"formula_id": "formula_13", "formula_text": "L \ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61 = -ln \ud835\udc5b \ud835\udc57=1 exp(\ud835\udc60 \ud835\udc5b-\ud835\udc57+1 ) \ud835\udc5b-\ud835\udc57+1 \ud835\udc5a=1 exp(\ud835\udc60 \ud835\udc5a ) = - \ud835\udc5b \u2211\ufe01 \ud835\udc57=1 ln exp(\ud835\udc60 1 + \ud835\udc5b-\ud835\udc57 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) \ud835\udc5b-\ud835\udc57+1 \ud835\udc5a=1 exp(\ud835\udc60 1 + \ud835\udc5a-1 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) = - \ud835\udc5b \u2211\ufe01 \ud835\udc57=1 ln exp( \ud835\udc5b-\ud835\udc57 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) \ud835\udc5b-\ud835\udc57+1 \ud835\udc5a=1 exp( \ud835\udc5a-1 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) = - \ud835\udc5b \u2211\ufe01 \ud835\udc57=1 \ud835\udc5b-\ud835\udc57 \u2211\ufe01 \ud835\udc58=0 \ud835\udeff \ud835\udc58 -ln( \ud835\udc5b-\ud835\udc57+1 \u2211\ufe01 \ud835\udc5a=1 exp( \ud835\udc5a-1 \u2211\ufe01 \ud835\udc58=0 \ud835\udeff \ud835\udc58 )) .", "formula_coordinates": [6.0, 87.13, 224.04, 173.61, 126.52]}, {"formula_id": "formula_14", "formula_text": "\ud835\udf15L \ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61 \ud835\udf15\ud835\udeff \ud835\udc56 = - \ud835\udc5b-\ud835\udc56 \u2211\ufe01 \ud835\udc57=1 1 - \ud835\udc5b-\ud835\udc57+1 \ud835\udc5a=\ud835\udc56+1 exp( \ud835\udc5a-1 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) \ud835\udc5b-\ud835\udc57+1 \ud835\udc5a=1 exp( \ud835\udc5a-1 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) < 0. (10", "formula_coordinates": [6.0, 73.31, 450.92, 217.32, 27.92]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [6.0, 290.62, 460.95, 3.42, 7.94]}, {"formula_id": "formula_16", "formula_text": "\ud835\udc5b-\ud835\udc57+1 \u2211\ufe01 \ud835\udc5a=\ud835\udc56+1 exp( \ud835\udc5a-1 \u2211\ufe01 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ) < \ud835\udc5b-\ud835\udc57+1 \u2211\ufe01 \ud835\udc5a=1 exp( \ud835\udc5a-1 \u2211\ufe01 \ud835\udc58=0 \ud835\udeff \ud835\udc58 ).", "formula_coordinates": [6.0, 100.87, 501.43, 145.49, 26.03]}, {"formula_id": "formula_17", "formula_text": "\ud835\udc36 = 1 \ud835\udc40 \ud835\udc40 \u2211\ufe01 \ud835\udc56=1 (z i -z)(z i -z) \u22a4 ,", "formula_coordinates": [11.0, 122.66, 389.5, 101.89, 24.75]}], "doi": "10.1109/ICCV48922.2021.00946"}
