{"FedUD: Exploiting Unaligned Data for Cross-Platform Federated Click-Through Rate Prediction": "Wentao Ouyang Alibaba Group Beijing, China maiwei.oywt@alibaba-inc.com Ri Tao Alibaba Group Beijing, China taori.tr@alibaba-inc.com", "ABSTRACT": "Click-through rate (CTR) prediction plays an important role in online advertising platforms. Most existing methods use data from the advertising platform itself for CTR prediction. As user behaviors also exist on many other platforms, e.g., media platforms, it is beneficial to further exploit such complementary information for better modeling user interest and for improving CTR prediction performance. However, due to privacy concerns, data from different platforms cannot be uploaded to a server for centralized model training. Vertical federated learning (VFL) provides a possible solution which is able to keep the raw data on respective participating parties and learn a collaborative model in a privacy-preserving way. However, traditional VFL methods only utilize aligned data with common keys across parties, which strongly restricts their application scope. In this paper, we propose FedUD, which is able to exploit unaligned data, in addition to aligned data, for more accurate federated CTR prediction. FedUD contains two steps. In the first step, FedUD utilizes aligned data across parties like traditional VFL, but it additionally includes a knowledge distillation module. This module distills useful knowledge from the guest party's high-level representations and guides the learning of a representation transfer network. In the second step, FedUD applies the learned knowledge to enrich the representations of the host party's unaligned data such that both aligned and unaligned data can contribute to federated model training. Experiments on two real-world datasets demonstrate the superior performance of FedUD for federated CTR prediction.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Online advertising .", "KEYWORDS": "Online advertising; Click-through rate (CTR) prediction; Vertical federated learning Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '24, July 14-18, 2024, Washington, DC, USA https://doi.org/10.1145/3626772.3657941", "Rui Dong": "Alibaba Group Beijing, China kailu.dr@alibaba-inc.com", "Xiangzheng Liu": "Alibaba Group Beijing, China xiangzheng.lxz@alibaba-inc.com", "ACMReference Format:": "Wentao Ouyang, Rui Dong, Ri Tao, and Xiangzheng Liu. 2024. FedUD: Exploiting Unaligned Data for Cross-Platform Federated Click-Through Rate Prediction. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24), July 1418, 2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages. https: //doi.org/10.1145/3626772.3657941", "1 INTRODUCTION": "Click-through rate (CTR) prediction is one of the most central tasks in online advertising platforms [12, 31]. Most existing methods use data collected from the advertising platform itself for CTR prediction [3, 7, 12, 16, 19, 20, 22, 24, 28, 31]. As user behaviors also exist on many other platforms, e.g., media platforms, it is beneficial to further exploit such complementary information for improving CTR prediction performance [13, 25]. However, due to user privacy concerns, data from different platforms cannot be uploaded to a server for centralized model training. Vertical federated learning (VFL) provides a feasible solution which allows different participating parties to keep raw data locally but train a collaborative model in a privacy-preserving way [26]. However, traditional VFL methods only utilize aligned data with common keys across parties, which strongly restricts their application scope. In this paper, we propose FedUD, which is able to exploit unaligned data, in addition to aligned data for federated CTR prediction. For simplicity, we consider a two-party setting, where the target advertising platform serves as the host party and another media platform servers as the guest party . FedUD contains two steps. First, FedUD utilizes aligned data across parties like traditional VFL, but it additionally includes a knowledge distillation module. Second, FedUD applies the learned knowledge to enrich the representations of the host party's unaligned data. In summary, the main contributions of this paper are \u00b7 Wepropose FedUD for federated CTR prediction in a privacypreserving way. FedUD is able to distill knowledge from aligned data' federated representations to enrich the host party's unaligned data' representations, such that both aligned and unaligned data can contribute to model training. \u00b7 During inference, FedUD benefits not only aligned inference data (like traditional VFL methods), but also unaligned inference data because of its representation transfer ability. \u00b7 We conduct experiments on two real-world datasets to evaluate the effectiveness of various methods. pCTR Guest party Host party label loss 1 Emb MLP Bottom model Top model MLP Emb MLP Bottom model loss 2 Knowledge distillation MLP \ud835\udc65 \u0bc1 \u0bd4 \ud835\udc21\u0bc1 \u0bd4 \ud835\udc21\u0bc0 \u0bd4 \ud835\udc21\u0bc0 \u0bd4 \ud835\udc21\u0bc0 \u0bd4 ~ \ud835\udc65 \u0bc0 \u0bd4 \ud835\udc66 \u0bc1 \u0bd4 \ud835\udc66 \u0bc1 \u0bd4 Sigmoid ^ \ud835\udc53 \u0bc1\u0bd5 \ud835\udc53 \u0bc1\u0be7 \ud835\udc53 \u0bc0 \ud835\udc45\ud835\udc52\ud835\udc5d Concat. (a) Step 1 Host party loss 3 \ud835\udc21\u0bc1 \u0bd4 \ud835\udc65 \u0bc1 \u0be8 \ud835\udc21\u0bc1 \u0be8 Bottom model \ud835\udc53 \u0bc1\u0bd5 Emb MLP \ud835\udc65 \u0bc1 \u0bd4 Emb MLP MLP \ud835\udc45\ud835\udc52\ud835\udc5d Guest party Emb MLP Bottom model \ud835\udc21\u0bc0 \u0bd4 \ud835\udc65 \u0bc0 \u0bd4 \ud835\udc53 \u0bc0 MLP Sigmoid Concat. MLP Sigmoid Concat. Top model \ud835\udc53 \u0bc1\u0be7 pCTR label \ud835\udc66 \u0bc1 \u0bd4 \ud835\udc66 \u0bc1 \u0bd4 pCTR label \ud835\udc66 \u0bc1 \u0be8 \ud835\udc66 \u0bc1 \u0be8 \ud835\udc21\u0bc0 \u0bd4 shared ^ ^ \ud835\udc21\u0bc0 \u0be8 ~ shared (b) Step 2", "2 PROBLEM STATEMENT": "For ease of illustration, we consider a two-party VFL setting. The host party has labels and features, and the guest party only has additional features. The two parties' samples have different feature spaces. Each sample has a key. In federated CTR prediction, the sample key could be the user's hashed device ID . VFL needs to first perform secured key intersection, e.g., private set intersection (PSI) [4, 15], to find aligned data with common keys across parties. We denote the aligned data across parties as D \ud835\udc4e = { \ud835\udc65 \ud835\udc4e \ud835\udc3b , \ud835\udc66 \ud835\udc4e \ud835\udc3b , \ud835\udc65 \ud835\udc4e \ud835\udc3a } , where { \ud835\udc65 \ud835\udc4e \ud835\udc3b } , { \ud835\udc66 \ud835\udc4e \ud835\udc3b } are the aligned samples and labels of the host party and { \ud835\udc65 \ud835\udc4e \ud835\udc3a } is the aligned samples of the guest party. Traditional VFL methods utilize aligned data to train a federated model \ud835\udc53 (D \ud835\udc4e ) for the host party with enriched features. Wedenotetheunaligned data of the host party as D \ud835\udc62 = { \ud835\udc65 \ud835\udc62 \ud835\udc3b , \ud835\udc66 \ud835\udc62 \ud835\udc3b } where { \ud835\udc65 \ud835\udc62 \ud835\udc3b } , { \ud835\udc66 \ud835\udc62 \ud835\udc3b } are the unaligned samples and labels. In this paper, we aim to further exploit the useful information contained in unaligned data, in addition to aligned data, to train a federated model \ud835\udc53 (D \ud835\udc4e , D \ud835\udc62 ) for the host party with improved performance.", "3 MODEL DESIGN": "", "3.1 Step 1: Federated Learning using Aligned Data with Knowledge Distillation": "We illustrate this step in Figure 1(a). For privacy protection, each party has its own local models. The guest party has only a local bottom model \ud835\udc53 \ud835\udc3a , whose aim is to extract high-level representations based on the guest party's local data. For simplicity, the guest party's local model \ud835\udc53 \ud835\udc3a contains an embedding layer and a multilayer perceptron (MLP) with several fully-connected layers (ReLU nonlinear activation) [11]. We denote the high-level representation of the data in the guest party as The host party has a local bottom model \ud835\udc53 \ud835\udc3b\ud835\udc4f , whose aim is to extract high-level representations based on the host party's local data. Similarly, \ud835\udc53 \ud835\udc3b\ud835\udc4f contains an embedding layer and an MLP. We denote the high-level representation of the data in the host party as The host party has an additional top model \ud835\udc53 \ud835\udc3b\ud835\udc61 , which takes the high-level representations h \ud835\udc4e \ud835\udc3b and h \ud835\udc4e \ud835\udc3a as input, and produces a CTR prediction logit as The host party's top model \ud835\udc53 \ud835\udc3b\ud835\udc61 contains a concatenation layer and an MLP. The predicted CTR is given by So far, we only utilize aligned data but ignores possibly useful information contained in unaligned data. However, it is non-trivial to utilize unaligned data of the host party because these data do not have corresponding features in the guest party. To tackle this problem, we aim to learn a representation transfer network \ud835\udc45\ud835\udc52\ud835\udc5d where the input is the high-level representation h \ud835\udc4e \ud835\udc3b in the host party, and the output is e h \ud835\udc4e \ud835\udc3a = \ud835\udc45\ud835\udc52\ud835\udc5d ( h \ud835\udc4e \ud835\udc3b ) which mimics the high-level representation h \ud835\udc4e \ud835\udc3a in the guest party in certain metric. In this way, the knowledge in h \ud835\udc4e \ud835\udc3a is distilled into e h \ud835\udc4e \ud835\udc3a and such knowledge guides the learning of the representation transfer network. For simplicity, this network contains an MLP. In Step 1 of FedUD, we have two aims: 1) learn accurate high-level representations and 2) learn an accurate representation transfer network with knowledge distillation. To achieve the first aim, we optimize the prediction loss as follows where \ud835\udc4c \ud835\udc4e \ud835\udc3b = { \ud835\udc66 \ud835\udc4e \ud835\udc3b } . To achieve the second aim, we optimize the mean squared error (MSE) loss between two representations as follows Both \ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60 1 and \ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60 2 are applied only on aligned data across parties. The overall loss in Step 1 of FedUD is given by where \ud835\udefc is a tunable balancing hyperparameter.", "3.2 Step 2: Federated Learning using Both Aligned and Unaligned Data": "We illustrate this step in Figure 1(b). For unaligned data \ud835\udc65 \ud835\udc62 \ud835\udc3b of the host party, we can only obtain the host party's representation h \ud835\udc62 \ud835\udc3b but not the guest party's representation. This makes unaligned data useless for traditional VFL methods. Differently, we have learned a representation transfer network \ud835\udc45\ud835\udc52\ud835\udc5d in Step 1. Although \ud835\udc45\ud835\udc52\ud835\udc5d is learned based on aligned data, we apply it to unaligned data to infer e h \ud835\udc62 \ud835\udc3a based on h \ud835\udc62 \ud835\udc3b . By doing so, we can use both aligned and unaligned data for model training. For aligned data D \ud835\udc4e = { \ud835\udc65 \ud835\udc4e \ud835\udc3b , \ud835\udc66 \ud835\udc4e \ud835\udc3b , \ud835\udc65 \ud835\udc4e \ud835\udc3a } , the high-level representation of the host party and that of the guest party are given by respective local bottom models The host party's top model then generates the predicted CTR \u02c6 \ud835\udc66 \ud835\udc4e \ud835\udc3b as For unaligned data D \ud835\udc62 = { \ud835\udc65 \ud835\udc62 \ud835\udc3b , \ud835\udc66 \ud835\udc62 \ud835\udc3b } , the high-level representation of the host party is given by its local bottom model, but the highlevel representation of the guest party is given by the representation transfer network \ud835\udc45\ud835\udc52\ud835\udc5d of the host party The host party's top model then generates the predicted CTR \u02c6 \ud835\udc66 \ud835\udc62 \ud835\udc3b as We then optimize the prediction loss based on both aligned and unaligned data as where \ud835\udc4c \ud835\udc4e \ud835\udc3b = { \ud835\udc66 \ud835\udc4e \ud835\udc3b } , \ud835\udc4c \ud835\udc62 \ud835\udc3b = { \ud835\udc66 \ud835\udc62 \ud835\udc3b } and \ud835\udefd is a tunable balancing hyperparameter. When we optimize \ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60 3, we load the learned parameters of the representation transfer network \ud835\udc45\ud835\udc52\ud835\udc5d and keep them frozen.", "3.3 Privacy": "FedUD preserves all the privacy properties of traditional VFL [26] because the guest party only sends the high-level representations h \ud835\udc4e \ud835\udc3a of aligned data to the host party and the host party only sends the corresponding partial gradients \u2207 h \ud835\udc4e \ud835\udc3a to the guest party. No raw data is communicated between the two parties. Unaligned data of the host party is only utilized by the host party itself.", "4 EXPERIMENTS": "", "4.1 Datasets": "(1) Avazu dataset [21]. We split it as training, validation and testing datasets each with 8 days, 1 day and 1 day of data. The host party and the guest party have 10 and 12 feature slots respectively. The sample key is hashed device ID. (2) Industrial dataset. We split it as training, validation and testing datasets each with 7 days, 1 day and 1 day of data. The host party is a news feed advertising platform. The guest party is a media platform. The host party and the guest party have 22 and 12 feature slots respectively. The sample key is hashed device ID.", "4.2 Methods in Comparison": "\u00b7 DNN [3]. Deep Neural Network. It contains an embedding layer, several fully connected layers and an output layer. \u00b7 Wide&Deep [3]. It combines logistic regression and DNN. \u00b7 DeepFM [6]. It combines factorization machine and DNN. \u00b7 AutoInt [20]. It consists of a multi-head self-attentive network with residual connections and DNN. \u00b7 FedSplitNN [8]. A classical VFL method using aligned data. \u00b7 FedCTR [25]. It exploits user features on other platforms to improve CTR prediction on the advertising platform. \u00b7 SS-VFL [1]. Self-Supervised Vertical Federated Learning. Step 1: each party uses its local data to pre-train network parameters using self-supervised learning. Step 2: aligned data are used to train a downstream prediction task. \u00b7 FedHSSL [8]. Federated Hybrid Self-Supervised Learning. Step 1: cross-party self-supervised learning [2, 5] using aligned data. Step 2: cross-party-guided local self-supervised learning using local data. Step 3: partial model aggregation. \u00b7 FedUD . The proposed federated learning with unaligned data method in this paper. Among these methods, DNN, Wide&Deep, DeepFM and AutoInt are local methods which use only the host party's data to train a local model. The others are VFL methods. In particular, FedSplitNN and FedCTR use only aligned data across parties. SS-VFL, FedHSSL and FedUD use both aligned and unaligned data. All the above methods are based on standard CTR features per sample.", "4.3 Settings": "Parameters. We set the embedding dimension as 10, the layer dimensions in the top model as {256, 128} and those in other models as {512, 256, 128}. Evaluation Metric. We use AUC and LogLoss as the evaluation metrics for CTR prediction.", "4.4 Experimental Results": "4.4.1 Effectiveness . In Table 2, 'overall', 'aligned' and 'unaligned' mean AUC/LogLoss is computed on all the test data, only aligned test data (which have the guest party's features) and only unaligned test data (which do not have the guest party's features) respectively. It is observed that most federated methods (e.g., FedHSSL) perform better than local methods (e.g., AutoInt) on aligned test data because of the inclusion of the guest party' features. However, most federated methods perform worse than local methods on unaligned test data because of the absence of the guest party's features. Local methods do not use the guest party's features at all. Although SSVFL and FedHSSL also exploit unaligned data, they only use such data for self-supervised learning, and thus they also do not perform well on unaligned test data. Differently, FedUD explicitly transfers Table 2: Test AUCs on experimental datasets. 'overall', 'aligned' and 'unaligned' mean AUC is computed on all the test data, only aligned test data (which have the guest party's features) and only unaligned test data (which do not have the guest party's features) respectively. The best result is in bold font. The second best result is underlined. A small improvement in AUC (e.g., 0.0020) can lead to a significant increase in online CTR (e.g., 3%) [3]. * indicates the statistical significance for \ud835\udc5d \u2264 0 . 01 compared with the second best result over paired t-test. 2 4 6 8 10 12 Number of the guest party's feature slots 0.7300 0.7320 0.7340 0.7360 0.7370 AUC (a) Avazu 2 4 6 8 10 12 Number of the guest party's feature slots 0.8020 0.8030 0.8040 0.8050 0.8060 AUC (b) Industrial 0.3M0.6M 1.1M 1.7M 2.2M 2.8M Number of unaligned training samples 0.7280 0.7300 0.7320 0.7340 0.7360 AUC (a) Avazu 15M 30M 60M 90M 120M 150M Number of unaligned training samples 0.8020 0.8030 0.8040 0.8050 0.8060 AUC (b) Industrial knowledge from aligned data to unaligned data and exhibits the best performance on both aligned and unaligned data. It is observed that the AUC improvement of FedUD over DNN (a local method) on aligned data is higher than that on unaligned data. It is because aligned data have real guest party's features but unaligned data have inferred guest party's representations which are more noisy. 4.4.2 Effect of the Guest Party's Feature Slots . Figure 2 shows the overall AUCs vs. the number of the guest party's feature slots. AUCs on aligned / unaligned test data show similar trends. Due to space limitation, we omit these figures. It is observed that more guest party's feature slots generally provide useful additional information and lead to improved performance. But adding more feature slots may also include noise and possibly lead to flatten or even slightly degraded performance. 4.4.3 Effect of the Host Party's Unaligned Samples . In this experiment, we use all the aligned samples but different numbers of unaligned samples for training. Figure 3 shows the overall AUCs vs. the number of the host party's unaligned samples. The rightmost point on the x-axis denotes the maximum number of unaligned samples in the training set. It is observed that more unaligned samples generally lead to improved prediction performance. But after certain amount, the improvement becomes less obvious.", "5 RELATED WORK": "CTR prediction. CTR prediction has attracted lots of attention in recent years. Methods range from shallow models such as LR [18], FM [17], Field-weighted FM [14] to deep models such as Wide & Deep [3], DeepFM [6], xDeepFM [10], AutoInt [20] and DIL [29]. These methods are based on standard CTR features per sample. Some other methods exploit auxiliary information. DIN [31] and DIEN [30] consider user historical click behaviors. BERT4CTR [23] considers pre-trained language model information. All these methods are centralized, which process all the data in a central server. Vertical federated learning (VFL). VFL provides a feasible solution for cross-platform federated CTR prediction. It is able to keep the raw data locally and learn a collaborative model in a privacy-preserving way [9, 26, 27]. However, FedSplitNN [8] and FedCTR [25] only utilize aligned data with common keys across parties, which strongly restricts their application scope. SS-VFL [1] and FedHSSL [8] exploit both aligned and unaligned data, but they only use unaligned data for local self-supervised learning. Differently, FedUD proposed in this paper explicitly transfers knowledge learned from cross-party aligned data to unaligned data. FedUD is thus able to benefit both aligned and unaligned inference data.", "6 CONCLUSION": "In this paper, we propose FedUD for federated CTR prediction. FedUD is able to transfer the knowledge from aligned data across parties to enrich the representations of unaligned data such that both aligned and unaligned data can contribute to federated model training. It also benefits both aligned and unaligned data during inference. Experimental results demonstrate the effectiveness of FedUD for federated CTR prediction. FedUD: Exploiting Unaligned Data for Cross-Platform Federated CTR Prediction", "REFERENCES": "[1] Timothy Castiglia, Shiqiang Wang, and Stacy Patterson. 2022. Self-Supervised Vertical Federated Learning. In Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022) . [2] Xinlei Chen and Kaiming He. 2021. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR) . 15750-15758. [3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS) . ACM, 7-10. [4] Emiliano De Cristofaro and Gene Tsudik. 2010. Practical private set intersection protocols with linear complexity. In International Conference on Financial Cryptography and Data Security . Springer, 143-159. [5] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a new approach to self-supervised learning. In Advances in neural information processing systems (NeurIPS) . 21271-21284. [6] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. Deepfm: a factorization-machine based neural network for ctr prediction. In International Joint Conferences on Artificial Intelligence (IJCAI) . 1725-1731. [7] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the International Workshop on Data Mining for Online Advertising (ADKDD) . ACM, 1-9. [8] Yuanqin He, Yan Kang, Xinyuan Zhao, Jiahuan Luo, Lixin Fan, Yuxing Han, and Qiang Yang. 2023. A hybrid self-supervised learning framework for vertical federated learning. arXiv preprint arXiv:2208.08934v2 (2023). [9] Yan Kang, Yuanqin He, Jiahuan Luo, Tao Fan, Yang Liu, and Qiang Yang. 2022. Privacy-preserving federated adversarial domain adaptation over feature groups for interpretability. IEEE Transactions on Big Data (2022). [10] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xDeepFM: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) . ACM, 1754-1763. [11] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML) . 807-814. [12] Wentao Ouyang, Xiuwu Zhang, Li Li, Heng Zou, Xin Xing, Zhaojie Liu, and Yanlong Du. 2019. Deep spatio-temporal neural networks for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) . ACM, 2078-2086. [13] Wentao Ouyang, Xiuwu Zhang, Lei Zhao, Jinmei Luo, Yu Zhang, Heng Zou, Zhaojie Liu, and Yanlong Du. 2020. MiNet: Mixed Interest Network for Cross-Domain Click-Through Rate Prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management (CIKM) . ACM, 2669-2676. [14] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu. 2018. Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising. In Proceedings of the 2018 World Wide Web Conference (WWW) . IW3C2, 1349-1357. [15] Benny Pinkas, Thomas Schneider, and Michael Zohner. 2018. Scalable private set intersection based on OT extension. ACM Transactions on Privacy and Security (TOPS) 21, 2 (2018), 1-35. [16] Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Yong Yu. 2020. User Behavior Retrieval for Click-Through Rate Prediction. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . ACM, 2347-2356. [17] Steffen Rendle. 2010. Factorization machines. In IEEE 10th International Conference on Data Mining (ICDM) . IEEE, 995-1000. [18] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting clicks: estimating the click-through rate for new ads. In Proceedings of the 16th International Conference on World Wide Web (WWW) . IW3C2, 521-530. [19] Parikshit Shah, Ming Yang, Sachidanand Alle, Adwait Ratnaparkhi, Ben Shahshahani, and Rohit Chandra. 2017. A practical exploration system for search advertising. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) . ACM, 1625-1631. [20] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via selfattentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM) . ACM, 1161-1170. [21] Will Cukierski Steve Wang. 2014. Click-Through Rate Prediction. https://kaggle. com/competitions/avazu-ctr-prediction [22] Zhen Tian, Ting Bai, Wayne Xin Zhao, Ji-Rong Wen, and Zhao Cao. 2023. EulerNet: Adaptive Feature Interaction Learning via Euler's Formula for CTR Prediction. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . 1376-1385. [23] Dong Wang, Kav\u00e9 Salamatian, Yunqing Xia, Weiwei Deng, and Qi Zhang. 2023. BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) . 5039-5050. [24] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the International Workshop on Data Mining for Online Advertising (ADKDD) . ACM, 12. [25] Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie. 2022. FedCTR: Federated native ad CTR prediction with cross-platform user behavior data. ACM Transactions on Intelligent Systems and Technology (TIST) 13, 4 (2022), 1-19. [26] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST) 10, 2 (2019), 1-19. [27] Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao. 2021. A survey on federated learning. Knowledge-Based Systems 216, 106775. [28] Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep learning over multi-field categorical data. In European Conference on Information Retrieval (ECIR) . Springer, 45-57. [29] Yang Zhang, Tianhao Shi, Fuli Feng, Wenjie Wang, Dingxian Wang, Xiangnan He, and Yongdong Zhang. 2023. Reformulating CTR Prediction: Learning Invariant Feature Interactions for Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . 1386-1395. [30] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) , Vol. 33. 5941-5948. [31] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) . ACM, 1059-1068."}
