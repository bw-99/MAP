{"title": "CAM2: Conformity-Aware Multi-Task Ranking Model for Large-Scale Recommender Systems", "authors": "Ameya Raul; Amey Porobo Dharwadker; Brad Schumitsch", "pub_date": "", "abstract": "Learning large-scale industrial recommender system models by fitting them to historical user interaction data makes them vulnerable to conformity bias. This may be due to a number of factors, including the fact that user interests may be difficult to determine and that many items are often interacted with based on ecosystem factors other than their relevance to the individual user. In this work, we introduce CAM2, a conformity-aware multi-task ranking model to serve relevant items to users on one of the largest industrial recommendation platforms. CAM2 addresses these challenges systematically by leveraging causal modeling to disentangle users' conformity to popular items from their true interests. This framework is generalizable and can be scaled to support multiple representations of conformity and user relevance in any largescale recommender system. We provide deeper practical insights and demonstrate the effectiveness of the proposed model through improvements in offline evaluation metrics compared to our production multi-task ranking model. We also show through online experiments that the CAM2 model results in a significant 0.50% increase in aggregated user engagement, coupled with a 0.21% increase in daily active users on Facebook Watch, a popular video discovery and sharing platform serving billions of users.", "sections": [{"heading": "INTRODUCTION", "text": "Large-scale recommender systems provide personalized recommendations to users by considering their historical interactions and learning predictive models to fit that data. Such systems could result in strong conformity bias as they fail to take into account the fact that previous user interactions may have been influenced by multiple factors causing them to be inconsistent with user preferences. For example, a user may watch a video just because it has a lot of views, even if it doesn't align with their true interests. This, in turn, can result in the system recommending more popular items rather than those that may be more relevant to the individual user, thereby limiting users' exposure to long-tail content [3].\nExisting methods to disentangle conformity and interest address this primarily by eliminating popularity bias using a static global term on the item side [2], while ignoring differing levels of conformity among users. To solve this issue, a recent work [19] decomposes user interactions into two factors -conformity and interest, considering both users and items, and learns separate embedding representations for them with cause-specific data. This helps model the personalized conformance effect and captures the fact that different users have different levels of influence on their judgment. It computes user-item matching scores for both causes and sums them up to estimate the overall score on whether a user will interact with the item.\nIn this work, we present CAM2, a conformity-aware multi-task ranking model to improve user recommendations on Facebook Watchfoot_0 by extending the above idea of cause-specific embeddings. The main contributions of this work are as follows:\n\u2022 This work disentangles conformity and relevance based representations learning in the same model over the full training data by separating statistical interaction-based features from rich attribute-based and content-based features. \u2022 We present a novel loss formulation for our causal representation modules. In contrast to previous works that rely on aggregating all losses, we train these modules independent of the user interaction tasks' losses. We modify gradient back-propagation from the main network to each of the causal modules to ensure that the performance of the predicted user interaction tasks are optimized without ignoring the causal losses. \u2022 We provide deeper insights into efficiently using the personalized conformity-aware embeddings in our multi-task ranking model to achieve the best predictive performance across tasks. \u2022 Through experiments on our large-scale video recommendation platform, we validate the effectiveness of the proposed approach to improve prediction performance across user interaction tasks, leading to improvements in online metrics.", "publication_ref": ["b2", "b1", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "METHODOLOGY 2.1 Problem Formulation", "text": "Facebook Watch, with more than 1.25 billion monthly viewers [15] is one of the largest global destinations for discovering and sharing video content. We use a typical two-stage recommendation ranking system design with a candidate generation stage and a ranking stage [6,11]. This paper focuses on the ranking stage, where the recommender has a few hundred promising candidates retrieved from the candidate generation stage. It applies a sophisticated, largecapacity model to rank these candidates and sort them in descending order of their relevance to users [14].\nTo effectively learn multiple types of user behavior and relevance dimensions, we use a multi-task deep neural network ranking model to predict multiple binary user interaction objectives such as user clicks, user interaction level with recommended video, user comments, etc. Similar to other multi-task ranking models, our system subsequently combines these predictions to compute a final relevance score for ranking.\nSuppose we have a dataset of \ud835\udc5b i.i.d. training data samples and \ud835\udc47 is the number of user interaction tasks to model. All tasks share an input space {\ud835\udc65 \ud835\udc56 } consisting of user, item and user-item interaction features for example \ud835\udc56 and {\ud835\udc66 \ud835\udc56 \ud835\udc61 } is the binary user interaction label for the \ud835\udc61 \ud835\udc61\u210e task for example \ud835\udc56. The goal of our multi-task model is to learn a causal-aware scoring function \ud835\udc53 (\ud835\udc65, \ud835\udc66, \ud835\udc61 |\ud835\udf03 ) which predicts the probability of user interaction \ud835\udc61 occurring on each example the model is applied on, parameterized by shared and task-specific parameters \ud835\udf03 . Our goal is to optimize predictions of the model for each interaction task objective by considering both conformity and relevance, to maximize recommendation performance metrics on future test data, that is not i.i.d. with the training data.", "publication_ref": ["b14", "b5", "b10", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Conformity-Aware Model Training", "text": "Our existing multi-task deep neural network recommendation model serving production traffic on Facebook Watch consists of a sequence of multi-layer perceptrons, composed of a sequence of fully connected layers and an activation function applied componentwise. The model has a combination of hard and soft parameter sharing [16] in the lower layers to exploit task relatedness better [1,5] and enable some form of regularization as different tasks share model parameters. We input simple binary and continuous features directly into the model as real normalized values. In addition, embeddings are used to process the input of sparse categorical features into appropriate fixed-width dense representations. The model is trained with logistic regression under normalized cross-entropy (NE) loss. The blue box in Figure 1 shows a simplified view of our existing production model architecture.\nIn order to improve relevance of recommendations by learning better representations, we need to better understand the underlying cause of user interactions. Our goal is to disentangle interactions due to user's alignment to conformity from those primarily due to user relevance that provide higher user value. We learn personalized causal embeddings in our multi-task ranking model corresponding to conformity and relevance based components using the steps  outlined below. The orange box in Figure 1 shows the proposed causal architecture addition to the production model.", "publication_ref": ["b15", "b0", "b4"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Causal Embeddings Model Architecture", "text": "In this work, we train two separate cause-specific residual network (ResNet) modules [10] as auxiliary tasks to learn decoupled causal embeddings indicating conformity alignment and relevance. We also introduce dedicated conformity and relevance losses to train these modules. They are only used to train the causal architectures for learning embedding representations jointly with the main model parameters, and not for model inference. We also disable gradient back-propagation from the task architectures to the causal modules to decouple the causal modules and prevent incorporating any bias in them from the task architectures. This allows us to train our causal embeddings on all available data without partitioning data into factor-specific data subsets based on the underlying reasons of user engagement.\nFirst, the conformity module models the probability of user engagement due to conformity. There are two types of conformity:\n\u2022 User Conformity, denoted by \ud835\udc62, is the global likelihood that a user will engage with any given item. \u2022 Item Conformity, denoted by \ud835\udc56, is the global likelihood that an item will be engaged with by any user.\nThe conformity module predicts a combined conformity \ud835\udc50 by combining the losses from both of these types of conformity. The total conformity loss (\ud835\udc3f \ud835\udc36 ) is calculated as:\n\ud835\udc3f \ud835\udc36 (\ud835\udc62, \ud835\udc56) = (\ud835\udc50 -\u00fb + \u00ee )\nwhere \u2225.\u2225 denotes L2-norm and \u00fb and \u00ee are the predictions for user and item conformity respectively.\nSecond, the relevance module models the probability of a user engaging with an item based on the user's interest affinity with the item. Our model predicts whether the item is aligned with the user's interests or not. The total relevance loss (\ud835\udc3f \ud835\udc45 ) is calculated as:\n\ud835\udc3f \ud835\udc45 (\ud835\udc62, \ud835\udc56) = \ud835\udc58 \u2211\ufe01 \ud835\udc65=0 \ud835\udc5f \ud835\udc65 -( \u00fb\ud835\udc65 \u2022 \u00ee \ud835\udc65 )\nwhere \u2225.\u2225 denotes L2-norm, \ud835\udc5f \ud835\udc65 denotes the true engagement rate of a user with interest \ud835\udc65, while \u00fb\ud835\udc65 is the predicted probability Pr(User Engagement | Interest \ud835\udc65) and \u00ee \ud835\udc65 is the predicted probability that the item belongs to interest \ud835\udc65. Since each item can be associated with multiple interests \ud835\udc58, we aggregate over all these interests to obtain the final loss for a user-item pair.\nCAM2 predicts multiple user engagement tasks in the multi-task model, hence we take a weighted sum of these individual causespecific losses along with our dedicated task losses to get the total loss \ud835\udc3f.\n\ud835\udc3f = \ud835\udc47 \u2211\ufe01 \ud835\udc61 =1 (\ud835\udc64 \ud835\udc61 \u2022 \ud835\udc3f \ud835\udc61 ) + \ud835\udc64 \ud835\udc36 \u2022 \ud835\udc3f \ud835\udc36 + \ud835\udc64 \ud835\udc45 \u2022 \ud835\udc3f \ud835\udc45\nwhere \ud835\udc3f \ud835\udc61 is the loss for task \ud835\udc61 and \ud835\udc64 \ud835\udc61 are tunable weights. To simplify the above formulation, we model this as a classification problem and define our causal labels by decomposing the probability of user engagement task \ud835\udc61 into conformity and relevance as:\nPr(\ud835\udc61) = \ud835\udc64 1 \u2022 Pr(\ud835\udc61 | Conformity) + \ud835\udc64 2 \u2022 Pr(\ud835\udc61 | Relevance) = \ud835\udc64 1 \u2022 Pr(\ud835\udc61 | \ud835\udc4b >= \ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc60\u210e) + \ud835\udc64 2 \u2022 Pr(\ud835\udc61 | \ud835\udc4b < \ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc60\u210e)\nwhere \ud835\udc4b is a scalar derived from user video historical engagement, used to distinguish the conformity and relevance separation with a static threshold \ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc60\u210e. Parameters \ud835\udc64 1 and \ud835\udc64 2 denote Pr(Conformity) and Pr(Relevance) respectively and are learnt jointly with the tasks. The conformity label is 1 if a user engagement happens and \ud835\udc4b >= \ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc60\u210e and 0 otherwise, while the relevance label is 1 if user engagement happens and \ud835\udc4b < \ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc60\u210e and 0 otherwise. Hence for the relevance loss, \ud835\udc5f \ud835\udc65 is our label as defined above and our model predicts the product ( \u00fb\ud835\udc65 \u2022 \u00ee \ud835\udc65 ). For conformity loss, \ud835\udc50 is our label as defined above and \u00fb + \u00ee is our model's prediction.\nThis causal embedding training approach improves the main (non-auxiliary) tasks' predictions in the model, which are then combined to compute a final relevance score for ranking and recommending top-k items to the given user. The framework is generalizable and can be scaled to support multiple representations of conformity and user relevance using the above formulations.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Input Features Partitioning", "text": "The DICE framework [19] disentangled representations for conformity and relevance by partitioning the training data into causespecific parts, and trained different embeddings with cause-specific data. In contrast, we present a unique way to achieve this using all available training data samples by partitioning our features into two types: We hypothesize that statistical engagement-based features on both the video and user side are sufficient to learn user conformity alignment, while rich attribute-based and content-based features characterizing user and video properties enable user relevance learning. Combining these features results in a higher degree of entanglement and tends to skew the model towards popular and over-represented training data samples. Our input features partitioning approach allows the ranking model to learn more accurate embedding representations for both popular and long-tail groups by separating each cause of user engagement.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS 3.1 Experimental Setup", "text": "We perform both offline and online evaluations on Facebook Watch, a real-world video recommendation system serving billions of users. We compare results against our production multi-task deep neural network ranking model, which is comparable to other large-scale industry video recommender ranking models [6]. The offline dataset contains 1.2B+ users, 100B+ instances and 35B+ user video engagements. It is split chronologically into the train set for model training and the next-day holdout test set for model performance evaluation. The models are trained recurrently on each day of additional data, starting with the previous network weights and optimizer state.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Offline Experiments", "text": "We use normalized cross-entropy (NE) as our primary offline evaluation metric as we expect accurate predictions, rather than just getting the optimal ranking order [11]. The lower the NE value, the better the model prediction.\nThe basic conformity-aware multi-task ranking model as illustrated in Figure 1, consists of feature decomposition at the input layer, and two dedicated cause-specific conformity and relevance residual network (ResNet) modules trained with dedicated loss functions. The disentangled embeddings extracted from the corresponding ResNets are used to train the task architectures after concatenating with the main network's shared bottom arch outputs. For simplicity, we refer to this variant as CAM2-Proposed. We conduct ablation studies to comparatively evaluate our modeling choices.\n\u2022 Embedding Usage: Instead of concatenating the embeddings with the shared bottom arch outputs, variant CAM2-TaskArch experiments with concatenating them directly at the final layer of the main engagement task architectures. \u2022 Causal Losses: In order to understand the importance of task independent causal losses, we experiment with a joint loss defined using a combination of existing engagement task labels and our causal labels in variant CAM2-JointLoss. \u2022 Input Features Partitioning: Variant CAM2-AllFeats experiments with adding all features to conformity and relevance modules. It serves as an ablation study on the input features partitioning we propose.\nThe ablation studies in Table 1 show that our proposed CAM2 model that leverages causal embeddings to learn every task by concatenating with the shared bottom arch outputs, improves over the existing production model. Additionally, using input features partitioning and separate auxiliary conformity and relevance losses improve the model performance further. Our hypothesis is that leveraging disentangled representations in all stages of the model learning can encode different aspects of the reason for user engagement, resulting in better prediction performance across tasks.", "publication_ref": ["b10"], "figure_ref": ["fig_0"], "table_ref": ["tab_2"]}, {"heading": "Variant", "text": "Tasks NE (Aggregated) CAM2-TaskArch -0.060% CAM2-JointLoss -0.016% CAM2-AllFeats +0.029% CAM2-Proposed -0.139% ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Online Experiments", "text": "We deploy CAM2 on Meta's dedicated inference cloud [9] and conduct three weeks of A/B testing to rank videos for users on Facebook Watch. The model predictions are used for scoring and generating a ranked list of videos displayed to users. The same ranking strategy and business logic is applied on control and treatment groups for fair comparison. Online A/B test shows that the causal model results in substantial wins across various metrics, all statistically significant with 95% confidence interval.\nCompared to production multi-task ranking model control, the treatment group using CAM2 for ranking videos in Watch shows 0.50% increase in aggregated user engagement metric and 0.21% increase in daily active users on Facebook Watch (Figure 2), advancing our billion-scale video recommender system by a large margin.  Our conformity-aware multi-task model reduces popularity bias by serving more long-tail and new videos. The number of videos resulting in 50% and 75% aggregated user engagements increased by 2.35% and 2.06% respectively. As shown in Table 2, we also notice significant increase in user engagement metric on new videos compared to control. This demonstrates that the causal embedding can learn better long-tail representation and capture relevant user interests by separating conformity information, instead of recommending irrelevant popular videos.\nCasual (low activity) users are defined as those users who have been active on Facebook Watch for 1 to 2 days in the last 28 days. To measure engagement as well as retention metrics on casual users, we freeze user activity levels just before the start of the experiment. Our model can better generalize to casual users by leveraging causespecific embeddings, resulting in significant 0.83% aggregated user engagement metric gains and 0.62% increase in daily active users on Facebook Watch on this user cohort. This indicates that casual users have a better overall experience and are encouraged to return to the platform due to better recommendation quality.", "publication_ref": ["b8"], "figure_ref": ["fig_2"], "table_ref": ["tab_3"]}, {"heading": "RELATED WORK", "text": "In recommender systems, it is important to consider that conformity can distort user ratings [12]. When users are influenced by other factors in the ecosystem, their interactions may not be an accurate representation of their own interests [13]. This can lead to biases in user ratings, and ultimately irrelevant recommendations. Existing approaches address this issue as an attempt to eliminate popularity bias through inverse propensity scoring (IPS) [4,7] or causal approaches [8,17,18]. IPS-based methods re-weight popular items by the reciprocal of their popularity score to compensate for the fact that more popular items are more likely to be recommended. However, these methods are very sensitive to the weighting and don't consider that not all popularity biases are bad, for example, viral and trending videos could be relevant to users and deserve to be recommended more. The causal methods mainly focus on confounding effects but lack fine-grained consideration of how to leverage popularity bias systematically to improve recommendation relevance.\nRecent work on DICE [19] which disentangles user conformity and interest representations is most relevant to our work. However, our proposed formulation of training personalized conformityaware embeddings in the model as auxiliary tasks on the entire training data with input features partitioning, independent of the user interaction task losses has not been explored to the best of our knowledge.", "publication_ref": ["b11", "b12", "b3", "b6", "b7", "b16", "b17", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this work, we describe conformity bias as a real-world challenge in the design and development of large-scale recommender systems. We present a conformity-aware multi-task ranking model to address this issue and serve the most relevant recommendations. Our scalable and generalizable framework disentangles representations of user conformity and relevance in the model over the entire training data with input features partitioning and dedicated auxiliary losses. We demonstrate the effectiveness of our proposed method to improve user engagement and ecosystem value metrics on Facebook Watch, an industrial video discovery and sharing platform.\nCAM2 develops a unique understanding of users by leveraging its input feature partitioning and loss formulation techniques. In the future, we will explore incorporating other user interaction causes in the model and investigate their impact on long-term metrics.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A model of inductive bias learning", "journal": "Journal of artificial intelligence research", "year": "2000", "authors": "Jonathan Baxter"}, {"ref_id": "b1", "title": "The bellkor 2008 solution to the netflix prize", "journal": "Statistics Research Department at AT&T Research", "year": "2008", "authors": "M Robert; Yehuda Bell; Chris Koren;  Volinsky"}, {"ref_id": "b2", "title": "Recommender systems survey", "journal": "Knowledge-based systems", "year": "2013", "authors": "Jes\u00fas Bobadilla; Fernando Ortega; Antonio Hernando; Abraham Guti\u00e9rrez"}, {"ref_id": "b3", "title": "Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising", "journal": "Journal of Machine Learning Research", "year": "2013", "authors": "L\u00e9on Bottou; Jonas Peters; Joaquin Qui\u00f1onero-Candela; Denis X Charles; Max Chickering; Elon Portugaly; Dipankar Ray; Patrice Simard; Ed Snelson"}, {"ref_id": "b4", "title": "Multitask learning", "journal": "Machine learning", "year": "1997", "authors": "Rich Caruana"}, {"ref_id": "b5", "title": "Deep neural networks for youtube recommendations", "journal": "", "year": "2016", "authors": "Paul Covington; Jay Adams; Emre Sargin"}, {"ref_id": "b6", "title": "Offline evaluation to make decisions about playlistrecommendation algorithms", "journal": "", "year": "2019", "authors": "Alois Gruson; Praveen Chandar; Christophe Charbuillet; James Mcinerney; Samantha Hansen; Damien Tardieu; Ben Carterette"}, {"ref_id": "b7", "title": "CauSeR: Causal Session-based Recommendations for Handling Popularity Bias", "journal": "", "year": "2021", "authors": "Priyanka Gupta; Ankit Sharma; Pankaj Malhotra; Lovekesh Vig; Gautam Shroff"}, {"ref_id": "b8", "title": "Applied machine learning at facebook: A datacenter infrastructure perspective", "journal": "IEEE", "year": "2018", "authors": "Kim Hazelwood; Sarah Bird; David Brooks; Soumith Chintala; Utku Diril; Dmytro Dzhulgakov; Mohamed Fawzy; Bill Jia; Yangqing Jia; Aditya Kalro"}, {"ref_id": "b9", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b10", "title": "Practical lessons from predicting clicks on ads at facebook", "journal": "", "year": "2014", "authors": "Xinran He; Junfeng Pan; Ou Jin; Tianbing Xu; Bo Liu; Tao Xu; Yanxin Shi; Antoine Atallah; Ralf Herbrich; Stuart Bowers"}, {"ref_id": "b11", "title": "When sheep shop: measuring herding effects in product ratings with natural experiments", "journal": "", "year": "2018", "authors": "Gael Lederrey; Robert West"}, {"ref_id": "b12", "title": "Are you influenced by others when rating? Improve rating prediction by conformity modeling", "journal": "", "year": "2016", "authors": "Yiming Liu; Xuezhi Cao; Yong Yu"}, {"ref_id": "b13", "title": "Deep learning recommendation model for personalization and recommendation systems", "journal": "", "year": "2019", "authors": "Maxim Naumov; Dheevatsa Mudigere; Michael Hao-Jun; Jianyu Shi; Narayanan Huang; Jongsoo Sundaraman; Xiaodong Park; Udit Wang; Carole-Jean Gupta; Alisson G Wu;  Azzolini"}, {"ref_id": "b14", "title": "The Evolution of Facebook Watch", "journal": "", "year": "2020-10-27", "authors": "Paresh Rajwat"}, {"ref_id": "b15", "title": "An overview of multi-task learning in deep neural networks", "journal": "", "year": "2017", "authors": "Sebastian Ruder"}, {"ref_id": "b16", "title": "Deconvolving feedback loops in recommender systems", "journal": "Advances in neural information processing systems", "year": "2016", "authors": "Ayan Sinha; David F Gleich; Karthik Ramani"}, {"ref_id": "b17", "title": "Causal inference for recommender systems", "journal": "", "year": "2020", "authors": "Yixin Wang; Dawen Liang; Laurent Charlin; David M Blei"}, {"ref_id": "b18", "title": "Disentangling user interest and conformity for recommendation with causal embedding", "journal": "", "year": "2021", "authors": "Yu Zheng; Chen Gao; Xiang Li; Xiangnan He; Yong Li; Depeng Jin"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Proposed conformity-aware multi-task ranking model architecture with existing prod model architecture (blue box on the right) and proposed causal model addition to it (orange box on left).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Overall user engagement metric and daily active users improvements on Facebook Watch in online A/B test.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Offline Results for ablation studies relative to production. Best results for the metric are indicated in bold.", "figure_data": "[0-1 day) [1-3 days) [3-10 days) [10+ days)+2.43%+1.17%-0.28%Neutral"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "User engagement metric by video age relative to production.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\ud835\udc3f \ud835\udc36 (\ud835\udc62, \ud835\udc56) = (\ud835\udc50 -\u00fb + \u00ee )", "formula_coordinates": [2.0, 392.46, 586.8, 86.74, 9.54]}, {"formula_id": "formula_1", "formula_text": "\ud835\udc3f \ud835\udc45 (\ud835\udc62, \ud835\udc56) = \ud835\udc58 \u2211\ufe01 \ud835\udc65=0 \ud835\udc5f \ud835\udc65 -( \u00fb\ud835\udc65 \u2022 \u00ee \ud835\udc65 )", "formula_coordinates": [2.0, 384.54, 685.68, 102.58, 24.75]}, {"formula_id": "formula_2", "formula_text": "\ud835\udc3f = \ud835\udc47 \u2211\ufe01 \ud835\udc61 =1 (\ud835\udc64 \ud835\udc61 \u2022 \ud835\udc3f \ud835\udc61 ) + \ud835\udc64 \ud835\udc36 \u2022 \ud835\udc3f \ud835\udc36 + \ud835\udc64 \ud835\udc45 \u2022 \ud835\udc3f \ud835\udc45", "formula_coordinates": [3.0, 107.78, 198.16, 131.33, 25.96]}, {"formula_id": "formula_3", "formula_text": "Pr(\ud835\udc61) = \ud835\udc64 1 \u2022 Pr(\ud835\udc61 | Conformity) + \ud835\udc64 2 \u2022 Pr(\ud835\udc61 | Relevance) = \ud835\udc64 1 \u2022 Pr(\ud835\udc61 | \ud835\udc4b >= \ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc60\u210e) + \ud835\udc64 2 \u2022 Pr(\ud835\udc61 | \ud835\udc4b < \ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc60\u210e)", "formula_coordinates": [3.0, 68.39, 277.23, 210.61, 23.24]}], "doi": "10.1145/3543873.3584657"}
