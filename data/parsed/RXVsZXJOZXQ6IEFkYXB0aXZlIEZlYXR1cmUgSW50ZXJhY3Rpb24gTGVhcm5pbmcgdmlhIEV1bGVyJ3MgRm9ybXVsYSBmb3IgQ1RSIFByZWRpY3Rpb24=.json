{"EulerNet: Adaptive Feature Interaction Learning via Euler's Formula for CTR Prediction": "Zhen Tian \u2020 Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China chenyuwuxinn@gmail.com Ting Bai Beijing University of Posts and Telecommunications Beijing, China baiting@bupt.edu.cn \u2217\u2021 Wayne Xin Zhao \u2217\u2020 Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China batmanfly@gmail.com", "Ji-Rong Wen": "\u2020 Gaoling School of Artificial Intelligence, Renmin University of China Beijing, China jrwen@ruc.edu.cn Zhao Cao Poisson Lab, Huawei Beijing, China caozhao1@huawei.com", "ABSTRACT": "Learning effective high-order feature interactions is very crucial in the CTR prediction task. However, it is very time-consuming to calculate high-order feature interactions with massive features in online e-commerce platforms. Most existing methods manually design a maximal order and further filter out the useless interactions from them. Although they reduce the high computational costs caused by the exponential growth of high-order feature combinations, they still suffer from the degradation of model capability due to the suboptimal learning of the restricted feature orders. The solution to maintain the model capability and meanwhile keep it efficient is a technical challenge, which has not been adequately addressed. To address this issue, we propose an adaptive feature interaction learning model, named as EulerNet , in which the feature interactions are learned in a complex vector space by conducting space mapping according to Euler's formula. EulerNet converts the exponential powers of feature interactions into simple linear combinations of the modulus and phase of the complex features, making it possible to adaptively learn the high-order feature interactions in an efficient way. Furthermore, EulerNet incorporates the implicit and explicit feature interactions into a unified architecture, which achieves the mutual enhancement and largely boosts the model capabilities. Such a network can be fully learned from data, with no need of pre-designed form or order for feature interactions. \u2217 Ting Bai (baiting@bupt.edu.cn) and Wayne Xin Zhao (batmanfly@gmail.com) are the corresponding authors. \u2020 Also with Beijing Key Laboratory of Big Data Management and Analysis Methods. \u2021 Also with Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '23, July 23-27, 2023, Taipei, Taiwan \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9408-6/23/07...$15.00 https://doi.org/10.1145/3539618.3591681 Extensive experiments conducted on three public datasets have demonstrated the effectiveness and efficiency of our approach. Our code is available at: https://github.com/RUCAIBox/EulerNet.", "CCS CONCEPTS": "\u00b7 Information systems \u2192 Recommender systems ; \u00b7 Computing methodologies \u2192 Neural networks .", "KEYWORDS": "Feature Interaction, CTR Prediction, Recommender Systems, Neural Networks", "ACMReference Format:": "Zhen Tian, Ting Bai, Wayne Xin Zhao, Ji-Rong Wen and Zhao Cao. 2023. EulerNet: Adaptive Feature Interaction Learning via Euler's Formula for CTR Prediction. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23), July 23-27, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3539618.3591681", "1 INTRODUCTION": "Click-Through Rate (CTR) prediction, which aims to predict the probability of a user clicking on an item, is a very critical task in online e-commerce platforms. In the literature, various approaches have been proposed for effective CTR prediction [7, 10, 20, 32]. The key of CTR prediction is to accurately model the complicated context data by capturing underlying feature relationships. Typically, these methods either learn explicit feature interaction by manually setting the interaction form/order via factorization based models [25, 34], or implicit feature interaction by directly modeling the fusion of all the features via deep neural networks [7, 40]. Despite the progress, these methods still have limitations in learning complicated feature relationships ( e.g., high-dimensional varied contexts). Firstly, due to an exponential growth of combinational complexity, explicit learning methods usually set a small interaction order, which cannot scale to the cases requiring highorder feature interaction modeling. Further, they only model the integer-order interactions, thus leading to an inaccurate modeling SIGIR '23, July 23-27, 2023, Taipei, Taiwan Zhen Tian et al. of real-world scenarios. Secondly, due to the lack of effective design in interaction mechanisms, implicit learning methods are shown to be less effective than explicit learning methods [30]. A major challenge in modeling high-order interactions among raw features is the incurred high computational cost due to the exponential feature combinations as the number of raw features increases. In a practical scenario, raw features tend to be very sparse and have hundreds of fields with millions of dimensions. For example, identifier features like user ID or item ID become very sparse when encoded as one-hot vectors, so are the multi-field features extracted from the user behavior logs. Calculating highorder interactions on such sparse features with hundreds of fields is computationally intensive and time-consuming. Considering the above limitations, several studies [22, 35, 37] manually assign a maximal order, and further remove useless interactions from them. However, they still suffer from the degradation of model capability due to the restricted feature orders. As a promising approach, a recent study AFN [8] leverages logarithmic neural network (LNN [12]) to adaptively learn the order of feature interactions. It can automatically learn the orders of feature interactions, but at the expense of limited feature representation space, i.e., only positive feature embeddings can be learned in logarithmic space transformation, which requires a large consumption of logarithmic neurons for retaining the performance. To address these issues, in this paper, we propose an adaptive feature interaction learning model, named as EulerNet , for automatically learning arbitrary-order feature interactions. Unlike prior work, the core idea of EulerNet is to model the feature interaction in a complex vector space by conducting space mapping according to Euler's formula. Specially, EulerNet converts the exponential powers of feature interactions into simple linear combinations of the modulus and phase of the complex features, making it feasible to capture complicated feature interactions in an efficient, flexible way. Based on such an idea, we develop an Euler interaction layer that performs the above transformation, which can be stacked to form a capable interaction learning network. Such a network can be fully learned from data, with no need of pre-designed form or order for feature interactions. Furthermore, Euler interaction layer can be extended to integrate the implicit feature interactions. Different from previous explicit-implicit hybrid approaches, our model can fuse the feature representations from the two ways in the Euler interaction layer, instead of simply keeping two separate feature interaction models. The contributions are summarized as follows: \u00b7 We propose an adaptive feature interaction learning model EulerNet. It can automatically learn the arbitrary-order feature interactions from data. Meanwhile, our model can jointly capture the explicit and implicit feature interactions in a unified model architecture. \u00b7 We propose to model the feature interaction in the complex vector space, by conducting space mapping according to Euler's formula. It enables EulerNet to convert the complicated exponential powers into simple linear computation. \u00b7 Weconductextensive experiments on three widely used datasets. EulerNet consistently outperforms a number of competitive baselines with much fewer parameters, showing the effectiveness and efficiency of our model.", "2 PRELIMINARY": "We first introduce the CTR prediction task, then present the formulations for explicit and implicit feature interactions in existing work, and finally introduce the Euler's formula used in our model. CTR Prediction. The task of the click-through rate (CTR) prediction aims to estimate the probability that a user will click on an item. It takes as input a vector of context features ( e.g., user and item features), denoted as \ud835\udc99 = { \ud835\udc65 1 , \ud835\udc65 2 , ..., \ud835\udc65 \ud835\udc5a } , where \ud835\udc5a is the number of feature fields and \ud835\udc65 \ud835\udc57 is the \ud835\udc57 -th feature, the label \ud835\udc66 \u2208 { 0 , 1 } represents whether the item is clicked or not and it is predicted from the input feature \ud835\udc99 . We further apply a look-up operation to each feature \ud835\udc65 \ud835\udc57 by mapping it into a \ud835\udc51 -dimensional embedding \ud835\udc86 \ud835\udc57 \u2208 R \ud835\udc51 . In this way, the original feature vector can be represented as a list of feature embeddings { \ud835\udc86 1 , \ud835\udc86 2 , .., \ud835\udc86 \ud835\udc5a } . Explicit Feature Interactions. The key of CTR prediction is to learn the effective feature interactions, which is a fundamental problem for this task [41]. According to the interaction forms, existing methods can be roughly divided into explicit and implicit feature interactions. Explicit feature interactions is usually modeled by a pre-designed interaction formula with a controllable order, such as FM [29], HOFM [2] and IM [39]. We introduce a special symbol \u0394 \ud835\udc52\ud835\udc65 to denote the explicit feature interaction, generally defined as:  where \ud835\udf36 = [ \ud835\udefc 1 , \ud835\udefc 2 , ..., \ud835\udefc \ud835\udc5a ] consists of the orders for each feature in \ud835\udc99 , \u2299 is the element-wise product. Based on \u0394 \ud835\udc52\ud835\udc65 , another prediction function \ud835\udc53 (\u00b7) ( i.e., sigmoid function) can be employed to generate the predicted label \u02c6 \ud835\udc66 in [ 0 , 1 ] . Here, A is the set of all planned interactions by a CTR model. Most CTR models require the interaction orders to be non-negative integers, i.e., \ud835\udefc \ud835\udc57 \u2208 N 0 . For example, FM [29] only considers second-order interaction, which specify A = { \ud835\udf36 | \u02dd \ud835\udc5a \ud835\udc57 = 1 \ud835\udefc \ud835\udc57 = 2 , \u2200 \ud835\udefc \ud835\udc57 \u2208 { 0 , 1 }} . Different from most existing methods [25, 29, 34], we aim to learn the arbitrary-order feature interactions in an adaptive learning way, i.e., \ud835\udf36 could be arbitrary real values that are automatically learned from data. Implicit Feature Interactions. As another form of feature interaction, implicit feature interactions are commonly modeled by feedforward neural networks, e.g., the multi-layer perceptron (MLP) used in xDeepFM [22], DCNV2 [37] and DeepIM [39]. Different from explicit feature interactions, it does not specify the concrete interaction forms in the model. Formally, given the concatenation of all feature embeddings, i.e., \ud835\udc9b ( 0 ) = [ \ud835\udc86 1; \ud835\udc86 2; ... ; \ud835\udc86 \ud835\udc5a ] , the implicit feature interaction process \u0394 \ud835\udc56\ud835\udc5a can be formulated as:   where \ud835\udc59 \u2208 [ 1 , \ud835\udc3f ] , \ud835\udc3f is the layer depth and \ud835\udf0e is the activation function. Euler's Formula. Euler's formula is a mathematical formula that establishes the relationships between different expressions of complex vectors, and can be formulated as:  EulerNet: Adaptive Feature Interaction Learning via Euler's Formula for CTR Prediction SIGIR '23, July 23-27, 2023, Taipei, Taiwan where \ud835\udf40 \ud835\udc52 \ud835\udc56 \ud835\udf3d and \ud835\udf40 cos \ud835\udf3d + \ud835\udc56 ( \ud835\udf40 sin \ud835\udf3d ) are the representations of a complex vector in the polar form and the rectangular form respectively. Here, \ud835\udc56 is the imaginary unit, \ud835\udf40 and \ud835\udf3d are the modulus and phase of a complex vector. For a complex vector \ud835\udc93 + \ud835\udc56 \ud835\udc91 , we set the real part \ud835\udc93 = \ud835\udf40 cos \ud835\udf3d and imaginary part \ud835\udc91 = \ud835\udf40 sin \ud835\udf3d . The modulus \ud835\udf40 and phase \ud835\udf3d can be represented as:  where atan2 ( \ud835\udc9a , \ud835\udc99 ) is the two-argument arctangent function. The transformation via Euler's formula makes it feasible to convert the complex vectors from the rectangular form to the polar form, providing a way to encode the features in the polar space.", "3 METHODOLOGY": "To adaptively learn the arbitrary-order feature interactions, we propose a feature interaction learning model via Euler's formula, named as EulerNet . We first present a general introduction of our model, and then introduce the technical details in each part. Log Input Linear Exp Inverse Euler Transformation Euler Transformation Linear Add & Norm ReLU Output Embedding Layer Space Mapping Euler Interaction Layer - 1 \u2026 Euler Interaction Layer - L Prediction Layer Add Input Features: \ud835\udc651, \ud835\udc65 \u0de1 \u0ddc \ud835\udc66 Explicit \ud835\udc93 \ud835\udc93 \ud835\udc91 \ud835\udf40 \ud835\udf3d \u2102 \ud835\udc1d \u211d \ud835\udc1d Sigmoid Implicit \ud835\udc91 2 , \u2026 , \ud835\udc65\ud835\udc5a Figure 1: The overall architecture of EulerNet.", "3.1 Overview of EulerNet": "The overview architecture of EulerNet is shown in Figure 1. EulerNet is designed by stacking the key structure of Euler interaction layer . The core idea of Euler interaction layer is to transform explicit interaction of feature embeddings (Eq. (1)) in a complex vector space according to Euler's formula (Eq. (4)). As such, we can model complicated feature relationships in a flexible way, without the constraints in existing work ( e.g., non-negativity or integer). Further, exponential computation can be simplified as linear computation, making it possible to adaptively learn the high-order feature interactions in an efficient way. Further, Euler interaction layers can be extended to incorporate implicit feature interaction learning, which can naturally integrate the two kinds of feature interaction. In what follows, we introduce the details of explicit feature interaction (Section 3.2) and implicit feature interaction (Section 3.3).", "3.2 Explicit Feature Interaction Learning": "Previous works [9, 22, 29, 37] mainly learn the feature interactions in the real vector space, which limits the expressiveness of features, lacking the ability to adaptively capture the arbitrary-order feature interactions. To address this issue, we first map the input features from the real vector space to the complex vector space, and then learn the explicit feature interactions in the complex vector space. 3.2.1 Complex Vector Representation of Features. As discussed in Section 2, the original input vector \ud835\udc99 can be mapped into a list of feature embeddings { \ud835\udc86 1 , \ud835\udc86 2 , .., \ud835\udc86 \ud835\udc5a } via an embedding layer. Based on the embedding representations, we next discuss how to map them into complex space and further conduct Euler interaction. Complex Space Mapping. To improve the expressiveness of features, we map the feature embeddings from the real vector space to the complex vector space . Given a feature embedding, \ud835\udc86 \ud835\udc57 , in the complex vector space, we utilize two real vectors \ud835\udc93 and \ud835\udc91 to represent the real and imaginary parts of the complex vector respectively ( i.e., \u02dc \ud835\udc86 \ud835\udc57 = \ud835\udc93 \ud835\udc57 + \ud835\udc56 \ud835\udc91 \ud835\udc57 ). To transform a feature embedding into a complex vector, the key idea is to consider it as the phase and incorporate a learnable parameter (or parameter vector) \ud835\udf07 \ud835\udc57 as the modulus following Euler's formula in Eq. (4):  According to Eq. (4), we can obtain the corresponding complex representation of \ud835\udc86 \ud835\udc57 by introducing the modulus parameter \ud835\udf07 \ud835\udc57 :  where we have \ud835\udc93 \ud835\udc57 = \ud835\udf07 \ud835\udc57 cos ( \ud835\udc86 \ud835\udc57 ) and \ud835\udc91 \ud835\udc57 = \ud835\udf07 \ud835\udc57 sin ( \ud835\udc86 \ud835\udc57 ) . To enhance the field-specific semantics, we let the feature embeddings corresponding to the same field share the same modulus parameter. After complex space mapping, each feature is represented by a complex vector \u02dc \ud835\udc86 \ud835\udc57 . We utilize the complex feature representations { \u02dc \ud835\udc86 \ud835\udc57 } \ud835\udc5a \ud835\udc57 = 1 = { \ud835\udc93 \ud835\udc57 + \ud835\udc56 \ud835\udc91 \ud835\udc57 } \ud835\udc5a \ud835\udc57 = 1 for subsequent interaction modeling. 3.2.2 Euler Interaction Layer. Euler interaction layer is the core component of our proposed EulerNet, which enables the adaptive learning of explicit feature interactions. An Euler interaction layer performs the feature interaction under the complex space one time, taking as input a complex representation and outputting a transformed complex representation. In this way, we can stack multiple Euler interaction layers for enhancing the model capacity. Next, we describe the transformation process with an Euler interaction layer. Euler Transformation. In order to adaptively learn the explicit feature interactions, we utilize Euler Transformation to transform the complex feature representations from the rectangular form to the polar form. This step can convert exponential multiplications into simplified linear computation, making it feasible to adaptive capture complicated feature interactions. Given the input complex representation \ud835\udc93 \ud835\udc57 + \ud835\udc56 \ud835\udc91 \ud835\udc57 of feature embedding \ud835\udc86 \ud835\udc57 , we use Euler's formula in Eq. (4) to obtain the polar-form representations:  SIGIR '23, July 23-27, 2023, Taipei, Taiwan Zhen Tian et al. In this form, the explicit feature interaction can be formulated as:  where \ud835\udf40 \ud835\udc57 = \u221a\ufe03 \ud835\udc93 2 \ud835\udc57 + \ud835\udc91 2 \ud835\udc57 (always non-negative) and \ud835\udf3d \ud835\udc57 = atan2 ( \ud835\udc91 \ud835\udc57 , \ud835\udc93 \ud835\udc57 ) are the modulus and phase vectors of the complex features in the polar form. In this way, explicit feature interaction has been cast into a linear weighted combination of modulus and phase values in the polar space, and the original interaction order ( i.e., \ud835\udf36 ) becomes the combination coefficients. Note that, to achieve the similar formulation, we can also perform the log operation on the original feature interaction (Eq. (1)), while it requires the feature embeddings to be non-negative , which do not always hold for all the cases. Such a transform provides a possibility to model complicated feature interaction in a more simplified way. Generalized Multi-order Transformation. In the above, we have discussed the case with an order vector \ud835\udf36 . In this part, we generalize such a transformation into a group of \ud835\udc5b order vectors { \ud835\udf36 \ud835\udc58 } \ud835\udc5b \ud835\udc58 = 1 , where \ud835\udefc \ud835\udc58,\ud835\udc57 denotes the \ud835\udc57 -th order of the \ud835\udc58 -th vector \ud835\udf36 \ud835\udc58 . Formally, we introduce the \ud835\udf4d \ud835\udc58 and \ud835\udc8d \ud835\udc58 to generalize Eq. (9) as follows:  where \ud835\udf39 \ud835\udc58 and \ud835\udf39 \u2032 \ud835\udc58 are learnable bias vectors that are incorporated for enhancing the representations. With this generalized extension, we can obtain the explicit interaction with \ud835\udf36 \ud835\udc58 in the polar form:  Inverse Euler Transformation. Since the above feature interactions are in the polar form, we do not directly perform the corresponding interactions with a group of multi-order coefficients { \ud835\udf36 \ud835\udc58 } \ud835\udc5b \ud835\udc58 = 1 . We further utilize inverse Euler transformation to convert them into the original complex vectors in the rectangular form as:  where \u02c6 \ud835\udc93 \ud835\udc58 and \u02c6 \ud835\udc91 \ud835\udc58 are the real and imaginary vectors. In this way, the Euler interaction layer can model \ud835\udc5b explicit feature interactions. The generalized explicit feature interactions with a group of multiorder coefficients { \ud835\udf36 \ud835\udc58 } \ud835\udc5b \ud835\udc58 = 1 learned in Euler interaction layer can be described as:  This formula is the core of the proposed EulerNet model for explicit feature interactions. Unlike prior work, the order of the interactions ( i.e., \ud835\udefc \ud835\udc58,\ud835\udc57 ) can be set to arbitrary real value, without additional limits such as non-negativity . Instead of manually setting the order coefficients, we adaptively learn them from data, and use the number of order vectors \ud835\udc5b to control the model complexity. Furthermore, we can also set varying \ud835\udc5b at different layers to increase the model flexibility.", "3.3 Integrating Implicit Interactions": "Considering that feature relationship in the real scenarios is very complicated, we further incorporate implicit feature interactions into our model. Different from previous studies [9, 22, 37], which model the explicit and implicit feature interactions in different architectures, we integrate them in each Euler interaction layer to enhanced the representation capacity. 3.3.1 Fusing Explicit and Implicit Interactions. To model more complicated feature relationship, we construct a neural network component for capturing implicit feature interactions. Given the input complex features { \u02dc \ud835\udc86 \ud835\udc57 } \ud835\udc5a \ud835\udc57 = 1 = { \ud835\udc93 \ud835\udc57 + \ud835\udc56 \ud835\udc91 \ud835\udc57 } \ud835\udc5a \ud835\udc57 = 1 , we can obtain the input of the implicit interaction by concatenating the these vectors as: \ud835\udc93 = [ \ud835\udc93 1; \ud835\udc93 2 , ... ; \ud835\udc93 \ud835\udc5a ] ( real part ) and \ud835\udc91 = [ \ud835\udc91 1; \ud835\udc91 2 , ... ; \ud835\udc91 \ud835\udc5a ] ( imaginary part ). Then, we feed the real and imaginary parts of feature representations into the same linear layer with a subsequent non-linear activation function:  where \ud835\udc58 \u2208 { 1 , \u00b7 \u00b7 \u00b7 \ud835\udc5b } , and \ud835\udc7e \ud835\udc58 \u2208 R \ud835\udc51 \u00d7 \ud835\udc5a\ud835\udc51 is the weight matrix and \ud835\udc83 \ud835\udc58 \u2208 R \ud835\udc51 is the bias. Finally, in order to integrate the two kinds of feature interaction, we add the explicit and implicit representations (See Eq. (12) and Eq. (14)) by real and imaginary parts accordingly as:  We can stack multiple Euler interaction layers by taking the output features of the previous layer as the input for the next layer and optionally applying normalization methods such as BatchNorm [17] or LayerNorm [1] to adjust the distribution as it passes through each layer. 3.3.2 Output for CTR Predictions. In order to predict the CTR value, we further perform linear regression on the output representations \u02dc \ud835\udc90 = { \u02dc \ud835\udc90 \ud835\udc58 } \ud835\udc5b \ud835\udc58 = 1 = { \u02dc \ud835\udc93 \ud835\udc58 + \ud835\udc56 \u02dc \ud835\udc91 \ud835\udc58 } \ud835\udc5b \ud835\udc58 = 1 . Specially, we concatenate the real and imaginary vectors accordingly, and introduce a regression weight vector \ud835\udc98 \u2208 R \ud835\udc5b\ud835\udc51 , so as to obtain a scalar value for both the real and imaginary parts:  EulerNet: Adaptive Feature Interaction Learning via Euler's Formula for CTR Prediction SIGIR '23, July 23-27, 2023, Taipei, Taiwan where \ud835\udc67 \ud835\udc5f\ud835\udc52 and \ud835\udc67 \ud835\udc56\ud835\udc5a are the real and imaginary part of \ud835\udc67 respectively. The prediction for CTR by integrating both explicit and implicit interactions can be given as:  For training, we utilize the binary cross entropy loss with a regularization term to train our model, which is formulated as:  where \ud835\udc66 \ud835\udc57 and \u02c6 \ud835\udc66 \ud835\udc57 are the ground-truth label and predicted result of \ud835\udc57 -th training sample respectively, and \u0398 denotes the set of the parameters and \ud835\udefe is the \ud835\udc3f 2-norm penalty.", "3.4 Discussion": "3.4.1 Intuitive Explanation of Feature Interaction. To have an intuitive understanding of our approach, we consider a simple case when the embedding dimension \ud835\udc51 = 1. Further, since we apply normalization at the Euler interaction layer, the modulus is around 1, so that we can omit the corresponding \ud835\udf06 from Eq. (9). The forms of explicit and implicit interaction can be simplified as:   As we can see, explicit interaction \ud835\udc54 \ud835\udc52\ud835\udc65 (\u00b7) mainly affects the phase of features ( i.e., \ud835\udf03 \ud835\udc57 and \ud835\udf03 \ud835\udc58 ), which can be approximately considered as the rotations in the complex vector space, while implicit interaction \ud835\udc54 \ud835\udc56\ud835\udc5a (\u00b7) performs a parallelogram-like transformation in the complex vector space, which mainly affects the modulus instead of the phase (due to the limits in first quadrant). By integrating both implicit and explicit feature interactions, our approach can model the effect in both phase and modulus , thus leading to an improved capacity due to mutual enhancement. Figure 2 presents a geometric interpretation of explicit and implicit feature interactions. Figure 2: The visual understanding of the interactions in EulerNet. \u2102 \ud835\udc1d Explicit Interaction \u0de4 \ud835\udc86\ud835\udc8b 0 p \ud835\udf3d \ud835\udf3d Implicit Interaction \ud835\udc76\ud835\udc98 r 0 0 \u0de4 \ud835\udc86\ud835\udc8c To further understand the explicit interaction, we present an illustrative example with a simple interaction \u02dc \ud835\udc86 0 . 33 1 \u2299 \u02dc \ud835\udc86 0 . 25 2 with two feature vectors: \u02dc \ud835\udc86 1 = [-8 , 1 ] \u22a4 and \u02dc \ud835\udc86 2 = [-16 , 4 ] \u22a4 . With some mathematical computations, we can get the following representation: \u02dc \ud835\udc86 0 . 33 1 \u2299 \u02dc \ud835\udc86 0 . 25 2 = \u02c6 \ud835\udc93 + \ud835\udc56 \u02c6 \ud835\udc91 = [-0 . 99 , 1 . 41 ] \u22a4 + \ud835\udc56 [ 3 . 85 , 0 ] \u22a4 . Table 1: Comparison of different CTR methods. 'Unified' denotes the integrated learning of explicit and implicit interactions, 'UR' means 'Unrestricted' indicating the elements of feature embeddings could be arbitrary real value, and 'SV' means 'Single-vector' indicating that each feature is represented by only one embedding vector. 3.4.2 Novelty and differences. In Table 1, we compare our approach with existing feature interaction methods. To the best of our knowledge, it is the first attempt that adaptively captures arbitrary-order feature interactions in the complex vector space. Although AFN+ [8] leverages the LNN [12] to learn arbitrary-order feature interactions adaptively, it constrains the feature representations to positive real vectors. This approach not only degrades the model performance, but also requires additional feature embeddings for implicit interactions. Furthermore, most studies [9, 22, 37, 39] model the explicit and implicit interactions in different architectures and seldom integrate them in a joint approach. As a comparison, EulerNet is more general, unified in integrating the modeling of implicit and explicit feature interactions, via the enhanced Euler interaction layer in Section 3.3. In general, our approach provides a more capable solution to model complicated feature interactions. 3.4.3 Complexity Analysis. We also compare the time complexities of different CTR methods in Table 1. For ease of analysis, we assume that the hidden size of different components is set to the same number. Specially, \ud835\udc5a is the number of feature fields, \ud835\udc51 is the embedding dimension, \ud835\udc3f and \ud835\udc47 are the layer depth of the explicit and implicit component respectively, \ud835\udc3b is the hidden size of MLP, \ud835\udc3e is the number of logarithmic neurons of AFN+ [8], and \ud835\udc5b is the number of order vectors of EulerNet. Note that \ud835\udc3e is much larger than \ud835\udc5a \u00b7 \ud835\udc51 , leading to a very high complexity of AFN+ [8]. In contrast, \ud835\udc5b is very small, which can be set to \ud835\udc5a in practice. The complexity of EulerNet for a training instance can be estimated as \ud835\udc42 ( \ud835\udc5a 2 \ud835\udc51 2 \ud835\udc3f ) , which is comparable to mainstream efficient methods such as FmFM [34] and DCNV2 [37] (See Table 3 for experimental analysis).", "4 EXPERIMENTS": "We conduct extensive experiments to show the effectiveness of EulerNet, and analyze the effect of each learning component in it.", "4.1 Experimental Settings": "We introduce the experimental settings, including the datasets, baseline approaches, and the details of hyper-parameters. 4.1.1 Datasets. We utilize three real world datasets in our experiments: Criteo 1 , Avazu 2 , MovieLens-1M 3 . Table 2 summarizes the dataset statistics information. 1 http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset 2 http://www.kaggle.com/c/avazu-ctr-prediction 3 https://grouplens.org/datasets/movielens SIGIR '23, July 23-27, 2023, Taipei, Taiwan Zhen Tian et al. \u00b7 Criteo. The most popular CTR prediction benchmark dataset contains user logs over a period of 7 days. \u00b7 Avazu. It contains user logs over a period of 7 days, which was used in the Avazu CTR prediction competition. \u00b7 MovieLens-1M. The most popular dataset for recommendation systems research. Table 2: The statistics of datasets. 4.1.2 Compared Models. We compare EulerNet with state-of-theart methods in CTR prediction task, including: \u00b7 FwFM [25] improves FM by considering field information and uses field-specific weights to capture the field-wise relationship. \u00b7 FmFM [34] replaces the field scalar weight in FwFM with a kernel matrix, allowing for modeling more informative interactions. \u00b7 DeepFM [9] uses FM to model the second-order interactions, and incorporates DNNs to model the high-order interactions. \u00b7 DeepIM [39] utilizes Newton's identity to implement highorder FM, and incorporate implicit interactions via an MLP. \u00b7 xDeepFM [22] encodes high-order interactions into multiple feature maps and combine an MLP to model implicit interactions. \u00b7 DCNV2 [37] takes the kernel product of concatenated feature vectors to model high-order interactions and combine an MLP to model implicit interactions. \u00b7 FiBiNet [16] uses the bilinear operation to model pair-wise interactions and uses SENet [14] to capture the feature importance. \u00b7 AutoInt [33] uses the self-attention mechanism to learn highorder interactions. AutoInt+ improves it by combining an MLP. \u00b7 FiGNN [21] represents the features into a full-connected graph, and uses gated GNNs to model the high-order feature interactions. \u00b7 AFN[8] encodes features into a logarithmic space to adaptively learn the arbitrary-order feature interactions. AFN+ improves the base model by using an MLP to model implicit interactions. The above models we compared in our experiments have covered different types of feature interaction methods. FwFM and FmFM are shallow models that only model the second-order explicit interactions. DeepFM, DeepIM, xDeepFM and DCNV2 are ensemble methods that learn both the explicit interactions by an empirically designed component and implicit interactions by an MLP. FiBiNet, AutoInt, and FiGNN have the ability to learn the importance of feature interactions. AFN encodes features into a logarithmic space to adaptively learn the arbitrary-order feature interactions. Different from them, our proposed EulerNet represents the features in a complex vector space, in which the exponential computation can be simplified as linear computation, making it possible to adaptively learn the arbitrary-order feature interactions in an efficient way. 4.1.3 Implementation Details. All methods are implemented in Pytorch [27]. The size of feature embedding is 16. The learning rate is in {1e-3, 1e-4, 1e-5}. The \ud835\udc3f 2 penalty weight is in {1e-3, 1e-5, 1e-7}. The batch size is 1024. The training optimizer is Adam [19]. The hidden layer of MLP component is 400 \u00d7 400 \u00d7 400 and the dropout rate is 0.1. For DeepIM, the interaction order is in {2, 3, 4}. For xDeepFM, the depth of CIN is in {1, 2, 3, 4, 5} and the hidden size is in {100, 200, 400}. For DCNV2, the depth of CrossNet is in {1, 2, 3, 4, 5}. For FiGNN, the graph interaction step is in {1, 2, 3, 4, 5}. For AutoInt+, the depth, number of head and attention size is 2, 2, 40 respectively. For AFN, the number of logarithmic neurons is in {40, 400, 800, 1000}. For EulerNet, the number of Euler interaction layer is in {1, 2, 3, 4, 5}, and the number of order vectors is set as {7, 23, 39} for MovieLens-1M, Avazu and Criteo datasets respectively. Our implementation is also available at RecBole [42, 43].", "4.2 Overall Performance": "We present the experimental results of different methods for CTR prediction in Table 3, and have the following observations: (1) Compared to the DNN-based methods, FwFM and FmFM perform worst due to the limited ability to only capture the secondorder explicit feature interactions. (2) Ensemble methods ( i.e., DeepFM[9], DeepIM [44], DCNV2 [37] and xDeepFM [22]) achieve competitive performance across on all three datasets, which shows the effectiveness of integrating implicit feature interactions. (3) For the feature importance learning methods ( i.e., FiBiNet [16], FiGNN [21] and AutoInt+ [33]), their performance largely varies across different datasets. AutoInt+ performs very well on all three datasets, while FiGNN and FiBiNet lose the advantage on the Criteo and MovieLens-1M datasets respectively. This indicates that the self-attention mechanism is more capable in modeling high-order feature interactions. (4) AFN+ outperforms all the other baseline methods on the Avazu and MovieLens-1M datasets, demonstrating the effectiveness of adaptively learning the arbitrary-order feature interactions in the CTR prediction task. However, its advantage on the Criteo dataset is small. This may be caused by the restriction of positive values assigned in the feature embeddings, which hinders the representation capability of the model. (5) Our proposed EulerNet consistently performs better than all of the compared methods. It shows the effectiveness of encoding the features into the complex vectors via Euler's formula and conducting transformations in the polar space. As for the model efficiency, we can see that the latency of FwFM, FmFM, DeepFM, DeepIM and DCNV2 are relatively small. They are more efficient due to the simple architecture and fewer parameters learned in the model. For AutoInt, FiGNN, FiBiNet and xDeepFM, the latency of them is much larger due to the complex model architecture or the complicated training strategy. For AFN+, due to the limited feature representation space, i.e., only positive feature embeddings can be learned in logarithmic space transformation, it requires a large amount of parameters for retaining the performance. This makes it impractical in the industrial scenarios. In contrast, the latency of EulerNet is much less than AFN+ ( i.e., under 10 . 2%) and it is comparable to many efficient methods such as DeepFM and DeepIM. With the highest accuracy and lower complexity, EulerNet has a great potential to be applied into large-scale industrial recommender systems. EulerNet: Adaptive Feature Interaction Learning via Euler's Formula for CTR Prediction SIGIR '23, July 23-27, 2023, Taipei, Taiwan Table 3: Performance comparisons. A higher AUC or lower Logloss at 0.001-level is regarded significant, as stated in previous studies [7, 9, 33, 36].", "4.3 Experimental Analysis": "Weconductexperiments to investigate the interaction orders learned in EulerNet, and then visualize the learned feature representations to show their correlation with the feature importance. 4.3.1 Arbitrary-Order Learning Analysis. Learning effective highorder feature interactions is very crucial in the CTR prediction task. To verify the orders learned in EulerNet, we visualize the learned feature interaction orders ( i.e., the total order of each learnable coefficient vector \ud835\udf36 \ud835\udc58 in Eq. (10)) in the explicit feature interaction component. Note that the orders adaptively learned in our model can be arbitrary values in [-\u221e , +\u221e] , we cluster them by setting the interval to 0.5 for better presentation. As shown in Figure 3, we can see that our model not only learns the integer-order feature interactions, but also can adaptively learn the fractional-order feature interactions in a fine-grained way. Specially, the feature interaction orders learned in our model varies from [ 0 , 3 . 5 ] on MovieLens-1M dataset and [ 0 . 5 , 3 . 5 ] on Avazu dataset. Fine-grained feature interaction learning can improve the capability of our model and enable it to capture more effective information for CTR prediction. contains one thousand features, and each feature is assigned a probability that affects the likelihood of a click-through event occurring. For a given record \ud835\udc65 \ud835\udc56 = [ \ud835\udc5d 1 , \ud835\udc5d 2 , ..., \ud835\udc5d 7 ] , its label \ud835\udc66 \ud835\udc56 is generated by sampling from a probability distribution that is pre-defined by one of the patterns, i.e., \ud835\udc45 \u2208 [ \ud835\udc45 1 , \ud835\udc45 2 , \ud835\udc45 3 ] (See Table 4). We compare the interaction orders learned in the explicit feature interaction component of AFN+ and EulerNet, and utilize fitting deviation to evaluate the difference between the orders in different learning algorithms and the ground-truth pattern \ud835\udc45 . From Table 4, we can see that the deviation in EulerNet is much smaller than AFN+, demonstrating that EulerNet has the ability to adaptively learn more meaningful feature interactions. Figure 3: The statistics of the interaction orders learned in EulerNet. [0.0,0.5) [0.5,1.0) [1.0,1.5) [1.5,2.0) [2.0,2.5) [3.0,3.5) Order 0 1 2 3 4 5 6 Count (a) MovieLens-1M [0.5,1.0) [1.0,1.5) [1.5,2.0) [2.0,2.5) [2.5,3.0) [3.0,3.5) Order 0 5 10 15 20 25 30 Count (b) Avazu 4.3.2 Verification on Synthetic Dataset. Since it can not identify the ground-truth of meaningful feature interactions in real-world public datasets, we further conduct an experiment using synthetic data to verify the degree of coincidence with the learned orders in EulerNet. The synthetic dataset consists of 1 million synthesized click-through records with 7 fields ( \ud835\udc39 = [ \ud835\udc53 1 , \ud835\udc53 2 , ..., \ud835\udc53 7 ] ) that simulate real click-through records. Each field is independently created and Specifically, we present the order vectors of EulerNet after training on the synthetic dataset defined by the pattern \ud835\udc45 3 in Figure 4. Different rows represent the explicit feature interactions learned by different order vectors (See Eq. (10)). For example, the most important feature interactions learned by the order vector \ud835\udf36 1 is \ud835\udc5d 1 . 12 1 \ud835\udc5d 1 . 90 3 \ud835\udc5d 1 . 48 5 . We can see that the combinational feature interactions learned by a group of multi-order vectors { \ud835\udf36 1 , \ud835\udf36 2 , \ud835\udf36 7 } ( i.e., \ud835\udc5d 1 . 12 1 \ud835\udc5d 1 . 90 3 \ud835\udc5d 1 . 48 5 + \ud835\udc5d 0 . 36 2 \ud835\udc5d 0 . 37 4 \ud835\udc5d 0 . 38 6 + \ud835\udc5d 0 . 67 7 ) are quite similar (average order deviation is 0.47) to the ground truth interaction pattern ( i.e., \ud835\udc45 3 = 1 3 ( \ud835\udc5d 1 . 3 1 \ud835\udc5d 2 . 2 3 \ud835\udc5d 1 . 7 5 + \ud835\udc5d 0 . 5 2 \ud835\udc5d 0 . 5 4 \ud835\udc5d 0 . 5 6 + \ud835\udc5d 7 ) ) in the data, showing the ability of EulerNet to learn the effective feature interactions. Table 4: The pattern for creating the synthetic dataset and the deviation comparisons between different models. 4.3.3 Visualization of Feature Embeddings. EulerNet not only can adaptively learn the arbitrary-order feature interactions, but also have the ability to capture the importance of features. We visualize the learned feature embeddings in EulerNet and show its ability in learning the importance of features. The heat map in Figure 5(a) illustrates the mutual information scores between feature fields and labels on the MovieLens-1M dataset, which represents the strength SIGIR '23, July 23-27, 2023, Taipei, Taiwan Zhen Tian et al. Figure 4: Visualization of the order vectors in EulerNet under the distribution pattern \ud835\udc45 3 , which is defined in Table 4. f1 f2 f3 f4 f5 f6 f7 \u03b1 1 \u03b1 2 \u03b1 3 \u03b1 4 \u03b1 5 \u03b1 6 \u03b1 7 1.12 -0.04 1.90 -0.05 1.48 -0.05 -0.15 -0.03 0.36 -0.05 0.37 -0.05 0.38 -0.06 -0.08 0.15 -0.13 0.17 -0.14 0.18 0.26 -0.01 -0.03 0.01 -0.01 -0.02 -0.05 -0.07 -0.01 -0.03 -0.05 -0.05 -0.00 -0.04 -0.08 0.00 -0.02 0.03 -0.03 -0.00 0.02 -0.03 -0.02 -0.02 -0.04 -0.02 -0.04 -0.02 0.67 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 of each field on the prediction results. We can observe that the fields item_id , user_id and zip_code have the strongest effect on the click results. The distributions of feature embeddings are plotted with Gaussian kernel density estimation (KDE) in two-dimensional space in Figure 5(b). The more dispersive the distribution of feature embeddings, the less influence of it has on the prediction results due to the low information quantity in the random varies. It can be seen that for the important fields ( i.e., item_id , user_id and zip_code ) in Figure 5(a), the distribution of feature embeddings in Figure 5(b) is more concentrated and has a smaller variance. While for the fields with less importance ( i.e., age , occupation and release_year ), they are chaotically distributed, and their variance is relatively large. This indicates that the feature embeddings, which also represent the phase of the complex features as defined in Eq. (6), can reflect the feature importance to a certain extent. In EulerNet, the phase of the complex features is effectively controlled by explicit feature interactions (See Section 3.4.1), which enables it to capture the meaningful feature relationship and improves the model capabilities.", "4.4 Ablation Study": "We conduct ablation studies to explore the impact of each component or hyper-parameter on the model performance. 4.4.1 Effect of Implicit and Explicit Feature Interactions. EulerNet contains both the explicit and implicit interaction learning components. In order to investigate the impact of each interaction type, we conduct experiments on the two variants of EulerNet, termed as EulerNet \ud835\udc38 and EulerNet \ud835\udc3c , in which the implicit and explicit learning parts are removed respectively. As shown in Table 5, we can see that the model performance has a decrease for both EulerNet \ud835\udc3c and EulerNet \ud835\udc38 , showing the mutual complementary effects of them, which is consistent with the observations in Section 3.4.1. Besides, EulerNet \ud835\udc3c shows a larger decrease in performance than EulerNet \ud835\udc38 on the Avazu and MovieLens-1M datasets, but the decrease is smaller on the Criteo dataset, showing both the implicit and explicit interactions are important for CTR prediction. 4.4.2 Impact of the Interaction Layer Number. EulerNet is designed by stacking the key structure of the Euler interaction layer. We study (a) Mutual information score between fields and label in MovieLens-1M. user_id age gender occupation zip_code item_id release_year 0.02 0.04 0.06 0.08 0.10 0.12 (b) The distribution of feature embeddings in different feature fields. user_id age occupation zip_code item_id release_year Figure 5: Visualization of the relationships between feature importance and the distribution of feature embeddings in EulerNet. Table 5: Performance comparison between different interactions. the impact of the Euler interaction layer number, which reflects the intricacy of feature interactions, on the model performance. As shown in Figure 6, we can observe that the performance of EulerNet increases as the number of layers increases. EulerNet achieves the best model performance with 5 interaction layers. When the number of layers exceeds 5, the model performance decreases due to the overfitting issue caused by incorporating more parameters. 4.4.3 Impact of the Number of Order Vectors. As introduced in Section 3.2.2, we use multiple order vectors to adaptively learn the arbitrary-order feature interactions. The number of order vectors is denoted as \ud835\udc5b (See Eq. (13)), which controls the number of explicit feature interactions in each Euler interaction layer. As illustrated in Figure 7, the performance of EulerNet on the Avazu dataset increases as the number of order vectors increases from 20 to 60. Whereas on the Criteo dataset, EulerNet achieves the best performance as the number of order vectors increases to 40. However, the model performance decreases when adding more order vectors. This indicates that including too many feature combinations in the multi-order transformation may incorporate the useless feature interactions that hurt the model performance. EulerNet: Adaptive Feature Interaction Learning via Euler's Formula for CTR Prediction SIGIR '23, July 23-27, 2023, Taipei, Taiwan Figure 6: Impact of the interaction layer number. 1 2 3 4 5 6 Depth 0.8120 0.8125 0.8130 0.8135 0.8140 0.8145 0.8150 AUC on Criteo Criteo Avazu 0.7826 0.7833 0.7840 0.7847 0.7854 0.7861 0.7868 AUC on Avazu (a) AUC 1 2 3 4 5 6 Depth 0.4380 0.4385 0.4390 0.4395 0.4400 0.4405 0.4410 LogLoss on Criteo Criteo Avazu 0.3760 0.3765 0.3770 0.3775 0.3780 0.3785 0.3790 LogLoss on Avazu (b) LogLoss AUC on Avazu Figure 7: Impact of the number of order vectors. 20 40 60 80 100 120 Number of order vectors per layer 0.8120 0.8125 0.8130 0.8135 0.8140 0.8145 0.8150 AUC on Criteo Criteo Avazu 0.7840 0.7845 0.7850 0.7855 0.7860 0.7865 0.7870 (a) AUC 20 40 60 80 100 120 Number of order vectors per layer 0.4361 0.4368 0.4375 0.4382 0.4389 0.4396 0.4403 LogLoss on Criteo Criteo Avazu 0.3760 0.3765 0.3770 0.3775 0.3780 0.3785 0.3790 LogLoss on Avazu (b) LogLoss", "5 RELATED WORK": "Explicit Feature Interaction Learning. This line of research explicitly enumerates feature combinations and uses vector operations such as inner product to capture their relationships. Early CTR models [3, 4, 11, 31] mainly relied on manually designing feature combinations with simple architectures. For example, FM [29] assigns an embedding vector to each feature that mainly captures second-order interactions. Inspired by FM, many variants of factorization machines have been proposed [6, 13, 18, 23, 38]. Among them, FFM [18] assigns multiple embeddings to explicitly model field-wise feature interactions. Besides, FwFM [25] and FmFM [34] are proposed to model the field information to improve FM in a parameter-efficient way. These factorization based methods mainly model second-order interactions, which severely limits their performance. To capture more effective feature interactions, xDeepFM [22] proposes the CIN to model the high-order feature interactions by incorporating lots of learnable parameters. Besides, DCNV2 [37] proposes the CrossNet to capture the high-order feature interactions in an efficient way. Although these methods leverage high-order feature interactions to achieve great performance, their interaction components are empirically predefined, which may lead to the suboptimal learning of restricted feature interactions. As a promising approach, AFN [8] uses logarithmic neural networks (LNN) [12] to adaptively model the arbitrary-order interactions, but at the expense of restricting feature embeddings to positive real vectors, which may degrade the expressiveness of feature representations and require much more parameters to retain the performance. Different from them, our proposed EulerNet models the feature interactions in a complex vector space by conducting the space mapping via Euler's formula. The feature interactions in our model are adaptively learned from data without additional restrictions, which could largely improve its capacity and better balance the effectiveness and efficiency. Implicit Feature Interaction Learning. In recent years, many deep learning based models [5, 7, 9, 15, 24, 40] have been proposed to model the high-order feature interactions via a deep neural network (DNN) component. Among them, the Wide & Deep [7] network combines the logit value of a linear regression model with the output of a DNN. Besides, PNN [28] introduces an MLP to improve the output of its explicit interaction component, and NFM [10] stacks deep neural networks after FMs to model the high-order feature interactions. Different from the explicit feature interactions, the implicit feature interactions modeled by deep neural networks lack good interpretability. Additionally, some recent study [30] has found that it is more challenging for an MLP to effectively learn the high-order feature interactions compared to using an inner product in FM. Most deep learning based methods [9, 22, 26, 39] leverage the implicit feature interactions as the supplemental signal of the explicit feature interaction component. Different from them, in EulerNet, the explicit and implicit feature interactions are learned in a unified architecture: both of them perform the linear transformations on the features in different forms ( i.e., the polar form for the explicit feature interactions and the rectangular form for the implicit feature interactions). Euler's formula establishes the relationship between different representation forms and also builds a bridge between the explicit and implicit feature interactions. It is observed that there exists a complementary effect between the explicit and implicit interactions in EulerNet, which enables them to promote each other and further improve the model capabilities.", "6 CONCLUSION": "In this paper, we proposed an adaptive feature interaction learning neural network EulerNet. Different from prior work, EulerNet modeled the arbitrary-order feature interactions in a complex vector space by conducting space mapping according to Euler's formula. In EulerNet, the exponential powers of feature interactions were converted into simple linear combinations of the modulus and phase of the complex features, enabling it to adaptively learn the arbitraryorder feature interactions in an efficient way. Furthermore, EulerNet integrated the implicit and explicit feature interactions into a unified architecture, which can achieve the mutual enhancement and largely boost the model capabilities. As the major contribution, we proposed to conduct feature interaction learning in the complex vector space, which provides a way to enhance the representation capability of models and promote the feature interaction learning in this area. As future work, we consider incorporating the user behavior features into our method, and further explore the use of attention mechanism in the complex vector space to capture more informative correlations for various recommendation tasks.", "ACKNOWLEDGMENTS": "This work was partially supported by National Natural Science Foundation of China under Grant No. 62222215 and 62102038, Beijing Natural Science Foundation under Grant No. 4222027, and Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098. SIGIR '23, July 23-27, 2023, Taipei, Taiwan Zhen Tian et al.", "REFERENCES": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 (2016). [2] Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. 2016. Higher-order factorization machines. Advances in Neural Information Processing Systems 29 (2016). [3] Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. 2010. Training and testing low-degree polynomial data mappings via linear SVM. Journal of Machine Learning Research 11, 4 (2010). [4] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining . 785-794. [5] Wenqiang Chen, Lizhang Zhan, Yuanlong Ci, Minghua Yang, Chen Lin, and Dugang Liu. 2019. FLEN: leveraging field for scalable CTR prediction. arXiv preprint arXiv:1911.04690 (2019). [6] Chen Cheng, Fen Xia, Tong Zhang, Irwin King, and Michael R Lyu. 2014. Gradient boosting factorization machines. In Proceedings of the 8th ACM Conference on Recommender systems . 265-272. [7] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems . 7-10. [8] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive factorization network: Learning adaptive-order feature interactions. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 3609-3616. [9] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [10] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse predictive analytics. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval . 355-364. [11] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising . 1-9. [12] J Wesley Hines. 1996. A logarithmic neural network architecture for unbounded non-linear function approximation. In Proceedings of International Conference on Neural Networks (ICNN'96) , Vol. 2. IEEE, 1245-1250. [13] Fuxing Hong, Dongbo Huang, and Ge Chen. 2019. Interaction-aware factorization machines for recommender systems. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 3804-3811. [14] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition . 7132-7141. [15] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management . 2333-2338. [16] Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In Proceedings of the 13th ACM Conference on Recommender Systems . 169-177. [17] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning . PMLR, 448-456. [18] Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Fieldaware factorization machines for CTR prediction. In Proceedings of the 10th ACM conference on recommender systems . 43-50. [19] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [20] Zeyu Li, Wei Cheng, Yang Chen, Haifeng Chen, and Wei Wang. 2020. Interpretable click-through rate prediction through hierarchical attention. In Proceedings of the 13th International Conference on Web Search and Data Mining . 313-321. [21] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-gnn: Modeling feature interactions via graph neural networks for ctr prediction. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 539-548. [22] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1754-1763. [23] Wantong Lu, Yantao Yu, Yongzhe Chang, Zhen Wang, Chenhui Li, and Bo Yuan. 2021. A dual input-aware factorization machine for CTR prediction. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence . 3139-3145. [24] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, CaroleJean Wu, Alisson G Azzolini, et al. 2019. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 [44] Jieming Zhu, Jinyang Liu, Weiqi Li, Jincai Lai, Xiuqiang He, Liang Chen, and Zibin Zheng. 2020. Ensembled CTR prediction via knowledge distillation. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 2941-2958."}
