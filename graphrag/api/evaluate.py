# Copyright (c) 2024 Microsoft Corporation.
# Licensed under the MIT License
# Evaluating the snippet from OnePaper/graphrag/cli/index.py. Written by SH Han (2025.01.13)

"""
Evaluating Engine API.

This API provides access to the query engine of graphrag, allowing external applications
to hook into graphrag and run queries over a knowledge graph generated by graphrag.

Contains the following functions:
 - global_search: Perform a global search.
 - global_search_streaming: Perform a global search and stream results back.
 - local_search: Perform a local search.
 - local_search_streaming: Perform a local search and stream results back.

WARNING: This API is under development and may undergo changes in future releases.
Backwards compatibility is not guaranteed at this time.
"""

from pathlib import Path
from typing import TYPE_CHECKING, Any
import pandas as pd
from pydantic import validate_call
from graphrag.index.config.embeddings import core_concept_embedding

from graphrag.config.models.graph_rag_config import GraphRagConfig
from graphrag.logger.print_progress import PrintProgressLogger
from graphrag.evaluate.factory import get_evaluate_search_engine
from graphrag.utils.graph import find_all_parents, get_embeddings, get_all_paper_titles

from graphrag.query.indexer_adapters import (
    read_indexer_communities,
    read_indexer_entities,
    read_indexer_reports,
)

if TYPE_CHECKING:
    from graphrag.query.structured_search.base import SearchResult

logger = PrintProgressLogger("")

def _get_extracted_entities(entities: pd.DataFrame, eval_paper_titles: list[str], viztree: pd.DataFrame) -> pd.DataFrame:
    ee = entities[entities["title"].isin(eval_paper_titles)][["id", "title"]]
    ee = pd.merge(
        ee,
        find_all_parents(ee["id"].tolist(), viztree), on="id", how="left"
    )
    return ee.explode("parents")

def _enrich_with_embedding(extracted_entities: pd.DataFrame, embedding_df: pd.DataFrame) -> pd.DataFrame:
    extracted_entities["parents"] = extracted_entities["parents"].astype(str)
    return extracted_entities.merge(
        embedding_df[["id", "vector", "text"]],
        left_on="parents", right_on="id", how="left",
        suffixes=("", "_embedding")
    )

def _post_process_entities(extracted_entities: pd.DataFrame) -> pd.DataFrame:
    deduped = extracted_entities.drop_duplicates(subset=["id", "title"])
    return deduped.groupby(["id", "title"]).agg({"text": list, "vector": list}).reset_index()

@validate_call(config={"arbitrary_types_allowed": True})
async def evaluate_keyword(
    config: GraphRagConfig,
    entities: pd.DataFrame,
    viztree: pd.DataFrame,
) -> float:
    embedding_df = get_embeddings(config, core_concept_embedding)
    eval_paper_titles = get_all_paper_titles()
    extracted_entities = _get_extracted_entities(entities, eval_paper_titles, viztree)
    extracted_entities = _enrich_with_embedding(extracted_entities, embedding_df)
    extracted_entities = _post_process_entities(extracted_entities)
    extracted_entities.to_feather("extracted_entities_with_explain.ftr")

    logger.info(f"Extracted {len(extracted_entities)} keywords per document")
    logger.info(extracted_entities.head())
    # TODO: 실제 keyword 가져오고, 임베딩 추출하기
    return 0.0

@validate_call(config={"arbitrary_types_allowed": True})
async def evaluate_graph(
    config: tuple[GraphRagConfig, GraphRagConfig],
    nodes: tuple[pd.DataFrame, pd.DataFrame],
    entities: tuple[pd.DataFrame, pd.DataFrame],
    communities: tuple[pd.DataFrame, pd.DataFrame],
    community_reports: tuple[pd.DataFrame, pd.DataFrame],
    response_type: str,
) -> tuple[
    str | dict[str, Any] | list[dict[str, Any]],
    str | list[pd.DataFrame] | dict[str, pd.DataFrame],
]:
    """Evaluate two graphs.

    Parameters
    ----------
    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)
    - nodes (pd.DataFrame): A DataFrame containing the final nodes (from create_final_nodes.parquet)
    - entities (pd.DataFrame): A DataFrame containing the final entities (from create_final_entities.parquet)
    - communities (pd.DataFrame): A DataFrame containing the final communities (from create_final_communities.parquet)
    - community_reports (pd.DataFrame): A DataFrame containing the final community reports (from create_final_community_reports.parquet)
    - response_type (str): The type of response to return.

    Returns
    -------
    """
    # Make sure your data be placed in the right order (exp, ctl)
    communities_exp = read_indexer_communities(communities[0], nodes[0], community_reports[0])
    communities_ctl = read_indexer_communities(communities[1], nodes[1], community_reports[1])

    reports_exp = read_indexer_reports(
        community_reports[0],
        nodes[0],
        community_level=None,
        dynamic_community_selection=False,
    )
    reports_ctl = read_indexer_reports(
        community_reports[1],
        nodes[1],
        community_level=None,
        dynamic_community_selection=False,
    )

    entities_exp = read_indexer_entities(nodes[0], entities[0], community_level=None)
    entities_ctl = read_indexer_entities(nodes[1], entities[1], community_level=None)

    # default setting is on the first config
    map_prompt = _load_search_prompt(config[0].root_dir, config[0].global_search.map_prompt)
    reduce_prompt = _load_search_prompt(config[0].root_dir, config[0].global_search.reduce_prompt)
    knowledge_prompt = _load_search_prompt(config[0].root_dir, config[0].global_search.knowledge_prompt)
    evaluate_prompt = _load_search_prompt(config[0].root_dir, config[0].global_search.evaluate_prompt)

    if True:
        # combine the two dataframes
        communities_ = communities_exp + communities_ctl
        reports_ = reports_exp + reports_ctl
        entities_ = entities_exp + entities_ctl
    else:
        raise NotImplementedError("Not implemented yet")

    search_engine = get_evaluate_search_engine(
        config[0],
        reports=reports_,
        entities=entities_,
        communities=communities_,
        response_type=response_type,
        dynamic_community_selection=False,
        map_system_prompt=map_prompt,
        reduce_system_prompt=reduce_prompt,
        general_knowledge_inclusion_prompt=knowledge_prompt,
    )
    result: SearchResult = await search_engine.asearch(query=evaluate_prompt)
    response = result.response
    context_data = _reformat_context_data(result.context_data)  # type: ignore
    return response, context_data


def _reformat_context_data(context_data: dict) -> dict:
    """
    Reformats context_data for all query responses.

    Reformats a dictionary of dataframes into a dictionary of lists.
    One list entry for each record. Records are grouped by original
    dictionary keys.

    Note: depending on which query algorithm is used, the context_data may not
          contain the same information (keys). In this case, the default behavior will be to
          set these keys as empty lists to preserve a standard output format.
    """
    final_format = {
        "reports": [],
        "entities": [],
        "relationships": [],
        "claims": [],
        "sources": [],
    }
    for key in context_data:
        records = (
            context_data[key].to_dict(orient="records")
            if context_data[key] is not None and not isinstance(context_data[key], dict)
            else context_data[key]
        )
        if len(records) < 1:
            continue
        final_format[key] = records
    return final_format


def _load_search_prompt(root_dir: str, prompt_config: str | None) -> str | None:
    """
    Load the search prompt from disk if configured.

    If not, leave it empty - the search functions will load their defaults.

    """
    if prompt_config:
        prompt_file = Path(root_dir) / prompt_config
        if prompt_file.exists():
            return prompt_file.read_bytes().decode(encoding="utf-8")
    return None
