# Copyright (c) 2024 Microsoft Corporation.
# Licensed under the MIT License
# Evaluating the snippet from OnePaper/graphrag/cli/index.py. Written by SH Han (2025.01.13)

"""
Evaluating Engine API.

This API provides access to the query engine of graphrag, allowing external applications
to hook into graphrag and run queries over a knowledge graph generated by graphrag.

Contains the following functions:
 - global_search: Perform a global search.
 - global_search_streaming: Perform a global search and stream results back.
 - local_search: Perform a local search.
 - local_search_streaming: Perform a local search and stream results back.

WARNING: This API is under development and may undergo changes in future releases.
Backwards compatibility is not guaranteed at this time.
"""

from collections.abc import AsyncGenerator
from pathlib import Path
from typing import TYPE_CHECKING, Any

import pandas as pd
from pydantic import validate_call

from graphrag.config.models.graph_rag_config import GraphRagConfig
from graphrag.logger.print_progress import PrintProgressLogger
from graphrag.evaluate.factory import get_evaluate_search_engine
from util.process_paper.const import PARSED_DIR
from glob import glob
from util.fileio import decode_paper_title

from graphrag.query.indexer_adapters import (
    read_indexer_communities,
    read_indexer_entities,
    read_indexer_reports,
)
from itertools import chain

if TYPE_CHECKING:
    from graphrag.query.structured_search.base import SearchResult

logger = PrintProgressLogger("")

def get_all_parents(node_id: str, parent_dict: dict[str, any]) -> list[str]:
    parents = []
    while node_id in parent_dict:
        parent_nodes = parent_dict[node_id]
        if len(parent_nodes) == 1 and parent_nodes[0] == '-1':
            break
        parents.append(parent_nodes)
    return list(chain.from_iterable(parents))

@validate_call(config={"arbitrary_types_allowed": True})
async def evaluate_keyword(
    entities: pd.DataFrame,
    viztree: pd.DataFrame,
) -> tuple[str | dict[str, Any] | list[dict[str, Any]], str | list[pd.DataFrame] | dict[str, pd.DataFrame]]:
    eval_paper_paths = glob(f"{PARSED_DIR}/*.json")
    eval_paper_titles = [decode_paper_title(Path(paper_path).stem).strip().upper() for paper_path in eval_paper_paths]
    parent_dict = viztree.groupby("id").agg({"parent": list}).to_dict()["parent"]

    extracted_entities = entities[entities["title"].isin(eval_paper_titles)][["id", "title"]]
    extracted_entities.to_csv("extracted_entities.csv", index=False)
    extracted_entities["all_parents"] = extracted_entities["id"].apply(get_all_parents, parent_dict=parent_dict)
    extracted_entities.to_csv("extracted_entities_with_parents.csv", index=False)

    exploded_entities = extracted_entities.explode("all_parents")
    explain_augmented = exploded_entities.merge(viztree, left_on="all_parents", right_on="id", how="inner")
    extracted_keyword_per_doc = explain_augmented.groupby(["id", "title"]).agg({"explain": list})
    extracted_keyword_per_doc.to_csv("extracted_keyword_per_doc.csv", index=False)
    logger.info(f"Extracted {len(extracted_keyword_per_doc)} keywords per document")
    logger.info(extracted_keyword_per_doc.head())

@validate_call(config={"arbitrary_types_allowed": True})
async def evaluate_graph(
    config: tuple[GraphRagConfig, GraphRagConfig],
    nodes: tuple[pd.DataFrame, pd.DataFrame],
    entities: tuple[pd.DataFrame, pd.DataFrame],
    communities: tuple[pd.DataFrame, pd.DataFrame],
    community_reports: tuple[pd.DataFrame, pd.DataFrame],
    response_type: str,
) -> tuple[
    str | dict[str, Any] | list[dict[str, Any]],
    str | list[pd.DataFrame] | dict[str, pd.DataFrame],
]:

    """Evaluate two graphs.

    Parameters
    ----------
    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)
    - nodes (pd.DataFrame): A DataFrame containing the final nodes (from create_final_nodes.parquet)
    - entities (pd.DataFrame): A DataFrame containing the final entities (from create_final_entities.parquet)
    - communities (pd.DataFrame): A DataFrame containing the final communities (from create_final_communities.parquet)
    - community_reports (pd.DataFrame): A DataFrame containing the final community reports (from create_final_community_reports.parquet)
    - response_type (str): The type of response to return.

    Returns
    -------
    """
    # Make sure your data be placed in the right order (exp, ctl)
    communities_exp = read_indexer_communities(communities[0], nodes[0], community_reports[0])
    communities_ctl = read_indexer_communities(communities[1], nodes[1], community_reports[1])

    reports_exp = read_indexer_reports(
        community_reports[0],
        nodes[0],
        community_level=None,
        dynamic_community_selection=False,
    )
    reports_ctl = read_indexer_reports(
        community_reports[1],
        nodes[1],
        community_level=None,
        dynamic_community_selection=False,
    )

    entities_exp = read_indexer_entities(nodes[0], entities[0], community_level=None)
    entities_ctl = read_indexer_entities(nodes[1], entities[1], community_level=None)

    # default setting is on the first config
    map_prompt = _load_search_prompt(config[0].root_dir, config[0].global_search.map_prompt)
    reduce_prompt = _load_search_prompt(
        config[0].root_dir, config[0].global_search.reduce_prompt
    )
    knowledge_prompt = _load_search_prompt(
        config[0].root_dir, config[0].global_search.knowledge_prompt
    )
    evaluate_prompt = _load_search_prompt(
        config[0].root_dir, config[0].global_search.evaluate_prompt
    )

    if True:
        # combine the two dataframes
        communities_ = communities_exp + communities_ctl
        reports_ = reports_exp + reports_ctl
        entities_ = entities_exp + entities_ctl
    else:
        raise NotImplementedError("Not implemented yet")

    search_engine = get_evaluate_search_engine(
        config[0],
        reports=reports_,
        entities=entities_,
        communities=communities_,
        response_type=response_type,
        dynamic_community_selection=False,
        map_system_prompt=map_prompt,
        reduce_system_prompt=reduce_prompt,
        general_knowledge_inclusion_prompt=knowledge_prompt,
    )
    result: SearchResult = await search_engine.asearch(query=evaluate_prompt)
    response = result.response
    context_data = _reformat_context_data(result.context_data)  # type: ignore
    return response, context_data

def _reformat_context_data(context_data: dict) -> dict:
    """
    Reformats context_data for all query responses.

    Reformats a dictionary of dataframes into a dictionary of lists.
    One list entry for each record. Records are grouped by original
    dictionary keys.

    Note: depending on which query algorithm is used, the context_data may not
          contain the same information (keys). In this case, the default behavior will be to
          set these keys as empty lists to preserve a standard output format.
    """
    final_format = {
        "reports": [],
        "entities": [],
        "relationships": [],
        "claims": [],
        "sources": [],
    }
    for key in context_data:
        records = (
            context_data[key].to_dict(orient="records")
            if context_data[key] is not None and not isinstance(context_data[key], dict)
            else context_data[key]
        )
        if len(records) < 1:
            continue
        final_format[key] = records
    return final_format


def _load_search_prompt(root_dir: str, prompt_config: str | None) -> str | None:
    """
    Load the search prompt from disk if configured.

    If not, leave it empty - the search functions will load their defaults.

    """
    if prompt_config:
        prompt_file = Path(root_dir) / prompt_config
        if prompt_file.exists():
            return prompt_file.read_bytes().decode(encoding="utf-8")
    return None
