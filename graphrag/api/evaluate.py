# Copyright (c) 2024 Microsoft Corporation.
# Licensed under the MIT License
# Evaluating the snippet from OnePaper/graphrag/cli/index.py. Written by SH Han (2025.01.13)

"""
Evaluating Engine API.

This API provides access to the query engine of graphrag, allowing external applications
to hook into graphrag and run queries over a knowledge graph generated by graphrag.

Contains the following functions:
 - global_search: Perform a global search.
 - global_search_streaming: Perform a global search and stream results back.
 - local_search: Perform a local search.
 - local_search_streaming: Perform a local search and stream results back.

WARNING: This API is under development and may undergo changes in future releases.
Backwards compatibility is not guaranteed at this time.
"""

from pathlib import Path
from typing import TYPE_CHECKING, Any
import numpy as np
from graphrag.index.operations.embed_text import embed_text
import pandas as pd
from pydantic import validate_call
from graphrag.index.config.embeddings import core_concept_embedding, keyword_embedding
from datashaper import NoopVerbCallbacks
from graphrag.config.models.graph_rag_config import GraphRagConfig
from graphrag.logger.print_progress import PrintProgressLogger
from graphrag.evaluate.factory import get_evaluate_search_engine
from graphrag.utils.graph import find_all_parents, get_embeddings, get_all_paper_titles
from graphrag.index.create_pipeline_config import _get_embedding_settings
from graphrag.evaluate.metrics import metrics_recall, metrics_mrr, metrics_ndcg, metrics_map

from graphrag.query.indexer_adapters import (
    read_indexer_communities,
    read_indexer_entities,
    read_indexer_reports,
)

if TYPE_CHECKING:
    from graphrag.query.structured_search.base import SearchResult

logger = PrintProgressLogger("")


def _get_extracted_entities(
    entities: pd.DataFrame, eval_paper_titles: pd.Series, viztree: pd.DataFrame
) -> pd.DataFrame:
    ee = entities[entities["title"].isin(eval_paper_titles)][["id", "title"]]
    ee = pd.merge(ee, find_all_parents(ee["id"].tolist(), viztree), on="id", how="left")
    return ee.explode("parents")


# 예측 키워드 임베딩 가져오기
def _get_pred_embeddings(
    entities: pd.DataFrame, all_paper_df: pd.DataFrame, viztree: pd.DataFrame, embedding_df: pd.DataFrame
) -> pd.DataFrame:
    extracted_entities = _get_extracted_entities(entities, all_paper_df["title_upper"], viztree)
    extracted_entities["parents"] = extracted_entities["parents"].astype(str)
    extracted_entities = extracted_entities.merge(
        embedding_df[["id", "vector", "text"]],
        left_on="parents",
        right_on="id",
        how="left",
        suffixes=("", "_embedding"),
    )
    extracted_entities = extracted_entities.dropna(subset=["text"])
    extracted_entities = (
        extracted_entities.drop_duplicates(subset=["id", "title", "text"])
        .groupby(["id", "title"])
        .agg({"text": list, "vector": list})
        .reset_index()
    )
    logger.info(f"Extracted {len(extracted_entities)} keywords per document")
    return extracted_entities


# 정답 키워드 임베딩 추출
async def _get_gt_embeddings(
    extracted_entities: pd.DataFrame, all_paper_df: pd.DataFrame, config: GraphRagConfig
) -> pd.DataFrame:
    eval_paper_df = all_paper_df[all_paper_df["title_upper"].isin(extracted_entities["title"])]
    eval_paper_df["keywords"] = eval_paper_df["keyword"].apply(lambda x: " ".join(x))
    eval_paper_df["id"] = np.arange(len(eval_paper_df))
    eval_paper_df = eval_paper_df.replace("", pd.NA).dropna(subset=["keywords"])
    eval_paper_df["embedding"] = await embed_text(
        eval_paper_df.loc[:, ["id", "keywords"]],
        callbacks=NoopVerbCallbacks(),
        cache=None,
        embed_column="keywords",
        embedding_name=keyword_embedding,
        strategy=_get_embedding_settings(config.embeddings)["strategy"],
    )
    return eval_paper_df


@validate_call(config={"arbitrary_types_allowed": True})
async def evaluate_keyword(
    config: GraphRagConfig,
    entities: pd.DataFrame,
    viztree: pd.DataFrame,
    k_lst: list[int] = [3, 5, 10],
) -> pd.DataFrame:
    embedding_df = get_embeddings(config, core_concept_embedding)
    all_paper_df = get_all_paper_titles()
    all_paper_df["title_upper"] = all_paper_df["title"].apply(lambda x: x.strip().upper())

    extracted_entities = _get_pred_embeddings(entities, all_paper_df, viztree, embedding_df)
    eval_paper_df = await _get_gt_embeddings(extracted_entities, all_paper_df, config)
    extracted_entities = extracted_entities[extracted_entities["title"].isin(eval_paper_df["title_upper"])].reset_index(
        drop=True
    )

    if any(k > len(eval_paper_df) for k in k_lst):
        logger.warning(f"k is greater than the number of GT keywords. Setting k to {len(eval_paper_df)}")
        k_lst = [min(k, len(eval_paper_df)) for k in k_lst]

    # similarity score 계산 & top-k index 추출
    topk_idx_lst, gt_idx_lst = [], []
    gt_embeddings = np.stack(eval_paper_df["embedding"])
    for _, row in extracted_entities.iterrows():
        pred_embeddings = np.array(row["vector"])
        pred_norm = pred_embeddings / np.linalg.norm(pred_embeddings, axis=1, keepdims=True)
        gt_norm = gt_embeddings / np.linalg.norm(gt_embeddings, axis=1, keepdims=True)
        # 추출된 키워드 n개 중 GT 키워드와 가장 높은 k개만 뽑으면 된다.
        # 그러니 여러 개의 예측 키워드 중 가장 높은 similarity를 사용한다.
        sims = np.max(pred_norm @ gt_norm.T, axis=0)
        topk_idx = np.argpartition(-sims, kth=max(k_lst) - 1, axis=0)[: max(k_lst)]
        topk_idx_lst.append(topk_idx)
        gt_idx_lst.append(eval_paper_df[eval_paper_df["title_upper"] == row["title"]]["id"].values[0])

    topk_idx_lst = np.array(topk_idx_lst)
    gt_idx_lst = np.array(gt_idx_lst)

    # 각 k마다 retrieval 성능을 찍는다.
    results = []
    for k in k_lst:
        recall = metrics_recall(topk_idx_lst[:, :k], gt_idx_lst)
        mrr = metrics_mrr(topk_idx_lst[:, :k], gt_idx_lst)
        ndcg = metrics_ndcg(topk_idx_lst[:, :k], gt_idx_lst)
        map_ = metrics_map(topk_idx_lst[:, :k], gt_idx_lst)

        results.append(
            {
                "K": k,
                "Recall": recall,
                "MRR": mrr,
                "NDCG": ndcg,
                "MAP": map_,
            }
        )

    return pd.DataFrame(results)


@validate_call(config={"arbitrary_types_allowed": True})
async def evaluate_graph(
    config: tuple[GraphRagConfig, GraphRagConfig],
    nodes: tuple[pd.DataFrame, pd.DataFrame],
    entities: tuple[pd.DataFrame, pd.DataFrame],
    communities: tuple[pd.DataFrame, pd.DataFrame],
    community_reports: tuple[pd.DataFrame, pd.DataFrame],
    response_type: str,
) -> tuple[
    str | dict[str, Any] | list[dict[str, Any]],
    str | list[pd.DataFrame] | dict[str, pd.DataFrame],
]:
    """Evaluate two graphs.

    Parameters
    ----------
    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)
    - nodes (pd.DataFrame): A DataFrame containing the final nodes (from create_final_nodes.parquet)
    - entities (pd.DataFrame): A DataFrame containing the final entities (from create_final_entities.parquet)
    - communities (pd.DataFrame): A DataFrame containing the final communities (from create_final_communities.parquet)
    - community_reports (pd.DataFrame): A DataFrame containing the final community reports (from create_final_community_reports.parquet)
    - response_type (str): The type of response to return.

    Returns
    -------
    """
    # Make sure your data be placed in the right order (exp, ctl)
    communities_exp = read_indexer_communities(communities[0], nodes[0], community_reports[0])
    communities_ctl = read_indexer_communities(communities[1], nodes[1], community_reports[1])

    reports_exp = read_indexer_reports(
        community_reports[0],
        nodes[0],
        community_level=None,
        dynamic_community_selection=False,
    )
    reports_ctl = read_indexer_reports(
        community_reports[1],
        nodes[1],
        community_level=None,
        dynamic_community_selection=False,
    )

    entities_exp = read_indexer_entities(nodes[0], entities[0], community_level=None)
    entities_ctl = read_indexer_entities(nodes[1], entities[1], community_level=None)

    # default setting is on the first config
    map_prompt = _load_search_prompt(config[0].root_dir, config[0].global_search.map_prompt)
    reduce_prompt = _load_search_prompt(config[0].root_dir, config[0].global_search.reduce_prompt)
    knowledge_prompt = _load_search_prompt(config[0].root_dir, config[0].global_search.knowledge_prompt)
    evaluate_prompt = _load_search_prompt(config[0].root_dir, config[0].global_search.evaluate_prompt)

    if True:
        # combine the two dataframes
        communities_ = communities_exp + communities_ctl
        reports_ = reports_exp + reports_ctl
        entities_ = entities_exp + entities_ctl
    else:
        raise NotImplementedError("Not implemented yet")

    search_engine = get_evaluate_search_engine(
        config[0],
        reports=reports_,
        entities=entities_,
        communities=communities_,
        response_type=response_type,
        dynamic_community_selection=False,
        map_system_prompt=map_prompt,
        reduce_system_prompt=reduce_prompt,
        general_knowledge_inclusion_prompt=knowledge_prompt,
    )
    result: SearchResult = await search_engine.asearch(query=evaluate_prompt)
    response = result.response
    context_data = _reformat_context_data(result.context_data)  # type: ignore
    return response, context_data


def _reformat_context_data(context_data: dict) -> dict:
    """
    Reformats context_data for all query responses.

    Reformats a dictionary of dataframes into a dictionary of lists.
    One list entry for each record. Records are grouped by original
    dictionary keys.

    Note: depending on which query algorithm is used, the context_data may not
          contain the same information (keys). In this case, the default behavior will be to
          set these keys as empty lists to preserve a standard output format.
    """
    final_format = {
        "reports": [],
        "entities": [],
        "relationships": [],
        "claims": [],
        "sources": [],
    }
    for key in context_data:
        records = (
            context_data[key].to_dict(orient="records")
            if context_data[key] is not None and not isinstance(context_data[key], dict)
            else context_data[key]
        )
        if len(records) < 1:
            continue
        final_format[key] = records
    return final_format


def _load_search_prompt(root_dir: str, prompt_config: str | None) -> str | None:
    """
    Load the search prompt from disk if configured.

    If not, leave it empty - the search functions will load their defaults.

    """
    if prompt_config:
        prompt_file = Path(root_dir) / prompt_config
        if prompt_file.exists():
            return prompt_file.read_bytes().decode(encoding="utf-8")
    return None
