-Goal-
Given a text document that discusses machine learning models or systems, extract only the entities that are directly and specifically related to a single focal entity named TOKEN_b0, and describe the relationships between TOKEN_b0 and those entities.

-Steps-
1. From the text, identify all entities that are **directly related to TOKEN_b0**. Disregard entities that are mentioned elsewhere in the text but not clearly connected to TOKEN_b0. For each identified entity, extract:
- entity_name: The capitalized name of the entity
- entity_type: One of the following types: [people, concept, system]
- entity_description: A full explanation of the entityâ€™s characteristics or role in relation to TOKEN_b0
Format each entity as:
("entity"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>)

2. For each of these entities, define the relationship where TOKEN_b0 is either the source or the target. For each such relationship, extract:
- source_entity: the name of the source (typically TOKEN_b0)
- target_entity: the related entity name
- relationship_description: a clear explanation of how TOKEN_b0 and the target entity are related
- relationship_strength: an integer between 1 and 10 representing how **specific or unique** this relationship is to TOKEN_b0 (e.g., a model-specific design or configuration = 10, a common setup across models = 3)
Format each relationship as:
("relationship"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_strength>)

3. **Do not extract relationships or entities that do not involve TOKEN_b0.** Only include entities that are directly and unambiguously connected to TOKEN_b0.

4. Output results in English using **{record_delimiter}** between each item, and close the output with {completion_delimiter}.

5. Translate only the descriptions if the input is in another language.

-Examples-
######################

entity_types: [people, concept, system]
text:
TOKEN_b9 generates feature interactions using the proposed Compressed Interaction Network (CIN) and further combines a CIN and a basic DNN into one unified model. TOKEN_b20 designs an interest extractor layer and an interest evolving layer to capture interests from behavior sequences. TOKEN_b1 disentangles the representation learning and feature interaction modeling via the co-action unit. TOKEN_b17 learns different representations for different operations. We use AUC and RelaImpr TOKEN_b16 as the metrics in offline experiments, CTR and CPM (Cost-Per-Mille) as metrics in online experiments. TOKEN_b0 implements DBPMaN 1 by Tensorflow. For all models, TOKEN_b0 uses Adam as the optimizer with a learning rate of 0.001. TOKEN_b0 initializes the model parameters with a Gaussian distribution (with a mean of 0 and a standard deviation of 0.01). TOKEN_b0 sets the item embedding dimension to 18.

------------------------
output:
("entity"{tuple_delimiter}TOKEN_B0{tuple_delimiter}SYSTEM{tuple_delimiter}TOKEN_b0 is a system that implements DBPMaN using Tensorflow and configures optimization and initialization settings for all models in the experiment.)
{record_delimiter}
("entity"{tuple_delimiter}DBPMAN 1{tuple_delimiter}CONCEPT{tuple_delimiter}DBPMaN 1 is the machine learning model that TOKEN_b0 implements.)
{record_delimiter}
("entity"{tuple_delimiter}TENSORFLOW{tuple_delimiter}CONCEPT{tuple_delimiter}Tensorflow is the framework used by TOKEN_b0 to implement DBPMaN.)
{record_delimiter}
("entity"{tuple_delimiter}ADAM{tuple_delimiter}CONCEPT{tuple_delimiter}Adam is the optimization algorithm used by TOKEN_b0 to train models.)
{record_delimiter}
("entity"{tuple_delimiter}LEARNING RATE{tuple_delimiter}CONCEPT{tuple_delimiter}Learning rate of 0.001 is the specific value TOKEN_b0 assigns when using Adam optimizer.)
{record_delimiter}
("entity"{tuple_delimiter}GAUSSIAN DISTRIBUTION{tuple_delimiter}CONCEPT{tuple_delimiter}Gaussian distribution with mean 0 and standard deviation 0.01 is used by TOKEN_b0 to initialize model parameters.)
{record_delimiter}
("entity"{tuple_delimiter}ITEM EMBEDDING DIMENSION{tuple_delimiter}CONCEPT{tuple_delimiter}The embedding dimension for items is set to 18 by TOKEN_b0.)
{record_delimiter}
("relationship"{tuple_delimiter}TOKEN_B0{tuple_delimiter}DBPMAN 1{tuple_delimiter}TOKEN_b0 implements the DBPMaN model{tuple_delimiter}9)
{record_delimiter}
("relationship"{tuple_delimiter}TOKEN_B0{tuple_delimiter}TENSORFLOW{tuple_delimiter}TOKEN_b0 uses Tensorflow as the implementation framework{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}TOKEN_B0{tuple_delimiter}ADAM{tuple_delimiter}TOKEN_b0 uses Adam as the optimizer in the training pipeline{tuple_delimiter}5)
{record_delimiter}
("relationship"{tuple_delimiter}TOKEN_B0{tuple_delimiter}LEARNING RATE{tuple_delimiter}TOKEN_b0 explicitly sets the learning rate to 0.001, making the configuration specific{tuple_delimiter}7)
{record_delimiter}
("relationship"{tuple_delimiter}TOKEN_B0{tuple_delimiter}GAUSSIAN DISTRIBUTION{tuple_delimiter}TOKEN_b0 initializes model weights using a specific Gaussian distribution{tuple_delimiter}8)
{record_delimiter}
("relationship"{tuple_delimiter}TOKEN_B0{tuple_delimiter}ITEM EMBEDDING DIMENSION{tuple_delimiter}TOKEN_b0 defines the item embedding dimension to be 18{tuple_delimiter}6)
{completion_delimiter}
######################

-Real Data-
######################
entity_types: [people, concept, system]
text: {input_text}
######################
output: