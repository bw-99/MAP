<h1>title</h1> <p>k-Pareto Optimality-Based Sorting with Maximization of Choice </p><h1>authors</h1> <p>Jean Ruppert; Marharyta Aleksandrova; Thomas Engel </p><h1>pub_date</h1> <p> </p><h1>abstract</h1> <p>Topological sorting 1 is an important technique in numerous practical applications, such as information retrieval, recommender systems, optimization, etc. In this paper, we introduce a problem of generalized topological sorting with maximization of choice, that is, of choosing a subset of items of a predefined size that contains the maximum number of equally preferable options (items) with respect to a dominance relation. We formulate this problem in a very abstract form and prove that sorting by k-Pareto optimality yields a valid solution. Next, we show that the proposed theory can be useful in practice. We apply it during the selection step of genetic optimization and demonstrate that the resulting algorithm outperforms existing state-of-the-art approaches such as NSGA-II and NSGA-III. We also demonstrate that the provided general formulation allows discovering interesting relationships and applying the developed theory to different applications. 1 Topological sorting [E. Knuth, 1997]  here means the process of sorting a set of items with respect to a preference or dominance relation. We use the terms preference and dominance interchangeably. 2 We use the terms item and element interchangeably. </p><h1>sections</h1><h2>heading</h2> <p>Introduction </p><h2>text</h2> <p>In the modern era of information overload, the task of choosing a subset of the most useful items is extremely important. Various tools were developed with the aim to assist a user with this task, for example, text search engines [Croft et al., 2010] and recommender systems [Resnick and Varian, 1997]. In most of the cases, such systems suggest to the user a small set of elements 2 . Thereby, if the number of equally preferable options is large, a heuristic is used to discard a fraction of them. However, in some applications the user might be willing to analyze all equally preferable options with the aim to choose the best one. This can happen, for example, in the case of choosing a habitation.
A similar problem of choosing a subset of most preferable elements also arises as an important step when solving various practical tasks. A straightforward example would be the selection step in genetic optimization algorithms [Mitchell, 1998]. At this step, a subset of the current population is chosen to advance to the next generation. Having the chosen subset made up of elements with large fitness values guides the evolution process in the desired direction. At the same time, selecting a subset with the largest variety of genes ensures variability of characteristics and allows faster exploration of the search space.
These examples bring us to the problem of generalizedfoot_0 topological sorting with choice maximisation which we also refer to as maximum choice problem. This problem aims to choose a subset of a predefined maximum size, consisting of most preferable items and containing the largest number of equally preferred elementsfoot_1 . To the best of our knowledge, this problems has not yet been studied in the literature. In this text, we propose a theoretical solution to the maximum choice problem and demonstrate how both the problem and its solution can be applied in practice.
The contributions of this work are the following:
1. We formulate the maximum choice problem in a broad sense for arbitrary elements, preference relations R, and measures µ indicating the set sizes, see Section 2.
2. We propose a solution based on the concept of k-Pareto optimality, whose definition relies on the relation R, see Section 3.
3. We further investigate the proposed solution from a theoretical point of view and discover interesting characteristics, such as the relationship between k-Pareto optimal elements and the arc of hyperbola, see Section 4.
4. Finally, we demonstrate the applicability of our approach to real-world problems by considering genetic optimization, see Section 5. </p><h2>publication_ref</h2> <p>['b1', 'b39'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Sorting with Choice Maximization </p><h2>text</h2> <p>To formally define the problem of maximization of choice, we introduce several definitions in Sections 2.1 and 2.2. The resulting formalization of this problem is abstract and quite general. However, this generality allows discovering novel connections and applying the developed theory to numerous practical problems. We also illustrate the defined concepts with an example in Section 2.3. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Definitions </p><h2>text</h2> <p>We consider a set X with a binary relation R. Intuitively xRy means that x is preferable to y. The case xRy and not yRx means that x is strictly preferable to y. This situation is denoted by xR * y. We also consider a positive and σ-finite measure µ [Halmos, 2013] defined on X. Thus, we have a measure space (X, Σ, µ), where Σ is a set of subsets of X, and µ intuitively indicates the size of these subsets. To ensure measurability, throughout this text the characteristic function 1 R of the relation R is assumed to be sufficiently regular 5 . The µ can be defined in different ways. Important examples are the counting measure and probability measures P . Depending on the definition of µ, it can indicate the following characteristics of the elements in X: how many?, how likely?, how important?, or what volume?
To illustrate these definitions, we consider the following example. Let X be a set of possible habitations of which the user has to choose the best according to his preferences encoded by the relation R. In such a situation, the relation R can be multidimensional. Let us assume, for simplicity, that an optimal habitation for the user is close to a given location, for example, his workplace (relation R l ), is situated in a district with a smaller population size (relation R p ), and is close to a river (relation R r ). Thus, the user's preferences can be represented by the preorder relation R = R l &R p &R r . In our example, all available habitations from X can be mapped onto points in a 3dimensional space of Proximity to the location × Population × River. The fact that R is a preorder rela-5 1R is equal to 1 if xRy and is equal to 0 otherwise.
tion means that some elements of X can be comparable, while others not. For example, the habitation x with coordinates (50, 100, T rue) is strictly preferable to y with coordinates (60, 100, T rue), that is xR * y.
At the same time, the habitation z with coordinates (40, 100, F alse) is incomparable with x. Indeed, z is better with respect to R l , it is situated closer to the required location, but x is better with respect to R r , as the latter is situated near a river.
Having the task to find a subset of X that is 'best' according to R, a rational solution can be formulated with the following recursive expression: if an element x is selected, then all elements that are strictly preferable to x should be also selected. In our example, this translates into the task of finding a subset of habitations S R that might be suitable for the user. Naturally, if y ∈ S R , then x ∈ S R as the latter corresponds better to the preferences of the user defined by the relation R. We formalize this rationality condition by defining selections as follows.
Definition 1. A selection S is a subset of X such that x ∈ S and yR * x implies y ∈ S. The set of all selections in Σ is denoted by Sfoot_2 . </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>The Maximum Choice Problem </p><h2>text</h2> <p>As discussed in Section 1, in practical applications when selecting a subset of X one might want not only to respect the above rationality constraint, but also to maximize the number of incomparable pairs. The latter condition is equivalent to the maximization of the diversity of the selected subset, or the maximization of the provided choice. In our example with habitations, if both x and z are presented to the user, then he can choose an appropriate habitation by himselffoot_3 .
In terms of our notations, this will be translated into the condition of selecting as many pairs x, y such that neither x is strictly preferable to y (¬xR * y) nor y is strictly preferable to x (¬yR * x). This means that there is freedom of choice between x and y (xRy = yRx). This motivates the following quantitative definition of choice for measurable subsets of X.
Definition 2. Choice offered by a set A is the number
cho(A) = (µ × µ)({(x, y) ∈ A 2 |xRy = yRx}).
The choice offered by a measurable set A essentially measures how many pairs of items offering choice can be extracted from A. Additionally, if one wants to restrict the size of the selected subset, in our example, to present to the user a small set of suitable habitations with µ(S R ) ≤ m, then this leads us to the definition of the maximum choice problem: Maximum Choice Problem. For a given m find all selections T such that cho(T ) = max S∈S,µ(S)≤m cho(S).
Any such selection T will be said to offer maximum choice for m.
In practical applications, it might be more insightful to consider the concept of diversity that is functionally related to the concept of choice. Definition 3. For any measurable set A, the diversity of A is the ratio div(A) = cho(A)/µ(A) 2 .
Thus div(A) is the likelihood that there is choice between the two randomly chosen elements. The main ingredient of our solution to the maximum choice problem is the following concept. Definition 4. The k-Pareto optimality of an element x ∈ X is the measure of the subset of X containing all elements strictly preferable to x:
po(x) = µ({y|yR * x}).
If µ is the probability measure, the k-Pareto optimality of an element x, po(x), is the likelihood an element drawn at random from X is strictly preferable to x.
Finally, we introduce the sets of at least k-Pareto optimal elements. Such sets consist of all elements in X with po ≤ k. In Section 3 we show that this concept yields a solution for the maximum choice problem. Definition 5. The at least k-Pareto optimal elements T k form the measurable set defined as follows:
T k = {x ∈ X| po(x) ≤ k} 8 . </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Example </p><h2>text</h2> <p>In this subsection, we discuss an illustrative example to demonstrate the concepts defined in Sections 2.1 and 2.2. Let us consider a finite subset X of R 2 , counting measure µ, and the relation R defined as follows:
(x 1 , x 2 )R (y 1 , y 2 ) iff x 1 ≤ y 1 and x 2 ≤ y 2 , see Fig. 1.
In economics, x is Pareto optimal if there is no y in X such that xR * y. In our language, this means that po(x) = 0. Thus, k-Pareto optimality indicates how much an element is away from being Pareto optimal.
Let X be comprised of six points presented in Fig. 2. As we are considering the counting measure, µ(X) = 6.  Sorting the set X by k-Pareto optimality of its elements will produce the following result: ({A, B, C}, {E}, {D, F }). This sorting is different from sorting by Pareto fronts. The latter approach is widely used in practice and is the basis of all Pareto dominance-based genetic optimization algorithms [Li et al., 2015]. Sorting by Pareto fronts is done in the following way. First, the first Pareto front, which is the set of non-dominated points, is identified. Next, the points from this front are removed from the consideration and the process is repeated until all points are assigned to a front. Sorting the points from Fig. 2
k that µ(T k ) = m.
We prove the above stated theorem in several steps. First, we show that for selections the computation of choice can be simplified. It only requires to compute a simple integral instead of a double integral.
Integral Formula. If S is a measurable selection and µ(S) < +∞, then
cho(S) = S (µ(S) -2 po(x))dµ(x).(1)
Proof. The fact that set S is a selection means that ∀y ∈ S : {x ∈ S|xR * y} = {x ∈ X|xR * y}. That is, any element x from X strictly preferable to any element y in S, also belong to S (x ∈ S). Using the definition of choice from Def. 2 and µ(S) < +∞ we obtain
cho(S) = µ(S) 2 -2(µ × µ)({(x, y) ∈ S 2 |xR * y}) = µ(S) 2 -2(µ × µ)({(x, y) ∈ S × X|yR * x}).
Fubini's theorem [Halmos, 2013] indicates that
(µ×µ)({(x, y) ∈ S × X|yR * x} = = S X 1 R * d(µ(y)) dµ(x) = S po(x)dµ(x),
where 1 R * is the characteristic function of R *foot_5 .
Finally, the integral formula results from the fact that µ(S) 2 -2 S po(x)dµ(x) = S (µ(S) -2 po(x))dµ(x).
Let's now consider the function c defined on Σ for any A of finite measure by
c(A) = A (µ(A) -2 po(x))dµ(x).
The integral formula defined in Eq. ( 1) means that for any selection S, we have c(S) = cho(S). The second step of our proof of Theorem 1 is to show that T k is the largest measurable set that maximizes c for its respective measure. More precisely, we will prove the following lemma. Lemma 1. For any k such as µ(T k ) < +∞ we have
c(T k ) = max A∈Σ,µ(A)≤µ(T k ) c(A). Moreover, if µ(A) ≤ µ(T k ) and c(A) = c(T k ), then A ⊆ T k almost-everywhere.
The context of this lemma is very similar to the knapsack problem [Martello, 1990]. In this problem, one needs to find a subset A of a finite set of items {x 1 , ..., x n } maximizing the total value Σ xi∈A v(x i ) under the constraint that the total weight Σ xi∈A w(x i ) of A does not exceed a predefined maximum weight w * .
In Lemma 1, the total value is c(A), the ratio of an element's value to its weight becomes µ(A)-2 po(x), and the weight constraint is expressed as µ(A) ≤ µ(T k ).
The solutions given by the lemma correspond to those yielded for the knapsack problem by George Dantzig's greedy approximation algorithm [Dantzig, 1957]. This algorithm consists of ordering elements by decreasing value-to-weight ratio and then taking the N first elements. N is chosen in such a way, that taking one more element would cause excessive weight. The process of proving Lemma 1 is similar to proving that George Dantzig's solutions are optimal for their respective weights.
Proof of Lemma 1. Let's consider T k such that µ(T k ) < +∞. To prove c(T k ) is the maximum, we need to show for any
A ∈ Σ, that c(A) ≤ c(T k ) if µ(A) ≤ µ(T k ). As A = (A ∩ T k ) ∪ (A \ T k ) and T k = (A ∩ T k ) ∪ (T k \ A), requiring c(A) ≤ c(T k ) is equivalent to requiring A\T k (µ(A)-2 po(x))dµ(x) ≤ T k \A (µ(T k )-2 po(x))dµ(x).
(2) By definition of T k , we have that po
(x) > k for x ∈ A \ T k , while po(x) ≤ k for x ∈ T k \ A. Therefore, A\T k (µ(A) -2 po(x))dµ(x) ≤ µ(A \ T k )(µ(A) -2k),
(3) and
T k \A (µ(T k ) -2 po(x))dµ(x) ≥ µ(T k \ A)(µ(T k ) -2k). (4) The constraint µ(A) ≤ µ(T k ) means that µ(A \ T k ) ≤ µ(T k \ A). Thus, we have µ(A \ T k )(µ(A) -2k) ≤ µ(T k \ A)(µ(T k ) -2k). (5)
Finally, combining Eq. ( 5) with Eq. (3) and Eq. ( 4) guarantees the inequality in Eq. ( 2) under the constraint µ(A) ≤ µ(T k ).
Let us now consider A ∈ Σ such that µ(A) ≤ µ(T k ) and c(A) = c(T k ). We now proceed to show that A ⊆ T k almost everywhere. If c(A) = c(T k ), the inequality in Eq. ( 2) must be an equality. Under the assumption µ(A) ≤ µ(T k ), we again have the inequalities in Eq. (3), Eq. ( 4) and Eq. ( 5), which must be equalities if Eq. ( 2) is an equality. However, because po(x) > k for x ∈ A \ T k , the inequality in Eq. ( 3) can only become an equality if µ(A \ T k ) = 0. Now everything is in place to prove the theorem.
Proof of Theorem 1. Let us first prove that the at least k-Pareto optimal elements T k offer maximum choice. Lemma 1 says that on Σ the set T k maximises c for its respective measure. We assumed T k is a selection. At the same time, for any selection c = cho. It means that T k offers maximum choice for its respective measure. Moreover, from Lemma 1 and from S ⊆ Σ directly results that if a selection A offers maximum choice for µ(T k ), then A ⊆ T k almost everywhere.
The theorem does not guarantee uniqueness. For example, in the case of the relation ≤ on R and the Lebesgue measure, selections are all left-unbounded intervals and the choice of any selection is 0. However, if for any selection
A ⊂ T k & µ(A) < µ(T k ) =⇒ cho(A) < cho(T k ), (6)
then T k is a unique maximum. This is a direct consequence from the fact that T k contains any other selection offering maximum choice for µ(T k ) < +∞. Also, the proof of the theorem indicates that transitivity is not necessary. The set T k is only required to be a selection. This is the case for Lebesgue area measure and the non-transitive relation R defined on the unit square by (
x 1 , x 2 )R (y 1 , y 2 ) iff y1 2 ≤ x 1 ≤ y 1 and y2 2 ≤ x 2 ≤ y 2 . The sets T k fulfill the requirement of be- ing selections if R * satisfies the weakened transitivity condition xR * y =⇒ po(x) ≤ po(y).
It is possible to prove that for the counting measure, finite sets, and partial order relations, all selections offering maximum choice can be obtained when sorting by k-Pareto optimality and taking the first l elements. However, for discrete measures with nonconstant weights the above construction might not yield all selections offering maximum choice. A relative example is given in Fig. 3. The partial order relation represented by its Hasse diagram. [Davey and Priestley, 2002]. The set S is not of the form with A ⊆ T k \ T * k . Nevertheless, it offers maximum choice for m = 3.
T * k ∪ A • (1, 1) • (1, 1) • (1, 0) • (4, 0) S </p><h2>publication_ref</h2> <p>['b29', 'b5'] </p><h2>figure_ref</h2> <p>['fig_1', 'fig_1', 'fig_2'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Further Theoretical Explorations </p><h2>text</h2> <p> </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Efficient Computation of Choice </p><h2>text</h2> <p>Computation of choice may be simplified by performing the change of variable y = po(x) in the integral formula (1).
Proposition 1. For any selection of at least k-Pareto optimal elements T k such that µ(T k ) < +∞ cho(T k ) = µ(T k ) 2 -2 [0,k] xd(po * µ)(x),(7)
where po * µ is the image measure defined by
(po * µ)([a, b]) = µ(po -1 ([a, b])). </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Characteristics of Random Vectors </p><h2>text</h2> <p>Let us study a probability space (Ω, Σ, P ). We consider two random variables X 1 and X 2 , as well as the partial order relation R Ω defined on Ω as follows:
ω x R Ω ω y iff X 1 (ω x ) ≤ X 1 (ω y ) and X 2 (ω x ) ≤ X 2 (ω y ).
For convenience, we use the following notations interchangeably: (X 1 (ω x ), X 2 (ω x )), (x 1 , x 2 ) or simply x.
In this case, the selections are the sets situated below any decreasing curve, like for example the hyperbola x 1 x 2 = 1/5 and the curve defined by max(x 1 , x 2 ) = 2/5 in Fig. 4. The k-Pareto optimality po((x 1 , x 2 )) is the joint cumulative probability distribution function F (x 1 , x 2 ). Finally, the at least k-Pareto optimal elements are situated below the curve F (x 1 , x 2 ) = k.
Having choice between x with coordinates (x 1 , x 2 ) and y with coordinates (y 1 , y 2 ) means the rectangles ((0, 0), (x 1 , 0), x, (0, x 2 )) and ((0, 0), (y 1 , 0), y, (0, y 2 )) are not nested, as depicted in Fig. 4a. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>['fig_3', 'fig_3'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Continuous Independent Variables </p><h2>text</h2> <p>If X 1 and X 2 are continuous, then the function po can be considered to be a linear extension of R Ω . Indeed, if x and y are chosen independently and at random
x y x 1 x 2 = 1 5 (0, 0) (1, 1) (a)
x and y offer choice.
x y from Ω, then P (po(x) = po(y)) = 0. On the other hand, it can be shown that div(Ω) = 1/2. This means that in half of the cases the relation R Ω cannot tell which element is preferable out of two elements extracted independently and at random from Ω.
T max 2 5 (0, 0) (1, 1) (b) y is preferable to x.
Moreover if X 1 and X 2 are independent, then the condition for uniqueness from Eq. ( 6) holds. In the special case where X 1 and X 2 are uniformly distributed on the interval [0, 1], the joint cumulative probability distribution function takes form of F (x 1 , x 2 ) = x 1 x 2 and the at least k-Pareto optimal elements are situated below a hyperbola x 1 x 2 = k. Combining this fact with Corollary 1 yields a surprising characterization of hyperbola, see Fig. 4 and the statement below. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>['fig_3'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Characterization of Hyperbolas. </p><h2>text</h2> <p>Out of all descending functions f from [0, 1] to [0, 1] delimiting an area 1 0 f (x)dx = c, the arc of hyperbola is the one offering the highest likelihood the rectangles ((0, 0), (x 1 , 0), x, (0, x 2 )) and ((0, 0), (y 1 , 0), y, (0, y 2 )) are not nested for two points x and y being drawn independently and at random from the delimited area. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Independence on Marginal Distribution </p><h2>text</h2> <p>Sklar's theorem [Sklar, 1959, Durante et al., 2013] states that the cumulative distribution function F (x 1 , x 2 ) can be represented as C(F 1 (x 1 ), F 2 (x 2 )) for a copula C. Marginas of X 1 and X 2 are fully described by the marginal cumulative probability distributions F 1 and F 2 , whereas the copula describes the dependence structure between X 1 and X 2 . The copula can be considered as a joint cumulative distribution function having two uniform marginal distributions on [0, 1]. Below we show that the introduced concepts do not depend on the marginal distribution of X 1 and X 2 .
Proposition 2. For a continuous random vector (X 1 , X 2 ) and the relation R Ω , k-Pareto optimality, choice, diversity and selections offering maximum choice only depend on the copula C of X 1 , X 2 .
Proof. Let us consider the mapping
G : Ω → [0, 1] 2 , ω x → (F 1 (x 1 ), F 2 (x 2 )).
We consider R Ω and P defined on Ω. At the same time, on [0, 1] 2 we consider R defined by (x 1 , x 2 )R (y 1 , y 2 ) iff x 1 ≤ x 2 and y 1 ≤ y 2 , as well as the image measure G * P defined on [0, 1] 2 by (G * P )(A) = P (G -1 )(A). The map G preserves probabilities in the sense that for any measurable A in Ω we have (G * P )(G(A)) = P (A). Moreover, G preserves the relations in the sense that xR Ω y iff G(x)R G(y). Selections are preserved in the sense that if S is a selection for R Ω , then G(S) is a selection for R . Ignoring negligible subsets, this mapping between selections is one-to-one. Therefore, G also preserves selections, k-Pareto optimality, choice, and diversity.
The proposition finally results from the fact that G * P only depends on the copula. This is a consequence of the fact that for any (a 1 , a 2 ) ∈ [0, 1] 2 , we have
(G * P )([0, a 1 ] × [0, a 2 ]) = C(a 1 , a 2 ).
This equality is a result of the following statements: 1) continuity which guarantees that a 1 and a 2 can be written as F 1 (x 1 ) and F 2 (x 2 ) for some appropriate x 1 and x 2 ; 2) the definition of image measure; 3) the fact that G = (F 1 , F 2 ) • (X 1 , X 2 ); and 4) the equality
F (x 1 , x 2 ) = C(F 1 (x 1 ), F 2 (x 2 )).
Proposition 3. If X 1 and X 2 are two continuous independent random variables, then for the relation
R Ω P (T k ) = k -k ln(k), cho(T k ) = (k -k ln(k)) 2 -k 2 1 2 -ln k .
Proof. Let us again consider the map G. The image measure G * P induced by G on [0, 1] 2 is the Lesbegue area measure. Independently of P , we have
G(T k ) = {(x 1 , x 2 ) ∈ [0, 1] 2 |x 1 x 2 ≤ k}.
Simple integration for Eq. ( 7) yields
P (po -1 (] -∞, x])) = P (T x ), = (x1,x2)∈[0,1] 2 (G(T x )) = x -x ln(x).
Therefore, po * P = -ln(x)dx and Eq. ( 7) results in
cho(T k ) = (k -k ln(k)) 2 -2 k 0 x(-ln(x))dx, = (k -k ln(k)) 2 -k 2 (1/2 -ln k) .
Now, it is possible to show that lim k→0 div(T k ) = 1.
The fact that diversity slowly tends to the maximum 0 0.2 0.4 0.6 0.8 1 0.0 0.2 0.4 0.6 0.8 1.0
x 1 x 2 min(x 1 , x 2 ) max(x 1 , x 2 ) (x 1 + x 2 )/2 x 1 x 2 (po)
min(x 1 , x 2 ) max(x 1 , x 2 ) (x 1 + x 2 )/2 x 1 x 2 (po)
Figure 6: Fraction of the best elements and diversity.
possible value of 1 as k tends to zero becomes even more surprising if one looks at the selections
T min a = {(x 1 , x 2 ) ∈ [0, 1] 2 | min(x 1 , x 2 ) ≤ a}, T max a = {(x 1 , x 2 ) ∈ [0, 1] 2 | max(x 1 , x 2 ) ≤ a}.
Here, lim a→0 div(T min a ) = 3 4 , and div(T max a ) = 1 2 . To further illustrate this observation, let us consider selections of a fixed measure m, which represent the best m * 100% of elements, defined with different sorting criteria: minimum min(x 1 , x 2 ), maximum max(x 1 , x 2 ), average x1+x2 2 , and Pareto optimality x 1 x 2foot_6 . As we can see in Fig. 6, min(x 1 , x 2 ) delimits selections containing too many large values, that is, extremes are overvalued. On the other hand, all other sorting criteria except po undervalue extremes and include too many elements situated around the diagonal
x 1 = x 2 .
Finally, in Fig. 6 we demonstrate how diversity of the selections defined above depends on the fraction of selected elements mfoot_7 . We can see that diversity is the largest when sorting by k-Pareto optimality. This is a direct consequence of Theorem 1. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Further Practical Explorations </p><h2>text</h2> <p> </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Computation Complexity </p><h2>text</h2> <p>A common solution for ranking n elements of a set X according to a partial order relation R is to rank the elements according to their average ranking with respect to all linear extensions of R. However, the total number of linear extensions exponentially increases with n, and the resulting algorithms are complex and slow [de Loof, 2010, p. 48]. For example, random sampling of linear extensions has an expected running time of O(n 3 log n) [Huber, 2006]. Below we show that sorting by k-Pareto optimality offers an efficient alternative.
The basic algorithm for the k-Pareto optimality based sorting is straightforward. In the case of an arbitrary relation R, po(x) is computed by summing up the measures of the items that are strictly preferable to x. This requires one pass through the whole set X for every element x ∈ X with computation complexity O(n 2 ). The complexity of sorting X by increasing values of po is O(n log n). Therefore, the total computational complexity is O(n 2 ).
The case of composite relations defined on the probability space allows constructing even faster sorting procedures. We illustrate this idea for R = R l &R p &R r from our housings example. We define the component relations as follows: for i ∈ {l, r, p}, aR i b iff X i (a) ≤ X i (b), where the real valued random variable X l represents proximity to the location, X p represents population size, and X r (x) is 0 when x is close to a river and 1 otherwise. For independent X i , we have: po(x) = P ({y|yR * x}), = P ({y|yRx}) -P ({y|yRx and xRy}), = i∈{1,...,m}
P (X i ≤ x i ) - i∈{1,...,m} P (X i = x i ).
The cumulative probability distributions F i (x) = P (X i ≤ x) can be approximated by the respective empirical cumulative probability distributions Fi (x). The computation complexity of estimating Fi is O(n log n). This needs to be done for every component relation R i , resulting in the total complexity of O(n log n). </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Application to Genetic Optimization </p><h2>text</h2> <p>In Section 1, we hypothesized that sorting with choice maximization can be beneficial for genetic optimization. Indeed, this strategy results in the maximization of the population diversity and allows exploring the search space more efficiently. Additionally, Pareto dominance-based many-objective 12 genetic optimiza-tion algorithms are known to suffer from the lack of selection pressure [Palakonda et al., 2018]. When the number of objectives increases, the number of incomparable solutions grows exponentially. However, as shown in Section 4.2, sorting random independent vectors by their Pareto optimality can be considered as a liner extension of the defined preference relation. The fact that P (po(x) = po(y)) = 0 means that such sorting rarely produces ties and for any two solutions either x is preferable to y or vice versa. In the rest of this subsection, we demonstrate that the proposed approach indeed improves the performance of genetic algorithms in the case of independent objectives.
To evaluate the proposed sorting procedure, we use it in NSGA-II instead of Pareto dominance-based sorting. We experiment with two measures µ: counting and probability measures. This gives us two versions of genetic algorithms referred to as PO-count and PO-prob respectively. These algorithms are compared with implementations of the state-of-the-art algorithms NSGA-II and NSGA-III [Deb and Jain, 2013] from the deap python library 13 . For the experimental evaluation, we use the 0/1 knapsack problem with independent objectives as defined in [Zitzler and Thiele, 1999]. The number of knapsacks (objectives) is varied within the following set n k ∈ {2 -8, 10, 15, 25} and the number of items is set to 250. We adopt random selection with replacement and uniform crossover with mutation probability 0.01. We set the population size to 250 and the number of generations to 500. All results are the average among 30 independent runs.
Below we analyze the performance of different algorithms in terms of the classical hypervolume metric [Shang et al., 2020] with the origin of coordinates as a reference point. In our setup, this metric is to be maximized. We choose NSGA-II as the baseline, and present the relative changes in the hypervolume indicator for the rest of the algorithms in Fig. 7 (increase: positive number, decrease: negative number). We notice that despite having been developed for the manyobjective optimization, NSGA-III almost always results in lower values of hypervolume, even for a large number of knapsacks. This confirms a similar observation from [Ishibuchi et al., 2016], and supports our choice of NSGA-II as a baseline for implementation and comparison instead of NSGA-III. Further, we see that the value of relative increase for PO-count is always very close to 0. It means that PO-count yields a population covering the same hypervolume as NSGA-II. Contrarily, PO-prob improves the hypervolume, as compared to NSGA-II. This difference is visible for small n k (+4% for n k = 2) and is especially prominent for large n k (+60% for n k = 25). For n k between 13 https://deap.readthedocs.io/en/master/ 1 2 3 4 5 6 7 8  5 and 7, PO-prob results in lower values of hypervolume than NSGA-II. However, the relative decrease in thes cases does not exceed -1.63%. Also, within this range, PO-count performs slightly better than other algorithms. These results demonstrate that the proposed approach improves the performance of genetic algorithms, especially in the case of many-objective optimization. It also suggests that the choice of the measure µ has a large impact on the performance. The latter relationship will be studied in future work. </p><h2>publication_ref</h2> <p>['b37', 'b9', 'b45', 'b41', 'b21'] </p><h2>figure_ref</h2> <p>['fig_5'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Conclusion </p><h2>text</h2> <p>In this paper, we formulate the problem of generalized topological sorting with choice maximization, which, to the best of our knowledge, was not considered in the literature before. We also prove that the at least k-Pareto optimal sets provide unique solutions. Further theoretical analysis of this problem leads us to an interesting relationship between the diversity of random points and the arc of hyperbola. Additionally, we propose a computationally efficient algorithm for the calculation of k-Pareto optimality for probability measures. Finally, we demonstrate a successful application of the developed theory. We show that sorting by k-Pareto optimality can drastically improve the performance of many-objective genetic optimization algorithms. In our experiments, the proposed solution based on the probability measure allows increasing the value of hypervolume by up to 60% for 25 objectives. This result can be considered as a potential solution to the problem of searchability deterioration in Paretodominance optimization.
We also believe that the proposed general framework can be used in different applications. In future work, we plan to study the applicability of k-Pareto optimality for constrained optimization, scheduling problems, recommender systems, and the development of statistical indicators. Maximization of choice might be also useful when studying causality and fairness. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>A.2 Cone-based Relations </p><h2>text</h2> <p>Let us again consider the unit square, the Lebesgue measure, and a positive constant a. However, this time the preference relation is defined as follows: yR a x iff y 2 ≤ x 2 and x 2 -y 2 ≥ a(y 1 -x 1 ). The above relation R a is an example of a cone-based relation illustrated in Fig. 9. This relation has the intuitive meaning of giving up (x 1 , x 2 ) for getting (y 1 , y 2 ) if the improvement (diminution) in the second characteristic is at least a times the trade-off (increase) in the first characteristic. In this case, selections are sets delimited by descending curves Let us now consider the sets of at least k-Pareto optimal elements of measure 0.25 for the three values of a: a = 1 10 , a = 1 2 , and a = 2, see Fig. 8b. Larger values of a represent higher maximum accepted trade-offs. This is represented by the gradual degeneration of the hyperbola into a straight horizontal line when a increases. As shown in the figure, the three sets demonstrate plausible behavior. In the situation discussed in Appendix A.1, the relation R corresponds to the extreme case of the relation R a with a = 0.
x 2 = f (x 1 ) such that -1 a ≤ df dx1 ≤ 0. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>['fig_6'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>A.3 Transitive relations </p><h2>text</h2> <p>In general, it is possible to show that if R * is transitive, then for any k, the set T k is a selection. In particular, if R is a partial order relation, µ is strictly positive, and X is countable, we obtain a linear extension [Dantzig, 1957] of R when sorting X by increasing values of po and sorting ties in any order. Selections are represented by downsets. The latter are obtained when topologically sorting X and taking the first n elements, for any n. If µ is the counting measure, then po(x) is simply the number of elements that can be reached by following downwards the edges of the corresponding Hasse diagram. An example of such a relation represented by its Hasse diagram is depicted in Fig. 10.    Figure 13: Sorting 500 uniformly distributed points. Points in black belong to the first 10 equivalence classes (fronts). Note, that the total number of equivalence classes is larger for po-based sorting. The latter approach results in fewer ties.
results of sorting for uniformly distributed points, we observe that both sorting methods result in hyperbola-like selections, see Fig. 13. This means that sorting by Pareto fronts is more sensitive to the topological structure of the analyzed space, while sorting by po preserves its characteristics. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>['fig_7', 'fig_2', 'fig_2'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>B.2 Further Solutions of the Maximum Choice Problem </p><h2>text</h2> <p> </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Further similar solutions </p><h2>text</h2> <p>It is possible to prove that Theorem 1 also holds for T * k defined with a strict inequality (<) as follows, see Def. 5 for comparison.
T * k = {x ∈ X| po(x) < k}. In this case, the proof of the fact that T * k is the largest in Lemma 1 requires analysis of inequality (4) instead of inequality (3). However, the proof of the fact that any selection T such that T * k ⊆ T ⊆ T k offers maximum choice for µ(T ) becomes a bit more technical. Moreover, it is possible to prove that T is the largest selection of this kind. Precisely, for any other selection S offering maximum choice for µ(T ), S ⊆ T for some T such that µ(T ) = µ(T ) and
T * k ⊆ T ⊆ T k . </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Completeness of the Solutions </p><h2>text</h2> <p>In the case condition in Eq. ( 6) holds, and if for any selection S there is a k and there are selections T such that T * k ⊆ T ⊆ T k and µ(T ) = µ(S), then those selections T are the only selections offering maximum choice. Therefore, we have a complete list of selections offering maximum choice. This is the case for the typical example of the relation R defined in Section 2.3 and the Lebesgue area measure defined on the unit square [0, 1] 2 . This also holds for any discrete measure with constant non-zero weights, for example, for the counting measure on a finite set with a partial order relation. In the latter case, topologically sorting by increasing values of po and then taking the first n elements results in a set offering maximum choice. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Existence of Solutions of a Different Nature </p><h2>text</h2> <p>Let us consider again the example in Fig. 3. Here, the set S is not of the form
T * k ∪ A with A ⊆ T k \ T * k .
Nevertheless, it offers maximum choice for m = 3. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>['fig_2'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>B.3 Diversity </p><h2>text</h2> <p>As it was discussed in Section 2.2, the concept of maximum choice is functionally related to the concept of diversity, see Def. 3. In Theorem 1 we cannot simply replace choice by diversity. However, by considering only selections of a fixed measure, we obtain the following straightforward corollary of Theorem 1.
Corollary 1. If R is a transitive relation, then for any set of at least k-Pareto optimal elements T k such that µ(T k ) < +∞, we have
div(T k ) = max S∈S,µ(S)=µ(T k ) div(S).
Moreover T k is the unique such maximum. Precisely if S is a selection such that µ(S) = µ(T k ), and div(S) = div(T k ), then S = T k almost everywhere.
C Further Practical Exploration of at least k-Pareto optimal elements </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>C.1 Additional Results for Genetic Optimization </p><h2>text</h2> <p>In Section 5.2, we evaluate the performance of the genetic algorithms using the hypervolume indicator. In this section, we further analyze the behavior of both the state-of-the-art and the proposed algorithms with respect to other metrics. In particular, we study the fraction of solutions dominated by the solution of alternative algorithms and analyze the time complexity of the sorting procedure.
We calculate the percentage of dominated solutions as follows. For a given pair of algorithms algorithm1 and algorithm2, we calculate how many solutions of algorithm2 (dominated algorithm) are dominated by solutions of algorithm1 (dominating algorithm). After that, we average the obtained results among all dominating algorithms to get an average fraction of dominated solutions, denoted by θ. Naturally, lower values of θ indicate better performance. We present the corresponding results in Fig. 14. We notice the following tendencies. NSGA-II and PO-count behave very similarly. For n k = 2, the value of θ for these algorithms is around 20%. After that, it starts increasing and reaches its peak of approximately 45% for n k = 7. Finally, it gradually decreases to 24% for n k = 25. NSGA-III starts at a similar level and reaches its peak of approximately 30% for n k = 5. After that, it decreases below 10% for n k = 7 and stays relatively close to 0 for the larger numbers of knapsacks. These results demonstrate the superiority of NSGA-III over NSGA-II in the case of many-objective optimization. PO-prob starts at around 16%. However, for n k = 4 the value of θ it already almost 0 and does not go up for larger numbers of knapsacks. This shows that the solutions produced by this algorithm are rarely dominated. Thereby, PO-prob is an effective approach for many-objective optimization problems. In Fig. 15, we demonstrate the dependence of sorting time on the population size for values of pop size ranging from 50 to 500. The reported values are the averages over 100 independent executions of one iteration of the corresponding genetic algorithm. From the figure, we can see that PO-prob requires much less time than all other algorithm. The results for NSGA-II and PO-count tend to be very close, as in other experiments. This observation also has theoretical explanation. Indeed, choosing the next generation for NSGA-II and NSGA-III has time complexity of O(N 2 M ) and max{O(N 2 M ), O(N 2 log M -2 N )} respectively where M stands for number of objectives and N is the population size, see [Deb et al., 2002,Deb andJain, 2013]. At the same time, sorting in PO-prob comes down to independent sorting procedures with respect to every objective. The time complexity of this procedure is O(N M log(N )). These results are in line with the theoretical analysis presented in Section 5.1 and prove the computational efficiency of the approximate ranking calculation procedure used in PO-prob.
The maximum choice theorem (Theorem 1) has an intuitive interpretation in the context of genetic algorithms. Assume that the selection step is required to pick a selection of a given maximum size for breeding offspring, and both parents are chosen independently and at random form this selection. Then selections obtained via k-Pareto optimality-based sorting yield most offspring with parents offering choice. Choice here means that every parent is strictly superior to the other with respect to at least one objective, or both have the same values of all objectives. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>['fig_11', 'fig_12'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>C.2 Kendall's τ Rank Correlation Coefficient and Statistical Tests </p><h2>text</h2> <p>Let us again consider the case of 2 continuous random variables introduced in Section 4.2. Let us assume that X 2 = f (X 1 ) for some increasing function f . For almost all (x, y), either xRy or yRx holds. Thus div(Ω) = 0. Moreover, diversity only depends on the copula which encodes the dependency structure between X 1 and X 2 , see Section 4.2.2 and Appendix B.3. Therefore, a value of div(Ω) close to zero indicates X 1 and X 2 are strongly correlated via an increasing function. It leads to the idea that Kendall's τ rank correlation coefficient [Kendall, 1948] and diversity are strongly related concepts.
Let us consider a sample of n points X = {(x i1 , x i2 ) i∈{1,2,...,n} }. Duplicates almost never occur and the order in which points are drawn has no importance. Therefore, X should be treated like a set. We consider the counting measure # and the relation R define on X. Diversity and choice of X are denoted by div # and cho # .
Kendall's τ correlation coefficient is defined as follows
τ = # con -# dis n 0 ,
where # con is the number of concordant pairs (pairs that do not offer choice in our terminology), # dis is the number of discordant pairs (pairs that do offer choice), and n 0 is the total number of pairs. As duplicates are discarded and the pairs are not ordered, n 0 = n(n -1)/2.
From the above remarks, we have cho # = 2# dis +n and # con +# dis = n 0 . Combining these equalities, we obtain the following relation
τ = n 2 + n -2 cho # n 2 -n .
Dividing the numerator and the denominator by n 2 and then neglecting 1 n , we obtains that approximately
τ ≈ 1 -2 div # .
This result means that the theory developed in this paper can be used for constructing non-parametric statistical tests generalizing Kendall's τ rank correlation coefficient and can be used for testing partial correlation. Below, we illustrate this property by building an indicator for distinguishing between wealthy and non-wealthy states.
A group of states might be considered wealthy if the following two conditions hold.
1. In the group there is no positive correlation between per capita income and the indicator representing education and health.
2. If a state belongs to a group of wealthy states, then all states having higher per capita income and better value of education and health indicator, must also belong to that group. Now, we define the set of all wealthy states as the largest group of states that are wealthy. If Kendall's τ is used to compute correlation, and correlation is considered to be positive if div # < 1 2 , then the elements of the above set can be easily identified. Indeed, Corollary 1 says that the set of wealthy states must be a set of at-least k-Pareto optimal states for the relation higher income and better education and health indicator. For the year 2015foot_9 , we took Gross National Income (GNI) per capita at purchasing power parity (PPP) as the income indicator, and the square root of the education and life expectancy as the education and health indicatorfoot_10 . The scatter plot in Fig. 16 shows the resulting division of states into wealthy and non-wealthy. We can observe that the wealthy states are defined as the states with GNI ≥ 20 000$. This seems perfectly plausible.  </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>['fig_13'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>C.3 Application to Recommender Systems </p><h2>text</h2> <p>Let us again consider the housing example introduced in Section 2.1. If we aim to provide to the user a full set of possible alternative houses that might fit his preferences, then, according to Theorem 1, sorting available habitations by the increasing value of po is the best strategy. As it was discussed in Section 5.1, in the case of independent components of the underlying composite relations, the computation of po can be simplified by using tools from probability theory. Apart from computational efficience, estimating po in this way has several additional advantages.
• Such sorting results in fewer ties and a meaningful score. Indeed, sorting items x by increasing values of po(x), is the same as sorting by decreasing values of -log(po(x)). The self-information -log(F i (x)) [Jones, 1979], which is additive, indicates how much a characteristic i is valued. In this case, there is no need to introduce any arbitrary coefficients as it is done when sorting by a weighted mean of the characteristics x i .
• If the condition of independence holds, then rarer characteristics get valued more. This makes sense from the economic point of view and is intuitively necessary for maximizing choice.
• If beyond the relation R, there is complete uncertainty about the user's complex needs, tastes and desires, then offering him a selection of maximum choice maximizes the likelihood he finds an appropriate item. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>C.4 Constrained Multi-Objective Genetic Algorithms </p><h2>text</h2> <p> </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>C.4.1 Problem Definition </p><h2>text</h2> <p>A Multiobjective Constrained Optimization Problem (CMOP) is a mathematical problem that is defined as follows [Kumara et al., 2020]:
Minimize f 1 (x), f 2 (x), . . . , f M (x) subject to g i (x) ≤ 0, i ∈ {1, 2, ..., ng}, h j = 0, j ∈ {ng + 1, ng + 2, ..., ng + nh}, L k ≤ x k ≤ U k , h ∈ {1, . . . , D},
where
• f i represents the i-th objective function,
• M is the total number of conflicting objective functions,
• x = (x 1 , x 2 , . . . , x D ) is a solution vector of length D,
• L k and U k are the lower and upper bounds of the search space at the k-th dimension.
Numerically, we consider a constraint h j to be verified iff h j ∈ [-, ]. A solution is feasible iff all ng + nh constraints g i and h j are verified. </p><h2>publication_ref</h2> <p>['b27'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>C.4.2 Problem Re-Definition with Preorder Relations </p><h2>text</h2> <p>In a more general setting, we can represent a constraint g i ≤ 0 by the preorder relation R gi defined as follows:
xR gi y iff g i (x) ≤ 0 or g i (x) ≤ g i (y). (8
)
And a constraint h j ∈ [a j , b j ] can be represented by the preorder relation R hj defined as follows:
xR hj y iff      h j (x) ∈ [a j , b j ] or h j (y) ≤ h j (x) ≤ a j or b j ≤ h j (x) ≤ h j (y).(9)
Then, the combination of the constraints g i ≤ 0, i ∈ {1, 2, ..., ng} and h j = 0, j ∈ {ng + 1, ng + 2, ..., ng + nh} can be represented by the preorder relation R c defined as follows.
xR c y iff xR g1 y and . . . and xR gng y and xR hng+1 y and . . . and xR h ng+nh y.
The objective consisting in minimizing f i is represented by the preodrer relation R fi :
xR fi y iff f i (x) ≤ f i (y).
Minimization of all M objectives f 1 , . . . , f M is represented by the preorder relation R f defined as follows  </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>C.4.3 Solution </p><h2>text</h2> <p>To solve the problem defined above, we can use the standard Adaptive Differential Evolution Algorithm jDE [Noman et al., 2011] with k-Pareto optimality for R cf as a fitness function. For any point x, k-Pareto optimality of x is the likelihood a point drawn at random from the population strictly Pareto dominates x for R cf . Smaller values of po mean better fitness. Under the independence assumption of objectives and constraints, we can easily compute k-Pareto optimality po(x). When saying P ({y|yR = cf x}) = 0, we assume the considered objectives and constraints are not constant on too large sets. Without this simplification, the computation becomes longer, see the derivation below. Any cumulative probability distribution defined above, F i for f i and G i for g i , can be estimated via its empirical cumulative probability distribution. Note, for a population {x 1 , . . . , x k , . . . , x ps } of size ps, and for any real valued function f , the empirical cumulative probability distribution F of f is defined as follows:
F (z) = #({x k |f (x k ) ≤ z}) ps .
In a similar way, P (y|yR hj x) can be estimated by H j * (h j (x)), where H j * (z) is defined as (10)
H j * (z) =
The computation of every Fi can be performed as follows:
• sort the values f (x k ) in increasing order, and store them in an array;
• create two new arrays;
• loop over the sorted values f (x k ); each time a new distinct value f (x k ) is encountered:
append the previously encountered f (x k ) to the first array, append to the second array the loop counter, which is equal to the value of the empirical cumulative probability distribution Fi of the previously encountered value f (x k ).
Thus, retrieving Fi (x) can be performed via a binary lookup with run time O(log ps). Computation of H j * can be performed in the same way. In this case, all three cases of the definition in Eq. ( 10 Finally, it is possible to show that the total run time of the k-Pareto optimality based sorting is O((ng + nh + M )ps log ps). </p><h2>publication_ref</h2> <p>['b35'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>C.5 Exploratory Database Queries </p><h2>text</h2> <p>Simple database queries q, objectives, and constraints in optimization problems often consist in requiring a continuous attribute to be in a given interval, or a discrete attribute to be equal to a given value. Conceptually, those queries are boolean functions. Complex queries are often conjunctions of the form r = q 1 ∧ q 2 ∧ • • • ∧ q n .
In our formalism, these simple queries translate into simple pre-order relations of the form xRy. Requiring an element to be in an interval can be represented by xR q y iff x is in the desired interval, or x is not situated further from the interval than yfoot_11 . Requiring an attribute to be equal to a given value translates into the relation xR q y if for x the attribute takes the required valuefoot_12 . Complex queries then translate into the composite relations of the form R r = R q1 ∧ R q2 ∧ • • • ∧ R qn . The simple sub-relations R q2 are pre-order relations, and, therefore, R r is also a pre-order relation and is transitive. However, these relations are not partial order relations, as reflexivity does not necessarily hold. A "topological" sorting according to our partial order relation R r can be viewed as a valid fuzzy relaxation of the strict functional query. There are many possible fuzzy relaxations and the problem is to find one that is suitable for a given application. The k-Pareto optimality is one of such fuzzy extensions of the query. It is 0 if all criteria are satisfied, and higher values of k-Pareto optimality indicate worse results. The maximum choice theorem applies here, and the user is offered the maximum choice. This is of particular interest for exploratory queries, such as job search, especially, if there are no items in the database that satisfy all the criteria. Direct brute-force search for selections offering maximum choice is unfeasible as there are too many selections to consider.
Moreover, in the above formalism, one can treat classical optimization objectives in the same way. Maximizing an attribute x can be represented by the relation xRy iff x ≥ y, and the minimization can be represented by the relation ≤. In the above framework, negation can be represented via the relation R -1 defined by xR -1 y iff yRx. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>C.6 Scheduling Algorithms </p><h2>text</h2> <p>In the case of scheduling algorithms, xRy can be given the meaning 'x depends on y'. Then, selections represent sets of tasks that remain to be processed. Having a large choice means having much freedom to parallelize tasks or having flexibility in case the rescheduling is required. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Supplementary Materials A Further Examples for Simple Relations and Measures </p><h2>text</h2> <p>In the first two supplementary examples, we consider a situation typical in economics or multi-objective optimization. Later, we show how the proposed concepts apply to arbitrary transitive relations. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>A.1 Continuous Measures </p><h2>text</h2> <p>Let us again consider R as defined in Section 2.3. The relation R models preference for small values of x 1 and x 2 . However, instead of assuming X to be a finite subset of R 2 , we now study the unit square [0, 1] × [0, 1] with three continuous measures: the Lebesgue area measure dx 1 dx 2 , as well as 2x 2 dx 1 dx 2 and 4x 1 x 2 dx 1 dx 2 . In each case, the total measure of the unit square equals to one. The Lebesgue area measure represents elements with two uniformly distributed characteristics x 1 and x 2 ; 2x 2 dx 1 dx 2 represents rarefaction of items having small values of x 2 , whereas 4x 1 x 2 dx 1 dx 2 represents rarefaction of items having small values of both x 1 and x 2 .
For each of the cases defined above, in Fig. 8a we show the set of at least k-Pareto optimal elements of measure 0.1, which corresponds to selecting the 10 best percent. All three sets demonstrate the qualitative behaviours expected from sets delimited by indifference curves when the corresponding rarefaction occurs. Indeed, the curve corresponding to the uniform distribution and the Lebesgue area measure dx 1 dx 2 is symmetric. Also, in this case, po(x 1 , x 1 ) = x 1 x 2 , and the sets of at least k-Pareto optimal elements are the sets situated below arcs of hyperbola defined by the equation x 1 x 2 = k, see Sections 4.2 and 4.2.1 for more details. Applying rarefaction with respect to x 2 prioritises smaller values of this characteristic. This is represented by shifting upwards the right part of the hyperbola arc, see the curve for 2x 2 dx 1 dx 2 . Indeed, in this case, the small values of x 2 are observed less often. This results in selecting additional elements with large values of x 1 but relatively small values of x 2 to compensate for this rarefaction. Finally, rarefaction with respect to both x 1 and x 2 results in the fact that the small values of both characteristics are observed less often. Thus, elements with larger values of x 1 and x 2 should be selected to generate a selection of the required measure. It results in the shift of the hyperbola upwards following the direction of the main diagonal, see the curve for 4x 1 x 2 dx 1 dx 2 .   </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h1>references</h1><h2>ref_id</h2> <p>b0 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2010 </p><h2>authors</h2> <p> Croft </p><h2>ref_id</h2> <p>b1 </p><h2>title</h2> <p>Search engines: Information retrieval in practice </p><h2>journal</h2> <p>Addison-Wesley Reading </p><h2>year</h2> <p>2010 </p><h2>authors</h2> <p>W B Croft; D Metzler; T Strohman </p><h2>ref_id</h2> <p>b2 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>1957 </p><h2>authors</h2> <p> Dantzig </p><h2>ref_id</h2> <p>b3 </p><h2>title</h2> <p>Discretevariable extremum problems </p><h2>journal</h2> <p>Operations research </p><h2>year</h2> <p>1957 </p><h2>authors</h2> <p>G B Dantzig </p><h2>ref_id</h2> <p>b4 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2002 </p><h2>authors</h2> <p>Priestley Davey </p><h2>ref_id</h2> <p>b5 </p><h2>title</h2> <p>Introduction to lattices and order </p><h2>journal</h2> <p>Cambridge university press </p><h2>year</h2> <p>2002 </p><h2>authors</h2> <p>B A Davey; H A Priestley </p><h2>ref_id</h2> <p>b6 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2010 </p><h2>authors</h2> <p> De Loof </p><h2>ref_id</h2> <p>b7 </p><h2>title</h2> <p>Efficient computation of rank probabilities in posets </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2010 </p><h2>authors</h2> <p>K De Loof </p><h2>ref_id</h2> <p>b8 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2013 </p><h2>authors</h2> <p>Deb ; Jain  </p><h2>ref_id</h2> <p>b9 </p><h2>title</h2> <p>An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: solving problems with box constraints </p><h2>journal</h2> <p>IEEE transactions on evolutionary computation </p><h2>year</h2> <p>2013 </p><h2>authors</h2> <p>K Deb; H Jain </p><h2>ref_id</h2> <p>b10 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2002 </p><h2>authors</h2> <p>Deb  </p><h2>ref_id</h2> <p>b11 </p><h2>title</h2> <p>A fast and elitist multiobjective genetic algorithm: Nsga-ii </p><h2>journal</h2> <p>IEEE transactions on evolutionary computation </p><h2>year</h2> <p>2002 </p><h2>authors</h2> <p>K Deb; A Pratap; S Agarwal; T Meyarivan </p><h2>ref_id</h2> <p>b12 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2013 </p><h2>authors</h2> <p>Durante  </p><h2>ref_id</h2> <p>b13 </p><h2>title</h2> <p>A topological proof of sklar's theorem </p><h2>journal</h2> <p>Applied Mathematics Letters </p><h2>year</h2> <p>2013 </p><h2>authors</h2> <p>F Durante; J Fernandez-Sanchez; C Sempi </p><h2>ref_id</h2> <p>b14 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>1997 </p><h2>authors</h2> <p>E Knuth </p><h2>ref_id</h2> <p>b15 </p><h2>title</h2> <p>The Art of Computer Programming </p><h2>journal</h2> <p>Addison-Wesley </p><h2>year</h2> <p>1997 </p><h2>authors</h2> <p>E Knuth; D  </p><h2>ref_id</h2> <p>b16 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2013 </p><h2>authors</h2> <p> Halmos </p><h2>ref_id</h2> <p>b17 </p><h2>title</h2> <p>Measure theory </p><h2>journal</h2> <p>Springer </p><h2>year</h2> <p>2013 </p><h2>authors</h2> <p>P R Halmos </p><h2>ref_id</h2> <p>b18 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2006 </p><h2>authors</h2> <p> Huber </p><h2>ref_id</h2> <p>b19 </p><h2>title</h2> <p>Fast perfect sampling from linear extensions </p><h2>journal</h2> <p>Discrete Mathematics </p><h2>year</h2> <p>2006 </p><h2>authors</h2> <p>M Huber </p><h2>ref_id</h2> <p>b20 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2016 </p><h2>authors</h2> <p> Ishibuchi </p><h2>ref_id</h2> <p>b21 </p><h2>title</h2> <p>Performance comparison of nsga-ii and nsga-iii on various manyobjective test problems </p><h2>journal</h2> <p>IEEE </p><h2>year</h2> <p>2016 </p><h2>authors</h2> <p>H Ishibuchi; R Imada; Y Setoguchi; Y Nojima </p><h2>ref_id</h2> <p>b22 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>1979 </p><h2>authors</h2> <p> Jones </p><h2>ref_id</h2> <p>b23 </p><h2>title</h2> <p>Elementary information theory </p><h2>journal</h2> <p>Clarendon Press </p><h2>year</h2> <p>1979 </p><h2>authors</h2> <p>D S Jones </p><h2>ref_id</h2> <p>b24 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>1948 </p><h2>authors</h2> <p> Kendall </p><h2>ref_id</h2> <p>b25 </p><h2>title</h2> <p>Rank correlation methods </p><h2>journal</h2> <p> </p><h2>year</h2> <p>1948 </p><h2>authors</h2> <p>M G Kendall </p><h2>ref_id</h2> <p>b26 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2020 </p><h2>authors</h2> <p> Kumara </p><h2>ref_id</h2> <p>b27 </p><h2>title</h2> <p>Guidelines for real-world multi-objective constrained optimisation competition </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2020 </p><h2>authors</h2> <p>A Kumara; G Wub; M Alic; Q Luob; R Mallipeddid; P N Suganthane; Swagatam Das; S  </p><h2>ref_id</h2> <p>b28 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2015 </p><h2>authors</h2> <p> Li </p><h2>ref_id</h2> <p>b29 </p><h2>title</h2> <p>Many-objective evolutionary algorithms: A survey </p><h2>journal</h2> <p>ACM Computing Surveys (CSUR) </p><h2>year</h2> <p>2015 </p><h2>authors</h2> <p>B Li; J Li; K Tang; X Yao </p><h2>ref_id</h2> <p>b30 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>1990 </p><h2>authors</h2> <p> Martello </p><h2>ref_id</h2> <p>b31 </p><h2>title</h2> <p>Knapsack problems: algorithms and computer implementations </p><h2>journal</h2> <p> </p><h2>year</h2> <p>1990 </p><h2>authors</h2> <p>S Martello </p><h2>ref_id</h2> <p>b32 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>1998 </p><h2>authors</h2> <p> Mitchell </p><h2>ref_id</h2> <p>b33 </p><h2>title</h2> <p>An introduction to genetic algorithms </p><h2>journal</h2> <p>MIT press </p><h2>year</h2> <p>1998 </p><h2>authors</h2> <p>M Mitchell </p><h2>ref_id</h2> <p>b34 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2011 </p><h2>authors</h2> <p> Noman </p><h2>ref_id</h2> <p>b35 </p><h2>title</h2> <p>An adaptive differential evolution algorithm </p><h2>journal</h2> <p>IEEE </p><h2>year</h2> <p>2011 </p><h2>authors</h2> <p>N Noman; D Bollegala; H Iba </p><h2>ref_id</h2> <p>b36 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2018 </p><h2>authors</h2> <p> Palakonda </p><h2>ref_id</h2> <p>b37 </p><h2>title</h2> <p>Pareto dominance-based moea with multiple ranking methods for many-objective optimization </p><h2>journal</h2> <p>IEEE </p><h2>year</h2> <p>2018 </p><h2>authors</h2> <p>V Palakonda; S Ghorbanpour; R Mallipeddi </p><h2>ref_id</h2> <p>b38 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>1997 </p><h2>authors</h2> <p>Varian Resnick </p><h2>ref_id</h2> <p>b39 </p><h2>title</h2> <p>Recommender systems </p><h2>journal</h2> <p>Communications of the ACM </p><h2>year</h2> <p>1997 </p><h2>authors</h2> <p>P Resnick; H R Varian </p><h2>ref_id</h2> <p>b40 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>2020 </p><h2>authors</h2> <p> Shang </p><h2>ref_id</h2> <p>b41 </p><h2>title</h2> <p>A survey on the hypervolume indicator in evolutionary multiobjective optimization </p><h2>journal</h2> <p>IEEE Transactions on Evolutionary Computation </p><h2>year</h2> <p>2020 </p><h2>authors</h2> <p>K Shang; H Ishibuchi; L He; L M Pang </p><h2>ref_id</h2> <p>b42 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>1959 </p><h2>authors</h2> <p> Sklar </p><h2>ref_id</h2> <p>b43 </p><h2>title</h2> <p>Fonctions de repartition an dimensions et leurs marges </p><h2>journal</h2> <p>Publ. inst. statist. univ </p><h2>year</h2> <p>1959 </p><h2>authors</h2> <p>M Sklar </p><h2>ref_id</h2> <p>b44 </p><h2>title</h2> <p> </p><h2>journal</h2> <p> </p><h2>year</h2> <p>1999 </p><h2>authors</h2> <p>Thiele Zitzler </p><h2>ref_id</h2> <p>b45 </p><h2>title</h2> <p>Multiobjective evolutionary algorithms: a comparative case study and the strength pareto approach </p><h2>journal</h2> <p>IEEE transactions on Evolutionary Computation </p><h2>year</h2> <p>1999 </p><h2>authors</h2> <p>E Zitzler; L Thiele </p><h1>figures</h1><h2>figure_label</h2> <p> </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_0 </p><h2>figure_caption</h2> <p>Points A, B, and C are not dominated by any other point. This means that po(A) = µ({y|yR * A}) = 0 and po(B) = po(C) = po(A) = 0. Point E is dominated by a single point C, that is po(E) = µ({C}) = 1. Finally, points F and D are dominated by two other points each, resulting in po(F ) = µ({C, E}) = 2 and po(D) = µ({A, B}) = 2.8 If R * is transitive, then for any k, T k is a selection. </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>2 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_1 </p><h2>figure_caption</h2> <p>Figure 2 :2Figure 1: Illustration of the relation R . </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>3 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_2 </p><h2>figure_caption</h2> <p>Figure 3 :3Figure 3: S is a selection that offers maximum choice but cannot be constructed from T k . Point are encoded with their measure and k-Pareto optimality: (µ, po). </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>4 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_3 </p><h2>figure_caption</h2> <p>Figure 4 :4Figure 4: Characterization of hyperbola. </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>5 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_4 </p><h2>figure_caption</h2> <p>Figure 5 :5Figure 5: Selections of the best 40% according to different sorting criteria. </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>7 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_5 </p><h2>figure_caption</h2> <p>Figure 7 :7Figure 7: Increase in hypervolume compared to NSGA-II. </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>9 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_6 </p><h2>figure_caption</h2> <p>Figure 9 :9Figure 9: An illustration of a cone-based relation R a . </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>10 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_7 </p><h2>figure_caption</h2> <p>Figure 10 :10Figure 10: An illustration of simple partial order relation. The values of po are shown by numbers, and the set of at least 2-Pareto optimal elements T 2 is delimited by a curve. </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p> </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_8 </p><h2>figure_caption</h2> <p>(a) Sorting by Pareto fronts. (b) Sorting by po. </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>11 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_9 </p><h2>figure_caption</h2> <p>Figure 11 :11Figure 11: Sorting points of a grid. The equivalence classes (fronts) are represented by numbers. </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>12 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_10 </p><h2>figure_caption</h2> <p>Figure 12 :12Figure 12: Selections of µ = 0.2 for the set X composed of points in the shaded area. </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>14 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_11 </p><h2>figure_caption</h2> <p>Figure 14 :14Figure 14: Average percentage of solutions dominated by other algorithms, θ. </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>15 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_12 </p><h2>figure_caption</h2> <p>Figure 15 :15Figure 15: Sorting duration as a function of population size for 10 knapsacks, n k = 10. </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p>16 </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_13 </p><h2>figure_caption</h2> <p>Figure 16 :16Figure 16: Separation between wealthy and non-wealthy states based on div # . </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p> </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_14 </p><h2>figure_caption</h2> <p>xR f y iff xR f1 y and . . . and xR f M y. Thus, the above CMOP can be represented by the lexicographic preorder relation xR cf y iff xR * c y or (xR = c y and xR f y), where xR * c y means "xR c y and not yR c x", and xR = c y means "xR c y and yR c x". For the given R cf , constrained Pareto optimal solutions [Kumara et al., 2020] are solutions that are not Pareto dominated by any other solution. </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p> </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_15 </p><h2>figure_caption</h2> <p>Ppo(x) = P ({y|yR * cf x}), = P ({y|yR cf x} -P ({y|yR = cf x}), = P ({y|yR cf x} -0, = P ({y|yR * c x} ∪ {y|yR = c x and yR f x}), = P ({y|yR * c x}) + P ({y|yR = c x and yR f x}).Thus, if x satisfies all constraints, which means xR = c (0, . . . , 0, a ng+1 , . . . , a ng+nh ), then po(x) = P ({y|yR = c x and yR f x}), = P ({y|yR = c (0, . . . , 0, a ng+1 , . . . , a ng+nh )})P ({y|yR f x}).Now, let F i (z) = P ({y|f i (y) ≤ z}) be the cumulative probability distribution of f i . Then,P ({y|yR f x}) (y|f i (y) ≤ f i (x)). = i∈{1,...,M } F i (f i (x)).Otherwise, if at least one constraint is not satisfied by x, then P ({y|yR = c x}) = 0 and po(x) = P ({y|yR * c x}) = P ({y|yR c x}) = i∈{1,...,ng} G i (g(x)) j∈{ng+1,...,ng+nh} P (y|yR hj x). </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p> </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_16 </p><h2>figure_caption</h2> <p>k |z≤hj (x k )≤bj } ps if z < a j , #({x k |aj ≤hj (x k )≤bj ]} ps if a j ≤ z ≤ b j , #({x k |aj ≤hj (x k )≤z} ps if z > b j . </p><h2>figure_data</h2> <p> </p><h2>figure_label</h2> <p> </p><h2>figure_type</h2> <p>figure </p><h2>figure_id</h2> <p>fig_17 </p><h2>figure_caption</h2> <p>) are treated separately. Moreover, we have the estimation P ({y|yR = c (0, . . . , 0, a ng+1 , . . . , a ng+nh ) </p><h2>figure_data</h2> <p> </p><h1>formulas</h1><h2>formula_id</h2> <p>formula_0 </p><h2>formula_text</h2> <p>cho(A) = (µ × µ)({(x, y) ∈ A 2 |xRy = yRx}). </p><h2>formula_coordinates</h2> <p>[2.0, 315.0, 608.76, 196.31, 10.31] </p><h2>formula_id</h2> <p>formula_1 </p><h2>formula_text</h2> <p>po(x) = µ({y|yR * x}). </p><h2>formula_coordinates</h2> <p>[3.0, 63.0, 309.07, 233.99, 20.69] </p><h2>formula_id</h2> <p>formula_2 </p><h2>formula_text</h2> <p>T k = {x ∈ X| po(x) ≤ k} 8 . </p><h2>formula_coordinates</h2> <p>[3.0, 63.0, 456.21, 115.85, 11.22] </p><h2>formula_id</h2> <p>formula_3 </p><h2>formula_text</h2> <p>(x 1 , x 2 )R (y 1 , y 2 ) iff x 1 ≤ y 1 and x 2 ≤ y 2 , see Fig. 1. </p><h2>formula_coordinates</h2> <p>[3.0, 63.0, 552.04, 233.99, 9.65] </p><h2>formula_id</h2> <p>formula_4 </p><h2>formula_text</h2> <p>k that µ(T k ) = m. </p><h2>formula_coordinates</h2> <p>[4.0, 85.47, 251.83, 79.68, 9.65] </p><h2>formula_id</h2> <p>formula_5 </p><h2>formula_text</h2> <p>cho(S) = S (µ(S) -2 po(x))dµ(x).(1) </p><h2>formula_coordinates</h2> <p>[4.0, 103.84, 358.23, 193.16, 17.23] </p><h2>formula_id</h2> <p>formula_6 </p><h2>formula_text</h2> <p>cho(S) = µ(S) 2 -2(µ × µ)({(x, y) ∈ S 2 |xR * y}) = µ(S) 2 -2(µ × µ)({(x, y) ∈ S × X|yR * x}). </p><h2>formula_coordinates</h2> <p>[4.0, 67.02, 454.02, 225.96, 26.99] </p><h2>formula_id</h2> <p>formula_7 </p><h2>formula_text</h2> <p>(µ×µ)({(x, y) ∈ S × X|yR * x} = = S X 1 R * d(µ(y)) dµ(x) = S po(x)dµ(x), </p><h2>formula_coordinates</h2> <p>[4.0, 67.45, 510.64, 225.1, 41.32] </p><h2>formula_id</h2> <p>formula_8 </p><h2>formula_text</h2> <p>c(A) = A (µ(A) -2 po(x))dµ(x). </p><h2>formula_coordinates</h2> <p>[4.0, 107.87, 664.96, 144.26, 17.23] </p><h2>formula_id</h2> <p>formula_9 </p><h2>formula_text</h2> <p>c(T k ) = max A∈Σ,µ(A)≤µ(T k ) c(A). Moreover, if µ(A) ≤ µ(T k ) and c(A) = c(T k ), then A ⊆ T k almost-everywhere. </p><h2>formula_coordinates</h2> <p>[4.0, 315.0, 143.48, 234.0, 45.81] </p><h2>formula_id</h2> <p>formula_10 </p><h2>formula_text</h2> <p>A ∈ Σ, that c(A) ≤ c(T k ) if µ(A) ≤ µ(T k ). As A = (A ∩ T k ) ∪ (A \ T k ) and T k = (A ∩ T k ) ∪ (T k \ A), requiring c(A) ≤ c(T k ) is equivalent to requiring A\T k (µ(A)-2 po(x))dµ(x) ≤ T k \A (µ(T k )-2 po(x))dµ(x). </p><h2>formula_coordinates</h2> <p>[4.0, 314.45, 463.64, 244.24, 82.76] </p><h2>formula_id</h2> <p>formula_11 </p><h2>formula_text</h2> <p>(x) > k for x ∈ A \ T k , while po(x) ≤ k for x ∈ T k \ A. Therefore, A\T k (µ(A) -2 po(x))dµ(x) ≤ µ(A \ T k )(µ(A) -2k), </p><h2>formula_coordinates</h2> <p>[4.0, 315.0, 560.73, 234.0, 59.4] </p><h2>formula_id</h2> <p>formula_12 </p><h2>formula_text</h2> <p>T k \A (µ(T k ) -2 po(x))dµ(x) ≥ µ(T k \ A)(µ(T k ) -2k). (4) The constraint µ(A) ≤ µ(T k ) means that µ(A \ T k ) ≤ µ(T k \ A). Thus, we have µ(A \ T k )(µ(A) -2k) ≤ µ(T k \ A)(µ(T k ) -2k). (5) </p><h2>formula_coordinates</h2> <p>[4.0, 314.12, 655.45, 234.88, 85.4] </p><h2>formula_id</h2> <p>formula_13 </p><h2>formula_text</h2> <p>A ⊂ T k & µ(A) < µ(T k ) =⇒ cho(A) < cho(T k ), (6) </p><h2>formula_coordinates</h2> <p>[5.0, 68.11, 457.75, 228.89, 9.65] </p><h2>formula_id</h2> <p>formula_14 </p><h2>formula_text</h2> <p>x 1 , x 2 )R (y 1 , y 2 ) iff y1 2 ≤ x 1 ≤ y 1 and y2 2 ≤ x 2 ≤ y 2 . The sets T k fulfill the requirement of be- ing selections if R * satisfies the weakened transitivity condition xR * y =⇒ po(x) ≤ po(y). </p><h2>formula_coordinates</h2> <p>[5.0, 63.0, 567.4, 234.0, 47.01] </p><h2>formula_id</h2> <p>formula_15 </p><h2>formula_text</h2> <p>T * k ∪ A • (1, 1) • (1, 1) • (1, 0) • (4, 0) S </p><h2>formula_coordinates</h2> <p>[5.0, 266.22, 81.78, 261.47, 660.39] </p><h2>formula_id</h2> <p>formula_16 </p><h2>formula_text</h2> <p>Proposition 1. For any selection of at least k-Pareto optimal elements T k such that µ(T k ) < +∞ cho(T k ) = µ(T k ) 2 -2 [0,k] xd(po * µ)(x),(7) </p><h2>formula_coordinates</h2> <p>[5.0, 315.0, 320.19, 234.0, 55.95] </p><h2>formula_id</h2> <p>formula_17 </p><h2>formula_text</h2> <p>(po * µ)([a, b]) = µ(po -1 ([a, b])). </p><h2>formula_coordinates</h2> <p>[5.0, 315.0, 397.71, 137.27, 10.31] </p><h2>formula_id</h2> <p>formula_18 </p><h2>formula_text</h2> <p>ω x R Ω ω y iff X 1 (ω x ) ≤ X 1 (ω y ) and X 2 (ω x ) ≤ X 2 (ω y ). </p><h2>formula_coordinates</h2> <p>[5.0, 315.21, 491.81, 233.59, 9.65] </p><h2>formula_id</h2> <p>formula_19 </p><h2>formula_text</h2> <p>x y x 1 x 2 = 1 5 (0, 0) (1, 1) (a) </p><h2>formula_coordinates</h2> <p>[6.0, 69.88, 75.99, 102.08, 124.24] </p><h2>formula_id</h2> <p>formula_20 </p><h2>formula_text</h2> <p>T max 2 5 (0, 0) (1, 1) (b) y is preferable to x. </p><h2>formula_coordinates</h2> <p>[6.0, 184.98, 75.99, 102.08, 124.24] </p><h2>formula_id</h2> <p>formula_21 </p><h2>formula_text</h2> <p>G : Ω → [0, 1] 2 , ω x → (F 1 (x 1 ), F 2 (x 2 )). </p><h2>formula_coordinates</h2> <p>[6.0, 359.93, 94.47, 144.15, 23.18] </p><h2>formula_id</h2> <p>formula_22 </p><h2>formula_text</h2> <p>(G * P )([0, a 1 ] × [0, a 2 ]) = C(a 1 , a 2 ). </p><h2>formula_coordinates</h2> <p>[6.0, 315.0, 326.35, 166.37, 9.65] </p><h2>formula_id</h2> <p>formula_23 </p><h2>formula_text</h2> <p>F (x 1 , x 2 ) = C(F 1 (x 1 ), F 2 (x 2 )). </p><h2>formula_coordinates</h2> <p>[6.0, 315.0, 398.08, 133.93, 9.65] </p><h2>formula_id</h2> <p>formula_24 </p><h2>formula_text</h2> <p>R Ω P (T k ) = k -k ln(k), cho(T k ) = (k -k ln(k)) 2 -k 2 1 2 -ln k . </p><h2>formula_coordinates</h2> <p>[6.0, 340.53, 428.93, 203.89, 59.13] </p><h2>formula_id</h2> <p>formula_25 </p><h2>formula_text</h2> <p>G(T k ) = {(x 1 , x 2 ) ∈ [0, 1] 2 |x 1 x 2 ≤ k}. </p><h2>formula_coordinates</h2> <p>[6.0, 315.0, 526.69, 234.0, 21.61] </p><h2>formula_id</h2> <p>formula_26 </p><h2>formula_text</h2> <p>P (po -1 (] -∞, x])) = P (T x ), = (x1,x2)∈[0,1] 2 (G(T x )) = x -x ln(x). </p><h2>formula_coordinates</h2> <p>[6.0, 332.44, 570.07, 199.13, 40.44] </p><h2>formula_id</h2> <p>formula_27 </p><h2>formula_text</h2> <p>cho(T k ) = (k -k ln(k)) 2 -2 k 0 x(-ln(x))dx, = (k -k ln(k)) 2 -k 2 (1/2 -ln k) . </p><h2>formula_coordinates</h2> <p>[6.0, 333.1, 642.1, 197.8, 39.46] </p><h2>formula_id</h2> <p>formula_28 </p><h2>formula_text</h2> <p>x 1 x 2 min(x 1 , x 2 ) max(x 1 , x 2 ) (x 1 + x 2 )/2 x 1 x 2 (po) </p><h2>formula_coordinates</h2> <p>[7.0, 69.04, 93.37, 210.91, 96.05] </p><h2>formula_id</h2> <p>formula_29 </p><h2>formula_text</h2> <p>min(x 1 , x 2 ) max(x 1 , x 2 ) (x 1 + x 2 )/2 x 1 x 2 (po) </p><h2>formula_coordinates</h2> <p>[7.0, 237.83, 245.24, 52.21, 48.5] </p><h2>formula_id</h2> <p>formula_30 </p><h2>formula_text</h2> <p>T min a = {(x 1 , x 2 ) ∈ [0, 1] 2 | min(x 1 , x 2 ) ≤ a}, T max a = {(x 1 , x 2 ) ∈ [0, 1] 2 | max(x 1 , x 2 ) ≤ a}. </p><h2>formula_coordinates</h2> <p>[7.0, 82.09, 456.01, 195.81, 27.9] </p><h2>formula_id</h2> <p>formula_31 </p><h2>formula_text</h2> <p>x 1 = x 2 . </p><h2>formula_coordinates</h2> <p>[7.0, 259.9, 619.94, 36.38, 9.65] </p><h2>formula_id</h2> <p>formula_32 </p><h2>formula_text</h2> <p>P (X i ≤ x i ) - i∈{1,...,m} P (X i = x i ). </p><h2>formula_coordinates</h2> <p>[7.0, 381.78, 518.23, 154.61, 20.53] </p><h2>formula_id</h2> <p>formula_33 </p><h2>formula_text</h2> <p>x 2 = f (x 1 ) such that -1 a ≤ df dx1 ≤ 0. </p><h2>formula_coordinates</h2> <p>[11.0, 63.0, 154.25, 158.41, 14.0] </p><h2>formula_id</h2> <p>formula_34 </p><h2>formula_text</h2> <p>T * k ⊆ T ⊆ T k . </p><h2>formula_coordinates</h2> <p>[13.0, 143.69, 554.54, 61.84, 12.55] </p><h2>formula_id</h2> <p>formula_35 </p><h2>formula_text</h2> <p>T * k ∪ A with A ⊆ T k \ T * k . </p><h2>formula_coordinates</h2> <p>[13.0, 428.99, 717.67, 120.01, 12.55] </p><h2>formula_id</h2> <p>formula_36 </p><h2>formula_text</h2> <p>div(T k ) = max S∈S,µ(S)=µ(T k ) div(S). </p><h2>formula_coordinates</h2> <p>[14.0, 236.49, 162.95, 139.01, 15.72] </p><h2>formula_id</h2> <p>formula_37 </p><h2>formula_text</h2> <p>τ = # con -# dis n 0 , </p><h2>formula_coordinates</h2> <p>[15.0, 265.78, 511.86, 80.44, 23.22] </p><h2>formula_id</h2> <p>formula_38 </p><h2>formula_text</h2> <p>τ = n 2 + n -2 cho # n 2 -n . </p><h2>formula_coordinates</h2> <p>[15.0, 259.47, 608.5, 93.06, 23.89] </p><h2>formula_id</h2> <p>formula_39 </p><h2>formula_text</h2> <p>τ ≈ 1 -2 div # . </p><h2>formula_coordinates</h2> <p>[15.0, 272.13, 661.58, 67.74, 9.65] </p><h2>formula_id</h2> <p>formula_40 </p><h2>formula_text</h2> <p>Minimize f 1 (x), f 2 (x), . . . , f M (x) subject to g i (x) ≤ 0, i ∈ {1, 2, ..., ng}, h j = 0, j ∈ {ng + 1, ng + 2, ..., ng + nh}, L k ≤ x k ≤ U k , h ∈ {1, . . . , D}, </p><h2>formula_coordinates</h2> <p>[17.0, 63.0, 146.49, 330.64, 87.08] </p><h2>formula_id</h2> <p>formula_41 </p><h2>formula_text</h2> <p>• f i represents the i-th objective function, </p><h2>formula_coordinates</h2> <p>[17.0, 72.96, 266.27, 185.86, 10.32] </p><h2>formula_id</h2> <p>formula_42 </p><h2>formula_text</h2> <p>xR gi y iff g i (x) ≤ 0 or g i (x) ≤ g i (y). (8 </p><h2>formula_coordinates</h2> <p>[17.0, 250.62, 424.22, 294.14, 24.59] </p><h2>formula_id</h2> <p>formula_43 </p><h2>formula_text</h2> <p>) </p><h2>formula_coordinates</h2> <p>[17.0, 544.76, 431.8, 4.24, 8.74] </p><h2>formula_id</h2> <p>formula_44 </p><h2>formula_text</h2> <p>xR hj y iff      h j (x) ∈ [a j , b j ] or h j (y) ≤ h j (x) ≤ a j or b j ≤ h j (x) ≤ h j (y).(9) </p><h2>formula_coordinates</h2> <p>[17.0, 229.91, 484.15, 319.09, 41.5] </p><h2>formula_id</h2> <p>formula_45 </p><h2>formula_text</h2> <p>xR fi y iff f i (x) ≤ f i (y). </p><h2>formula_coordinates</h2> <p>[17.0, 255.93, 617.01, 100.13, 9.65] </p><h2>formula_id</h2> <p>formula_46 </p><h2>formula_text</h2> <p>F (z) = #({x k |f (x k ) ≤ z}) ps . </p><h2>formula_coordinates</h2> <p>[18.0, 247.7, 604.29, 118.84, 22.31] </p><h2>formula_id</h2> <p>formula_47 </p><h2>formula_text</h2> <p>H j * (z) = </p><h2>formula_coordinates</h2> <p>[18.0, 201.22, 676.57, 40.4, 15.14] </p><h1>doi</h1> <p> </p>