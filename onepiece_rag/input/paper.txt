<h1>title</h1>kNN-Embed: Locally Smoothed Embedding Mixtures For Multi-interest Candidate Retrieval<h1>authors</h1>Ahmed El-Kishky; Thomas Markovich; Kenny Leung; Frank Portman; Aria Haghighi ⋆⋆; Ying Xiao ⋆⋆<h1>pub_date</h1>2023-08-05<h1>abstract</h1>Candidate retrieval is the first stage in recommendation systems, where a light-weight system is used to retrieve potentially relevant items for an input user. These candidate items are then ranked and pruned in later stages of recommender systems using a more complex ranking model. As the top of the recommendation funnel, it is important to retrieve a high-recall candidate set to feed into downstream ranking models. A common approach is to leverage approximate nearest neighbor (ANN) search from a single dense query embedding; however, this approach this can yield a low-diversity result set with many near duplicates. As users often have multiple interests, candidate retrieval should ideally return a diverse set of candidates reflective of the user's multiple interests. To this end, we introduce kNN-Embed, a general approach to improving diversity in dense ANN-based retrieval. kNN-Embed represents each user as a smoothed mixture over learned item clusters that represent distinct "interests" of the user. By querying each of a user's mixture component in proportion to their mixture weights, we retrieve a high-diversity set of candidates reflecting elements from each of a user's interests. We experimentally compare kNN-Embed to standard ANN candidate retrieval, and show significant improvements in overall recall and improved diversity across three datasets. Accompanying this work, we open source a large Twitter follow-graph dataset1, to spur further research in graph-mining and representation learning for recommender systems.<h1>sections</h1><h2>heading</h2>Introduction<h2>text</h2>Recommendation systems for online services such as e-commerce or social networks present users with suggestions in the form of ranked lists of items [5].<h2>publication_ref</h2>['b4']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Related Works<h2>text</h2>Traditionally, techniques for candidate retrieval rely on fast, scalable approaches to search large collections for similar sparse vectors [3,1]. Approaches apply indexing and optimization strategies to scale sparse similarity search. One such strategy builds a static clustering of the entire collection of items; clusters are retrieved based on how well their centroids match the query [25,20]. These methods either (1) match the query against clusters of items and rank clusters based on similarity to query or (2) utilize clusters as a form of item smoothing.
For embedding-based recommender systems [28], large-scale dense similarity search has been applied for retrieval. Some approaches proposed utilize hashing-based techniques such as mapping input and targets to discrete partitions and selecting targets from the same partitions as inputs [26]. With the advent of fast approximate nearest-neighbor search [21,13], dense nearest neighbor has been applied by recommender systems for candidate retrieval [5].
When utilizing graph-based embeddings for recommender systems [8], some methods transform single-mode embeddings to multiple modes by clustering user actions [23]. Our method extends upon this idea by incorporating nearest neighbor smoothing to address the sparsity problem of generating mixtures of embeddings for users with few engagements.
Smoothing via k-nearest-neighbor search has been applied for better language modeling [16] and machine translation [15]. We smooth low-engagement user representations by leveraging engagements from similar users.<h2>publication_ref</h2>['b2', 'b0', 'b24', 'b19', 'b27', 'b25', 'b20', 'b12', 'b4', 'b7', 'b22', 'b15', 'b14']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>kNN-Embed<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Preliminaries<h2>text</h2>Let U = {u 1 , u 2 , . . . u n } be the set of source entities (i.e., users in a recommender system) and I = {i 1 , i 2 , . . . i m } be the set of target entities (i.e., items in a recommender system). Let G constitute a bipartite graph representing the engagements between users (U) and items (I). For each user and item, we define a "relevance" variable in {0, 1} indicating an item's relevance to a particular user. An item is considered relevant to a particular user if a user, presented with an item, will engage with said item. Based on the engagements in G, each user, u j , is associated with a d-dimensional embedding vector u j ∈ R d ; similarly each target item i k is associated with an embedding vector i k ∈ R d . We call these the unimodal embeddings, and assume that they model user-item relevance p(relevance|u j , i k ) = f (u j , i k ) for a suitable function f .
Given the input user-item engagement graph, our goal is to learn mixtures of embeddings representations of users that better capture the multiple interests of a user as evidenced by higher recall in a candidate retrieval task.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Unimodal User and Item Embeddings:<h2>text</h2>While kNN-Embed presupposes a set of co-embedded user and item embeddings and is agnostic to the exact embedding technique used (the only constraint is that the embeddings must satisfy p(i k |u j ) = g(u j T i k ) for monotone g), for completeness we describe a simple approach we applied to co-embed users and items into the same space. We form a bipartite graph G of users and items, where an edge represents relevance (e.g., user follows content producer). We seek to learn an embedding vector (i.e., vector of learnable parameters) for each user (u j ) and item (i k ) in this bipartite graph; we denote these learnable embeddings for users and items as u j and i k respectively. A user-item pair is scored with a scoring function of the form f (u j , i k ). Our training objective seeks to learn u and i parameters that maximize a log-likelihood constructed from the scoring function for (u, i) ∈ G and minimize for (u, i) / ∈ G. For simplicity, we apply a dot product comparison between user and item representations. For a user-item pair e = (u j , i), this is defined by:
f (e) = f (u j , i k ) = u j ⊺ i k(1)
As seen in Equation 1, we co-embed users and items by scoring their respective embedded representations via dot product and perform edge (or link) prediction. We consume the input bipartite graph G as a set of user-item pairs of the form (u, i) which represent positive engagements between a user and item. The embedding training objective is to find user and item representations that are useful for predicting which users and items are linked via an engagement. While a softmax is a natural formulation to predict a user-item engagement, it is impractical due to the cost of computing the normalization over a large vocabulary of items. Following previous methods [22,10], negative sampling, a simplification of noise-contrastive estimation, can be used to learn the parameters u and i. We maximize the following negative sampling objective:
arg max u,i e∈G   log σ(f (e)) + e ′ ∈N (e) log σ(-f (e ′ ))  (2)
where:
N (u, i) = {(u, i ′ ) : i ′ ∈ I} ∪ {(u ′ , i) : u ′ ∈ U }. Equation 2 represents
the log-likelihood of predicting a binary "real" (edges in the network) or "fake" (negatively sampled edges) label. To maximize the objective, we learn u and i parameters to differentiate positive edges from negative, unobserved edges. Negative edges are sampled by corrupting positive edges via replacing either the user or item in an edge pair with a negatively sampled user or item. Following previous approaches, negative sampling is performed both uniformly and proportional to node prevalence in the training graph [4,18].<h2>publication_ref</h2>['b21', 'b9', 'b3', 'b17']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Smoothed Mixture of Embeddings<h2>text</h2>To use embeddings for candidate retrieval, we need a method of selecting relevant items given the input user. Ideally, we would like to construct a full distribution over all items for each user p(i k |u j ) and draw samples from it. The sheer number of items makes this difficult to do efficiently, especially when candidate retrieval strategies are meant to be light-weight. In practice, the most common method is to greedily select the top few most relevant items using an ANN search with the unimodal user embedding as query. A significant weakness of this greedy selection is that, by its nature, ANN search will return items that are similar not only to the user embedding, but also to each other; this drastically reduces the diversity of the returned items. This reduction in diversity is a side-effect of the way embeddings are trained -typically, the goal of training embeddings is to put users and relevant items close in Euclidean space; however, this also places similar users close in space, as well as similar items. We will repeatedly exploit this "locality implies similarity" property of embeddings in this paper to resolve this diversity issue.
Clustering Items: Since neighboring items are similar in the embedding space, if we apply a distance-based clustering to items, we can arrive at groupings that represent individual user preferences well. As such, we first cluster items using spherical k-means [6] where cluster centroids are placed on a high-dimensional sphere with radius one. Given these item clusters, instead of immediately collapsing the distribution p(i k |u j ) to a few items as ANN search does, we can write the full distribution p(i k |u j ) as a mixture over item clusters:
p(i k |u j ) = c p(c|u j ) • p(i k |u j , c)
where in each cluster, we learn a separate distribution over the items in the cluster p(i k |u j , c). Thus, we are modeling each user's higher level interests p(c|u), and then within each interest c, we can apply an efficient ANN-search strategy as before. In effect, we are interpolating between sampling the full preference distribution p(i k |u j ) and greedily selecting a few items in an ANN.<h2>publication_ref</h2>['b5']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Mixture of Embeddings via Cluster Engagements:<h2>text</h2>After clustering target entities, we learn p(c|u j ) through its maximum likelihood estimator (MLE):
p mle (c|u j ) = count(u i , c)/ c ′ ∈Mj count(u j , c ′ )(3)
where, count(u j , c) is the number of times u j has a relevant item in cluster c. For computational efficiency, we take M j to be u j 's top m most relevant clusters. We normalize these counts to obtain a proper cluster-relevance distribution.
Nearest Neighbor Smoothing: Unfortunately, we typically have few user-item engagements on a per-user basis; thus, while the MLE is unbiased and asymptotically efficient, it can also be high variance. To this end, we introduce a smoothing technique that once again exploits locality in the ANN search, this time for users. Figure 1 illustrates identifying k nearest-neighbors (K j ) to the query user u j 's, and leveraging the information from the neighbors' cluster engagements to augment the user's cluster relevance. We compute this distribution over item clusters by averaging the MLE probability for each nearest neighbor (item clusters that are not engaged with by a retrieved neighbor have zero probability).
p k N N (c|u j ) = 1 |K j | u ′ ∈Kj p mle (c|u ′ )(4)
We apply Jelinik-Mercer smoothing to interpolate between a user's MLE distribution with the aggregated nearest neighbor distribution [12].
p smoothed (c|u j ) = (1 -λ)p mle (c|u j ) + λp k N N (c|u j ),(5)
where λ ∈ [0, 1] represents how much smoothing is applied. It can be manually set or tuned on a downstream extrinsic task. Fig. 1: Example of retrieving two candidates. In an ANN, items 4 and 5 would be deterministically returned for user 1. In our proposed kNN-Embed, even though the distances to cluster 2 are larger, smoothing means that we will sometimes return items from that cluster, yielding more diverse items. Note in this case, we don't even require that user 1 has previously relevant items in cluster 2.
Sampling within Clusters Within each cluster there are many ways to retrieve items on a per user basis. A simple, but appealing, strategy is to represent each user as a normalized centroid of their relevant items in that cluster:
centroid(c, u j ) = m∈R(c,uj ) i m ∥ m∈R(c,uj ) i m ∥ ,(6)
where R(c, u j ) is the set of relevant items for user u j in cluster c. However, since we are applying smoothing to the cluster probabilities p(c|u j ), it may be case that u j has zero relevant items in a given cluster. Hence, we smooth the user centroid using neighbor infomation to obtain the final user representation u c j :
u c j = (1 -λ) centroid(c, u j ) + λ |K j | u ′ ∈Kj p mle (c|u ′ ) centroid(c, u ′ )(7)
Equation 7 shows the kNN-smoothed user-specific embedding for cluster c. This embedding takes the user-specific cluster representations from Equation 6, and performs a weighted averaging proportionate to each user's contribution to p smoothed (c|u j ). The final vector is once again normalized to unit norm.<h2>publication_ref</h2>['b11']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Evaluation Datasets and Metrics<h2>text</h2>We evaluate on three datasets which we describe below:<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>HEP-TH Citation Graph:<h2>text</h2>This paper citation network is collected from Arxiv preprints from the High Energy Physics category [9]. The dataset consists of: 34,546 papers and 421,578 citations.<h2>publication_ref</h2>['b8']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>DBLP Citation Graph:<h2>text</h2>This paper citation network is collected from DBLP [24] and consists of 5,354,309 papers and 48,227,950 citation relationships.
Twitter Follow Graph: We curate Twitter user-follows-user (available via API) by first selecting a number of 'highly-followed' users that we refer to as 'content producers'; these content producers serve as 'items' in our recommender systems terminology. We then sampled users that follow these content producer accounts. All users are anonymized with no other personally identifiable information (e.g., demographic features) present. Additionally, the timestamp of each follow edge was mapped to an integer that respects date ordering, but does not provide any information about the date that follow occurred. In total, we have 261M edges and 15.5M vertices, with a max-degree of 900K and a min-degree of 5. We hope that this dataset will be of useful to the community as a test-bed for large-scale retrieval research.<h2>publication_ref</h2>['b23']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Metrics:<h2>text</h2>We evaluate kNN-Embed on three aspects: (1) the recall (2) diversity and (3) goodness of fit of retrieved candidates. Below, we formalize these metrics.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Recall@K:<h2>text</h2>The most natural (and perhaps most important) metric for computing the efficacy of various candidate retrieval strategies is Recall@K. This metric is given by considering a fixed number of top candidates yield by a retrieval system (up to size K) and measuring what percent of these candidates are heldout relevant candidates. The purpose of most candidate retrieval systems is to collect a high-recall pool of items for further ranking, and thus recall is a relevant metric to consider. Additionally, recall provides an indirect way to measure diversity -to achieve high recall, one is obliged to return a large fraction of all relevant documents, which simple greedy ANN searches can struggle with.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Diversity:<h2>text</h2>To evaluate the diversity among the retrieved candidates, we measure the spread in the embeddings of the retrieved candidates by calculating the average distance retrieved candidates are from their centroid. The underlying idea is that when 'locality implies similarity'; as a corollary, if candidates are further in Euclidean distance, then they are likely to be different. As such, for a given set of candidates C, we compute diversity D as follows:
D(C) = 1 |C| i k ∈C ∥i k -î∥(8)
where C denotes the set of retrieved candidates and î = i k ∈C i k /|C| is the mean of the unimodal embeddings of the retrieved candidates.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Goodness of Fit:<h2>text</h2>In addition to diversity of retrieved items, we need to ensure that a user's mixture representation is an accurate model of their intereststhat is the mixture of embeddings identifies points in the embedding space where relevant items lie. Thus, we compare held out relevant items to the user's mixture representation we use to query. We measure this "goodness of fit" by computing the Earth Mover's Distance (EMD) [19] between a uniform distribution over a user's relevant items and the user's mixture distribution. The EMD measures the distance between two probability distributions over a metric space [17,7]. We measure the distance between a user's cluster distribution (e.g., Equation 3 and Equation 4), to a uniform distribution over a held-out set of relevant items: p(i|u j ) over a Euclidean space. We compute EMD by soft assigning all held-out relevant items of a user to clusters, minimizing the sum of item-cluster distances, with the constraint that the sum over soft assignments matches p(c|u j ). As seen in Figure 2, with standard unimodal representations, a single embedding vector is compared to the held-out items and the goodness of fit is the distance between the item embeddings and the singular user embedding. In comparison, for mixture representations (Figure 2, each user multiple user embeddings who each have fractional probability mass that in total sums to 1. The goodness of fit is then the distance achieved by allocating the mass in each item to the closest user embedding cluster with available probability mass. Observing unimodal representations in Fig. 2, a single unimodal embedding is situated in the embedding space and compared to held-out relevant items.
As shown, some held-out items are close to the unimodal embedding, while others are further away. In contrast, for mixture representations, each user has multiple user-embeddings and each of these embeddings lies close to a cluster of relevant items. The intuition is that if a user has multiple item clusters they are interested in, multiple user embeddings can better capture these interests.  <h2>publication_ref</h2>['b18', 'b16', 'b6']<h2>figure_ref</h2>['fig_0', 'fig_0', 'fig_0']<h2>table_ref</h2>[]<h2>heading</h2>Experiments<h2>text</h2>Experimental Setup: For our underlying ANN-based candidate retrieval system, we start by creating a bipartite graph between source entities and target entities for each dataset, with each edge representing explicit relevance between items (e.g., citing paper cites cited paper or user follows content producer). We then learn unimodal 100-dimensional embeddings for users and items by training over 20 epochs and cluster them via spherical k-means over 20 epochs [2].
Evaluation Task: We evaluate three candidate retrieval strategies -baseline ANN with unimodal embeddings (which is how most ANN-based candidate retrieval systems work), mixture of embeddings with no smoothing [23], and mixture of embeddings with smoothing (i.e., kNN-Embed). For each strategy, we compute the Recall@K, diversity, and fit in a link prediction task.<h2>publication_ref</h2>['b1', 'b22']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Research Hypotheses:<h2>text</h2>We explore two research hypotheses (as well as achieve some understanding of the hyperparameters): (1) Unimodal embeddings miss many relevant items due to the similarity of retrieved items. Mixtures yield more diverse and higher recall candidates. ( 2) Smoothing, by using information from neighboring users, further improves the recall of retrieved items. Approach R@10 R@20 R@50 R@10 R@20 R@50 R@10 R@20 R@50 Unimodal 20.0% 30.0% 45.7% 9.4% 13.9% 21.6% 0.58% Recall of unimodal vs mixture vs kNN-Embed-higher is better. HEP-TH (λ = 0.8, 2000 clusters, 5 embeddings). DBLP (λ = 0.8, 10000 clusters, 5 embeddings). Twitter-Follow (λ = 0.8, 40000 clusters, 5 embeddings).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Recall:<h2>text</h2>In Table 1, we report results when evaluating recall on citation prediction tasks. Results support the first hypothesis that unimodal embeddings may miss relevant items if they don't lie close to the user in the shared embedding space. Mixture of embeddings with no smoothing, yields a 14% relative improvement in R@10 for for HEP-TH, and 16% relative improvement for DBLP. Our second hypothesis (2) posits that data sparsity can lead to sub-optimal mixtures of embeddings, and that nearest-neighbor smoothing can mitigate this. Our experiments support this hypothesis, as we see a 25% relative improvement for HEP-TH in R@10, and 35% for DBLP and when using kNN-Embed. We see similar significant improvements over baselines in R@20 and R@50. For Twitter-Follow, the improvements in recall are dramatic -534% in relative terms going from unimodal embeddings to a mixture of embeddings in R@10. We suspect this significant improvement is because Twitter-Follow simultaneously has a much higher average degree than HEP-TH and DBLP and the number of unique nodes is much larger. It is a more difficult task to embed so many items, from many different interest clusters, in close proximity to a user. As such, we see a massive improvement by explicitly querying from each user's interest clusters. Applying smoothing provides an additional 74% in relative terms, and similar behaviours are observed in R@20 and R@50.<h2>publication_ref</h2>['b1']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_2']<h2>heading</h2>Diversity:<h2>text</h2>We apply Equation 8 to retrieved candidates and measure the spread of retrieved candidates' embedding vectors. As seen in Table 2, the candidates from unimodal retrieval are less diverse than candidates retrieved via multiple queries from mixture representations. This verifies our first research hypothesis that unimodal embeddings may retrieve many items that are clustered closely together as a by-product of ANN retrieval (i.e., diversity and recall is low). However, multiple queries from mixtures of embeddings broadens the search spatially; retrieved items are from different clusters, which are more spread out from each other. kNN-Embed (i.e., smooth mixture retrieval) results in slightly less diverse candidates than unsmoothed mixture retrieval. We posit that this is due to the high-variance of the maximum likelihood estimator of the p mle (c|u j ) multinomial (Equation 3). While this high-variance may yield more diverse candidates, this yields less relevant candidates as seen in Table 1 where kNN-Embed consistently yields better recall than unsmoothed mixture retrieval. While high diversity is necessary for high recall, it is insufficient on its own. <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_4', 'tab_2']<h2>heading</h2>Goodness of Fit:<h2>text</h2>We evaluate how well unimodal, mixture, and smoothed mixture embeddings model a user's interests. The main idea is that the better fit a user representation is, the closer it will be to the distribution of held out relevant items for that user. As seen in Table 3, the results validate the idea that unimodal user embeddings do not model user interests as well as mixtures over multiple embeddings. Multiple embeddings yield a significant EMD improvement over a single embedding vector when evaluated on held-out items. Smoothing further decreases the EMD which we posit is due to the smoothed embedding mixtures being lower-variance estimates as they leverage engagement data from similar users in constructing the representations. These results suggest that the higher recall of smoothed mixtures is due to better user preferences modeling. <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_5']<h2>heading</h2>Hyper-parameter Sensitivity Analysis:<h2>text</h2>We focus on recall as the sine qua non of candidate retrieval problems and analyze hyper-parameters on HEP-TH. In Figure 3a, we vary the smoothing parameter λ (same parameter for both the mixture probabilities and the cluster centroids) and see heavy smoothing improves performance significantly. This likely stems from the sparsity of HEP-TH where most papers have only a few citations. In Figure 3b, we vary the number of embeddings (i.e., the mixture size) and notice improved performance saturating at six mixture components. Out of all the hyperparameters, this seems to be the critical one in achieving high recall. In practice, latency constraints can be considered when selecting the number of embeddings per user, explicitly making the trade-off between diversity and latency.   <h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_3', 'fig_3']<h2>table_ref</h2>[]<h2>heading</h2>Conclusions<h2>text</h2>We present kNN-Embed, a method of transforming single user dense embeddings, into mixtures of embeddings, with the goal of better modeling user interests, increasing retrieval recall and diversity. This multi-embedding scheme represents a source entity with multiple distinct topical affinities by globally clustering items and aggregating the source entity's engagements with clusters. Recognizing that user-item engagements may often be sparse, we propose a nearest-neighbor smoothing to enrich these mixture representation. Our smoothed mixture representation better models user preferences retrieving a diverse set of candidate items reflective of a user's multiple interests. This significantly improves recall on candidate retrieval tasks on three datasets including Twitter-Follow, a dataset we curate and release to the community.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions<h2>journal</h2><h2>year</h2>2006<h2>authors</h2>A Andoni; P Indyk<h2>ref_id</h2>b1<h2>title</h2>k-means++: The advantages of careful seeding<h2>journal</h2><h2>year</h2>2006<h2>authors</h2>D Arthur; S Vassilvitskii<h2>ref_id</h2>b2<h2>title</h2>Scaling up all pairs similarity search<h2>journal</h2>WWW<h2>year</h2>2007<h2>authors</h2>R Bayardo; Y Ma; R Srikant<h2>ref_id</h2>b3<h2>title</h2>Translating embeddings for modeling multi-relational data<h2>journal</h2>NeurIPS<h2>year</h2>2013<h2>authors</h2>A Bordes; N Usunier; A Garcia-Duran; J Weston; O Yakhnenko<h2>ref_id</h2>b4<h2>title</h2>Deep neural networks for youtube recommendations<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>P Covington; J Adams; E Sargin<h2>ref_id</h2>b5<h2>title</h2>Concept decompositions for large sparse text data using clustering<h2>journal</h2>Machine learning<h2>year</h2>2001<h2>authors</h2>I Dhillon; D Modha<h2>ref_id</h2>b6<h2>title</h2>Massively multilingual document alignment with crosslingual sentence-mover's distance<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>A El-Kishky; F Guzmán<h2>ref_id</h2>b7<h2>title</h2>Twhin: Embedding the twitter heterogeneous information network for personalized recommendation<h2>journal</h2>KDD<h2>year</h2>2022<h2>authors</h2>A El-Kishky; T Markovich; S Park<h2>ref_id</h2>b8<h2>title</h2>Overview of the 2003 kdd cup<h2>journal</h2><h2>year</h2>2003<h2>authors</h2>J Gehrke; P Ginsparg; J Kleinberg<h2>ref_id</h2>b9<h2>title</h2>word2vec explained: deriving mikolov et al.'s negativesampling word-embedding method<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>Y Goldberg; O Levy<h2>ref_id</h2>b10<h2>title</h2>Embedding-based retrieval in facebook search<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>J Huang; A Sharma; S Sun; L Xia<h2>ref_id</h2>b11<h2>title</h2>Interpolated estimation of markov source parameters from sparse data<h2>journal</h2>PRIP<h2>year</h2>1980<h2>authors</h2>F Jelinek<h2>ref_id</h2>b12<h2>title</h2>Billion-scale similarity search with gpus<h2>journal</h2>BigData<h2>year</h2>2019<h2>authors</h2>J Johnson; M Douze; H Jégou<h2>ref_id</h2>b13<h2>title</h2>Candidate generation with binary codes for large-scale top-n recommendation<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>W Kang; J Mcauley<h2>ref_id</h2>b14<h2>title</h2>Nearest neighbor machine translation<h2>journal</h2>ICLR<h2>year</h2>2021<h2>authors</h2>U Khandelwal; A Fan; D Jurafsky; L Zettlemoyer; M Lewis<h2>ref_id</h2>b15<h2>title</h2>Generalization through memorization: Nearest neighbor language models<h2>journal</h2>ICLR<h2>year</h2>2020<h2>authors</h2>U Khandelwal; O Levy; D Jurafsky; L Zettlemoyer; M Lewis<h2>ref_id</h2>b16<h2>title</h2>From word embeddings to document distances<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>M Kusner; Y Sun; N Kolkin; K Weinberger<h2>ref_id</h2>b17<h2>title</h2>Pytorch-biggraph: A large-scale graph embedding system<h2>journal</h2>MLSys<h2>year</h2>2019<h2>authors</h2>A Lerer; L Wu; J Shen; T Lacroix; L Wehrstedt; A Bose; A Peysakhovich<h2>ref_id</h2>b18<h2>title</h2>The earth mover's distance is the mallows distance: Some insights from statistics<h2>journal</h2><h2>year</h2>2001<h2>authors</h2>E Levina; P Bickel<h2>ref_id</h2>b19<h2>title</h2>Cluster-based retrieval using language models<h2>journal</h2><h2>year</h2>2004<h2>authors</h2>X Liu; B Croft<h2>ref_id</h2>b20<h2>title</h2>Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs<h2>journal</h2>TPAMI<h2>year</h2>2018<h2>authors</h2>Y Malkov; D Yashunin<h2>ref_id</h2>b21<h2>title</h2>Distributed representations of words and phrases and their compositionality<h2>journal</h2>NeurIPS<h2>year</h2>2013<h2>authors</h2>T Mikolov; I Sutskever; K Chen; G Corrado; J Dean<h2>ref_id</h2>b22<h2>title</h2>Pinnersage: multi-modal user embedding framework for recommendations at pinterest<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>A Pal; C Eksombatchai; Y Zhou<h2>ref_id</h2>b23<h2>title</h2>Arnetminer: extraction and mining of academic social networks<h2>journal</h2><h2>year</h2>2008<h2>authors</h2>J Tang; J Zhang; L Yao; J Li; L Zhang; Z Su<h2>ref_id</h2>b24<h2>title</h2>Document clustering: An evaluation of some experiments with the cranfield 1400 collection<h2>journal</h2><h2>year</h2>1975<h2>authors</h2>C Van Rĳsbergen; W Bruce<h2>ref_id</h2>b25<h2>title</h2>Label partitioning for sublinear ranking<h2>journal</h2><h2>year</h2><h2>authors</h2>J Weston; A Makadia; H Yee<h2>ref_id</h2>b26<h2>title</h2>Practical diversified recommendations on youtube with determinantal point processes<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>M Wilhelm; A Ramanathan; A O Bonomo<h2>ref_id</h2>b27<h2>title</h2>Collaborative knowledge base embedding for recommender systems<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>F Zhang; N Yuan; D Lian; X Xie; W Ma<h1>figures</h1><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Fig. 2 :2Fig. 2: Goodness of fit of unimodal representation vs mixture representation.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Finally, in Figure we vary the number of k-means clusters; recall peaks at k = 2500 and then decreases. HEP-TH is a small dataset with only 34,546 items; it is likely that generating a very large number of clusters leads to excessively fine-grained and noisy sub-divisions of the items.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>(a) Varying lambda -R@50. (b) Varying mixtures R@20. (c) Varying clusters R@20.<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>Fig. 3 :3Fig. 3: We analyze the effect of three important hyper-parameters: (1) the λ smoothing (2) the number of embeddings in the mixture (3) the number of clusters for candidate retrieval in the HEP-TH dataset.<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>Recall of Retrieved Candidates<h2>figure_data</h2>HEP-THDBLPTwitter-Follow<h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_3<h2>figure_caption</h2><h2>figure_data</h2>1.02% 2.06%Mixture22.7% 33.4% 49.3% 10.9% 16.1% 25.1% 3.70% 5.53% 8.79%kNN-Embed<h2>figure_label</h2>2<h2>figure_type</h2>table<h2>figure_id</h2>tab_4<h2>figure_caption</h2>Diversity of Retrieved Candidates<h2>figure_data</h2>HEP-THDBLPTwitter-FollowApproachD@10 D@20 D@50 D@10 D@20 D@50 D@10 D@20 D@50Unimodal0.490.540.610.430.460.510.380.400.43Mixture0.580.630.680.510.560.600.560.540.58kNN-Embed0.540.600.660.460.520.570.470.520.55<h2>figure_label</h2>3<h2>figure_type</h2>table<h2>figure_id</h2>tab_5<h2>figure_caption</h2>Goodness of fit between user and held-out items as measured by earth mover's distance over a Euclidean embedding space. Lower EMD is better.<h2>figure_data</h2>ApproachHEP-TH DBLP Twitter-FollowUnimodal0.8970.8891.018Mixture0.8380.8300.952kNN-Embed0.8110.8080.940<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>f (e) = f (u j , i k ) = u j ⊺ i k(1)<h2>formula_coordinates</h2>[4.0, 255.5, 140.16, 225.09, 10.91]<h2>formula_id</h2>formula_1<h2>formula_text</h2>arg max u,i e∈G   log σ(f (e)) + e ′ ∈N (e) log σ(-f (e ′ ))  (2)<h2>formula_coordinates</h2>[4.0, 201.77, 304.72, 278.82, 34.15]<h2>formula_id</h2>formula_2<h2>formula_text</h2>N (u, i) = {(u, i ′ ) : i ′ ∈ I} ∪ {(u ′ , i) : u ′ ∈ U }. Equation 2 represents<h2>formula_coordinates</h2>[4.0, 168.42, 351.15, 312.17, 10.87]<h2>formula_id</h2>formula_3<h2>formula_text</h2>p(i k |u j ) = c p(c|u j ) • p(i k |u j , c)<h2>formula_coordinates</h2>[5.0, 236.41, 215.17, 142.54, 19.61]<h2>formula_id</h2>formula_4<h2>formula_text</h2>p mle (c|u j ) = count(u i , c)/ c ′ ∈Mj count(u j , c ′ )(3)<h2>formula_coordinates</h2>[5.0, 212.65, 352.54, 267.94, 22.13]<h2>formula_id</h2>formula_5<h2>formula_text</h2>p k N N (c|u j ) = 1 |K j | u ′ ∈Kj p mle (c|u ′ )(4)<h2>formula_coordinates</h2>[5.0, 233.19, 551.8, 247.4, 26.8]<h2>formula_id</h2>formula_6<h2>formula_text</h2>p smoothed (c|u j ) = (1 -λ)p mle (c|u j ) + λp k N N (c|u j ),(5)<h2>formula_coordinates</h2>[5.0, 197.78, 621.4, 282.81, 10.32]<h2>formula_id</h2>formula_7<h2>formula_text</h2>centroid(c, u j ) = m∈R(c,uj ) i m ∥ m∈R(c,uj ) i m ∥ ,(6)<h2>formula_coordinates</h2>[6.0, 230.41, 392.43, 250.18, 26.85]<h2>formula_id</h2>formula_8<h2>formula_text</h2>u c j = (1 -λ) centroid(c, u j ) + λ |K j | u ′ ∈Kj p mle (c|u ′ ) centroid(c, u ′ )(7)<h2>formula_coordinates</h2>[6.0, 163.25, 489.65, 317.34, 26.8]<h2>formula_id</h2>formula_9<h2>formula_text</h2>D(C) = 1 |C| i k ∈C ∥i k -î∥(8)<h2>formula_coordinates</h2>[7.0, 255.96, 534.63, 224.63, 27.47]<h1>doi</h1><h1>title</h1>𝜅HGCN:Tree-likeness Modeling via Continuous and Discrete Curvature Learning<h1>authors</h1>Menglin Yang; Min Zhou; Lujia Pan; Irwin King; Long Beach<h1>pub_date</h1>2023-07-17<h1>abstract</h1>The prevalence of tree-like structures, encompassing hierarchical structures and power law distributions, exists extensively in realworld applications, including recommendation systems, ecosystems, financial networks, social networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness modeling has garnered considerable attention owing to its exponential growth volume. Compared to the flat Euclidean space, the curved hyperbolic space provides a more amenable and embeddable room, especially for datasets exhibiting implicit tree-like architectures. However, the intricate nature of real-world tree-like data presents a considerable challenge, as it frequently displays a heterogeneous composition of tree-like, flat, and circular regions. The direct embedding of such heterogeneous structures into a homogeneous embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions. To mitigate the aforementioned shortage, this study endeavors to explore the curvature between discrete structure and continuous learning space, aiming at encoding the message conveyed by the network topology in the learning process, thereby improving tree-likeness modeling. To the end, a curvature-aware hyperbolic graph convolutional neural network, 𝜅HGCN, is proposed, which utilizes the curvature to guide message passing and improve long-range propagation. Extensive experiments on node classification and link prediction tasks verify the superiority of the proposal as it consistently outperforms various competitive models by a large margin.<h1>sections</h1><h2>heading</h2>INTRODUCTION<h2>text</h2>Tree-like structures refer to networks, systems, or data organizations that resemble a tree in their architecture, with nodes branching out from the root into multiple levels. They are widely observed in various real-world domains [1,3,44,53,64,101], such as recommendation systems, financial networks, and social networks.
Recently, the utilization of hyperbolic space for modeling treelike structures has garnered substantial attention [11,24,45,56,57,61,65,90,96]. Compared with the zero curvature Euclidean space, one key property of negative curvature hyperbolic space is that it expand exponentially, making it can be considered as a continuous tree and vice versa (as shown in Figure 1). In other words, hyperbolic space allows for the efficient representation of a tree-like structure, as it enables nodes to be spread apart as they move away from the root, preventing crowding and overlapping of nodes as is commonly observed in Euclidean space. Additionally, hyperbolic space allows for exponential growth in the number of nodes in a given area, which is well-suited for modeling the exponential growth of trees.
However, the real-world dataset often deviates from a pure tree configuration and manifests in a labyrinthine complexity, posing large challenges for tree-likeness modeling within hyperbolic space [82,103]. For instance, biological taxonomies, which depict the hierarchical relationships between different species from a for 𝑇 2 . By incorporating curvature 𝜅 into the neighboring aggregation, the detection of local structures is improved. This is achieved by assigning asymmetric weights (𝜅 1 𝑎,𝑜 vs 𝜅 1  𝑎,𝑏 ) to nodes at different levels and results in larger values for nodes in a circular shape (𝜅 1  𝑎,𝑜 vs 𝜅 2 𝑎,𝑜 and 𝜅 1 𝑎,𝑐 vs 𝜅 2 𝑎,𝑐 ).
global view, often exhibit both extensive local flat regions where multiple species are closely related and local circular regions where species connections are less defined. The local structure of a treelike graph exhibits a heterogeneous blend of tree-like, flat, and circular patterns, resulting in difficulties in uniformly embedding the data into a homogeneous embedding space and thereby engendering structural biases and distortions.
To mitigate the limitations, this study seeks to examine the intersection between the discrete structure of the data and the continuous learning space. The aim is to encode the information inherent in the network topology in a manner that is both effective and imbued with structural inductive bias, thereby enhancing the performance of downstream tasks. From a geometric perspective, the quality of the embedding in geometric learning depends on the compatibility between the intrinsic graph structure and the embedding space [26]. In light of this principle, we employs the concept of curvature to guide tree-likeness modeling in hyperbolic space. As shown in Figure 2, the incorporation of curvature information offers a more comprehensive grasp of the local shape characteristics, facilitating the representation of the shape and contours of diverse regions within the learning space.
In Riemannian geometry, curvature measures the deviation of a geometric object from being flat, originally defined on continuous smooth manifolds [40]. Smooth manifolds with positive, zero, or negative curvature are spherical, Euclidean, and hyperbolic spaces, respectively. This concept has been extended to discrete objects such as graphs, where curvature describes the deviation of a local pair of neighborhoods from a "flat" case.
Graph curvature, analogous to curvature in the realm of continuous geometry, consists of Gaussian curvature, Ricci curvature, and mean curvature. These components have unique roles: Gaussian curvature quantifies the local curvature at vertices, Ricci curvature allocates curvature to the edges, and mean curvature provides an overall metric for the entire graph. In this work, we focus on Ricci curvature for graph convolution and edge-based filtering. Several definitions have been proposed about Ricci curvature, including Ollivier Ricci curvature [58], Forman Ricci curvature [21], Balanced Forman curvature [74]. Ricci curvature controls the overlap between two distance balls by considering the radii of the balls and the distance between their centers [33]. Furthermore, the lower bound of Ricci curvature can reveal valuable global geometric and topological information [4]. It is also an effective indicator of treelike, flat, and cyclic areas, making it well-suited for integration into hyperbolic space to capture asymmetries, biases, and hierarchies.
Overall, we put forward a novel framework: curvature-aware hyperbolic graph convolutional neural network (𝜅HGCN) for effectively modeling tree-like datasets with complex structures. Specifically, 𝜅HGCN leverages the discrete Ricci curvature to guide message passing and dynamically adapts the global continuous hyperbolic curvature. Through empirical evaluations on diverse datasets and tasks, we confirm the superiority of the 𝜅HGCN, as it consistently outperforms existing baselines by substantial margins. The major contributions of this work are summarized as follows:
• We design a novel hyperbolic geometric learning framework that encapsulates the graph Ricci curvature into the continuous embedding space, producing less distortion, powerful expressiveness, and topology-aware embeddings; • We present a new message technique for hyperbolic graph embedding, and we further prove that it produces a smaller (larger) embedding distance when larger (smaller) curvature is involved, which well handles the inconsistency between the local structure and global curvature of embedding space; • Extensive experiments demonstrate that the proposed model 𝜅HGCN achieves significant improvements over various baselines on link prediction and node classification tasks.<h2>publication_ref</h2>['b0', 'b2', 'b43', 'b52', 'b63', 'b100', 'b10', 'b23', 'b44', 'b55', 'b56', 'b60', 'b64', 'b89', 'b95', 'b81', 'b102', 'b25', 'b39', 'b57', 'b20', 'b73', 'b32', 'b3']<h2>figure_ref</h2>['fig_0', 'fig_1']<h2>table_ref</h2>[]<h2>heading</h2>RELATED WORK<h2>text</h2>For tree-likeness modeling, we mainly review the latest research techniques including graph neural networks and hyperbolic geometry. In addition to this, we also review the recent advancements in curvature and curvature-based learning.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Graph Neural Networks<h2>text</h2>Tree-structured data can often be represented as graphs. In recent years, graph neural networks (GNNs) have gained significant attention within the graph learning community. The main concept behind GNNs is a message-passing mechanism that aggregates information from neighboring nodes. GNNs have demonstrated remarkable performance in various tasks, including node classification, link prediction, graph classifications, and graph reconstruction [23,25,28,35,41,42,48,52,67,69,78,88,95,[98][99][100]. They have also found wide applications in recommender systems, anomaly detection, social networks analysis, and more [12-16, 20, 35, 49, 66, 68, 69, 92]. The majority of GNNs learn graphs in Euclidean space due to their computational advantages and intuitiveness. However, Euclidean models are limited in their ability to represent complex patterns in graph [9].<h2>publication_ref</h2>['b22', 'b24', 'b27', 'b34', 'b40', 'b41', 'b47', 'b51', 'b66', 'b68', 'b77', 'b87', 'b94', 'b97', 'b98', 'b99', 'b8']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Hyperbolic Geometry<h2>text</h2>Hyperbolic representation learning has recently garnered considerable attention [17,102]. Hyperbolic geometry has been recognized as a continuous tree [37], exhibiting properties such as low distortion and small generalization errors when modeling treelike structured data [63,72,73]. Its applications span various domains [50,61,90], including computer vision [5,30,34], natural language processing [8,10,36,51,56,57,62], recommender systems [14,70,80,81,87,91], graph learning [7,11,27,43,45,89,96] and more [84]. In the graph learning domain, recent works [11,43,45,47,89,92,96,97] have generalized graph neural networks to hyperbolic space and demonstrated impressive performance, particularly on tree-like data. Some studies [60,71,82,103] propose learning graph in different embedding spaces or product spaces. Furthermore, researchers have also explored the use of ultrahyperbolic geometry for graph learning [38,39,85,86]. However, many existing methods fail to consider the local heterogeneous structure of graphs, resulting in significant distortion and low-quality embeddings.<h2>publication_ref</h2>['b16', 'b101', 'b36', 'b62', 'b71', 'b72', 'b49', 'b60', 'b89', 'b4', 'b29', 'b33', 'b7', 'b9', 'b35', 'b50', 'b55', 'b56', 'b61', 'b13', 'b69', 'b79', 'b80', 'b86', 'b90', 'b6', 'b10', 'b26', 'b42', 'b44', 'b88', 'b95', 'b83', 'b10', 'b42', 'b44', 'b46', 'b88', 'b91', 'b95', 'b96', 'b59', 'b70', 'b81', 'b102', 'b37', 'b38', 'b84', 'b85']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Graph Curvature<h2>text</h2>Graph curvature, resembling curvature in continuous geometry, includes Gaussian curvature, Ricci curvature, and average curvature. Each of these elements serves a distinct purpose: Gaussian curvature measures local curvature at vertices, Ricci curvature assigns curvature to edges, and average curvature offers a global measure for the entire graph. Applications of graph curvatures span various domains in network alignment, congestion and vulnerability detection, community detection, and robustness analysis [32,54,55].
The recent work of curvature graph neural network (CurvGN) [93] introduced the notion of Ricci curvature into the field of graph learning. The study in [74] showed that edges with negative curvature can contribute to the over-squashing problem in graph embeddings. Coinciding with the announcement of the accepted papers for WWW 2023, we noted a parallel work by Fu et al. [22] that introduces the idea of class-aware Ricci curvature for addressing hierarchy-imbalance in node classification, while in our work, we aim to explore the integration of more generalized ricci curvature with hyperbolic graph convolution and curvature-based filtering mechanism to enhance the performance of HGCN for a more range of tasks, including node classification and link prediction.<h2>publication_ref</h2>['b31', 'b53', 'b54', 'b92', 'b73', 'b21']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>BACKGROUND<h2>text</h2>In this part, we first briefly review the necessary definitions of differential geometry, primarily concentrating on hyperbolic geometry. A thorough and in-depth explanation can be found in [40]. We also give a short introduction about Ollivier Ricci curvature (ORC), which is a generalized Ricci curvature tailored for discrete objects (e.g., graphs) 1 . The readers may refer to [58] for more details.<h2>publication_ref</h2>['b39', 'b57']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Riemannian Geometry<h2>text</h2>Manifold and Tangent Space. Riemannian geometry, a subfield of differential geometry, denoted as M with a Riemannian metric 𝑔. An 𝑛-dimensional manifold (M, 𝑔) represents a smooth and real space, essentially an extension of a 2-D surface to higher dimensions, that can be locally approximated by R 𝑛 . For any point x on M, we define a tangent space T x M, acting as the first-order approximation of M in the vicinity of x. This tangent space is an 𝑛-dimensional vector space that is isomorphic to R 𝑛 . The metric 𝑔 on the Riemannian 
𝐿(𝛾) = ∫ 𝛽 𝛼 ∥𝛾 ′ (𝑡)∥ 𝑔 𝑑𝑡. Then the distance of u, v ∈ M, 𝑑 M (u, v) = inf 𝐿(𝛾) where 𝛾 is a curve that 𝛾 (𝑎) = u, 𝛾 (𝑏) = v.
Maps and Parallel Transport. The maps define the relationship between the hyperbolic space and the corresponding tangent space. Given a point x ∈ M and a vector v ∈ T x M, a unique geodesic 𝛾 : [0, 1] → M exists, satisfying 𝛾 (0) = x, 𝛾 ′ (0) = v. The exponential map, symbolized as exp x : T x M → M, is defined such that exp x (v) = 𝛾 (1). Conversely, the logarithmic map, denoted as log x , acts as the inverse of exp x . Furthermore, the parallel transport 𝑃𝑇 x→y : T xM → T yM achieves the transportation from point x to point y, while ensuring the preservation of the metric tensors.
Hyperbolic Models. Hyperbolic geometry describes a curved space with negative curvature. There are several mathematically equivalent ways to model hyperbolic geometry that emphasize different properties, but our methods apply to hyperbolic geometry in general and are not limited to any particular model. Formulas for concepts such as distance, maps, and parallel transport are summarized in Appendix B.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Graph Curvature<h2>text</h2>Curvature is a fundamental concept in smooth spaces that has also generalized to discrete objects (e.g., graphs). There are several distinct notions of discrete Ricci curvature for graphs or networks, such as the Forman-Ricci curvature [21] and Ollivier-Ricci curvature [58]. Here we mainly focus on ORC since it is more geometrical [33,58]. Another reason is ORC builds a bridge between continuous geometry and discrete structures [2,76]. Definition 3.1 (Ollivier-Ricci Curvature). Let 𝐺 = (𝑉 , 𝐸) be a locally finite, connected, and simple graph (i.e., 𝐺 contains no multiple edges or self-loops), for any two distinct vertices 𝑣 1 , 𝑣 2 , the ORC of 𝑣 1 and 𝑣 2 is defined as
𝜅 (𝑣 1 , 𝑣 2 ) = 1 - 𝑊 (𝑚 𝑣 1 , 𝑚 𝑣 2 ) 𝑑 (𝑣 1 , 𝑣 2 ) ∈ (-2, 1),(1)
where 𝑑 (𝑣 
𝑊 (𝑚 1 , 𝑚 2 ) = inf 𝜋 𝑖,𝑗 ∈Π ∑︁ 𝑣 𝑖 ,𝑣 𝑗 ∈𝑉 𝜋 𝑖,𝑗 (𝑣 𝑖 , 𝑣 𝑗 )𝑑 (𝑣 𝑖 , 𝑣 𝑗 ),(2)
where 𝜋 𝑖,𝑗 : 𝑉 × 𝑉 → [0, 1] is a transport plan, i.e., the probability measure of the amount of mass transferred from 𝑣 𝑖 to 𝑣 𝑗 . Then to seek an optimal transference plan (𝜋) that is to minimize the total cost of moving from 𝑣 𝑖 to 𝑣 𝑗 such that for every 𝑣 𝑖 , 𝑣 𝑗 in 𝑉 satisfying
∑︁ 𝑣 𝑖 ∈𝑉 𝜋 𝑖,𝑗 (𝑣 𝑖 , 𝑣 𝑗 ) = 𝑚 1 ; ∑︁ 𝑣 𝑗 ∈𝑉 𝜋 𝑖,𝑗 (𝑣 𝑖 , 𝑣 𝑗 ) = 𝑚 2 . (3
)
𝑇 ℎ ′ ℍ 𝑑 ′ ,𝐾 ℍ 𝑑,𝐾 x 𝐻 ∈ ℝ 𝑛×𝑑 h 𝐻 ∈ ℝ 𝑛×𝑑 ′ ℍ 𝑑 ′ ,𝐾 x 𝐸 ∈ ℝ 𝑛×𝑑 𝒛 ∈ 𝐻 𝑛x𝑐 x 𝐻 𝑇 𝑜 ℍ 𝑑,𝐾 AGG 𝜅 (h 𝑖 𝐻 ) h 𝑖 𝐻 log h ′ 𝐾 (⋅) exp h ′ 𝐾 (⋅) ෨ h 𝑖 𝐻 ℍ 𝑑 ′ ,𝐾 𝑑 ′ 𝑑 ′ 𝑑 ′ 2𝑑 ′ , 1 𝜅 𝑖𝑗 𝛼 𝑖𝑗 h 𝑖 𝐻 h 𝑗 𝐻 h 𝑖 𝐻 h 𝑗 𝐻 ℍ Figure 3: Schematic of 𝜅HGCN. (1)
The simplified algorithm flow of our method: consists of hyperbolic projection, feature transform, and aggregation. After that, a readout layer is applied to the embeddings for either a node classification or link prediction task. ( 2) The visualization of neighborhood aggregation procedure: first project information to hyperbolic space for transformation, then map messages to the tangent space, perform the aggregation in the tangent space with the guide of discrete curvature (and attention), and then map back to the hyperbolic space. (3) The details of Ricci Curvature-aware aggregation and its combination with feature-based attention.
Definition 3.3 (Probability Measure). Given 𝐺 = (𝑉 , 𝐸), for a vertex 𝑣 𝑖 in 𝑉 , denote 𝑑 𝑣 𝑖 the degree of 𝑣 𝑖 and 𝑁 (𝑣) the neighbors of 𝑣, for any 𝑝 ∈ [0, 1], the probability measure 𝑚 𝑣 𝑖 on 𝑉 is defined as:
𝑚 𝑣 𝑖 =        𝑝, if 𝑣 = 𝑣 𝑖 1-𝑝 𝑑 𝑣 , if (𝑣 𝑖 ) ∈ 𝑁 (𝑣). 0, otherwise(4)
Geometric Intuition. ORC seeks the most efficient transportation plan that preserves mass between two probability measures, which may be solved using linear programming. Intuitively, transporting messages between two nodes whose neighbors are highly overlapping, such as two nodes in the same cluster, is costless. On the other hand, if two nodes are situated in distinct groups or clusters, information flow between them is difficult.<h2>publication_ref</h2>['b20', 'b57', 'b32', 'b57', 'b1', 'b75']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>METHODOLOGY<h2>text</h2>The proposed method, 𝜅HGCN, combines discrete and continuous curvatures to improve tree-like modeling in hyperbolic space. Our approach emphasizes the strengthening of message passing in nodes with high local graph curvature and the weakening of message propagation in nodes with low local curvature. This curvatureguided approach enhances the formation of hierarchies and reduces the impact of structural incompatibility on the modeling process.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>𝜅HGCN<h2>text</h2>Our approach, named 𝜅HGCN, presents a novel curvature-aware hyperbolic graph network model, as depicted in Figure 3. Building upon the foundation of HGCN [11], we implement graph convolution operations via the tangential method [11,45] space. However, it is worth mentioning that 𝜅HGCN is flexible and can be applied to non-tangential methods as well [97]. Similar to other GNN and HGNN models, 𝜅HGCN also comprises three fundamental modules: hyperbolic feature transformation, curvature-aware neighbor aggregation, and non-linear activation.<h2>publication_ref</h2>['b10', 'b10', 'b44', 'b96']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Hyperbolic Feature Transformation. Hyperbolic feature transformation is formulated as:<h2>text</h2>h ℓ,H 𝑖 = W ℓ ⊗ 𝜅 ℓ -1 x ℓ -1,H 𝑖 ⊕ 𝜅 ℓ -1 b ℓ ,(5)
where ℓ denotes the ℓ-th layer, W is the trainable matrix and b is the bias.
W ⊗ 𝜅 x := exp 𝜅 o (W log 𝜅 o (x)) and x ⊕ 𝜅 b := exp 𝜅
x (𝑃𝑇 𝜅 o→x (b)) are matrix-vector multiplication and bias translation operations in hyperbolic space, respectively. The superscript H denotes the hyperbolic feature. space of the origin:
hℓ,H 𝑖 = exp 𝜅 𝑙 -1 o ∑︁ 𝑗 ∈ N 𝑖 κ𝑖,𝑗 • log 𝜅 𝑙 -1 o (h ℓ,H 𝑗 ) .(6)
Here κ𝑖,𝑗 denotes the curvature for capturing the local structure, computed by softmax operation within the neighbors N 𝑖 (N 𝑖 contains the node self):
κ𝑖,𝑗 = softmax 𝑗 ∈ N (𝑖 ) MLP(𝜅 𝑖,𝑗 ) ,(7)
where 𝜅 𝑖,𝑗 is the raw ORC value, and MLP (Multilayer Perceptron) is employed to make the curvature more adaptive to the overall negative curvature. This approach is referred to as Curv. When it comes to the case where the topology information and node features are inconsistent to a certain degree, e.g., the network is quite sparse or depends more on the node features, inspired by [94], we propose a feature-attention enhanced aggregation (CurvAtt), which encodes node state into the curvature:
κ′ 𝑖,𝑗 = 𝑤 𝜅 κ𝑖,𝑗 + 𝑤 𝛼 𝛼 𝑖,𝑗 𝑤 𝜅 + 𝑤 𝛼 ,(8)
where 𝛼 𝑖,𝑗 is hyperbolic feature attention, 𝑤 𝜅 and 𝑤 𝛼 are the trainable parameters that adjust structure information and feature correlation with the initial value 1.0. The hyperbolic feature attention 𝛼 𝑖,𝑗 is defined as:
𝛼 𝑖,𝑗 = softmax 𝑗 ∈ N 𝑖 MLP(log 𝜅 o (h H 𝑖 )∥ log 𝜅 o (h H 𝑗 )) .(9)
4.1.3 Hyperbolic Non-linear Activation. After that, we apply a nonlinear activation:
x ℓ,H 𝑖 = 𝜎 𝜅 ℓ -1 ,𝜅 ℓ ( hℓ,H ) = exp 𝜅 ℓ o 𝜎 (log 𝜅 ℓ -1 o ( hℓ,H )) . (10
)
Geometric Intuition. The real-world tree-like graphs with heterogeneous local structures are inevitably distorted if we directly embed them into a homogeneous manifold. For instance, the embedding of quasi-cycle graphs such as 𝑛 × 𝑛 square lattices (zero curvature) and 𝑛-node cycles (positive curvature) incur at least a multiplicative distortion of 𝑂 (𝑛/log 𝑛) in hyperbolic space [79]. Graph Ricci curvature is able to mitigate this distortion. The geometric intuition is that the more positive the curvature is, the more two distance balls centered at nearby points overlap, and therefore, the cheaper it is to transport the mass from one to the other. Theoretical results show that with the increasing number of triangles involved in the linked pair (𝑖, 𝑗), the lower bound of curvature will be increased [33], as stated in Theorem 4.1. It is easy to understand because when the two vertices share many triangles, then the transportation distance should be smaller, and the curvature, therefore, is correspondingly larger. Theorem 4.1 (Lower bound of ORC [33]). On a locally finite graph, for any pair of neighboring vertices i, j, let #(𝑖, 𝑗) := number of triangles which include 𝑖, 𝑗 as vertices for 𝑖 ∼ 𝑗. Then, we have the inequality, saying that
𝜅 𝑖,𝑗 ≥ -1 - 1 𝑑 𝑖 - 1 𝑑 𝑗 - #(𝑖, 𝑗) 𝑑 𝑖 ∧ 𝑑 𝑗 + -1 - 1 𝑑 𝑖 - 1 𝑑 𝑗 - #(𝑖, 𝑗) 𝑑 𝑖 ∨ 𝑑 𝑗 + + #(𝑖, 𝑗) 𝑑 𝑖 ∨ 𝑑 𝑗 , (11
)
where 𝑠 + := max(𝑠, 0), 𝑠 ∨ 𝑡 := max(𝑠, 𝑡), 𝑎𝑛𝑑 𝑠 ∧ 𝑡 := min(𝑠, 𝑡).
In this study, we make a theoretical analysis in Theorem 4.2, which further demonstrates the relations of ORC and embedding distance, i.e., when a large curvature is involved within the linked node, the closer of their embedding distance, which thus mitigates the distortion. Theorem 4.2 (Embedding Distance w.r.t ORC). Let (𝑖, 𝑗) ∈ 𝐸 be the linked pair, h 𝑖 , h 𝑗 ∈ R 𝑑 be the node state in the tangent space, 𝑑 𝑖 be the degree of node 𝑖, and 𝐷 be the distance of node 𝑖 and node 𝑗 in the tangent space, that is
𝐷 = ∥h 𝑖 -h 𝑗 ∥,(12)
where ∥ • ∥ is the Euclidean norm. Define a large ORC as κ𝑖,𝑗 > max{1/𝑑 𝑖 , 1/𝑑 𝑗 } and a small ORC as κ𝑖,𝑗 < min{1/𝑑 𝑖 , 1/𝑑 𝑗 }. Then, when the large ORC is involved, their embedding distance will get smaller if using 𝜅HGCN. On the contrary, when the small ORC is involved, their embedding distance will get larger.
Proof. In the following, we use 𝐷 𝑙 and 𝐷 𝑠 to denote the distance when large and small curvature κ are involved, respectively. The main idea is that when there is a large curvature involved, the node distance will be decreased compared with the original case (degreebased aggregation), that is 𝐷 𝑙 < 𝐷. At the same time, when there is a small curvature involved, the node distance will increase, that is 𝐷 𝑠 > 𝐷.
(1) When a large curvature (i.e., 𝜅 𝑖,𝑗 > max(1/𝑑 𝑖 , 1/𝑑 𝑗 )) is involved, more messages will be transferred, and we decompose the embedding, taking h 𝑖 as an example, into two components: one is from original h 𝑖 and another is the incremental parts from h 𝑗 , then
𝐷 𝑙 = ∥(h 𝑖 + 𝛼 𝑖 h 𝑗 ) -(h 𝑗 + 𝛼 𝑗 h 𝑖 )∥ = ∥(h 𝑖 -𝛼 𝑗 h 𝑖 ) -(h 𝑗 -𝛼 𝑖 h 𝑗 )∥,(13)
where 𝛼 𝑖 (𝛼 𝑗 ) is the difference between κ𝑖,𝑗 and 1/𝑑 𝑖 ( 1/𝑑 𝑗 ), i.e., 𝛼 𝑖 = κ𝑖,𝑗 -1/𝑑 𝑖 , 𝛼 𝑗 = κ𝑖,𝑗 -1/𝑑 𝑗 . Since κ𝑖,𝑗 > max(1/𝑑 𝑖 , 1/𝑑 𝑗 ), 𝛼 𝑖 and 𝛼 𝑗 are both positive. Let 𝛼 𝑖 𝑗 = 𝛼 𝑖 ≈ 𝛼 𝑗 , then
𝐷 𝑙 ≈ ∥(h 𝑖 -𝛼 𝑖 𝑗 h 𝑖 ) -(h 𝑗 -𝛼 𝑖 𝑗 h 𝑗 )∥ = (1 -𝛼 𝑖 𝑗 )∥h 𝑖 -h 𝑗 ∥ < 𝐷.(14)
Then, we easily know that when large curvature is involved, the distance will be reduced and two nodes will be closer to each other. What's more, the larger the curvature, the closer the nodes are.
(2) Similarly, when a small curvature (𝜅 𝑖,𝑗 < min(1/𝑑 𝑖 , 1/𝑑 𝑗 )) is involved, fewer messages will be transferred, and we decompose the embedding, taking h 𝑖 as an example, into two components: one is from original h 𝑖 and another is the reduction parts of h 𝑗 , that is
𝐷 𝑠 = ∥(h 𝑖 -𝛽 𝑖 h 𝑗 ) -(h 𝑗 -𝛽 𝑗 h 𝑖 )∥ ≈ (1 + 𝛽 𝑖 𝑗 )∥h 𝑖 -h 𝑗 ∥ > 𝐷,(15)
where 𝛽 𝑖 is the difference between 1/𝑑 𝑖 and κ𝑖,𝑗 , i.e., 𝛽 𝑖 = 1/𝑑 𝑖 -κ𝑖,𝑗 , 𝛽 𝑗 = 1/𝑑 𝑗 -κ𝑖,𝑗 . Both 𝛽 𝑖 and 𝛽 𝑗 are positive in that 𝜅 𝑖,𝑗 < min(1/𝑑 𝑖 , 1/𝑑 𝑗 ). Let 𝛽 𝑖 𝑗 = 𝛽 𝑖 ≈ 𝛽 𝑗 , then, we easily know that when small curvature is involved, the node pair will become more distant in the embedding space. What's more, the smaller the curvature, the more distant the nodes are. □<h2>publication_ref</h2>['b93', 'b78', 'b32', 'b32']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Curvature-based Homophily Constraint<h2>text</h2>In the degree-based learning paradigm, like GCN [35], the influence of a node on another node decays exponentially as their graph distance increases as shown by [31]. The analysis in [31] is limited degree-based aggregation and Euclidean space. The hyperbolic message passing learning paradigm of 𝜅HGCN also shows a similar phenomenon, which causes too much influence loss in long-term propagation. Especially, if the paths consist of numerous connections to other nodes, the node influence is minimal. For clarity, we term the hyperbolic message passing learning paradigm in 𝜅HGCN or original HGNNs [11,45,96] as HMP. The HMP is a local aggregation method, in which the influence of nodes decreases with increasing distance, as demonstrated in Theorem 4.3.<h2>publication_ref</h2>['b34', 'b30', 'b30', 'b10', 'b44', 'b95']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Theorem 4.3 (Decaying property of HMP).<h2>text</h2>Let 𝑝 be a path between node 𝑢 and node 𝑣, 𝑑 * 𝑔 be the shortest distance between 𝑢 and 𝑣, let 𝐶 be a constant and z be the embedding on the tangent space. Consider the node influence 𝐼 𝑢,𝑣 (𝐼 𝑢,𝑣 = ∥𝜕z 𝑢 /𝜕z 𝑣 ∥) from 𝑣 to 𝑢 using HMP, 𝐼 𝑢,𝑣 ≤ 𝐶𝛾 𝑑 * 𝑔 (0 < 𝛾 <= 1). The condition for equality is 𝑑 * 𝑔 = 1, and 𝑣 is the unique neighbors of node 𝑢, correspondingly 𝐶 = 1, 𝛾 = 1.
Proof. Recall the aggregation rule in Equations ( 6) and ( 7) (similar to that in origin HGNNs), we focus on the aggregation in the tangent space and ignore the previous logarithmic map and the later exponential map since they are applied before and after the whole aggregation process, respectively. Then for any node 𝑢 and 𝑣, the update rule in the tangent space can be formulated as:
z 𝑢 = ∑︁ 𝑗 ∈ N (𝑢 ) κ𝑢,𝑗 z 𝑗 = 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑖 ) exp( κ𝑢,𝑗 )z 𝑗 ,(16)
where 𝐾 𝑢𝑢 = 𝑗 ∈ N (𝑢 ) exp( κ𝑢,𝑗 ). 2 By an expansion of node in the neighbor N ( 𝑗), we have:
z 𝑢 = 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑢 ) exp( κ𝑢,𝑗 ) * 1 𝐾 𝑗 𝑗 ∑︁ 𝑘 ∈ N ( 𝑗 ) exp( κ𝑗,𝑘 )z 𝑘 . (17
)
2 For original HGNNs, the 𝜅 can be replaced with degree-based weight or attentionbased weight.
We completely expand it:
z 𝑢 = 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑢 ) exp( κ𝑖,𝑗 ) * • • • * 1 𝐾 𝑜𝑜 ∑︁ 𝑝 ∈ N (𝑜 )
exp( κ𝑜,𝑝 )z 𝑝 .
(18) Node influences 𝐼 𝑢,𝑣 of 𝑣 on 𝑢 in the message passing output is 𝐼 𝑢,𝑣 = ∥𝜕z 𝑢 /𝜕z 𝑣 ∥, where the norm is any subordinate norm and the node influence measures how a change in 𝑣 passes to a change in 𝑢. By equation (18), the node influence can be computed as:
𝐼 𝑢,𝑣 = ∥ 𝜕z 𝑢 𝜕z 𝑣 ∥ = ∥ 𝜕 𝜕z 𝑣 ( 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑢 ) exp( κ𝑖,𝑗 ) * • • • * 1 𝐾 𝑜𝑜 ∑︁ 𝑝 ∈ N (𝑜 ) exp( κ𝑜,𝑝 )z 𝑝 ) ∥. (19)
The partial derivative of the nodes in Equation ( 19) is zero if they are not on the path between node 𝑢 and 𝑣, and then the feature influence can be decomposed into the sum influence of all related paths. Suppose there are 𝑛 paths between 𝑢 and 𝑣, then
𝐼 𝑢,𝑣 = 𝜕 𝜕z 𝑣 𝐼 𝑝 1 + • • • + 𝐼 𝑝 𝑖 • • • + 𝐼 𝑝 𝑛 ,(20)
where
𝐼 𝑝 𝑖 = 1 𝐾 𝑢𝑢 exp( κ𝑢,𝑝 𝑖 𝑗 ) • • • 1 𝐾 𝑝 𝑖 𝑛 𝑖 𝑝 𝑖 𝑛 𝑖 exp( κ𝑝 𝑖 𝑛 𝑖 ,𝑣 ) 𝑆 (𝐼 𝑝 𝑖 ) z 𝑣 .
Note that, in Equation ( 20), the scalar term 𝑆 (𝐼 𝑝 𝑖 ) ranges from (0, 1] and all 𝐼 𝑝 𝑖 (1 ≤ 𝑖 ≤ 𝑛) have the term 𝑚 𝑣 , thus we separate 𝑆 (𝐼 𝑝 𝑖 ) and the rest derivative term and then uses the absolute homogeneous property, i.e., ∥𝛼𝑀 ∥ = |𝛼 |∥𝑀 ∥
𝐼 𝑢,𝑣 = 𝑆 (𝐼 𝑝 1 ) + • • • + 𝑆 (𝐼 𝑝 𝑖 ) • • • + 𝑆 (𝐼 𝑝 𝑛 ) ∥ 𝜕𝑧 𝑣 𝜕𝑧 𝑣 ∥ = 𝑆 (𝐼 𝑝 1 ) + • • • + 𝑆 (𝐼 𝑝 𝑖 ) • • • + 𝑆 (𝐼 𝑝 𝑛 ) ≤ |𝑛 * max(𝑆 (𝐼 𝑝 𝑖 ))| = |𝑛 * 𝛾 𝑛 𝑖 | ≤ |𝑛 * 𝛾 𝑑 * 𝑔 | = 𝐶𝛾 𝑑 * 𝑔 ,(21)
where 𝑑 𝑔 is the shortest path between 𝑢 and 𝑣, 𝑑 * ≤ 𝑛 𝑖 and 0 < 𝛾 ≤ 1, thus the second inequality holds on in Equation (21). For more generality, we use constant 𝐶 to denote the 𝑛. The condition for equality is if and only if 𝑑 * 𝑔 = 1 and the 𝑣 is the unique neighbor of node 𝑢, i.e., 𝛾 = 1 and 𝐶 = 1. □ Theorem 4.3 shows that the node influence using HMP exponentially decays as the shortest graph distance 𝑑 * 𝑔 between two nodes increases. In other words, distant nodes in dense areas will have less interaction, even if they are in a dense connected area. To alleviate the phenomenon, we propose a Curvature-based Homophily Constraint (𝜅HC) to enhance the connection within linked pairs. The basic idea is to push the embeddings of linked nodes closer if their ORC value is larger than a threshold. In this way, we can enforce disjoint node pairs in dense areas or clusters to have more Table 1: Comparisons of the abilities of models in terms of global tree-likeness modeling (Global), local heterogeneous structure learning (Local), and neighbor interaction (Neighbor) are indicated by ✓ for the presence of the ability and × for its absence.<h2>publication_ref</h2>['b17', 'b20']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Model Type Models Global Local Neighbor<h2>text</h2>Shallow models
EUC × × × HYP ✓ × × Euclidean GNN models GCN × × ✓ GAT × × ✓ SAGE × × ✓ SGC × × ✓ Curvature GNN models CurvGN × ✓ ✓ 𝜅GCN ✓ × ✓ Hyperbolic GNN models HGCN ✓ × ✓ LGCN ✓ × ✓ Curvature-aware HGNN model 𝜅HGCN ✓ ✓ ✓
influence on each other through their mutual neighbors, which is given by:
L 𝜅ℎ𝑐 + = - 1 |𝐸 𝜅 | ∑︁ (𝑖,𝑗 ) ∈𝐸 𝜅 log 𝑝 (x ℓ,H 𝑖 , x ℓ,H 𝑗 ),(22)
where 𝐸 𝜅 is the filtered edge set based on ORC threshold 𝜏 3 ; 𝑝 (•) is the Fermi-Dirac function, indicating the probability of two hyperbolic nodes (𝑢, 𝑣) link or not, which is given by:
𝑝 (x 𝑢 , x 𝑣 ) = exp (𝑑 2 H (x 𝑢 , x 𝑣 ) -𝑟 )/𝑡 + 1 -1 ,(23)
and 𝑑 H (x 𝑢 , x 𝑣 ) is the hyperbolic distance from 𝑢 to 𝑣, 𝑟 and 𝑡 is hyper parameters and we set it as previous work [11]. We also sample the same number of negative link pairs that they have no connections or the curvature is very small based on the results in [74]. Totally,
L 𝜅ℎ𝑐 = L 𝜅ℎ𝑐 + + L 𝜅ℎ𝑐 -.(24)
Geometric Intuition. HMP helps build the connection between the graph topology and the embedding space, adjust the curvature of the hyperbolic geometry, and guide the information flow. It also shortens the distance of two linked nodes in an area with many triangles, helping mitigate the distortion caused by hyperbolic space. Nonetheless, HMP is local inherently, and the proposed 𝜅HC further enhances the interactions of unconnected nodes, which is non-local.<h2>publication_ref</h2>['b10', 'b73']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>𝜅HGCN Architecture<h2>text</h2>Given the Euclidean feature x 𝐸 , we first project it into the hyperbolic manifold by the exponential map. 𝜅HGCN architecture takes layers of HMP as the encoder. Following the literature, the Fermi-Dirac function is used as a decoder in the link prediction task. For the node classification task, the final hyperbolic vector is mapped back to tangent space and decoded with MLP, which is the same with work [11]. 3 We select edges if their ORC value is larger than a threshold where edges can be constructed by multiple hop neighbors and the weight is added their curvature together based on their shortest distance. <h2>publication_ref</h2>['b10']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>EXPERIMENTS 5.1 Experimental Setup<h2>text</h2>Datasets. The evaluation of our work utilizes several datasets, including Disease, Airport, and three benchmark citation networks, namely PubMed and Cora. While Disease and Airport exhibit a more hierarchical structure, the citation networks are less so, making them suitable for demonstrating the generalization capability of our proposal. In Table 2, we provide the data statistics and hyperbolicity metric that measures the tree-likeness of each graph. For further details, please refer to Appendix A.
Baselines. We compare our proposed model with various baselines. ( 1) Shallow Euclidean and hyperbolic models, including Euclidean embeddings (EUC) and Poincaré embeddings (HYP) [56];
(2) Euclidean GNN models, i.e., GCN [35], GraphSAGE (SAGE) [28], Graph Attention Networks (GAT) [78], Simplified Graph Convolution, (SGC) [83]; (3) Curvature GNN models, including Curvature Graph Network (CurvGN) [93] which applies the discrete curvature in Euclidean model and ProdGCN which deploys GNNs to products of constant curvature spaces, both of them are close to our work; Hyperbolic GNNs, including HGCN [11], HGNN [45], HGAT [96],
LGCN [97]. Table 1 presents the different features of the aforementioned models regarding their capabilities for perceiving both global tree-likeness modeling and local heterogeneous structure, as well as their interactional aptitude with regard to neighboring information.
Experimental Details Data split. We evaluate 𝜅HGCN on both node classification and link prediction tasks. The data split is the same with the previous works [11]. More specifically, in link prediction, we randomly split edges into 85%, 5%, 10% for training, validation, and test sets, respectively. For node classification, we split nodes into 70%, 15%, 15% for Airport, 30%, 10%, 60% for Disease, and we use 20 labeled train examples per class for Cora, and PubMed. Implementation details. We closely follow the parameter settings as HGCN [11], fix the number of embedding dimensions to 16 and then perform hyper-parameter search on a validation set over learning rate, weight decay, dropout, and the number of layers. We also adopt the early stopping strategies based on the validation set as [11]. For baselines, we mainly refer to the reported results in the literature, and for the inconsistent cases (such as different embedding dimensions in H2H-GCN), we re-implement their official code in similar experimental settings. Evaluation metric. Following the literature, we report the F1-score for Disease and Airport datasets, and accuracy for the others in the node classification tasks. For the link predictions task, the Area Under Curve (AUC) is calculated. <h2>publication_ref</h2>['b55', 'b34', 'b27', 'b77', 'b82', 'b92', 'b10', 'b44', 'b95', 'b96', 'b10', 'b10', 'b10']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_2']<h2>heading</h2>Experimental Results<h2>text</h2>We report the results of 10 random experiments 4 , including standard deviations in TABLE 3 and4, where the Δ 𝐸 , Δ 𝜅 , Δ 𝐻 is the improvement of the proposed model 𝜅HGCN over the Euclidean GNNs, Curvature-related GNNs and Hyperbolic GNNs, respectively.
Node Classification. The experimental results of node classification are summarized in TABLE 3, where a lower hyperbolicity value corresponds to a more tree-like structure. The key findings are: (1) Overall, the proposed model performs impressively, surpassing previous models on four out of five datasets. Specifically, hyperbolic models (e.g., HGCN, LGCN) perform substantially better on the more hyperbolic dataset (e.g., Disease) than on the less hyperbolic dataset; Euclidean models (e.g., GCN, GAT) find more success on the less hyperbolic datasets (e.g., Cora) than on the more hyperbolic dataset; whereas our model performs better on both datasets, which is consistent with our motivation, namely, that the graph can be better learned under the guidance of curvature. In addition, from the improvements of Δ 𝜅 , we discovered that CurvGN with discrete curvature and 𝜅GCN with continuous curvature both perform worse than our method which validates the power of hyperbolic geometry and the curvature-aware learning. (3) When it comes to Cora, both hyperbolic models and the proposed 𝜅HGCN fail to outperform Euclidean GAT [78], indicating Euclidean geometry is more suitable for modeling data with scarcely hierarchical structures. Nevertheless, it is noted that 𝜅HGCN still outperforms well-known Euclidean GCN models, e.g., GCN [35], SGC [83], SAGE [28]. It is observed that the proposed method 𝜅HGCN also helps to narrow down the gap between hyperbolic models and Euclidean GAT.
Link Prediction. The experimental results of link prediction tasks are summarized in TABLE 4. In the link prediction task, we further have the following observations: (1) Compared with Euclidean 4 The results on Lorentz model are similar. counterparts, Our proposed 𝜅HGCN, and other hyperbolic models have achieved better performance. It is because hyperbolic space owns a larger embedding space, where the structural dependencies could be well preserved by the link prediction loss, providing more space or boundary for nodes to be well arranged; (2) In comparison with the advanced hyperbolic models, our model also obtains remarkable gains and refresh the records. According to the above extensive experiments, we are safe to conclude that equipping ORC with hyperbolic geometry further improves its generalization ability, obtaining high-quality representations for both tree-like and non-tree-like structured data. This confirms our primary motivation that the curvature carries rich information which is beneficial for graph representation learning in the embedding manifold. Specifically, incorporating the structure information featured by ORC helps the models developed in a continuous manifold with negative curvature to perceive the role of each node, accelerating the learning procedure, and reducing the distortion for graph embedding of less hierarchical networks.<h2>publication_ref</h2>['b0', 'b77', 'b34', 'b82', 'b27', 'b1']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_3', 'tab_4', 'tab_3']<h2>heading</h2>Effectiveness of Aggregations<h2>text</h2>In Section 4.1, we introduce two tangential aggregation strategies: the curvature-based approach (denoted as Curv) and the featureaugmented method (denoted as CurvAtt). The performance of  these two aggregation strategies is evaluated and reported in Table 5. Our results show that the feature-augmented approach outperforms the curvature-based one in most cases, which can be attributed to the complex nature of real-world systems and the incongruities between node features and topology. The feature-augmented method offers a more flexible and adaptive way to synthesize information from various sources, thus resulting in improved performance.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_5']<h2>heading</h2>Effectiveness of 𝜅HC<h2>text</h2>Figure 5 displays the results of adding 𝜅HC or not on 𝜅HGCN. As it observed, the performance degenerates substantially on DISEASE when applying 𝜅HC, while there are significant improvements on the three citation networks, i.e., PubMed, and Cora. This phenomenon can be understood as follows. Adding 𝜅HC as in node classification will force linked nodes to obtain more similar representations. For the pure tree-like dataset, i.e., DISEASE (without any triangle and circle), these node pairs belong to different levels, and adding 𝜅HC impairs the learning of asymmetric dependencies, which further affects the establishment of hierarchical awareness. When it comes to the citation networks instead, 𝜅HC helps to reduce the distortion caused by hyperbolic geometry and thus boost the learning.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_3']<h2>table_ref</h2>[]<h2>heading</h2>Case Study of Ricci Curvature Weights<h2>text</h2>In this section, we demonstrate the effectiveness of our method through a case study. We extract a subtree centered on a randomly sampled node (node 10, in this case), from the Disease dataset, where node 2 is from a higher level and the remaining nodes (42,43,44,45) belong to a lower level. The edges depict disease propagation paths. As shown in the upper sub-figures of Figure 6, we display the corresponding edge weights assigned by both 𝜅HGCN and HGCN during the node classification task. The comparison between the two reveals that 𝜅HGCN (upper right) effectively distinguishes node levels, as it assigns greater importance to the parent node (node 2) and equally emphasizes the child nodes from the same level, whereas HGCN (upper left) fails to make such distinctions. This substantiates the importance of ORC in facilitating hierarchical learning.
Furthermore, we illustrate a subgraph with triangles selected from the citation network, Cora, as depicted in the lower two sub-figures of Figure 6. This subgraph comprises nodes 𝑜, 𝑒, 𝑑 that form a triangle. We examine the edge weights around node 𝑜 and present the results in the lower two sub-figures of Figure 6. Observing these results, it is evident that 𝜅HGCN (as shown in the lower right) effectively identifies the local triangle structure and assigns larger weights to promote inter-node message exchange. In contrast, HGCN (as depicted in the lower left) fails to grasp the intricate topology in this area. These observations further attest to the efficacy of our proposed method in uncovering local clusters and mitigating the distortions imposed by hyperbolic geometry.<h2>publication_ref</h2>['b41', 'b42', 'b43', 'b44']<h2>figure_ref</h2>['fig_4', 'fig_4', 'fig_4']<h2>table_ref</h2>[]<h2>heading</h2>CONCLUSION<h2>text</h2>For modeling tree-like structures, the hyperbolic space has demonstrated its ability to capture hierarchical relationships. However, approximating a discrete tree-like graph with a hyperbolic manifold can result in inevitable distortions, as real-world tree-like graphs are inherently complex. In this work, we integrate the intrinsic graph structure into the continuous hyperbolic embedding space via the discrete Ricci curvature. As expected, the graph curvature facilitates the node to perceive the role and the hierarchy it belongs to, helping accelerate the hierarchical formation as well as alleviate the distortion in local clusters or cliques. The superiority of the proposal is demonstrated by extensive experiments. Curvature is a geometric notion with appealing and descriptive properties for both network and continuous space. Via the interaction of curvatures, we can build proper connections for a graph and the embedding space to obtain high-quality representations, which is a promising direction to advance geometric learning. In the future, we will consider the use of Ricci flow, a more sophisticated geometric concept derived from Ricci curvature, to further enhance graph embedding and graph machine learning in tree-likeness modeling.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>APPENDIX A DATASETS<h2>text</h2>In this section, we provide details about the datasets used in our study. The Disease dataset contains nodes that are labeled as infected or not infected with a disease, with features indicating their susceptibility to the disease. The disease-spreading network in this dataset displays a clear hierarchical structure, which makes it ideal for testing the effectiveness of hyperbolic embedding models. We use this dataset to validate our proposal. The Airport dataset consists of nodes representing airports, with edges indicating the existence of routes between two airports and labels reflecting the population of the respective country that the airport belongs to. In contrast, the PubMed and Cora datasets represent scientific papers as nodes, with edges indicating citations and labels corresponding to academic subfields. Additionally, the Disease and Airport datasets have imbalanced node labels, rendering accuracy an inadequate measure of model performance. Therefore, we use the F1 score as a more suitable measure for imbalanced datasets. Conversely, for the remaining datasets with balanced node classes, we use accuracy to evaluate the models. Furthermore, note that according to the official code of HGCN 5 , the data statistics for Disease in node classification and link prediction differ slightly, and we list them separately in Table 2 for clarity.
Hyperbolicity 𝛿 is a metric that quantifies how closely a graph resembles a tree structure, with lower values of 𝛿 indicating greater tree-like characteristics. A 𝛿 value of 0 corresponds to a tree, while higher hyperbolicity values indicate a less tree-like structure. The relationship between hyperbolicity and tree-like characteristics has been established in various studies, including [3,32,53].<h2>publication_ref</h2>['b2', 'b31', 'b52']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_2']<h2>heading</h2>B HYPERBOLIC GEOMETRY<h2>text</h2>The geometry of a Riemannian manifold is defined by its curvature: elliptic geometry for positive curvature, Euclidean geometry for zero curvature, and hyperbolic geometry for negative curvature. In this study, we concentrate on the latter, i.e. hyperbolic geometry. There exist several equivalent hyperbolic models that exhibit diverse characteristics, yet are mathematically isometric. Our main focus will be on two extensively researched hyperbolic models.: the Poincaré ball model [56] and the Lorentz model (also known as the hyperboloid model) [57]. Let ∥.∥ be the Euclidean norm and ⟨., .⟩ L denote the Minkowski inner product, respectively. The two models are denoted by Definition B.1 and Definition B.2. A compilation of the formulas and operations associated with these models, such as distance, mapping, and parallel transport, is presented in Table 6. These operations include Möbius addition [75] denoted by ⊕ 𝜅 and the gyration operator [75] <h2>publication_ref</h2>['b55', 'b56', 'b74', 'b74']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_7']<h2>heading</h2>C MORE ANALYSIS C.1 Computational Complexity of ORC<h2>text</h2>The computation of ORC is formulated as linear programming problems [18,46], and its computational complexity is 𝑂 (|𝐸|𝑑 3 max ), where 𝑑 max is the maximum degree of the graph. To illustrate the computational cost, we present the actual run-time on a machine with the environment Intel(R) Xeon(R) Gold 6132 CPU @ 2.60GHZ in TABLE 7. The results indicate that the time required for computing ORC is proportional to the size of the graph, and the cost for smaller graphs is correspondingly lower. Importantly, it is noteworthy that ORC only needs to be computed once prior to the training process, with the same computational complexity as HGCN [11] during both training and inference. To handle extremely large-scale graphs, approximation methods such as Sinkhorn [19] or Jaccard proxy [59] may be utilized. The efficacy of 𝜅HGCN and HGCN in learning representations for node classification is demonstrated through the visualization of their performance on the Disease and Cora datasets. To accomplish this, we employ the t-distributed Stochastic Neighbor Embedding (t-SNE) technique [77] to reduce the high-dimensional embeddings produced by the final layer of each model to a two-dimensional plane for visual examination. The results, shown in Figure 7, depict nodes as individual points, where each point is assigned a color that corresponds to its class. The visualization indicates that   <h2>publication_ref</h2>['b17', 'b45', 'b10', 'b18', 'b58', 'b76']<h2>figure_ref</h2>['fig_5']<h2>table_ref</h2>['tab_8']<h2>heading</h2>C.2 Embedding Visualization<h2>text</h2>= x ∈ R 𝑛 : ⟨x, x⟩ 2 < -1 𝜅 L 𝑛 𝜅 = x ∈ R 𝑛+1 : ⟨x, x⟩ L = 1 𝜅 Metric 𝑔 B𝜅 x = 𝜆 𝜅<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C.3 Curvature-wise Performance<h2>text</h2>The objective of the study is to perceive the local structure around nodes in tree-like graphs, encompassing local tree-like, zero-density, and densely connected structures. To demonstrate the effectiveness of the proposed method, we conducted further analysis by classifying nodes into defined local substructures. As an example, consider the following, we set the nodes to the following three types: tree-like (𝜅 𝑖 ≥ -0.01); zero-like (-0.01 < 𝜅 𝑖 ≤ 0.01); positive-like (𝜅 𝑖 > 0.01). The curvature of each node is defined as the sum of curvatures over its edges, that is:
𝜅 𝑖 = 1 |𝑁 𝑖 |
𝑗 ∈𝑁 𝑖 𝜅 𝑖 𝑗 , where |𝑁 𝑖 | is the number of neighbors of node 𝑖, and 𝜅 𝑖 𝑗 is the curvature of the edge between nodes 𝑖 and 𝑗. This curvature measure was used to determine the local substructure of each node. In the following, we evaluated the performance of three models (GCN, HGCN, and the proposed method) on both the Cora and Airport datasets. Specifically, we calculated the accuracy/F1-score on the test set for nodes in each local substructure.
The results are shown in Table 8 and Table 9. The "GT-Prop" row in the Tables show the proportion of nodes belonging to each Overall, the study found that the models achieved comparable accuracy to the best achievable, but performance varied across different substructures. Specifically, Euclidean GCN outperformed HGCN on zero-density and densely connected areas, but underperformed on tree-like nodes. In contrast, HGCN showed improved accuracy for tree-like nodes, but lower accuracy on other substructures. The proposed model achieved a balance of performance across all substructures, with high accuracy for both tree-like and non-tree-like nodes.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_9', 'tab_10']<h2>heading</h2>C.4 Connections of the Discrete and Continuous Curvatures<h2>text</h2>The discrete curvature 𝜅 is computed in advance, which can be regarded as an edge weight. The continuous curvature 𝑐 of the predefined hyperbolic space is learnable and differentiable. For simplicity, we denote the learnable parameters of our model 𝜅HGCN as 𝜃 . Let us consider node classification as an example. If the groundtruth label of node x is y and the predicted label is ȳ, then we have ȳ = 𝜅HGCN(x, 𝜅, 𝜃, 𝑐) and the loss is given by 𝐿(y, ȳ). We can take the derivative of 𝑐 with respect to the loss, i.e., 𝜕𝐿 (y,ȳ) 𝜕𝑐 = 𝜕 (y,𝜅HGCN(x,𝜅,𝜃,𝑐 ) ) 𝜕𝑐
. It is easy to see that the update of 𝑐 is constrained by 𝜅. In other words, we learn a good embedding space equipped with curvature 𝑐 that matches the graph structure through discrete Ricci curvature 𝜅.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>Metric tree-like structures in realworld networks: an empirical study<h2>journal</h2>Networks<h2>year</h2>2016<h2>authors</h2>Muad Abu; -Ata ; Feodor F Dragan<h2>ref_id</h2>b1<h2>title</h2>Ricci curvature and the manifold learning problem<h2>journal</h2>Advances in Mathematics<h2>year</h2>2019<h2>authors</h2>G Antonio; Micah W Ache;  Warren<h2>ref_id</h2>b2<h2>title</h2>Tree-like structure in large social and information networks<h2>journal</h2>IEEE<h2>year</h2>2013<h2>authors</h2>Blair D Aaron B Adcock; Michael W Sullivan;  Mahoney<h2>ref_id</h2>b3<h2>title</h2>A theorem of Myers<h2>journal</h2>Duke Mathematical Journal<h2>year</h2>1957<h2>authors</h2> Ambrose<h2>ref_id</h2>b4<h2>title</h2>Hyperbolic Image Segmentation<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Mina Ghadimi; Atigh ; Julian Schoep; Erman Acar; Nanne Van Noord; Pascal Mettes<h2>ref_id</h2>b5<h2>title</h2>Constant curvature graph convolutional networks<h2>journal</h2>PMLR<h2>year</h2>2020<h2>authors</h2>Gregor Bachmann; Gary Bécigneul; Octavian Ganea<h2>ref_id</h2>b6<h2>title</h2>HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction<h2>journal</h2><h2>year</h2>2023<h2>authors</h2>Qijie Bai; Changli Nie; Haiwei Zhang; Dongming Zhao; Xiaojie Yuan<h2>ref_id</h2>b7<h2>title</h2>Modeling heterogeneous hierarchies with relation-specific hyperbolic cones<h2>journal</h2>Advances in Neural Information Processing Systems<h2>year</h2>2021<h2>authors</h2>Yushi Bai; Zhitao Ying; Hongyu Ren; Jure Leskovec<h2>ref_id</h2>b8<h2>title</h2>On Approximate Reasoning Capabilities of Low-Rank Vector Spaces<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Guillaume Bouchard; Sameer Singh; Theo Trouillon<h2>ref_id</h2>b9<h2>title</h2>Low-Dimensional Hyperbolic Knowledge Graph Embeddings<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Ines Chami; Adva Wolf; Da-Cheng Juan; Frederic Sala; Sujith Ravi; Christopher Ré<h2>ref_id</h2>b10<h2>title</h2>Hyperbolic graph convolutional neural networks<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Ines Chami; Zhitao Ying; Christopher Ré; Jure Leskovec<h2>ref_id</h2>b11<h2>title</h2>Bipartite Graph Convolutional Hashing for Effective and Efficient Top-N Search in Hamming Space<h2>journal</h2><h2>year</h2>2023<h2>authors</h2>Yankai Chen; Yixiang Fang; Yifei Zhang; Irwin King<h2>ref_id</h2>b12<h2>title</h2>Learning binarized graph representations with multifaceted quantization reinforcement for top-k recommendation<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Yankai Chen; Huifeng Guo; Yingxue Zhang; Chen Ma; Ruiming Tang; Jingjie Li; Irwin King<h2>ref_id</h2>b13<h2>title</h2>Modeling scale-free graphs with hyperbolic geometry for knowledge-aware recommendation<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Yankai Chen; Menglin Yang; Yingxue Zhang; Mengchen Zhao; Ziqiao Meng; Jianye Hao; Irwin King<h2>ref_id</h2>b14<h2>title</h2>Attentive Knowledge-aware Graph Convolutional Networks with Collaborative Guidance for Personalized Recommendation<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Yankai Chen; Yaming Yang; Yujing Wang; Jing Bai; Xiangchen Song; Irwin King<h2>ref_id</h2>b15<h2>title</h2>WSFE: Wasserstein Sub-graph Feature Encoder for Effective User Segmentation in Collaborative Filtering<h2>journal</h2><h2>year</h2>2023<h2>authors</h2>Yankai Chen; Yifei Zhang; Menglin Yang; Zixing Song; Chen Ma; Irwin King<h2>ref_id</h2>b16<h2>title</h2>Hyperbolic Neural Networks: Theory, Architectures and Applications<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Nurendra Choudhary; Nikhil Rao; Karthik Subbian; H Srinivasan;  Sengamedu;  Chandan K Reddy<h2>ref_id</h2>b17<h2>title</h2>The Graph Curvature Calculator and the curvatures of cubic graphs<h2>journal</h2>Experimental Mathematics<h2>year</h2>2019<h2>authors</h2>David Cushing; Riikka Kangaslampi; Shiping Valtteri Lipiäinen; George W Liu;  Stagg<h2>ref_id</h2>b18<h2>title</h2>Sinkhorn distances: Lightspeed computation of optimal transport<h2>journal</h2>Advances in Neural Information Processing Systems<h2>year</h2>2013<h2>authors</h2>Marco Cuturi<h2>ref_id</h2>b19<h2>title</h2>Enhancing graph neural network-based fraud detectors against camouflaged fraudsters<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Yingtong Dou; Zhiwei Liu; Li Sun; Yutong Deng; Hao Peng; Philip S Yu<h2>ref_id</h2>b20<h2>title</h2>Bochner's method for cell complexes and combinatorial Ricci curvature<h2>journal</h2>Discrete and Computational Geometry<h2>year</h2>2003<h2>authors</h2>Robin Forman<h2>ref_id</h2>b21<h2>title</h2>Hyperbolic Geometric Graph Representation Learning for Hierarchy-imbalance Node Classification<h2>journal</h2><h2>year</h2>2023<h2>authors</h2>Xingcheng Fu; Yuecen Wei; Qingyun Sun; Haonan Yuan; Jia Wu; Hao Peng; Jianxin Li<h2>ref_id</h2>b22<h2>title</h2>MAGNN: Metapath aggregated graph neural network for heterogeneous graph embedding<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Xinyu Fu; Jiani Zhang; Ziqiao Meng; Irwin King<h2>ref_id</h2>b23<h2>title</h2>Hyperbolic neural networks<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Octavian Ganea; Gary Bécigneul; Thomas Hofmann<h2>ref_id</h2>b24<h2>title</h2>Predict then propagate: Graph neural networks meet personalized pagerank<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Johannes Gasteiger; Aleksandar Bojchevski; Stephan Günnemann<h2>ref_id</h2>b25<h2>title</h2>Learning mixed-curvature representations in product spaces<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Albert Gu; Frederic Sala; Beliz Gunel; Christopher Ré<h2>ref_id</h2>b26<h2>title</h2>Hyperbolic attention networks<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Caglar Gulcehre; Misha Denil; Mateusz Malinowski; Ali Razavi; Razvan Pascanu; Karl Moritz Hermann; Peter Battaglia; Victor Bapst; David Raposo; Adam Santoro<h2>ref_id</h2>b27<h2>title</h2>Inductive representation learning on large graphs<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Rex William L Hamilton; Jure Ying;  Leskovec<h2>ref_id</h2>b28<h2>title</h2>Inductive representation learning on large graphs<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Rex William L Hamilton; Jure Ying;  Leskovec<h2>ref_id</h2>b29<h2>title</h2>Capturing implicit hierarchical structure in 3D biomedical images with self-supervised hyperbolic representations<h2>journal</h2>Advances in Neural Information Processing Systems<h2>year</h2>2021<h2>authors</h2>Joy Hsu; Jeffrey Gu; Gong Wu; Wah Chiu; Serena Yeung<h2>ref_id</h2>b30<h2>title</h2>Graph meta learning via local subgraphs<h2>journal</h2>Advances in Neural Information Processing Systems<h2>year</h2>2020<h2>authors</h2>Kexin Huang; Marinka Zitnik<h2>ref_id</h2>b31<h2>title</h2>Euclidean versus hyperbolic congestion in idealized versus experimental networks<h2>journal</h2>Internet Mathematics<h2>year</h2>2011<h2>authors</h2>Edmond Jonckheere; Mingji Lou; Francis Bonahon; Yuliy Baryshnikov<h2>ref_id</h2>b32<h2>title</h2>Ollivier's Ricci curvature, local clustering and curvature-dimension inequalities on graphs<h2>journal</h2>Discrete & Computational Geometry<h2>year</h2>2014<h2>authors</h2>Jürgen Jost; Shiping Liu<h2>ref_id</h2>b33<h2>title</h2>Hyperbolic image embeddings<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Valentin Khrulkov; Leyla Mirvakhabova; Evgeniya Ustinova; Ivan Oseledets; Victor Lempitsky<h2>ref_id</h2>b34<h2>title</h2>Semi-Supervised Classification with Graph Convolutional Networks<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>N Thomas; Max Kipf;  Welling<h2>ref_id</h2>b35<h2>title</h2>Hy-perKG: Hyperbolic knowledge graph embeddings for knowledge base completion<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Prodromos Kolyvakis; Alexandros Kalousis; Dimitris Kiritsis<h2>ref_id</h2>b36<h2>title</h2>Curvature and temperature of complex networks<h2>journal</h2>Physical Review E<h2>year</h2>2009<h2>authors</h2>Dmitri Krioukov; Fragkiskos Papadopoulos; Amin Vahdat; Marián Boguná<h2>ref_id</h2>b37<h2>title</h2>Ultrahyperbolic Neural Networks<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Marc Law<h2>ref_id</h2>b38<h2>title</h2>Ultrahyperbolic representation learning<h2>journal</h2>Advances in neural information processing systems<h2>year</h2>2020<h2>authors</h2>Marc Law; Jos Stam<h2>ref_id</h2>b39<h2>title</h2>Introduction to Riemannian manifolds<h2>journal</h2>Springer<h2>year</h2>2018<h2>authors</h2>M John;  Lee<h2>ref_id</h2>b40<h2>title</h2>BSAL: A Framework of Bi-component Structure and Attribute Learning for Link Prediction<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Bisheng Li; Min Zhou; Shengzhong Zhang; Menglin Yang; Defu Lian; Zengfeng Huang<h2>ref_id</h2>b41<h2>title</h2>CSPM: Discovering compressing stars in attributed graphs<h2>journal</h2>Information Sciences<h2>year</h2>2022<h2>authors</h2>Jiahong Liu; Philippe Fournier-Viger; Min Zhou; Ganghuan He; Mourad Nouioua<h2>ref_id</h2>b42<h2>title</h2>Enhancing Hyperbolic Graph Embeddings via Contrastive Learning<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Jiahong Liu; Menglin Yang; Min Zhou; Shanshan Feng; Philippe Fournier-Viger<h2>ref_id</h2>b43<h2>title</h2>Discovering Representative Attribute-stars via Minimum Description Length<h2>journal</h2>IEEE<h2>year</h2>2022<h2>authors</h2>Jiahong Liu; Min Zhou; Philippe Fournier-Viger; Menglin Yang; Lujia Pan; Mourad Nouioua<h2>ref_id</h2>b44<h2>title</h2>Hyperbolic graph neural networks<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Qi Liu; Maximilian Nickel; Douwe Kiela<h2>ref_id</h2>b45<h2>title</h2>Ricci curvature on polyhedral surfaces via optimal transportation<h2>journal</h2>Axioms<h2>year</h2>2014<h2>authors</h2>Benoît Loisel; Pascal Romon<h2>ref_id</h2>b46<h2>title</h2>Differentiating through the fréchet mean<h2>journal</h2>PMLR<h2>year</h2>2020<h2>authors</h2>Aaron Lou; Isay Katsman; Qingxuan Jiang; Serge Belongie; Ser-Nam Lim; Christopher De Sa<h2>ref_id</h2>b47<h2>title</h2>Memory Augmented Graph Neural Networks for Sequential Recommendation<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Chen Ma; Liheng Ma; Yingxue Zhang; Jianing Sun; Xue Liu; Mark Coates<h2>ref_id</h2>b48<h2>title</h2>A comprehensive survey on graph anomaly detection with deep learning<h2>journal</h2>IEEE Transactions on Knowledge and Data Engineering<h2>year</h2>2021<h2>authors</h2>Xiaoxiao Ma; Jia Wu; Shan Xue; Jian Yang; Chuan Zhou; Z Quan; Hui Sheng; Leman Xiong;  Akoglu<h2>ref_id</h2>b49<h2>title</h2>Hyperbolic Deep Learning in Computer Vision: A Survey<h2>journal</h2><h2>year</h2>2023<h2>authors</h2>Pascal Mettes; Mina Ghadimi Atigh; Martin Keller-Ressel; Jeffrey Gu; Serena Yeung<h2>ref_id</h2>b50<h2>title</h2>Hyperbolic Temporal Knowledge Graph Embeddings with Relational and Time Curvatures<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Sebastien Montella; Lina M Rojas Barahona; Johannes Heinecke<h2>ref_id</h2>b51<h2>title</h2>Weisfeiler and leman go neural: Higher-order graph neural networks<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Christopher Morris; Martin Ritzert; Matthias Fey; Jan William L Hamilton; Gaurav Eric Lenssen; Martin Rattan;  Grohe<h2>ref_id</h2>b52<h2>title</h2>Large-scale curvature of networks<h2>journal</h2>Physical Review E<h2>year</h2>2011<h2>authors</h2>Onuttom Narayan; Iraj Saniee<h2>ref_id</h2>b53<h2>title</h2>Network alignment by discrete Ollivier-Ricci flow<h2>journal</h2>Springer<h2>year</h2>2018<h2>authors</h2> Chien-Chun; Yu-Yao Ni; Jie Lin; Xianfeng Gao;  Gu<h2>ref_id</h2>b54<h2>title</h2>Ricci curvature of the internet topology<h2>journal</h2>IEEE<h2>year</h2>2015<h2>authors</h2> Chien-Chun; Yu-Yao Ni; Jie Lin; Xianfeng David Gao; Emil Gu;  Saucan<h2>ref_id</h2>b55<h2>title</h2>Poincaré embeddings for learning hierarchical representations<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Maximillian Nickel; Douwe Kiela<h2>ref_id</h2>b56<h2>title</h2>Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Maximillian Nickel; Douwe Kiela<h2>ref_id</h2>b57<h2>title</h2>Ricci curvature of Markov chains on metric spaces<h2>journal</h2>Journal of Functional Analysis<h2>year</h2>2009<h2>authors</h2>Yann Ollivier<h2>ref_id</h2>b58<h2>title</h2>An efficient alternative to Ollivier-Ricci curvature based on the Jaccard metric<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Siddharth Pal; Feng Yu; Terrence J Moore; Ram Ramanathan; Amotz Bar-Noy; Ananthram Swami<h2>ref_id</h2>b59<h2>title</h2>Mix dimension in poincaré geometry for 3d skeleton-based action recognition<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Wei Peng; Jingang Shi; Zhaoqiang Xia; Guoying Zhao<h2>ref_id</h2>b60<h2>title</h2>Hyperbolic deep neural networks: A survey<h2>journal</h2>IEEE Transactions on Pattern Analysis and Machine Intelligence<h2>year</h2>2021<h2>authors</h2>Wei Peng; Tuomas Varanka; Abdelrahman Mostafa; Henglin Shi; Guoying Zhao<h2>ref_id</h2>b61<h2>title</h2>Representation Tradeoffs for Hyperbolic Embeddings<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Frederic Sala; Chris De Sa; Albert Gu; Christopher Re<h2>ref_id</h2>b62<h2>title</h2>Low distortion delaunay embedding of trees in hyperbolic plane<h2>journal</h2>Springer<h2>year</h2>2011<h2>authors</h2>Rik Sarkar<h2>ref_id</h2>b63<h2>title</h2>Hyperbolic embedding of internet graph for distance estimation and overlay construction<h2>journal</h2>IEEE/ACM Transactions on Networking<h2>year</h2>2008<h2>authors</h2>Yuval Shavitt; Tomer Tankel<h2>ref_id</h2>b64<h2>title</h2>Hyperbolic neural networks++<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Ryohei Shimizu; Yusuke Mukuta; Tatsuya Harada<h2>ref_id</h2>b65<h2>title</h2>Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Zixing Song; Irwin King<h2>ref_id</h2>b66<h2>title</h2>Semi-supervised Multi-label Learning for Graph-structured Data<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Zixing Song; Ziqiao Meng; Yifei Zhang; Irwin King<h2>ref_id</h2>b67<h2>title</h2>Graph-based semisupervised learning: A comprehensive review<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Zixing Song; Xiangli Yang; Zenglin Xu; Irwin King<h2>ref_id</h2>b68<h2>title</h2>Towards an optimal asymmetric graph structure for robust semi-supervised node classification<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Zixing Song; Yifei Zhang; Irwin King<h2>ref_id</h2>b69<h2>title</h2>HGCF: Hyperbolic Graph Convolution Networks for Collaborative Filtering<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Jianing Sun; Zhaoyue Cheng; Saba Zuberi; Felipe Pérez; Maksims Volkovs<h2>ref_id</h2>b70<h2>title</h2>A Self-supervised Mixed-curvature Graph Neural Network<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Li Sun; Zhongbao Zhang; Junda Ye; Hao Peng; Jiawei Zhang; Sen Su; Philip S Yu<h2>ref_id</h2>b71<h2>title</h2>Generalization error bound for hyperbolic ordinal embedding<h2>journal</h2>PMLR<h2>year</h2>2021<h2>authors</h2>Atsushi Suzuki; Atsushi Nitanda; Jing Wang; Linchuan Xu; Kenji Yamanishi; Marc Cavazza<h2>ref_id</h2>b72<h2>title</h2>Generalization bounds for graph embedding using negative sampling: Linear vs hyperbolic<h2>journal</h2>Advances in Neural Information Processing Systems<h2>year</h2>2021<h2>authors</h2>Atsushi Suzuki; Atsushi Nitanda; Linchuan Xu; Kenji Yamanishi; Marc Cavazza<h2>ref_id</h2>b73<h2>title</h2>Understanding over-squashing and bottlenecks on graphs via curvature<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Jake Topping; Francesco Di Giovanni; Benjamin Paul Chamberlain; Xiaowen Dong; Michael M Bronstein<h2>ref_id</h2>b74<h2>title</h2>The hyperbolic square and mobius transformations<h2>journal</h2>Banach Journal of Mathematical Analysis<h2>year</h2>2007<h2>authors</h2> Abraham A Ungar<h2>ref_id</h2>b75<h2>title</h2>Ollivier curvature of random geometric graphs converges to Ricci curvature of their Riemannian manifolds<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Pim Van Der Hoorn; Gabor Lippner; Carlo Trugenberger<h2>ref_id</h2>b76<h2>title</h2>Visualizing data using t-SNE<h2>journal</h2>JMLR<h2>year</h2>2008<h2>authors</h2>Laurens Van Der Maaten; Geoffrey Hinton<h2>ref_id</h2>b77<h2>title</h2>Graph Attention Networks<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Petar Veličković; Guillem Cucurull; Arantxa Casanova; Adriana Romero; Pietro Liò; Yoshua Bengio<h2>ref_id</h2>b78<h2>title</h2>Metric embedding, hyperbolic space, and social networks<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>Kevin Verbeek; Subhash Suri<h2>ref_id</h2>b79<h2>title</h2>HyperML: A Boosting Metric Learning Approach in Hyperbolic Space for Recommender Systems<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Lucas Vinh; Tran ; Yi Tay; Shuai Zhang; Gao Cong; Xiaoli Li<h2>ref_id</h2>b80<h2>title</h2>HyperSoRec: Exploiting Hyperbolic User and Item Representations with Multiple Aspects for Social-aware Recommendation<h2>journal</h2>TOIS<h2>year</h2>2021<h2>authors</h2>Hao Wang; Defu Lian; Hanghang Tong; Qi Liu; Zhenya Huang; Enhong Chen<h2>ref_id</h2>b81<h2>title</h2>Mixed-curvature multi-relational graph neural network for knowledge graph completion<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Shen Wang; Xiaokai Wei; Cicero Nogueira Nogueira Dos Santos; Zhiguo Wang; Ramesh Nallapati; Andrew Arnold; Bing Xiang; Philip S Yu; Isabel F Cruz<h2>ref_id</h2>b82<h2>title</h2>Simplifying graph convolutional networks<h2>journal</h2>PMLR<h2>year</h2>2019<h2>authors</h2>Felix Wu; Amauri Souza; Tianyi Zhang; Christopher Fifty; Tao Yu; Kilian Weinberger<h2>ref_id</h2>b83<h2>title</h2>Hyperbolic Embedding Inference for Structured Multi-Label Prediction<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Bo Xiong; M Cochez; Mojtaba Nayyeri; Steffen Staab<h2>ref_id</h2>b84<h2>title</h2>Ultrahyperbolic knowledge graph embeddings<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Bo Xiong; Shichao Zhu; Mojtaba Nayyeri; Chengjin Xu; Shirui Pan; Chuan Zhou; Steffen Staab<h2>ref_id</h2>b85<h2>title</h2>Pseudo-Riemannian Graph Convolutional Networks<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Bo Xiong; Shichao Zhu; Nico Potyka; Shirui Pan; Chuan Zhou; Steffen Staab<h2>ref_id</h2>b86<h2>title</h2>HICF: Hyperbolic informative collaborative filtering<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Menglin Yang; Zhihao Li; Min Zhou; Jiahong Liu; Irwin King<h2>ref_id</h2>b87<h2>title</h2>FeatureNorm: L2 Feature Normalization for Dynamic Graph Embedding<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Menglin Yang; Ziqiao Meng; Irwin King<h2>ref_id</h2>b88<h2>title</h2>Discrete-time Temporal Network Embedding via Implicit Hierarchical Learning in Hyperbolic Space<h2>journal</h2><h2>year</h2>1975<h2>authors</h2>Menglin Yang; Min Zhou; Marcus Kalander; Zengfeng Huang; Irwin King<h2>ref_id</h2>b89<h2>title</h2>Hyperbolic Graph Neural Networks: A Review of Methods and Applications<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Menglin Yang; Min Zhou; Zhihao Li; Jiahong Liu; Lujia Pan; Hui Xiong; Irwin King<h2>ref_id</h2>b90<h2>title</h2>HRCF: Enhancing Collaborative Filtering via Hyperbolic Geometric Regularization<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Menglin Yang; Min Zhou; Jiahong Liu; Defu Lian; Irwin King<h2>ref_id</h2>b91<h2>title</h2>Hyperbolic Temporal Network Embedding<h2>journal</h2>IEEE Transactions on Knowledge and Data Engineering<h2>year</h2>2022<h2>authors</h2>Menglin Yang; Min Zhou; Hui Xiong; Irwin King<h2>ref_id</h2>b92<h2>title</h2>Curvature Graph Network<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Ze Ye; Kin Sum Liu; Tengfei Ma; Jie Gao; Chao Chen<h2>ref_id</h2>b93<h2>title</h2>Adaptive structural fingerprints for graph attention networks<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Kai Zhang; Yaokang Zhu; Jun Wang; Jie Zhang<h2>ref_id</h2>b94<h2>title</h2>Link prediction based on graph neural networks<h2>journal</h2>Advances in Neural Information Processing Systems<h2>year</h2>2018<h2>authors</h2>Muhan Zhang; Yixin Chen<h2>ref_id</h2>b95<h2>title</h2>Hyperbolic graph attention network<h2>journal</h2>IEEE Transactions on Big Data<h2>year</h2>2021<h2>authors</h2>Yiding Zhang; Xiao Wang; Chuan Shi; Xunqiang Jiang; Yanfang Fanny; Ye <h2>ref_id</h2>b96<h2>title</h2>Lorentzian Graph Convolutional Networks<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Yiding Zhang; Xiao Wang; Chuan Shi; Nian Liu; Guojie Song<h2>ref_id</h2>b97<h2>title</h2>Graphadaptive rectified linear unit for graph neural networks<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Yifei Zhang; Hao Zhu; Ziqiao Meng; Piotr Koniusz; Irwin King<h2>ref_id</h2>b98<h2>title</h2>COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive Learning<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Yifei Zhang; Hao Zhu; Zixing Song; Piotr Koniusz; Irwin King<h2>ref_id</h2>b99<h2>title</h2>Spectral Feature Augmentation for Graph Contrastive Learning and Beyond<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Yifei Zhang; Hao Zhu; Zixing Song; Piotr Koniusz; Irwin King<h2>ref_id</h2>b100<h2>title</h2>TeleGraph: A benchmark dataset for hierarchical link prediction<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Min Zhou; Bisheng Li; Menglin Yang; Lujia Pan<h2>ref_id</h2>b101<h2>title</h2>Hyperbolic Graph Representation Learning: A Tutorial<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Min Zhou; Menglin Yang; Lujia Pan; Irwin King<h2>ref_id</h2>b102<h2>title</h2>Graph Geometry Interaction Learning<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Shichao Zhu; Shirui Pan; Chuan Zhou; Jia Wu; Yanan Cao; Bin Wang<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Figure 1 :1Figure 1: The (tree-like) graph 𝐺 in the left subfigure can be considered as a discrete approximation to the hyperbolic manifold M in the right subfigure; on the other hand, the hyperbolic manifold M can also be approximated as the graph 𝐺. ACM Reference Format: Menglin Yang, Min Zhou, Lujia Pan, and Irwin King. 2023. 𝜅HGCN:Treelikeness Modeling via Continuous and Discrete Curvature Learning. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23), August 6-10, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3580305.3599532<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Figure 2 :2Figure 2: Illustration of two tree-like graphs: 𝑇 1 (left) is pure and 𝑇 2 (right) has a local heterogeneous structure. The arrows indicate the aggregation flow and the values on the edges represent the edge curvature, denoted as 𝜅 1 for 𝑇 1 and 𝜅 2 for 𝑇 2 . By incorporating curvature 𝜅 into the neighboring aggregation, the detection of local structures is improved. This is achieved by assigning asymmetric weights (𝜅1 𝑎,𝑜 vs 𝜅 1 𝑎,𝑏 ) to nodes at different levels and results in larger values for nodes in a circular shape (𝜅 1 𝑎,𝑜 vs 𝜅 2 𝑎,𝑜 and 𝜅 1 𝑎,𝑐 vs 𝜅 2 𝑎,𝑐 ).<h2>figure_data</h2><h2>figure_label</h2>124<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>4. 1 . 2 Figure 4 :124Figure 4: Illustration of the proposed Curvature-aware aggregation. Compared with the traditional aggregation scheme, the proposed method further makes the aggregation process aware of the local structure.<h2>figure_data</h2><h2>figure_label</h2>5<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>Figure 5 :5Figure 5: The performance of node classification by 𝜅HGCN with 𝜅HC and without 𝜅HC.<h2>figure_data</h2><h2>figure_label</h2>6<h2>figure_type</h2>figure<h2>figure_id</h2>fig_4<h2>figure_caption</h2>Figure 6 :6Figure 6: A tree-like area (upper row) and a triangle-contained area (lower row) with edge weights by HGCN (left column) and 𝜅HGCN (right column).<h2>figure_data</h2><h2>figure_label</h2>7<h2>figure_type</h2>figure<h2>figure_id</h2>fig_5<h2>figure_caption</h2>Figure 7 :7Figure 7: Visualization of Cora and DISEASE. First row: Emof Cora by HGCN (left) and 𝜅HGCN (right)]; Second row: Embedding of DISEASE by HGCN (left) and 𝜅HGCN (right)<h2>figure_data</h2><h2>figure_label</h2>22212<h2>figure_type</h2>figure<h2>figure_id</h2>fig_6<h2>figure_caption</h2>x 2 I 2 1+𝜅 ∥x∥ 2 2 𝑔cosh - 1 2 √22212𝑛 where 𝜆 𝜅 x = L𝜅 x = 𝜂, where 𝜂 is𝐼 except 𝜂 0,0 = -1 𝜅 ⟨x, y⟩ L Logarithmic Map log 𝜅 x (y) = |𝜅 |𝜆 𝜅 tanh -1 √︁ |𝜅 | ∥-x ⊕ 𝜅 y∥ 2 -x⊕𝜅 y ∥ -x⊕𝜅 y∥ 2 log 𝜅 x (y) = cosh -1 (𝜅 ⟨x,y⟩ L ) sinh(cosh -1 (𝜅 ⟨x,y⟩ L )) y -𝜅 ⟨x, y⟩ L x Exponential Map exp 𝜅 x (v) = x ⊕ 𝜅 tanh √︁ , -x]𝑣 𝑃𝑇 𝜅 x→y (v) = v -𝜅 ⟨y,v⟩ L 1+𝜅 ⟨x,y⟩ L (x + y)<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>1 , 𝑣 2 ) is the shortest path between 𝑣 1 and 𝑣 2 on graph 𝐺, 𝑊 (𝑚 𝑣 1 , 𝑚 𝑣 2 ) is the Wasserstein distance (see Definition 3.2) between two probability measures (see Definition 3.3) 𝑚 𝑣 1 and 𝑚 𝑣 2 . Definition 3.2 (Wasserstein Distance). Let 𝑚 1 , 𝑚 2 be two probability measures on 𝑉 . The Wasserstein distance 𝑊 (𝑚 1 , 𝑚 2 ) between 𝑚 1 and 𝑚 2 is given by<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>Statistics of the datasets<h2>figure_data</h2>DatasetNodes Edges Classes Node features Hyperbolicity𝛿Disease (NC) 10441043210000Disease (LP) 26652664210000Airport318818631441PubMed19717 8865135003.5Cora270854297143311<h2>figure_label</h2>3<h2>figure_type</h2>table<h2>figure_id</h2>tab_3<h2>figure_caption</h2>Profiling evaluation on node classification. F1-score with standard deviation for Disease and Airport; accuracy for others (the higher, the better).<h2>figure_data</h2>DatasetDiseaseAirportPubMedCoraHyperbolocity (𝛿)013.511EUC32.5 ± 1.1 60.9 ± 3.4 48.2 ± 0.7 23.8 ± 0.7HYP [56]45.5 ± 3.3 70.2 ± 0.1 68.5 ± 0.3 22.0 ± 1.5GCN [35]69.7 ± 0.4 81.4 ± 0.6 78.1 ± 0.2 81.3 ± 0.3GAT [78]70.4 ± 0.4 81.5 ± 0.3 79.0 ± 0.3 83.0 ± 0.7SAGE [29]69.1 ± 0.6 82.1 ± 0.5 77.4 ± 2.2 77.9 ± 2.4SGC [83]69.5 ± 0.2 80.6 ± 0.1 78.9 ± 0.0 81.0 ± 0.1CurvGN [93]89.8 ± 2.9 84.7 ± 1.5 78.3 ± 0.3 82.0 ± 0.9𝜅GCN [6]82.1 ± 1.1 84.4 ± 0.4 78.3 ± 0.6 80.8 ± 0.6HGCN [11]74.5 ± 0.9 90.6 ± 0.2 80.3 ± 0.3 79.9 ± 0.2LGCN [97]84.4 ± 1.0 90.9 ± 1.0 78.8 ± 0.5 83.3 ± 0.5𝜅HGCN (Ours)92.3 ± 1.4 92.8 ± 0.4 82.1 ± 0.5 82.5 ± 0.6Δ 𝐸 (%)+31.1+13.0+3.9-0.6Δ 𝜅 (%)+2.8+9.6+4.9+0.6Δ 𝐻 (%)+2.2+3.6+5.0+5.4<h2>figure_label</h2>4<h2>figure_type</h2>table<h2>figure_id</h2>tab_4<h2>figure_caption</h2>Profiling evaluation on link prediction. AUC scores with standard deviation are reported (the higher, the better). The best is bold.<h2>figure_data</h2>DatasetDiseaseAirportPubMedCoraHyperbolicity(𝛿)013.511EUC60.9 ± 3.4 92.0 ± 0.0 83.3 ± 0.1 82.5 ± 0.3HYP [56]70.2 ± 0.1 94.5 ± 0.0 87.5 ± 0.1 87.6 ± 0.2GCN [35]64.7 ± 0.5 89.3 ± 0.4 91.1 ± 0.5 90.4 ± 0.2GAT [78]69.8 ± 0.3 90.5 ± 0.3 91.2 ± 0.1 93.7 ± 0.1SAGE [28]65.9 ± 0.3 90.4 ± 0.5 86.2 ± 1.0 85.5 ± 0.6SGC [83]65.1 ± 0.2 89.8 ± 0.3 94.1 ± 0.0 91.5 ± 0.1CurvGN [93]80.6 ± 0.8 89.5 ± 0.3 91.6 ± 0.4 72.5 ± 0.7𝜅GCN [6]92.0 ± 0.5 92.5 ± 0.5 94.9 ± 0.3 92.6 ± 0.4HGCN [11]90.8 ± 0.3 96.4 ± 0.1 96.3 ± 0.0 92.9 ± 0.1LGCN [97]96.6 ± 0.6 96.0 ± 0.6 96.6 ± 0.1 93.6 ± 0.4𝜅HGCN (Ours) 96.7 ± 0.1 98.2 ± 0.1 96.7 ± 0.1 95.0 ± 0.1Δ 𝐸 (%)+27.2+8.5+2.8+1.4Δ 𝜅 (%)+4.1+6.2+1.9+2.6Δ 𝐻 (%)+0.6+0.2+1.3+1.1<h2>figure_label</h2>5<h2>figure_type</h2>table<h2>figure_id</h2>tab_5<h2>figure_caption</h2>Performance of different aggregation methods: curvature (Curv) and feature-enhanced curvature (CurvAtt).<h2>figure_data</h2>DatasetNCLPCurvCurvAttCurvCurvAttDisease 91.8 ± 1.9 92.3 ± 1.4 95.7 ± 0.3 95.8 ± 0.1Airport 92.4 ± 1.0 92.8 ± 0.8 98.0 ± 0.1 98.2 ± 0.1PubMed 81.2 ± 0.1 82.1 ± 0.4 96.5 ± 0.1 96.7 ± 0.1Cora82.3 ± 0.6 82.5 ± 0.6 94.9 ± 0.3 95.0 ± 0.5<h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_6<h2>figure_caption</h2>denoted by gyr[., .]𝑣. 𝑛 : ∥x∥ 2 < -1/𝜅 . The metric tensor of this model is expressed as 𝑔 B = 𝜆 2 𝑔 𝐸 , where the conformal factor 𝜆 = 2 1+𝜅 |x| 2 and 𝑔 𝐸 = 𝐼 𝑛 represents the Euclidean metric. https://github.com/HazyResearch/hgcn Definition B.2 (Lorentz Model). The Lorentz model, also known as the hyperboloid model, is defined as a Riemannian manifold (L 𝑛 𝜅 , 𝑔 L ), where L 𝑛 𝐾 = x ∈ R 𝑛+1 : ⟨x, x⟩ L = 1 𝜅 . The metric tensor of the Lorentz model is given by 𝑔 L = diag([-1, 1, ..., 1]), where 𝜅 is the negative curvature constant.<h2>figure_data</h2>Definition B.1 (Poincaré Ball Model). The Poincaré Ball Modelwith negative curvature 𝜅 is defined as a Riemannian manifold(B 𝑛 𝜅 , 𝑔 B ), where B 𝑛 𝜅 is an open 𝑛-dimensional ball with radius 1/√︁|𝜅 |,B 𝑛 𝜅 = x ∈ R<h2>figure_label</h2>6<h2>figure_type</h2>table<h2>figure_id</h2>tab_7<h2>figure_caption</h2>Summary of operations in the Poincaré ball model and the Lorentz model (𝜅 < 0)<h2>figure_data</h2>Poincaré Ball ModelLorentz Model (Hyperboloid Model)ManifoldB 𝑛 𝜅<h2>figure_label</h2>7<h2>figure_type</h2>table<h2>figure_id</h2>tab_8<h2>figure_caption</h2>Computation cost of Ricci curvature where the unit of time is in seconds. NC: node classification; LP: link prediction.<h2>figure_data</h2>TaskDisease Airport PubMed CoraNC (s)0.82.745.52.3LP (s)1.52.241.32.1<h2>figure_label</h2>8<h2>figure_type</h2>table<h2>figure_id</h2>tab_9<h2>figure_caption</h2>Curvature-wise performance on cora dataset the representations learned by 𝜅HGCN exhibit sharper boundaries between different classes, thus showcasing the improved discriminative power of the proposed method compared to HGCN.<h2>figure_data</h2>MethodNegative Zero PositiveGT-Prop (%)0.5360.1380.326GCN (%)0.5120.0850.216HGCN (%)0.5230.0770.213𝜅HGCN (%)0.5350.0820.217<h2>figure_label</h2>9<h2>figure_type</h2>table<h2>figure_id</h2>tab_10<h2>figure_caption</h2>Curvature-wise performance on Airport dataset local substructure, with the proportions summing to 1. The values in the GCN, HGCN, and proposed model rows represent the proportion of nodes in each substructure that were correctly predicted by the respective models. In other words, the table presents the accuracy of each model in representing the local environment for different types of nodes.<h2>figure_data</h2>MethodNegative Zero PositiveGT-Prop (%)0.4180.4390.143GCN (%)0.2770.4240.118HGCN (%)0.3820.4050.117𝜅HGCN (%)0.3950.4100.123<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>𝐿(𝛾) = ∫ 𝛽 𝛼 ∥𝛾 ′ (𝑡)∥ 𝑔 𝑑𝑡. Then the distance of u, v ∈ M, 𝑑 M (u, v) = inf 𝐿(𝛾) where 𝛾 is a curve that 𝛾 (𝑎) = u, 𝛾 (𝑏) = v.<h2>formula_coordinates</h2>[3.0, 317.82, 153.37, 240.38, 21.68]<h2>formula_id</h2>formula_1<h2>formula_text</h2>𝜅 (𝑣 1 , 𝑣 2 ) = 1 - 𝑊 (𝑚 𝑣 1 , 𝑚 𝑣 2 ) 𝑑 (𝑣 1 , 𝑣 2 ) ∈ (-2, 1),(1)<h2>formula_coordinates</h2>[3.0, 367.77, 508.09, 190.97, 20.37]<h2>formula_id</h2>formula_2<h2>formula_text</h2>𝑊 (𝑚 1 , 𝑚 2 ) = inf 𝜋 𝑖,𝑗 ∈Π ∑︁ 𝑣 𝑖 ,𝑣 𝑗 ∈𝑉 𝜋 𝑖,𝑗 (𝑣 𝑖 , 𝑣 𝑗 )𝑑 (𝑣 𝑖 , 𝑣 𝑗 ),(2)<h2>formula_coordinates</h2>[3.0, 353.39, 610.29, 205.35, 21.67]<h2>formula_id</h2>formula_3<h2>formula_text</h2>∑︁ 𝑣 𝑖 ∈𝑉 𝜋 𝑖,𝑗 (𝑣 𝑖 , 𝑣 𝑗 ) = 𝑚 1 ; ∑︁ 𝑣 𝑗 ∈𝑉 𝜋 𝑖,𝑗 (𝑣 𝑖 , 𝑣 𝑗 ) = 𝑚 2 . (3<h2>formula_coordinates</h2>[3.0, 358.65, 687.75, 196.92, 21.67]<h2>formula_id</h2>formula_4<h2>formula_text</h2>)<h2>formula_coordinates</h2>[3.0, 555.57, 693.36, 3.17, 4.09]<h2>formula_id</h2>formula_5<h2>formula_text</h2>𝑇 ℎ ′ ℍ 𝑑 ′ ,𝐾 ℍ 𝑑,𝐾 x 𝐻 ∈ ℝ 𝑛×𝑑 h 𝐻 ∈ ℝ 𝑛×𝑑 ′ ℍ 𝑑 ′ ,𝐾 x 𝐸 ∈ ℝ 𝑛×𝑑 𝒛 ∈ 𝐻 𝑛x𝑐 x 𝐻 𝑇 𝑜 ℍ 𝑑,𝐾 AGG 𝜅 (h 𝑖 𝐻 ) h 𝑖 𝐻 log h ′ 𝐾 (⋅) exp h ′ 𝐾 (⋅) ෨ h 𝑖 𝐻 ℍ 𝑑 ′ ,𝐾 𝑑 ′ 𝑑 ′ 𝑑 ′ 2𝑑 ′ , 1 𝜅 𝑖𝑗 𝛼 𝑖𝑗 h 𝑖 𝐻 h 𝑗 𝐻 h 𝑖 𝐻 h 𝑗 𝐻 ℍ Figure 3: Schematic of 𝜅HGCN. (1)<h2>formula_coordinates</h2>[4.0, 53.8, 100.2, 456.62, 270.61]<h2>formula_id</h2>formula_6<h2>formula_text</h2>𝑚 𝑣 𝑖 =        𝑝, if 𝑣 = 𝑣 𝑖 1-𝑝 𝑑 𝑣 , if (𝑣 𝑖 ) ∈ 𝑁 (𝑣). 0, otherwise(4)<h2>formula_coordinates</h2>[4.0, 112.39, 487.98, 182.19, 38.12]<h2>formula_id</h2>formula_7<h2>formula_text</h2>h ℓ,H 𝑖 = W ℓ ⊗ 𝜅 ℓ -1 x ℓ -1,H 𝑖 ⊕ 𝜅 ℓ -1 b ℓ ,(5)<h2>formula_coordinates</h2>[4.0, 372.03, 597.94, 186.71, 12.67]<h2>formula_id</h2>formula_8<h2>formula_text</h2>W ⊗ 𝜅 x := exp 𝜅 o (W log 𝜅 o (x)) and x ⊕ 𝜅 b := exp 𝜅<h2>formula_coordinates</h2>[4.0, 336.96, 635.98, 176.55, 10.81]<h2>formula_id</h2>formula_9<h2>formula_text</h2>hℓ,H 𝑖 = exp 𝜅 𝑙 -1 o ∑︁ 𝑗 ∈ N 𝑖 κ𝑖,𝑗 • log 𝜅 𝑙 -1 o (h ℓ,H 𝑗 ) .(6)<h2>formula_coordinates</h2>[5.0, 98.9, 300.46, 195.69, 21.8]<h2>formula_id</h2>formula_10<h2>formula_text</h2>κ𝑖,𝑗 = softmax 𝑗 ∈ N (𝑖 ) MLP(𝜅 𝑖,𝑗 ) ,(7)<h2>formula_coordinates</h2>[5.0, 112.62, 366.41, 181.96, 9.43]<h2>formula_id</h2>formula_11<h2>formula_text</h2>κ′ 𝑖,𝑗 = 𝑤 𝜅 κ𝑖,𝑗 + 𝑤 𝛼 𝛼 𝑖,𝑗 𝑤 𝜅 + 𝑤 𝛼 ,(8)<h2>formula_coordinates</h2>[5.0, 133.5, 470.59, 161.09, 20.75]<h2>formula_id</h2>formula_12<h2>formula_text</h2>𝛼 𝑖,𝑗 = softmax 𝑗 ∈ N 𝑖 MLP(log 𝜅 o (h H 𝑖 )∥ log 𝜅 o (h H 𝑗 )) .(9)<h2>formula_coordinates</h2>[5.0, 81.28, 545.98, 213.3, 13.01]<h2>formula_id</h2>formula_13<h2>formula_text</h2>x ℓ,H 𝑖 = 𝜎 𝜅 ℓ -1 ,𝜅 ℓ ( hℓ,H ) = exp 𝜅 ℓ o 𝜎 (log 𝜅 ℓ -1 o ( hℓ,H )) . (10<h2>formula_coordinates</h2>[5.0, 80.85, 593.33, 210.32, 11.21]<h2>formula_id</h2>formula_14<h2>formula_text</h2>)<h2>formula_coordinates</h2>[5.0, 291.16, 596.76, 3.42, 4.09]<h2>formula_id</h2>formula_15<h2>formula_text</h2>𝜅 𝑖,𝑗 ≥ -1 - 1 𝑑 𝑖 - 1 𝑑 𝑗 - #(𝑖, 𝑗) 𝑑 𝑖 ∧ 𝑑 𝑗 + -1 - 1 𝑑 𝑖 - 1 𝑑 𝑗 - #(𝑖, 𝑗) 𝑑 𝑖 ∨ 𝑑 𝑗 + + #(𝑖, 𝑗) 𝑑 𝑖 ∨ 𝑑 𝑗 , (11<h2>formula_coordinates</h2>[5.0, 356.09, 221.16, 199.23, 46.72]<h2>formula_id</h2>formula_16<h2>formula_text</h2>)<h2>formula_coordinates</h2>[5.0, 555.32, 240.69, 3.42, 4.09]<h2>formula_id</h2>formula_17<h2>formula_text</h2>𝐷 = ∥h 𝑖 -h 𝑗 ∥,(12)<h2>formula_coordinates</h2>[5.0, 410.98, 401.14, 147.76, 8.4]<h2>formula_id</h2>formula_18<h2>formula_text</h2>𝐷 𝑙 = ∥(h 𝑖 + 𝛼 𝑖 h 𝑗 ) -(h 𝑗 + 𝛼 𝑗 h 𝑖 )∥ = ∥(h 𝑖 -𝛼 𝑗 h 𝑖 ) -(h 𝑗 -𝛼 𝑖 h 𝑗 )∥,(13)<h2>formula_coordinates</h2>[5.0, 376.08, 605.82, 182.66, 22.27]<h2>formula_id</h2>formula_19<h2>formula_text</h2>𝐷 𝑙 ≈ ∥(h 𝑖 -𝛼 𝑖 𝑗 h 𝑖 ) -(h 𝑗 -𝛼 𝑖 𝑗 h 𝑗 )∥ = (1 -𝛼 𝑖 𝑗 )∥h 𝑖 -h 𝑗 ∥ < 𝐷.(14)<h2>formula_coordinates</h2>[5.0, 373.93, 673.64, 184.81, 34.0]<h2>formula_id</h2>formula_20<h2>formula_text</h2>𝐷 𝑠 = ∥(h 𝑖 -𝛽 𝑖 h 𝑗 ) -(h 𝑗 -𝛽 𝑗 h 𝑖 )∥ ≈ (1 + 𝛽 𝑖 𝑗 )∥h 𝑖 -h 𝑗 ∥ > 𝐷,(15)<h2>formula_coordinates</h2>[6.0, 112.71, 169.63, 181.87, 34.0]<h2>formula_id</h2>formula_21<h2>formula_text</h2>z 𝑢 = ∑︁ 𝑗 ∈ N (𝑢 ) κ𝑢,𝑗 z 𝑗 = 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑖 ) exp( κ𝑢,𝑗 )z 𝑗 ,(16)<h2>formula_coordinates</h2>[6.0, 90.87, 594.16, 203.72, 22.46]<h2>formula_id</h2>formula_22<h2>formula_text</h2>z 𝑢 = 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑢 ) exp( κ𝑢,𝑗 ) * 1 𝐾 𝑗 𝑗 ∑︁ 𝑘 ∈ N ( 𝑗 ) exp( κ𝑗,𝑘 )z 𝑘 . (17<h2>formula_coordinates</h2>[6.0, 69.55, 654.99, 221.61, 22.46]<h2>formula_id</h2>formula_23<h2>formula_text</h2>)<h2>formula_coordinates</h2>[6.0, 291.16, 661.06, 3.42, 4.09]<h2>formula_id</h2>formula_24<h2>formula_text</h2>z 𝑢 = 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑢 ) exp( κ𝑖,𝑗 ) * • • • * 1 𝐾 𝑜𝑜 ∑︁ 𝑝 ∈ N (𝑜 )<h2>formula_coordinates</h2>[6.0, 326.95, 104.62, 164.57, 22.46]<h2>formula_id</h2>formula_25<h2>formula_text</h2>𝐼 𝑢,𝑣 = ∥ 𝜕z 𝑢 𝜕z 𝑣 ∥ = ∥ 𝜕 𝜕z 𝑣 ( 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑢 ) exp( κ𝑖,𝑗 ) * • • • * 1 𝐾 𝑜𝑜 ∑︁ 𝑝 ∈ N (𝑜 ) exp( κ𝑜,𝑝 )z 𝑝 ) ∥. (19)<h2>formula_coordinates</h2>[6.0, 319.42, 190.83, 239.32, 53.99]<h2>formula_id</h2>formula_26<h2>formula_text</h2>𝐼 𝑢,𝑣 = 𝜕 𝜕z 𝑣 𝐼 𝑝 1 + • • • + 𝐼 𝑝 𝑖 • • • + 𝐼 𝑝 𝑛 ,(20)<h2>formula_coordinates</h2>[6.0, 369.15, 301.24, 189.59, 17.56]<h2>formula_id</h2>formula_27<h2>formula_text</h2>𝐼 𝑝 𝑖 = 1 𝐾 𝑢𝑢 exp( κ𝑢,𝑝 𝑖 𝑗 ) • • • 1 𝐾 𝑝 𝑖 𝑛 𝑖 𝑝 𝑖 𝑛 𝑖 exp( κ𝑝 𝑖 𝑛 𝑖 ,𝑣 ) 𝑆 (𝐼 𝑝 𝑖 ) z 𝑣 .<h2>formula_coordinates</h2>[6.0, 350.86, 342.21, 174.15, 41.47]<h2>formula_id</h2>formula_28<h2>formula_text</h2>𝐼 𝑢,𝑣 = 𝑆 (𝐼 𝑝 1 ) + • • • + 𝑆 (𝐼 𝑝 𝑖 ) • • • + 𝑆 (𝐼 𝑝 𝑛 ) ∥ 𝜕𝑧 𝑣 𝜕𝑧 𝑣 ∥ = 𝑆 (𝐼 𝑝 1 ) + • • • + 𝑆 (𝐼 𝑝 𝑖 ) • • • + 𝑆 (𝐼 𝑝 𝑛 ) ≤ |𝑛 * max(𝑆 (𝐼 𝑝 𝑖 ))| = |𝑛 * 𝛾 𝑛 𝑖 | ≤ |𝑛 * 𝛾 𝑑 * 𝑔 | = 𝐶𝛾 𝑑 * 𝑔 ,(21)<h2>formula_coordinates</h2>[6.0, 354.02, 442.28, 204.72, 95.07]<h2>formula_id</h2>formula_29<h2>formula_text</h2>EUC × × × HYP ✓ × × Euclidean GNN models GCN × × ✓ GAT × × ✓ SAGE × × ✓ SGC × × ✓ Curvature GNN models CurvGN × ✓ ✓ 𝜅GCN ✓ × ✓ Hyperbolic GNN models HGCN ✓ × ✓ LGCN ✓ × ✓ Curvature-aware HGNN model 𝜅HGCN ✓ ✓ ✓<h2>formula_coordinates</h2>[7.0, 53.8, 168.71, 227.63, 124.13]<h2>formula_id</h2>formula_30<h2>formula_text</h2>L 𝜅ℎ𝑐 + = - 1 |𝐸 𝜅 | ∑︁ (𝑖,𝑗 ) ∈𝐸 𝜅 log 𝑝 (x ℓ,H 𝑖 , x ℓ,H 𝑗 ),(22)<h2>formula_coordinates</h2>[7.0, 98.83, 342.12, 195.75, 22.5]<h2>formula_id</h2>formula_31<h2>formula_text</h2>𝑝 (x 𝑢 , x 𝑣 ) = exp (𝑑 2 H (x 𝑢 , x 𝑣 ) -𝑟 )/𝑡 + 1 -1 ,(23)<h2>formula_coordinates</h2>[7.0, 93.45, 407.21, 201.13, 17.32]<h2>formula_id</h2>formula_32<h2>formula_text</h2>L 𝜅ℎ𝑐 = L 𝜅ℎ𝑐 + + L 𝜅ℎ𝑐 -.(24)<h2>formula_coordinates</h2>[7.0, 130.77, 486.03, 163.82, 10.23]<h2>formula_id</h2>formula_33<h2>formula_text</h2>= x ∈ R 𝑛 : ⟨x, x⟩ 2 < -1 𝜅 L 𝑛 𝜅 = x ∈ R 𝑛+1 : ⟨x, x⟩ L = 1 𝜅 Metric 𝑔 B𝜅 x = 𝜆 𝜅<h2>formula_coordinates</h2>[13.0, 94.93, 126.48, 384.75, 20.49]<h2>formula_id</h2>formula_34<h2>formula_text</h2>𝜅 𝑖 = 1 |𝑁 𝑖 |<h2>formula_coordinates</h2>[13.0, 178.52, 600.66, 32.12, 11.59]<h1>doi</h1>10.1145/3580305.3599532<h1>title</h1>i-Razor: A Differentiable Neural Input Razor for Feature Selection and Dimension Search in DNN-Based Recommender Systems<h1>authors</h1>Yao Yao; Bin Liu; Haoxun He; Dakui Sheng; Ke Wang; Li Xiao; Huanhuan Cao<h1>pub_date</h1>2023-11-12<h1>abstract</h1>Input features play a crucial role in DNN-based recommender systems with thousands of categorical and continuous fields from users, items, contexts, and interactions. Noisy features and inappropriate embedding dimension assignments can deteriorate the performance of recommender systems and introduce unnecessary complexity in model training and online serving. Optimizing the input configuration of DNN models, including feature selection and embedding dimension assignment, has become one of the essential topics in feature engineering. However, in existing industrial practices, feature selection and dimension search are optimized sequentially, i.e., feature selection is performed first, followed by dimension search to determine the optimal dimension size for each selected feature. Such a sequential optimization mechanism increases training costs and risks generating suboptimal input configurations. To address this problem, we propose a differentiable neural input razor (i-Razor) that enables joint optimization of feature selection and dimension search. Concretely, we introduce an end-to-end differentiable model to learn the relative importance of different embedding regions of each feature. Furthermore, a flexible pruning algorithm is proposed to achieve feature filtering and dimension derivation simultaneously. Extensive experiments on two large-scale public datasets in the Click-Through-Rate (CTR) prediction task demonstrate the efficacy and superiority of i-Razor in balancing model complexity and performance.<h1>sections</h1><h2>heading</h2>INTRODUCTION<h2>text</h2>I N this era of information overload, recommender systems powered by information technology and deep learning have become an effective way to retrieve potentially useful information for users from a huge range of options [1], [2], [3]. Deep learning recommender systems enhance recommendation performance by capturing complex correlations between features [4]. Experts resort to feature engineering, generating categorical, numerical, statistical, and cross features, to better understand the user's interests. However, due to the universal approximation property [5] of deep neural networks (DNN), feeding noisy features into a DNN model can adversely affect predictive performance [4], [6]. Meanwhile, because the embedding lookup table dominates both the size and inductive bias of DNN-based recommendation models [7], the embedding dimensions of input features are crucial to the performance and complexity of recommender systems. Therefore, it is highly desired to efficiently identify effective features from the original feature set and specify appropriate embedding dimensions. As illustrated in Figure 1, conventional industrial practices typically optimize feature selection and embedding dimension allocation separately. In contrast, this work explores jointly optimizing both in an end-to-end fashion. Feature selection is a crucial task in recommender systems and has been the subject of extensive research. Previous studies have categorized feature selection methods into three main categories: Filter, Wrapper, and Embedded methods [8], [9], [10]. Filter methods measure the discriminant attributes of features through criteria such as information gain [11] and feature consistency [12], so as to extract high-ranked features. Wrapper methods optimize feature subsets by using performance assessments such as the K-nearest neighbor (KNN) algorithm and linear discriminant analysis (LDA) instead of solely relying on evaluation criteria [13]. Compared with Filter and Wrapper, Embedded methods, such as the least absolute shrinkage and selection operator (LASSO) [14] and the gradient-boosting decision tree (GBDT) [15], [16], incorporate feature selection as part of the training process. Despite compensating for the drawbacks of low efficiency in Filter and high computational cost in Wrapper, the effectiveness of Embedded methods is substantially dependent on conductive parameters and strict model assumptions [9]. Although the aforementioned feature selection methods have proven effective in several scenarios, there could be a mismatch between the subset of features selected to optimize the feature selection model and the subset that would ideally serve the target DNN model. Recognizing that DNN models can capture more complex feature interactions, it would be logical to use the target DNN model to guide feature selection directly. In light of this, we propose a model-consistent feature selection approach that integrates the target DNN model into the feature selection process. Our approach belongs to the Embedded methods in a broad sense, but there is a notable difference between our study and previous work: we evaluate the importance of features from the point of view of each feature's optimal embedding dimension size under the target DNN model.
In recent years, there has been an increasing interest in studying embedding dimensions, as the embedding layer impacts on both the scale and the inductive bias of DNNbased recommendation models [7]. Specifying an adaptive embedding dimension for each feature is a problem of exponential complexity. A common practice is to set a unified value for all features by hyperparameter tuning experiments or relying on expert knowledge. However, such practice does not distinguish between different features and can be memory inefficient. Inspired by recent progress in neural architecture search (NAS) [17], methods based on reinforcement learning [18], differentiable search [19], [20], pruning [21], and so forth, have been proposed to perform dimension search at either field level or feature level 1 . Despite the ability of these methods to reduce model parameters without compromising model performance given a carefully selected feature subset, their effectiveness without feature selection remains unexplored. The dependency on feature selection effectively equates to assuming all input features are beneficial during the dimension search process. We believe that there is a necessity to conduct dimension search under the assumption that not all features in the original feature set are helpful, as the updating of features can be very frequent in industrial practice. With this objective, we propose a differentiable framework that identifies redundantly assigned embedding dimensions for pruning. Specifically, a feature with its embedding dimension pruned to 0 is considered unimportant. It is worth noting that features are typically grouped into fields for practical reasons in industrial recommender systems, and these fields can 1. Field-level methods assign the same dimension to all features within the same field while feature-level ones can assign diverse dimensions to different features within the same field.
have significantly varying cardinalities. For example, the field Gender generally contains two features "Female" and "Male", while the field Browsed Goods may have millions of features. While individual dimension assignment for each feature (i.e., feature-level methods) might boost performance, field selection and field-level dimension search are often prioritized in large-scale recommender systems to reduce complexity and enhance implementation ease. This paper emphasizes assigning different dimension sizes to fields based on their importance, and discarding less important fields during the retraining stage. Unless otherwise specified, the term "feature selection" hereafter refers to the selection of fields.
To guide the input configuration towards better optima, we propose a differentiable neural input razor (i-Razor) that is capable of simultaneously pruning unnecessary fields and assigning adaptive embedding dimensions. The training of i-Razor consists of two stages. i) In the pretraining stage, we refine the dimension search space to learn the importance of different fields and different embedding regions. Meanwhile, to control the number of model parameters and avoid overfitting, we introduce a novel regularization term to regularize the model complexity. ii) In the retraining stage, a proposed pruning algorithm is executed to remove redundant fields and preserve the desired embedding dimensions for the remaining fields according to the relative weights of each embedding region. We then reconstruct the DNN model and retrain it with the training dataset based on the derived input configuration. We conduct extensive experiments on two large-scale public datasets for the CTR prediction task. The experimental results show that i-Razor achieves higher prediction accuracy while saving fields than several state-ofthe-art methods.
The main contributions of this paper are threefold. • Traditionally, feature selection and embedding dimension search were treated as separate processes. Our research presents a pioneering approach that explores their joint optimization. This innovative leap not only bridges the gap between feature selection and dimension search but also hints at a higher level of alignment between the two, which has the potential to discover input configurations that more closely approach optimal solutions. The remainder of this paper is organized as follows: Section 2 discusses the related literature on feature selection and embedding dimension search. We formulate the input configuration optimization problem and present the framework of i-Razor in Section 3. In Section 4, we elaborate on our experimental setup and empirical results. Finally, we conclude our work and discuss its potential future extensions in Section 5.<h2>publication_ref</h2>['b0', 'b1', 'b2', 'b3', 'b4', 'b3', 'b5', 'b6', 'b7', 'b8', 'b9', 'b10', 'b11', 'b12', 'b13', 'b14', 'b15', 'b8', 'b6', 'b16', 'b17', 'b18', 'b19', 'b20']<h2>figure_ref</h2>['fig_0']<h2>table_ref</h2>[]<h2>heading</h2>RELATED WORK<h2>text</h2>In this section, we briefly review the works related to our study, including two branches of the literature: feature selection approaches and dimension search techniques.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Feature Selection Approaches<h2>text</h2>Feature selection refers to the process of finding out a subset from the original feature set without losing useful information [22]. Existing feature selection methods include the following three categories: Filter, Wrapper, and Embedded methods [23]. Filter methods measure the association between the feature and the class label via some specific evaluation criteria [22], such as information gain [24], minimum redundant maximum relevance [25], and spatiotemporal preserving representations [26]. This stream of research also includes feature selection methods based on statistical tests, e.g., Chi-squared [27], Pearson correlation, and ANOVA F-value test [28]. Input features that demonstrate a significant statistical relationship with the output variable are retained and utilized for further analysis, while others are discarded. Filter methods are expected to have the highest computational efficiency and tend to keep more features from the total feature space [9], [10]. By contrast, Wrapper methods adopt specific learning techniques such as KNN [29], Naive Bayes [30], LDA [31], and adaptive semisupervised feature analysis [32] to assess the performance of feature combinations [13]. Though more efficient for feature selection than Filter methods, Wrapper methods require higher computational resources [33]. In these two types of feature selection methods, the feature selection process and the model training process are distinctly separate.
Unlike Filter and Wrapper methods, Embedded methods such as LASSO [14], GBDT [15], and semi-supervised recurrent convolutional attention models [34] integrate the feature selection process with the model training process, i.e., feature selection is performed automatically during the model training process [8]. Recently, the advancement of NAS has spawned a new research hotspot in Embedded methods, the automatic search for useful features with AutoML techniques [10], [35]. For instance, AutoFIS [36] proposes a gating mechanism to identify and mask the intersection of useless features in factorization models. AutoField [37] assigns two operators to each field, one for selecting and the other for ignoring that field. By comparing the relative importance of these two operators, it determines whether to select that field or not. Operators of each field of AutoField [37] and AutoFIS [36] are independent of each other, while AdaFS [38] assigns associated operators to each field through softmax operation, and measures the relative importance of fields corresponding to each operator by comparing the weight of each operator, so as to screen features. Meanwhile, there has been a growing interest in automatic feature selection in deep learning-based sequential recommendation systems. For instance, NASR [35] made significant strides in designing a hybrid sequential recommendation model with the aim to ensemble the capacity of both self-attention and convolutional architectures. The proposed method can automatically select the architecture operation on each layer.
Inspired by these advancements, our approach, i-Razor, also belongs to the Embedded category, but with a unique perspective. i-Razor facilitates model-consistent feature selection by evaluating the importance of each feature concerning its optimal embedding dimension in the target DNN model. Building upon the foundation laid by AutoField, we propose a 0-dimension absorbing operator, which signifies the action of deselecting a specific feature. In addition, we enhance the existing design of the dimension search space, which commonly segments the embedding dimensions, by introducing the concept of the 0-dimension operator into this process. The weight associated with each operator measures the relative importance of these divisions, with 0 dimension indicating that all regions should be masked. By considering combinations of these operators, we derive the final input configuration. This method allows us to perform dimension search and feature selection concurrently, offering a more efficient manner to ensure that the chosen features and embedding dimensions contribute to the target model's performance.<h2>publication_ref</h2>['b21', 'b22', 'b21', 'b23', 'b24', 'b25', 'b26', 'b27', 'b8', 'b9', 'b28', 'b29', 'b30', 'b31', 'b12', 'b32', 'b13', 'b14', 'b33', 'b7', 'b9', 'b34', 'b35', 'b36', 'b36', 'b35', 'b37', 'b34']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Dimension Search Techniques<h2>text</h2>Since assigning unified embedding dimensions to all features is incompatible with their heterogeneity, the new paradigm of mixed dimensions has gained more and more attention [7], [18], [20], [39]. To be specific, in a mixed dimension scheme, different features can have different embedding dimensions. There are two types of dimension search methods: featurelevel and field-level. Feature-level methods aim to assign different embedding dimensions to different features within the same field. For example, NIS [18] applies reinforcement learning to generate decision sequences on the selection of dimensions for different features. Other heuristic approaches, such as MDE [39], DPG [40], and MGQE [41], attempt to assign higher dimensions to features with higher frequencies. However, these methods suffer from numerous unique values in each field, while the feature frequencies are highly dynamic and not pre-known in real-life situations. Although the frequency of different features can be estimated by sampling, the fine-grained dimension search at feature level introduces a sizeable computational overhead and additional complexity. In this work, we focus on the second group for its efficiency, which aims to allocate different dimensions to different fields, while different features in the same field share a common dimension size. Based on advances in NAS [42], the second type of dimension search method normally adopts an AutoML style to search for the desired embedding dimension from a set of predefined candidate embedding dimensions. Inspired by DARTS [17], [43], AutoDim [19] proposes a differentiable search method to analyze the suitability of different candidate embedding dimensions by calculating the attention coefficients of the corresponding operators. After the search, AutoDim assigns to each field the candidate dimension size whose corresponding operator has maximum attention. DNIS [7] eliminates the dependency on predefined candidate dimensions by directly pruning non-informative embedding blocks whose values are less than a given threshold. Although existing dimension search methods work well in the case of a well-selected subset of fields, their effectiveness in the presence of the original set of fields without feature selection remains understudied.
Similar to AutoDim, our objective is to automatically search for the optimal embedding dimension configuration on a given set of embedding dimension candidates. We differ from AutoDim: i) We consider dimension search on the original field set and avoid dimension assignment to useless fields by introducing a novel candidate dimension, i.e., dimension 0. After the search is completed, the redundant dimensions and the fields dominated by dimension 0 will be discarded, which means that dimension search and feature selection can be achieved simultaneously. ii) There is overlap between different candidate dimensions in AutoDim, i.e., the larger embedding dimensions cover the smaller ones, whereas the candidate dimensions in i-Razor do not overlap with each other. iii) Unlike AutoDim, which uses the argmax operator for dimension pruning, we propose a flexible pruning algorithm that allows more fine-grained dimension assignment.<h2>publication_ref</h2>['b6', 'b17', 'b19', 'b38', 'b17', 'b38', 'b39', 'b40', 'b41', 'b16', 'b42', 'b18', 'b6']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>METHODOLOGY<h2>text</h2>In this section, we first formulate the feature selection and embedding dimension search problem. Then, we describe the proposed i-Razor, a two-stage end-to-end framework to automatically select important fields and assign adaptive dimensions to the selected fields.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Problem Formulation<h2>text</h2>We assume that the original field set F consists of N fields, whether useful or not. Meanwhile, taking online resources into consideration, we can roughly give the search space of dimension search:
D = {d 1 , • • • , d K }, where K represents the number of candidate dimensions and d 1 < • • • < d K .
A solution A to the input configuration optimization problem involves not only filtering a field subset F ⊆ F in favor of the target DNN model, but also specifying the corresponding embedding dimension size for these selected fields. Denoting the configuration of A by C A = {(f i , d fi ) | f i ∈ F}, we can formulate the problem of finding the configuration that maximizes the performance of the target DNN model as follows:
min A L (C A , W (A)) ,(1)
where L is a task-specific loss function, W(A) denotes the optimal parameters of the DNN model under configuration A, and d fi specifies the unified dimension size assigned to all features in field f i .
Finding the optimal solution to the input configuration optimization problem is an NP-Hard problem, with an incredibly huge space (typical size of 2 N K ) to search for. For the purpose of automatically selecting essential features from a noisy feature set while learning proper dimensions simultaneously without loss of generality and model performance, we propose an end-to-end differentiable framework, i-Razor. The overall framework is illustrated in Figure 2, which is made up of two stages: the pretraining stage and the retraining stage. We design the search space and the differentiable architecture for dimension search in the pretraining stage. To be specific, after an embedding layer, we adopt the target DNN structure to get model-consistent field importance evaluation by learning the relative weights of different operators. After that, in the retraining stage, a pruning algorithm is proposed to abandon unnecessary fields and determine the dimensions of the selected fields so that the target model can be retrained with the ultimate configuration.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_1']<h2>table_ref</h2>[]<h2>heading</h2>Pretraining Stage<h2>text</h2>The input search problem is actually consistent with hyperparameter optimization [44] in a broad sense, since the decision of the field subset F and the corresponding dimension configuration C A can be regarded as model hyperparameters to be determined before retraining the target model. However, the main difference is that the search space of C A in our problem is much larger than that of conventional hyperparameter optimization problems.<h2>publication_ref</h2>['b43']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Search Space<h2>text</h2>Before performing the input search, a suitable search space should be given in advance for dimension search. Generally, previous work [7], [19] defined the dimension candidate set D as a set of positive integers, which implicitly assumes that every field is indispensable to the performance of the model. The assumption holds well under fine-grained feature engineering. However, in industrial recommender systems, features are iterated rapidly to continuously improve the user experience, but not all newly generated features/fields are beneficial. When only given the original field set F rather than a well-chosen subset F ⊆ F, allocating dimensions to noisy fields would introduce superfluous memory consumption and lead to suboptimal performance. To avoid forcing dimension assignment to noisy fields, we propose a masking operator corresponding to dimension 0, which helps identify and filter out fields detrimental to the target DNN model. Despite the fact that neural networks can automatically adjust weights related to useless fields, the introduction of dimension 0 makes it easier to learn and observable for us to understand the importance of the field. We can easily obtain the ideal F for the retraining stage by removing the fields in F that are dominated by dimension 0.<h2>publication_ref</h2>['b6', 'b18']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Embedding Division<h2>text</h2>In this paper, we assign the same search space to all fields for simplicity, and it is straightforward to introduce varying candidate sets. Instead of directly concatenating the K candidate embedding dimensions together, a sharing embedding architecture is introduced in order to reduce storage space and increase training efficiency. As shown in Figure 3, we allocate a d K -dimensional embedding to each field f i , denoted by e i , which is divided into K independent regions. Specifically, the first region covers the front d 1 embedding blocks (corresponding to no dimensional blocks in the case of d 1 = 0) and the j-th (1 < j ≤ K) region ranges from the (d j-1 + 1)-th digit to the d j -th digit of e i . To automatically learn the relative importance of each embedding region to the performance of the target DNN model, we assign a corresponding operator O i j ∈ R d K to the j-th region. The elements in these operators are either 1 or 0, where 1 in a operator reflects control over the embedding block at the corresponding position, while 0 indicates the embedding block at that position is masked so that the operator is independent of that block. By determining whether to keep each region in the retraining stage individually, the dimension search problem can be reformulated as an operator selection problem.
In contrast to existing dimension search methods [19], [20], where different operators may share the front dimensions, the proposed decoupled structure is tailored for learning the relative importance of each independent embedding region, allowing for greater flexibility in selecting the appropriate combination of embedding regions based on the importance weights. Notice that by setting D subtly, such as
d j | d j = j×(j-1) 2 , j ∈ [1, K] or d j | d j = 2 j-1 , j ∈ [1, K]
, the actual dimension search space we can get is able to cover all the integers in the interval [d 1 , d K ] when we aim to select some of the embedding regions to determine the final dimension size after pruning unconsidered regions. For example, given D = {0, 1, 3, 6}, we can get the dimension size of 5 by selecting the third region with 2 embedding blocks and the fourth region with 3 blocks. In contrast, previous methods can only select an embedding size that belongs to D because the regions controlled by different operators are overlapping, and the importance of these coupled operators is not independent of each other.
𝑒 ! " 𝑒 # " e $ " 𝑒 % " 𝑒 & " 𝑒 ' " 𝑒 ( " 𝑒 ) " 𝑒 ! Batch Norm 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 𝑒 ! ! 𝑂 ! " 𝑂 # " 𝑂 $ " 𝑂 % " 𝑂 & " Operators when 𝒟 = [0,1,2,4,8] Output Embedding 𝑒 ! " = (𝑎 " ! 𝑂 " ! + 𝑎 # ! 𝑂 # ! + 𝑎 $ ! 𝑂 $ ! + 𝑎 % ! 𝑂 % ! + 𝑎 & ! 𝑂 & ! ) * 𝑒 ! !
Fig. 3: Embedding transformation and division in i-Razor.<h2>publication_ref</h2>['b18', 'b19']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Continuous Relaxation<h2>text</h2>Inspired by DARTS [17], [43], we formulate the problem of joint optimization of feature selection and dimension search as an operator selection problem by incorporating continuous architecture parameters. Specifically, we introduce a soft layer that assigns varying weights
α i = α i 1 , • • • , α i K to learn the relative contribution of different embedding regions of field f i . Trainable variables w i = w i 1 , • • • , w i
K are introduced to calculate the value of each α i j :
α i j = exp(w i j /τ ) K j=1 exp(w i j /τ ) , for j = 1, 2, . . . , K,(2)
where τ > 0 is the temperature hyperparameter. Note that the magnitude of the values in e i can be different and easily coupled with other parameters in DNN, making it difficult for the architecture parameters α i to represent the relative importance of different embedding regions. Following [36],
[45], we adopt the batch normalization [46] technique before the embedding transformation to eliminate the scale issue. The normalized embedding e
i ′ = {e i ′ 1 , • • • , e i ′ d K } is calculated as: e i ′ j = e i j -µ i j (B) [σ i j (B)] 2 + ϵ , for j = 1, 2, . . . , d K ,(3)
where µ i j (B) and σ i j (B) are the mean and standard deviation of e i j over a mini-batch B, and ϵ is a small positive constant to avoid numerical overflow. With this trick, we can force each value in e i ′ to approximately follow the Gaussian distribution N(0, 1), making them comparable with each other. As illustrated in Figure 3, the transformed embedding e i of field f i can be computed as:
e i = e i ′ ⊙ K j=1 α i j O i j ,(4)
where e i ′ is the output of the batch normalization layer and ⊙ is the element-wise product.
By applying Equation (4) to all fields and feeding all output embedding to the target DNN structure, we can obtain the relative importance of different embedding regions in the target model with a training set. Such an end-to-end importance evaluation mechanism allows the DNN model to softly select different embedding regions during model training, and an ideal subset of fields F ⊆ F and the corresponding discrete mixed dimension scheme C A can then be derived after training.<h2>publication_ref</h2>['b16', 'b42', 'b35', 'b45']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Loss Function<h2>text</h2>In recommendation tasks, it is common for the number of features in different fields to vary greatly. For example, the field Gender usually has two features, while the field Browsed Goods may have millions of features. Therefore, varying even one embedding dimension on fields like Browsed Goods can lead to a significant change in the embedding lookup table size. In order to balance model performance and memory consumption, inspired by [42], we propose a novel regularization term L p to regularize the number of model parameters:
L p = N i=1 |f i | N j=1 |f j | × K m=1 c m α i m ,(5)
c m = d 1 , m = 1; d m -d m-1 , otherwise,(6)
where |f i | indicates the number of features contained in field f i and c m represents the size of the embedding region that operator O i m controls. We intuitively expect the weighting scheme of L p to penalize those fields and regions with large cardinality, thus facilitating the compression of the model. In this paper, we focus on the CTR prediction task, where cross-entropy loss is commonly used. We combine it with L p to define the loss function of the pretraining stage as follows:
L(y, ŷ) = -y log ŷ -(1 -y) log(1 -ŷ) + λL p ,(7)
where y ∈ {0, 1} is the real label, ŷ ∈ [0, 1] is the predicted probability of y = 1, and the hyperparameter λ is a coefficient to control the degree of regularization.<h2>publication_ref</h2>['b41']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Algorithm 1:<h2>text</h2>The CPT-based Pruning Algorithm
Input: The value of α i after pretraining, the threshold cpt (0 ≤ cpt ≤ 1), and the search space
D = {d 1 , • • • , d K }.
Output: a pruned dimension size d fi assigned to field f i . 1 Sort the architecture weights α i in the descending order:
α i = [α i j1 , • • • , α i j K ], satisfying α i j1 ≥ α i j2 ≥ • • • ≥ α i j K ; 2 sum = 0 ;
// The cumulative weight 3 d fi = 0 ; // The cumulative dimension size 4 for t ∈ [1, 2, . . . , K] do 5
d fi = d fi + c jt ;
// c jt is the embedding size of α i jt calculated from Eq.( 6) 
6 sum = sum + α i jt ; //<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>One-level Optimization<h2>text</h2>In this work, we reparameterize the problem of feature selection and dimension search as a structural search problem and relax the selection of different operators to be continuous and differentiable. We slightly abuse the symbol
A = [ α 1 , • • • , α N ]
to denote all the continuous architecture parameters and W(A) to represent the downstream DNN parameters. Different from DARTS-style works [7], [17], [19] that use bi-level optimization to iteratively update A and W(A), we follow the setting of [36] and use one-level optimization to simultaneously train A and W(A) on the training set T . In addition to being time-consuming, [36] empirically demonstrated that bi-level optimization might downgrade the final performance due to the accumulation of errors from multiple approximations. On the other hand, we argue that there are many causes that can affect the performance and stability of bi-level optimization, such as the size of the validation set and the switch interval during iterative training. Hence, we adopt one-level optimization for practicality and efficiency by simultaneously updating A and W(A) via gradient descent as follows:
∂ A L (T ; A, W (A)) and ∂ W(A) L (T ; A, W (A)). (8)<h2>publication_ref</h2>['b6', 'b16', 'b18', 'b35', 'b35']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Retraining Stage<h2>text</h2>After the pretraining stage, we can obtain the relative importance of different operators. We then seek to derive the optimal subset F ⊆ F and the corresponding dimension configuration C A . Retraining the target DNN model with F and C A aims to eliminate suboptimal influence since the unpicked fields in F and the redundant embedding dimensions are also involved during the pretraining stage.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Input Configuration Derivation<h2>text</h2>Despite the hard selector argmax being widely used in previous DARTS-style work [17], [19], we argue that it can lead to an inconsistency between the pretraining model and the retraining model, resulting in inferior performance. For instance, assume there is a field that has three candidate dimensions with corresponding weights of {0.33, 0.33, 0.34}. It is difficult to judge which operator is better since the gap is too small, and dropping all of the relatively smaller operators can lead to a dilemma that the total weight of the dropped operators is much higher than the selected one. Moreover, due to randomness during DNN model training, the derived optimal operators can also be unstable. To alleviate such issues, we set a cumulative probability threshold (CPT) to flexibly adjust the minimum amount of information kept in the retraining stage, supplemented with a pruning algorithm to derive the embedding dimension d fi for each field f i . The details are presented in Algorithm 1. The intuition behind this is that embedding regions with higher weights contain more effective information and should be retained in preference during pruning.
Recall that d fi = 0 means that the corresponding field f i is useless and thus can be removed in the retraining stage. By applying Algorithm 1 to each field separately, we simultaneously complete feature selection and dimension search to obtain the pruned neural architecture of the retraining model. Compared with those selection algorithms based on the argmax operator, the proposed CPT-based pruning algorithm can generate more fine-grained embedding dimension configurations. Meanwhile, our algorithm can cover some common embedding search methods by flexibly adjusting the threshold. For example, the argmax operator in DART-style work [17], [19] becomes a special case with cpt = 0, while cpt = 1 means retaining all regions.<h2>publication_ref</h2>['b16', 'b18', 'b16', 'b18']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Model Retraining<h2>text</h2>As shown in Figure 2, we discard useless fields and redundant dimensions to redefine the architecture of the retraining model. Specifically, we only allocate embedding dimensions to fields in F according to C A and concatenate all of these dimensions to feed into the subsequent DNN model. Note that as the dimension size of the embedding layer changes, the first layer of the DNN model needs to be adjusted accordingly. In addition, batch normalization is not used during retraining since there are no comparisons between different operators in each field. All parameters in the retraining model will be trained on the training set T by only minimizing the cross-entropy loss.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_1']<h2>table_ref</h2>[]<h2>heading</h2>Complexity Analysis and Discussion<h2>text</h2>Similar to FM-style approaches [47], [48], [49], the space complexity of i-Razor is near O(nd K ), where n is the total number of features contained in all N fields and d K is the largest candidate dimension in the search space. In terms of training speed, the continuous relaxation and embedding transformation in i-Razor has a complexity of O(N Kd K ), which slows down the training when setting up candidates of large embedding dimensions. Overall, the proposed i-Razor elegantly converts the joint optimization of feature selection and dimension search into an operator selection process, which improves the flexibility and interpretability of feature engineering, while goes beyond the convention of "feature selection before dimension search". Given a suitable search space, i-Razor is efficient and conforms to the demand of recommendation scenarios.<h2>publication_ref</h2>['b46', 'b47', 'b48']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>EXPERIMENTS<h2>text</h2>Extensive experiments were conducted on two well-known public datasets to answer the following research questions:
• RQ1: How does i-Razor perform when compared to other input search methods? • RQ2: How do key components, i.e., the setting of d 1 = 0 and the CPT-based pruning algorithm, affect the performance? • RQ3: How do different hyperparameters affect i-Razor?
• RQ4: How does the search space setting affect i-Razor?
• RQ5: What is the interpretability of i-Razor?
• RQ6: What is the compatibility of i-Razor with various deep recommendation models, and can the learned configurations from one model serve as a valuable reference for others on the same dataset?<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Datasets<h2>text</h2>Experiments were conducted on the following two public datasets:
• Criteo 2 : Criteo is a benchmark industry dataset, which contains one month of click logs. In our experiments, we use "data 6-12" as the training set while selecting "day 13" for evaluation. Due to the huge volume of data and the severe label imbalance (only 3% positive samples), negative down-sampling is applied to keep the positive ratio roughly at 50%. • Avazu 3 : Avazu dataset was provided for the CTR prediction challenge on Kaggle. 80% of randomly shuffled data is allotted for training and validation while the rest 20% for testing.
To make a fair comparison between different methods, we process the two datasets following the settings 4 in AutoFIS [36] and PIN [45]. Meanwhile, to demonstrate the capability of feature selection from noisy fields, we augment the number of fields in the two datasets by brute-forcing all cross fields, yielding 39 + C 2 39 = 780 fields in Criteo and 24 + C 2 24 = 300 fields in Avazu. We renamed the new datasets as Criteo-FG and Avazu-FG. Some key statistics of the aforementioned datasets are summarized in Table 1.  L1 and L2 control the weights of the L1-norm and L2-norm loss respectively; TopK represents the field search space, where each element means the retention of the topK fields; τ is the temperature; λ is the parameter that controls the weight of the regularization term L p ; cpt is the threshold in the retraining stage.<h2>publication_ref</h2>['b35', 'b44']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_2']<h2>heading</h2>Experimental Setup<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Baselines<h2>text</h2>To evaluate the effectiveness of i-Razor, we select the following state-of-the-art methods as baselines:
1) Feature selection methods. Fixed dimension embedding (FDE) specifies a uniform dimension size for all fields via traversal search and is paired with all feature selection methods to test the effectiveness of feature selection alone. FTRL [50] is a widely used feature selection algorithm that incorporates L1 and L2 regularization and has excellent sparsity and convergence properties. AutoField [37] is an AutoML framework that can adaptively evaluate the inclusion/exclusion probability for each feature. Following the experimental setup in the original paper, we evaluated model performance by keeping varying numbers of features and identified the configuration that achieved the best performance. AdaFS [38] employs a control network to score input data samples and adaptively select the most informative features. Mirroring AdaFS-hard [38] in the original study, we performed hard feature selection to retain different proportions of features in order of importance and discarded the remainder. AutoFIS [36] is a state-of-theart method to select important feature interactions in FMbased methods 5 . Notice that we tried different deletion ratios ranging from 0 to 1 with an interval of 0.1, and only presented the best results with the highest AUC metric. The feature selection performance of i-Razor is evaluated by just removing the fields dominated by dimension 0. 2) Dimension search methods. AutoDim [19] and DARTS [17] are two state-of-the-art methods that bear the closest resemblance to i-Razor and can automatically select appropriate dimensions for different fields. Both methods perform dimension search directly on all fields without feature selection to evaluate the validity of naive dimension optimization. Unlike AutoDim [19] and i-Razor, different dimension candidates in DARTS [17] do not share dimension blocks with each other. DARTS needs to allocate total K j=1 D j dimensions to each field in the pretraining model and relies on the argmax operator as Autodim for dimension selection.
3) Hybrid methods. In addition to our approach, we compare the training mechanism of feature selection followed by dimension search, and the corresponding baselines are designated as 'AutoFIS w/ AutoDim' and 'AutoFIS w/ i-Razor'.<h2>publication_ref</h2>['b49', 'b36', 'b37', 'b37', 'b35', 'b18', 'b16', 'b18', 'b16']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Evaluation Metrics<h2>text</h2>We evaluate the performance of our proposed method on the classical CTR prediction task. The evaluation metrics include Area Under the ROC Curve (AUC), the number of selected fields (Fields), the total embedding dimensions used in the retraining stage (Dims), and the parameter quantity of all embeddings (Params) 6 . A higher AUC indicates a better recommendation performance. It is noteworthy that a slightly higher AUC at 0.001-level is regarded as significant for the CTR prediction task [19], [51]. We naturally introduce Dims and Fields as straightforward metrics since all methods aim to reduce model parameters through feature selection or dimension search.<h2>publication_ref</h2>['b18', 'b50']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Implementation<h2>text</h2>We implement all algorithms in Tensorflow [52] and use the Adagrad [53] optimizer for gradient descent. For a fair comparison, for each baseline, we have reproduced it with reference to the original paper. Table 2 summarizes the key parameters used in our experiments. For AutoDim, we adopt the same annealing temperature τ = max(0.01, 1 -0.00005 • t) provided in the original paper [19]. The code is publicly available on https://github.com/YaoYao1995/i-Razor.git.
6. Dims and Params are roughly calculated by
N i=1 d f i and N i=1 |f i | × d f i , respectively.<h2>publication_ref</h2>['b51', 'b52', 'b18']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_4']<h2>heading</h2>Overall Performance (RQ1)<h2>text</h2>Performance improvement can be an arduous process in real-world recommender systems, so we present the best performance configuration corresponding to each baseline in Table 3 and Table 4. There are several observations:
• Proper feature selection leads to competitive or even superior performance compared with FDE. This indicates that filtering out less informative and predictive features is beneficial for recommendation models. Additionally, we observe that the discarded fields have a small cardinality (i.e., a small number of features per field) on Avazu, which limits the savings in Params. This emphasizes the importance of further exploring dimension search techniques. • Comparing all dimension search methods with FDE, we observe that dimension search methods generally achieve more significant parameter savings. However, on Criteo, existing dimension search methods assign dimensions to each field, which fails to eliminate the Feature selection eliminates less informative features without compromising model performance, while dimension search effectively reduces parameter quantities. The findings emphasize the importance of integrating these optimization processes, as evidenced by the superior performance of i-Razor compared to other methods.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_6', 'tab_7']<h2>heading</h2>Ablation Study (RQ2)<h2>text</h2>In i-Razor, the introduced dimension 0 in the search space plays an important role to filter out the useless fields. Meanwhile, the proposed CPT-based pruning algorithm makes it easy and flexible to derive the desired embedding configuration. To further study the role of these two components, we propose and compare the following four variants:
1) 'i-Razor w/ 0': using the CPT-based pruning algorithm while introducing dimension 0 into the search space; 2) 'i-Razor w/o 0': only using the CPT-based pruning algorithm; 3) 'argmax w/ 0': using the argmax selector while introducing dimension 0 into the search space; 4) 'argmax w/o 0': only using the argmax selector.
To be specific, we consider two kinds of candidate dimension sets, i.e., D 1 = {0, 1, 3, 6, 10} and D 2 = {1, 3, 6, 10}.
The best results of these variants on Criteo-FG and Avazu-FG are shown in Table 5 and we draw the following conclusions:
• By comparing variant (1) with ( 2) and comparing variant (3) with ( 4), we observe that the variants with dimension 0 (w/ 0) consistently outperform those without dimension 0 (w/o 0). The reason is that the introduced dimension 0 can increase the distinction between different fields and avoid assigning dimensions to redundant fields. This observation suggests that the introduction of dimension 0 can benefit recommender systems where noisy and redundant fields exist, especially in the face of thousands of fields in large-scale industrial recommender systems. • By comparing variant (1) with (3) and comparing variant
(2) with ( 4), we observe that the CPT-based pruning algorithm can achieve a performance gain at the expense of preserving more fields and parameters. Recall that the argmax-based pruning algorithm is a special case of the CPT-based pruning algorithm (by setting cpt = 0), we conclude that the CPT-based pruning algorithm is more suitable for real-world recommender systems due to its flexibility to obtain effective coordination between model performance and memory overhead. In summary, both components have their advantages, such as dimension 0 contributes to feature selection and parameter reduction, while the CPT-based pruning algorithm further helps the model to arrive at better input configurations.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_8']<h2>heading</h2>Hyperparameter Study (RQ3)<h2>text</h2>There are two main hyperparameters involved in i-Razor: the threshold cpt and the weight coefficient λ of the regularization term L p . To study the impact of cpt, we investigate how i-Razor performs with the change of cpt, while fixing other parameters (τ = 0.05, λ = 0.0001). As shown in Figure 4, larger cpt leads to larger value of Params, which is straightforward as more embedding regions and fields would be retained. It can be seen that when cpt is relatively small (cpt ≤ 0.3), AUC is improved with the increase of Params. However, when cpt ≥ 0.4, the performance of the model will decrease significantly. The potential reason is that with more fields and embedding dimensions preserved, the DNN model may require more data to eliminate the side-effect of redundant parameters. Meanwhile, the experimental results indicate that larger λ usually corresponds to less Params, which reflects the effectiveness of L p in controlling the number of parameters. However, when L p dominates the gradient update of the architecture parameters, model performance tends to drop sharply since some useful embedding regions or fields with large cardinality may be incorrectly filtered out. In short, both cpt and λ are crucial hyperparameters in i-Razor, and we empirically find them to be effective and parameter insensitive when taking smaller values. In practical use, the appropriate values can be found by performing a grid search on recommended intervals: cpt ∈ [0, 0.4] and λ ∈ [0, 0.01].<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>[]<h2>heading</h2>Candidate Dimension Set Study (RQ4)<h2>text</h2>In this subsection, we study how different dimension partitioning schemes for the candidate dimension sets affect model performance. Specifically, we consider three heuristic settings: uniform splits {0, 0 0 1 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 2 0 2 2 1 0 3 2 0 1 0 0 2 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 0 2 0 0 0 0 3 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 2 0 1 1 2 2 2 0 1 1 2 0 0 2 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 2 0 0 1 2 0 0 1 1 2 2 0 1 2 1 1 0 0 2 0 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0.0 0.5  <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Dimension Analysis (RQ5)<h2>text</h2>We visualize the output configuration of the CPT-based pruning algorithm to get a better understanding of i-Razor. The result on Avazu-FG with the search space D = {0, 1, 2, 4, 6} is presented in Figure 5 as an example, and similar results obtained with other settings are omitted due to limited space. We can observe that important single fields such as "site id", "app id", and "device ip", generate many cross fields to be kept with. It makes sense since item features, context features and position features are essential features in recommender systems. For continuous fields, we can infer that "C17" and "C20" are relatively more important than others as they generate more effective cross fields. Meanwhile, we can directly pick out useful cross fields according to their derived embedding dimension size. For instance, "site id * C14", "site domain * mday", "app category * device model", and "device model * C20" are expected to be beneficial cross fields, since they are assigned with the largest dimension size. Furthermore, we can find the configuration matrix is sparse, where only 75 out of the 300 fields are greater than 0. This means that we can directly increase the high-order interaction between different fields to enrich the feature space, and rely on i-Razor to remove the superfluous fields. The visualization result also supports the conclusion that i-Razor allows a fine-grained pruning procedure so that discriminative fields can be assigned the appropriate dimension according to their predictive capacity. Such findings demonstrate i-Razor's capability to automatically assess the quality of fields, which facilitates the screening of new features and can guide feature engineering iterations in industrial recommender systems.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_3']<h2>table_ref</h2>[]<h2>heading</h2>Compatibility and Transferability Study (RQ6)<h2>text</h2>In this subsection, we investigate the compatibility and transferability of i-Razor. We examine its applicability beyond multi-layer perceptron (MLP) models [54] and assess the effectiveness of the derived input configurations in different recommendation architectures. Our experiments focus on advanced deep recommendation models such as DeepFM [47], xDeepFM [55], and IPNN [45], [56] using the Avazu dataset.
To ensure dimension uniformity for feature interaction operations, we introduce different MLP layers for each field to perform linear transformations on embeddings. We compare three configurations in Table 7: FDE, which evenly allocates 30 dimensions to all fields; MLP, which transfers the input configuration learned from the MLP model; and model-consistent configurations specifically derived from each target model.
Our experiments yield the following key findings:
• Directly adopting the input configuration learned from the MLP model to other deep recommendation models results in the removal of irrelevant fields, parameter compression, and improved AUC performance. These findings demonstrate the instructiveness of the MLP input configuration for enhancing the performance of other deep recommender systems on the same dataset. • Model-consistent configurations derived specifically for each target model outperform the other two competitors. This highlights the compatibility of i-Razor with various deep recommendation models and emphasizes the importance of using model-specific configurations to achieve optimal performance. Overall, our experiments confirm the compatibility and transferability of i-Razor, showcasing its ability to generate input configurations that improve model performance and parameter efficiency across various deep models.<h2>publication_ref</h2>['b53', 'b46', 'b54', 'b44', 'b55']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_11']<h2>heading</h2>CONCLUSION<h2>text</h2>In this paper, we address the challenge of feature selection and dimension search in modern DNN-based recommender systems. To this end, we propose a differentiable framework that selects useful fields from a given set while assigning adaptive embedding dimensions. The proposed method acts on the embedding layer and can be integrated into various network architectures to improve the recommendation <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>Content-based recommendation systems<h2>journal</h2>Springer<h2>year</h2>2007<h2>authors</h2>M J Pazzani; D Billsus<h2>ref_id</h2>b1<h2>title</h2>Clcdr: Contrastive learning for cross-domain recommendation to cold-start users<h2>journal</h2>Springer<h2>year</h2>2022<h2>authors</h2>Y Chen; Y Yao; W K V Chan<h2>ref_id</h2>b2<h2>title</h2>Cdr-adapter: Learning adapters to dig out more transferring ability for cross-domain recommendation models<h2>journal</h2><h2>year</h2>2023<h2>authors</h2>Y Chen; Y Yao; W K V Chan; L Xiao; K Zhang; L Zhang; Y Ye<h2>ref_id</h2>b3<h2>title</h2>Autocross: Automatic feature crossing for tabular data in real-world applications<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Y Luo; M Wang; H Zhou; Q Yao; W.-W Tu; Y Chen; W Dai; Q Yang<h2>ref_id</h2>b4<h2>title</h2>Multilayer feedforward networks are universal approximators<h2>journal</h2>Neural networks<h2>year</h2>1989<h2>authors</h2>K Hornik; M Stinchcombe; H White<h2>ref_id</h2>b5<h2>title</h2>High performance feature selection algorithms using filter method for cloudbased recommendation system<h2>journal</h2>Cluster Computing<h2>year</h2>2019<h2>authors</h2>D Muthusankar; B Kalaavathi; P Kaladevi<h2>ref_id</h2>b6<h2>title</h2>Differentiable neural input search for recommender systems<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>W Cheng; Y Shen; L Huang<h2>ref_id</h2>b7<h2>title</h2>A survey on feature selection methods<h2>journal</h2>Computers and Electrical Engineering<h2>year</h2>2014<h2>authors</h2>G Chandrashekar; F Sahin<h2>ref_id</h2>b8<h2>title</h2>Bacterial colony algorithm with adaptive attribute learning strategy for feature selection in classification of customers for personalized recommendation<h2>journal</h2>Neurocomputing<h2>year</h2>2020<h2>authors</h2>H Wang; B Niu; L Tan<h2>ref_id</h2>b9<h2>title</h2>Automl for deep recommender systems: A survey<h2>journal</h2>ACM Transactions on Information Systems<h2>year</h2>2023<h2>authors</h2>R Zheng; L Qu; B Cui; Y Shi; H Yin<h2>ref_id</h2>b10<h2>title</h2>Igf-bagging: Information gain based feature selection for bagging<h2>journal</h2>International Journal of Innovative Computing, Information and Control<h2>year</h2>2011<h2>authors</h2>G Wang; J Ma; S Yang<h2>ref_id</h2>b11<h2>title</h2>Consistency-based search in feature selection<h2>journal</h2>Artificial intelligence<h2>year</h2>2003<h2>authors</h2>M Dash; H Liu<h2>ref_id</h2>b12<h2>title</h2>A survey on evolutionary computation approaches to feature selection<h2>journal</h2>IEEE Transactions on Evolutionary Computation<h2>year</h2>2015<h2>authors</h2>B Xue; M Zhang; W N Browne; X Yao<h2>ref_id</h2>b13<h2>title</h2>Pairwise constraint-guided sparse learning for feature selection<h2>journal</h2>IEEE transactions on cybernetics<h2>year</h2>2015<h2>authors</h2>M Liu; D Zhang<h2>ref_id</h2>b14<h2>title</h2>Greedy function approximation: a gradient boosting machine<h2>journal</h2>Annals of statistics<h2>year</h2>2001<h2>authors</h2>J H Friedman<h2>ref_id</h2>b15<h2>title</h2>Practical lessons from predicting clicks on ads at facebook<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>X He; J Pan; O Jin; T Xu; B Liu; T Xu; Y Shi; A Atallah; R Herbrich; S Bowers<h2>ref_id</h2>b16<h2>title</h2>Darts: Differentiable architecture search<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>H Liu; K Simonyan; Y Yang<h2>ref_id</h2>b17<h2>title</h2>Neural input search for large scale recommendation models<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>M R Joglekar; C Li; M Chen; T Xu; X Wang; J K Adams; P Khaitan; J Liu; Q V Le<h2>ref_id</h2>b18<h2>title</h2>Autodim: Field-aware embedding dimension searchin recommender systems<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>X Zhao; H Liu; H Liu; J Tang; W Guo; J Shi; S Wang; H Gao; B Long<h2>ref_id</h2>b19<h2>title</h2>Autoemb: Automated embedding dimensionality search in streaming recommendations<h2>journal</h2>IEEE<h2>year</h2>2021<h2>authors</h2>X Zhaok; H Liu; W Fan; H Liu; J Tang; C Wang; M Chen; X Zheng; X Liu; X Yang<h2>ref_id</h2>b20<h2>title</h2>Learnable embedding sizes for recommender systems<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>S Liu; C Gao; Y Chen; D Jin; Y Li<h2>ref_id</h2>b21<h2>title</h2>Feature selection in machine learning: A new perspective<h2>journal</h2>Neurocomputing<h2>year</h2>2018<h2>authors</h2>J Cai; J Luo; S Wang; S Yang<h2>ref_id</h2>b22<h2>title</h2>Dragonfly algorithm: theory, literature review, and application in feature selection<h2>journal</h2>Springer<h2>year</h2>2020<h2>authors</h2>M Mafarja; A A Heidari; H Faris; S Mirjalili; I Aljarah<h2>ref_id</h2>b23<h2>title</h2>Information gain and divergence-based feature selection for machine learning-based text categorization<h2>journal</h2>Information processing and management<h2>year</h2>2006<h2>authors</h2>C Lee; G G Lee<h2>ref_id</h2>b24<h2>title</h2>Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy<h2>journal</h2>IEEE Transactions on pattern analysis and machine intelligence<h2>year</h2>2005<h2>authors</h2>H Peng; F Long; C Ding<h2>ref_id</h2>b25<h2>title</h2>Making sense of spatio-temporal preserving representations for eeg-based human intention recognition<h2>journal</h2>IEEE transactions on cybernetics<h2>year</h2>2019<h2>authors</h2>D Zhang; L Yao; K Chen; S Wang; X Chang; Y Liu<h2>ref_id</h2>b26<h2>title</h2>A guide to chi-squared testing<h2>journal</h2>Biometrics<h2>year</h2>1996<h2>authors</h2>P E Greenwood; M S Nikulin<h2>ref_id</h2>b27<h2>title</h2>An analysis of variance test for normality (complete samples)<h2>journal</h2>Biometrika<h2>year</h2>1965<h2>authors</h2>S S Shaphiro; M B Wilk<h2>ref_id</h2>b28<h2>title</h2>The distance-weighted k-nearest-neighbor rule<h2>journal</h2>IEEE Transactions on Systems, Man, and Cybernetics<h2>year</h2>1976<h2>authors</h2>S A Dudani<h2>ref_id</h2>b29<h2>title</h2>An empirical study of the naive bayes classifier<h2>journal</h2><h2>year</h2>2001<h2>authors</h2>I Rish<h2>ref_id</h2>b30<h2>title</h2>Feature selection in omics prediction problems using cat scores and false nondiscovery rate control<h2>journal</h2>The Annals of Applied Statistics<h2>year</h2>2010<h2>authors</h2>M Ahdesmäki; K Strimmer<h2>ref_id</h2>b31<h2>title</h2>An adaptive semisupervised feature analysis for video semantic recognition<h2>journal</h2>IEEE transactions on cybernetics<h2>year</h2>2017<h2>authors</h2>M Luo; X Chang; L Nie; Y Yang; A G Hauptmann; Q Zheng<h2>ref_id</h2>b32<h2>title</h2>A hybrid model of fuzzy min-max and brain storm optimization for feature selection and data classification<h2>journal</h2>Neurocomputing<h2>year</h2>2019<h2>authors</h2>F Pourpanah; C P Lim; X Wang; C J Tan; M Seera; Y Shi<h2>ref_id</h2>b33<h2>title</h2>A semisupervised recurrent convolutional attention model for human activity recognition<h2>journal</h2>IEEE transactions on neural networks and learning systems<h2>year</h2>2019<h2>authors</h2>K Chen; L Yao; D Zhang; X Wang; X Chang; F Nie<h2>ref_id</h2>b34<h2>title</h2>Towards automatic discovering of deep hybrid network architecture for sequential recommendation<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>M Cheng; Z Liu; Q Liu; S Ge; E Chen<h2>ref_id</h2>b35<h2>title</h2>Autofis: Automatic feature interaction selection in factorization models for click-through rate prediction<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>B Liu; C Zhu; G Li; W Zhang; J Lai; R Tang; X He; Z Li; Y Yu<h2>ref_id</h2>b36<h2>title</h2>Autofield: Automating feature selection in deep recommender systems<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Y Wang; X Zhao; T Xu; X Wu<h2>ref_id</h2>b37<h2>title</h2>Adafs: Adaptive feature selection in deep recommender system<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>W Lin; X Zhao; Y Wang; T Xu; X Wu<h2>ref_id</h2>b38<h2>title</h2>Mixed dimension embeddings with application to memory-efficient recommendation systems<h2>journal</h2>IEEE<h2>year</h2>2021<h2>authors</h2>A Ginart; M Naumov; D Mudigere; J Yang; J Zou<h2>ref_id</h2>b39<h2>title</h2>Differentiable product quantization for end-to-end embedding compression<h2>journal</h2>PMLR<h2>year</h2>2020<h2>authors</h2>T Chen; L Li; Y Sun<h2>ref_id</h2>b40<h2>title</h2>Learning multi-granular quantized embeddings for largevocab categorical features in recommender systems<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>W.-C Kang; D Z Cheng; T Chen; X Yi; D Lin; L Hong; E H Chi<h2>ref_id</h2>b41<h2>title</h2>Atomnas: Fine-grained end-to-end neural architecture search<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>J Mei; Y Li; X Lian; X Jin; L Yang; A Yuille; J Yang<h2>ref_id</h2>b42<h2>title</h2>Darts+: Improved differentiable architecture search with early stopping<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>H Liang; S Zhang; J Sun; X He; W Huang; K Zhuang; Z Li<h2>ref_id</h2>b43<h2>title</h2>Bilevel programming for hyperparameter optimization and meta-learning<h2>journal</h2>PMLR<h2>year</h2>2018<h2>authors</h2>L Franceschi; P Frasconi; S Salzo; R Grazzi; M Pontil<h2>ref_id</h2>b44<h2>title</h2>Product-based neural networks for user response prediction over multi-field categorical data<h2>journal</h2>ACM Transactions on Information Systems (TOIS)<h2>year</h2>2018<h2>authors</h2>Y Qu; B Fang; W Zhang; R Tang; M Niu; H Guo; Y Yu; X He<h2>ref_id</h2>b45<h2>title</h2>Batch normalization: Accelerating deep network training by reducing internal covariate shift<h2>journal</h2>PMLR<h2>year</h2>2015<h2>authors</h2>S Ioffe; C Szegedy<h2>ref_id</h2>b46<h2>title</h2>Deepfm: a factorizationmachine based neural network for ctr prediction<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>H Guo; R Tang; Y Ye; Z Li; X He<h2>ref_id</h2>b47<h2>title</h2>Factorization machines<h2>journal</h2>IEEE<h2>year</h2>2010<h2>authors</h2>S Rendle<h2>ref_id</h2>b48<h2>title</h2>Deep learning over multi-field categorical data<h2>journal</h2>Springer<h2>year</h2>2016<h2>authors</h2>W Zhang; T Du; J Wang<h2>ref_id</h2>b49<h2>title</h2>Ad click prediction: a view from the trenches<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>H B Mcmahan; G Holt; D Sculley; M Young; D Ebner; J Grady; L Nie; T Phillips; E Davydov; D Golovin<h2>ref_id</h2>b50<h2>title</h2>Wide & deep learning for recommender systems<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>H.-T Cheng; L Koc; J Harmsen; T Shaked; T Chandra; H Aradhye; G Anderson; G Corrado; W Chai; M Ispir<h2>ref_id</h2>b51<h2>title</h2>Tensorflow: A system for large-scale machine learning<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>M Abadi; P Barham; J Chen; Z Chen; A Davis; J Dean; M Devin; S Ghemawat; G Irving; M Isard<h2>ref_id</h2>b52<h2>title</h2>Adaptive subgradient methods for online learning and stochastic optimization<h2>journal</h2>Journal of machine learning research<h2>year</h2>2011<h2>authors</h2>J Duchi; E Hazan; Y Singer<h2>ref_id</h2>b53<h2>title</h2>Deep neural networks for youtube recommendations<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>P Covington; J Adams; E Sargin<h2>ref_id</h2>b54<h2>title</h2>xdeepfm: Combining explicit and implicit feature interactions for recommender systems<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>J Lian; X Zhou; F Zhang; Z Chen; X Xie; G Sun<h2>ref_id</h2>b55<h2>title</h2>Product-based neural networks for user response prediction<h2>journal</h2>IEEE<h2>year</h2>2016<h2>authors</h2>Y Qu; H Cai; K Ren; W Zhang; Y Yu; Y Wen; J Wang<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Fig. 1 :1Fig. 1: Contrasting two strategies for optimizing model inputs. (a) Sequentially conducting feature selection and tuning feature dimensions. (b) Simultaneously optimizing feature selection and dimension allocation.<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Fig. 2 :2Fig. 2: The overall framework of i-Razor for joint optimization of feature selection and dimension search. (a) In the pretraining stage, we divide the embedding of each field into several parts and learn the relative importance of these regions by training directly under the guidance of the target model. (b) In the retraining stage, we use a pruning algorithm to filter the useless fields and keep only the desired dimensions to reconstruct the model. After retraining it with the training dataset, the ultimate serving model can be obtained.<h2>figure_data</h2><h2>figure_label</h2>4<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Fig. 4 :4Fig. 4: Hyperparameter study of cpt and λ on Avazu-FG.<h2>figure_data</h2><h2>figure_label</h2>5<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>Fig. 5 :5Fig. 5: Embedding dimensions allocated to single fields and cross fields on Avazu-FG.<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>Statistics of Evaluation Datasets<h2>figure_data</h2>Dataset#instances #features #fields positive ratioCriteo1 × 10 81 × 10 6390.50Criteo-FG1 × 10 81 × 10 87800.50Avazu4 × 10 76 × 10 5240.17Avazu-FG4 × 10 74 × 10 73000.17<h2>figure_label</h2>2<h2>figure_type</h2>table<h2>figure_id</h2>tab_4<h2>figure_caption</h2><h2>figure_data</h2>: Parameter SettingAvazuAvazu-FGCriteoCriteo-FGMLP=[700 × 5, 1]bias opt = ftrlbias lr = 0.01Commonnet opt = Adagradnet lr = 0.02L2 = 0.001batch size=128batch size=500LN=TrueLN=FalseFDEsp = [5, 10, • • • , 50]sp= [1, 2, • • • , 10]sp= [5, 10, • • • , 50]sp= [1, 2, • • • , 10]FTRLL1=1, L2=1AutoFieldTopK = [8, 10, • • • , 22]-TopK = [15, 18, • • • , 36] -AdaFSTopK = [8, 10, • • • , 22]-TopK = [15, 18, • • • , 36] -AutoFISL1=0.01 L2=0.01-L1=0.01 L2=0.01-AutoDimsp=[1, 2, 4, 8, 16, 30]sp=[1, 2, 4, 6]sp=[1, 2, 4, 8, 16, 25]sp=[1, 3, 6, 10]DARTSsp=[1, 2, 4, 8, 16, 30]sp=[1, 2, 4, 6]sp=[1, 2, 4, 8, 16, 25]sp=[1, 3, 6, 10]sp = [0, 1, 2, 4, 8, 16, 30]sp = [0, 1, 2, 4, 6]sp=[0, 1, 2, 4, 8, 16, 25]sp=[0, 1, 3, 6, 10]i-Razorτ = 0.05 λ = 0.0001τ = 0.05 λ = 0.0001τ = 0.1 λ = 0.0001τ = 0.2 λ = 0.0001cpt = 0.45cpt = 0.3cpt = 0.8cpt = 0.3LN represents layer normalization;sp represents the search space during the pretraining stage;<h2>figure_label</h2>3<h2>figure_type</h2>table<h2>figure_id</h2>tab_6<h2>figure_caption</h2>Model performance on Avazu and Avazu-FG. ⋆⋆ and ⋆ represent significance level p-value < 0.01 and p-value < 0.05 of comparing i-Razor with the best baseline (indicated by the underlined number). RelaChan represents the relative change of i-Razor compared to the underlined baseline. ↑: the higher the better; ↓: the lower the better.<h2>figure_data</h2>AvazuAvazu-FGMethodAUC ↑Fields ↓ Dims ↓ Params (M) ↓AUC ↑Fields ↓ Dims ↓ Params (M) ↓FDE0.78112472019.360.781430030043.89FTRL w/ FDE0.78122060019.270.784516016033.60AutoField w/ FDE0.78092060019.35----AdaFS w/ FDE0.78121236019.34----AutoFIS w/ FDE----0.783518918916.51i-Razor w/ FDE0.78121648019.350.7870757527.68AutoDim0.78202423916.630.7842300588184.03DARTS0.78152421117.970.7839300596201.12AutoFIS w/ AutoDim----0.783618939347.83AutoFIS w/ i-Razor----0.78488715922.27i-Razor (Ours)0.78231612212.320.78967510840.85RelaChan+0.0003 ⋆-33%-49%-26%+0.0048 ⋆⋆-14%-32%+83%<h2>figure_label</h2>4<h2>figure_type</h2>table<h2>figure_id</h2>tab_7<h2>figure_caption</h2>Model performance on Criteo and Criteo-FG. FTRL is not listed as performance degrades when any proportion of fields are deleted. ↑: the higher the better; ↓: the lower the better.<h2>figure_data</h2>CriteoCriteo-FG<h2>figure_label</h2>5<h2>figure_type</h2>table<h2>figure_id</h2>tab_8<h2>figure_caption</h2>Effectiveness of our model components. ↑: the higher the better; ↓: the lower the better. Fields ↓ Dims ↓ Params (M) ↓ AUC ↑ Fields ↓ Dims ↓ Params (M) ↓<h2>figure_data</h2>Avazu-FGCriteo-FGMethod AUC ↑ i-Razor w/ 0 (1) 0.78908020067.320.80195991487208.71i-Razor w/o 0 (2)0.782730045782.640.80157802148314.23argmax w/ 0 (3)0.78837416958.140.801325662584.11argmax w/o 0 (4) 0.782630044981.500.8012780957136.90interference of less predictive fields, leading to perfor-mance degradation. This highlights the necessity offeature selection.• Concerning hybrid methods, on Avazu-FG, we observea noticeable drop in AUC for 'AutoFIS w/ AutoDim'compared to AutoDim, and even larger differences inAUC between 'AutoFIS w/ i-Razor' and i-Razor. OnCriteo-FG, i-Razor consistently outperforms 'AutoFISw/ i-Razor' in all evaluation metrics, demonstrating theadvantage of joint optimization over separate sequentialoptimization.• Dexterously balancing predictive performance andmodel compression, i-Razor surpasses top baselines. OnAvazu-FG, i-Razor jointly optimizes to retain informativefields. With fewer fields and Dims, it tends to concentratemore dimensions on predictive high-cardinality fieldswhile allocating relatively fewer dimensions to otherretained fields, increasing Params yet notably boostingAUC. On the remaining three datasets compared tostrong baselines, i-Razor aggressively slashes Paramswhile maintaining or even improving AUC. Combiningobservations on Avazu-FG and the remaining datasetsdemonstrates i-Razor's adept negotiation of predictiveperformance and compression with flexibility.In summary, our experiments underscore the significanceand advantages of feature selection and dimension search.<h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_9<h2>figure_caption</h2>According to the complexity analysis in Section 3.4, when constructing the search space by dimension partitioning, the key factors to consider are the number of candidate dimensions K and the maximum dimension d K . Based on practical insights from real-world deployment, we make<h2>figure_data</h2>C1banner_possite_idsite_domainsite_categoryapp_idapp_domainapp_categorydevice_iddevice_ipdevice_modeldevice_typedevice_conn_typeC14C15C16C17C18C19C20C21mdayhourwdayC1banner_possite_idsite_domainsite_categoryapp_idapp_domainapp_categorydevice_iddevice_ipdevice_modeldevice_typedevice_conn_typeC14C15C16C17C18C19C20C21mdayhourwday4, 8, 12, 16, 20, 24, 28},linear expansion {0, 1, 3, 6, 10, 15, 21, 28}, and exponentialexpansion {0, 1, 2, 4, 8, 16, 32}. The performance of i-Razoron Avazu under these three schemes is summarized inTable 6, showing no significant differences. This experimentprovides empirical evidence that the model has a relativelylow sensitivity to dimension partitioning configurations. Onecontributing factor is the two-stage training procedure, whichseparates the search for optimal configurations in pretrainingfrom parameter tuning in retraining. The minor performancefluctuations imply the model's capability to reach similarsolutions despite varying partitioning schemes.<h2>figure_label</h2>6<h2>figure_type</h2>table<h2>figure_id</h2>tab_10<h2>figure_caption</h2>Impact of Candidate Dimension Set on Model Performance on Avazu. Fields ↓ Dims ↓ Params (M) ↓<h2>figure_data</h2>Metric<h2>figure_label</h2>7<h2>figure_type</h2>table<h2>figure_id</h2>tab_11<h2>figure_caption</h2>Compatibility and Transferability of i-Razor on Avazu. ↑: the higher the better; ↓: the lower the better. ⋆ indicates the statistically significant improvements (i.e., twosided t-test with p-value < 0.05) over the FDE config. Fields ↓ Dims ↓ Params (M) ↓ and reduce the size of parameters. The key idea is to introduce a soft selection layer that estimates the importance of different embedding regions and thus evaluates the field importance. Moreover, we design a CPTbased pruning algorithm to flexibly generate the mixed embedding configuration. While conceptually simple, i-Razor provides us with a concrete framework that is amenable to practicality and can be easily incorporated with various recommendation models. CTR prediction experiments on two real-world datasets verify the efficacy of i-Razor on the joint optimization of feature selection and dimension search. Since i-Razor can serve as an efficient plug-in for recommendation models, future work may include extension to other recommendation tasks.<h2>figure_data</h2>Metric<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>D = {d 1 , • • • , d K }, where K represents the number of candidate dimensions and d 1 < • • • < d K .<h2>formula_coordinates</h2>[4.0, 312.0, 391.12, 252.0, 21.19]<h2>formula_id</h2>formula_1<h2>formula_text</h2>min A L (C A , W (A)) ,(1)<h2>formula_coordinates</h2>[4.0, 395.44, 519.62, 169.51, 14.51]<h2>formula_id</h2>formula_2<h2>formula_text</h2>d j | d j = j×(j-1) 2 , j ∈ [1, K] or d j | d j = 2 j-1 , j ∈ [1, K]<h2>formula_coordinates</h2>[5.0, 317.54, 183.06, 246.65, 26.7]<h2>formula_id</h2>formula_3<h2>formula_text</h2>𝑒 ! " 𝑒 # " e $ " 𝑒 % " 𝑒 & " 𝑒 ' " 𝑒 ( " 𝑒 ) " 𝑒 ! Batch Norm 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 𝑒 ! ! 𝑂 ! " 𝑂 # " 𝑂 $ " 𝑂 % " 𝑂 & " Operators when 𝒟 = [0,1,2,4,8] Output Embedding 𝑒 ! " = (𝑎 " ! 𝑂 " ! + 𝑎 # ! 𝑂 # ! + 𝑎 $ ! 𝑂 $ ! + 𝑎 % ! 𝑂 % ! + 𝑎 & ! 𝑂 & ! ) * 𝑒 ! !<h2>formula_coordinates</h2>[5.0, 316.3, 350.47, 244.07, 141.08]<h2>formula_id</h2>formula_4<h2>formula_text</h2>α i = α i 1 , • • • , α i K to learn the relative contribution of different embedding regions of field f i . Trainable variables w i = w i 1 , • • • , w i<h2>formula_coordinates</h2>[5.0, 312.0, 600.77, 252.0, 35.27]<h2>formula_id</h2>formula_5<h2>formula_text</h2>α i j = exp(w i j /τ ) K j=1 exp(w i j /τ ) , for j = 1, 2, . . . , K,(2)<h2>formula_coordinates</h2>[5.0, 345.58, 654.17, 219.37, 28.57]<h2>formula_id</h2>formula_6<h2>formula_text</h2>i ′ = {e i ′ 1 , • • • , e i ′ d K } is calculated as: e i ′ j = e i j -µ i j (B) [σ i j (B)] 2 + ϵ , for j = 1, 2, . . . , d K ,(3)<h2>formula_coordinates</h2>[6.0, 48.0, 65.36, 252.95, 59.59]<h2>formula_id</h2>formula_7<h2>formula_text</h2>e i = e i ′ ⊙ K j=1 α i j O i j ,(4)<h2>formula_coordinates</h2>[6.0, 131.68, 221.57, 169.27, 29.41]<h2>formula_id</h2>formula_8<h2>formula_text</h2>L p = N i=1 |f i | N j=1 |f j | × K m=1 c m α i m ,(5)<h2>formula_coordinates</h2>[6.0, 90.15, 529.08, 210.8, 29.41]<h2>formula_id</h2>formula_9<h2>formula_text</h2>c m = d 1 , m = 1; d m -d m-1 , otherwise,(6)<h2>formula_coordinates</h2>[6.0, 89.74, 564.51, 211.21, 23.5]<h2>formula_id</h2>formula_10<h2>formula_text</h2>L(y, ŷ) = -y log ŷ -(1 -y) log(1 -ŷ) + λL p ,(7)<h2>formula_coordinates</h2>[6.0, 75.76, 696.07, 225.19, 9.65]<h2>formula_id</h2>formula_11<h2>formula_text</h2>D = {d 1 , • • • , d K }.<h2>formula_coordinates</h2>[6.0, 355.56, 87.2, 79.9, 9.65]<h2>formula_id</h2>formula_12<h2>formula_text</h2>α i = [α i j1 , • • • , α i j K ], satisfying α i j1 ≥ α i j2 ≥ • • • ≥ α i j K ; 2 sum = 0 ;<h2>formula_coordinates</h2>[6.0, 318.2, 135.16, 169.51, 35.12]<h2>formula_id</h2>formula_13<h2>formula_text</h2>d fi = d fi + c jt ;<h2>formula_coordinates</h2>[6.0, 340.9, 195.41, 67.36, 9.65]<h2>formula_id</h2>formula_14<h2>formula_text</h2>6 sum = sum + α i jt ; //<h2>formula_coordinates</h2>[6.0, 318.6, 217.63, 139.95, 12.25]<h2>formula_id</h2>formula_15<h2>formula_text</h2>A = [ α 1 , • • • , α N ]<h2>formula_coordinates</h2>[6.0, 312.0, 411.01, 75.54, 9.57]<h2>formula_id</h2>formula_16<h2>formula_text</h2>∂ A L (T ; A, W (A)) and ∂ W(A) L (T ; A, W (A)). (8)<h2>formula_coordinates</h2>[6.0, 325.55, 603.17, 239.41, 9.96]<h2>formula_id</h2>formula_17<h2>formula_text</h2>N i=1 d f i and N i=1 |f i | × d f i , respectively.<h2>formula_coordinates</h2>[9.0, 56.97, 726.28, 243.03, 21.56]<h1>doi</h1><h1>title</h1>iMARS: An In-Memory-Computing Architecture for Recommendation Systems<h1>authors</h1>Mengyuan Li; Ann Franchesca Laguna; Dayane Reis; Xunzhao Yin; Michael Niemier; Sharon Hu<h1>pub_date</h1>2022-02-18<h1>abstract</h1>Recommendation systems (RecSys) suggest items to users by predicting their preferences based on historical data. Typical RecSys handle large embedding tables and many embedding table related operations. The memory size and bandwidth of the conventional computer architecture restrict the performance of RecSys. This work proposes an in-memory-computing (IMC) architecture (iMARS) for accelerating the filtering and ranking stages of deep neural network-based RecSys. iMARS leverages IMC-friendly embedding tables implemented inside a ferroelectric FET based IMC fabric. Circuit-level and systemlevel evaluation show that iMARS achieves 16.8× (713×) endto-end latency (energy) improvement compared to the GPU counterpart for the MovieLens dataset.<h1>sections</h1><h2>heading</h2>I. INTRODUCTION<h2>text</h2>Recommendation systems (RecSys) have been used in various applications to suggest items such as movies, music, books, shopping items, websites, etc. based on a user's previous behavior, as well as historical data from other users. The large amount of available data allows RecSys to leverage deep neural networks (DNN). DNN-based RecSys have been widely adopted by companies like Facebook [1] and Google [2] to improve their online services.
The typical DNN-based RecSys inference process [2] includes two stages: filtering and ranking. In the filtering stage, the system selects a set of candidate items from a large item database to be recommended based on a user's behavior. The ranking stage then computes the probability of choosing each candidate item. The items with the highest probability are finally returned to the user. Both the filtering and ranking stage use embedding tables (ETs) to capture and store user behaviors and item characteristics. The large ETs make the operations on them memory-bandwidth limited.
Several algorithm and hardware solutions, e.g., ET compression [3], have been proposed to alleviate the memorybandwidth bottleneck. Near-memory computing has also been leveraged to solve the memory-bandwidth problem by bringing the compute units closer to the memory [4]. However, these solutions only focus on the ranking stage and not the filtering stage which are still impeded by memory bottlenecks due to the huge amount of data transfers.
In-memory-computing (IMC) is a computational paradigm that can alleviate the data transfer overhead between the memory and the compute unit by performing logic and arithmetic operations inside the memory unit itself. Different IMC kernels have been proposed, such as content addressable memories (CAMs), crossbars, general purpose computing-inmemory (GPCiM) and configurable memory arrays (CMAs). CAMs [5] can perform parallel content-based searches in the memory itself, crossbar arrays [6] can perform matrix-vector multiplications, and GPCiM [7] performs Boolean logic and arithmetic operations in memory. CMAs combine the functionalities of random access memory (RAM), CAM, GPCiM and crossbar in a single memory array. Ferroelectric fieldeffect transistors (FeFET) based crossbars, CAMS, GPCiM and CMAs have been designed [8], [9] and have shown that FeFET-based circuits are denser and faster than CMOS-based and ReRAM-based circuits. FeFETs can be easily integrated with the CMOS fabrication process and large-scale FeFET memories have also been fabricated in [10].
In this paper, we propose iMARS, an IMC architecture for RecSys. iMARS exploits an FeFET-based CMA fabric that combines the functionalities of RAM, ternary CAMs (TCAMs) and GPCiM. The FeFET-based CMA (from [9]) can switch its functionality to implement (i) TCAM-based searches to realize nearest neighbor search (NNS) in the filtering stage; and (ii) GPCiM-based arithmetic logic to implement the additions/accumulations in the filtering and ranking stage. Specific contributions of our work include (1) an integrated IMC fabric to simultaneously accelerate the filtering and ranking stages; (2) an IMC-friendly computation flow to facilitate mapping the RecSys algorithms to iMARS;
(3) support for all ET related operations in memory by combining TCAM and GPCiM functionality in the CMA fabric; (4) a two-level memory hierarchy and corresponding in-memory adder trees to store the large ETs.
We have evaluated the latency and energy benefits of iMARS based on two widely used RecSys models: YoutubeDNN [2] on the MovieLens dataset [11] and Facebook DLRM [1] on the Criteo Kaggle dataset. The results show that for the MovieLens dataset, iMARS achieves a 16.8× (713×) end-to-end speedup (energy improvement) against the GPU implementation. For the Criteo Kaggle dataset which is widely used for the ranking task, the ranking model in Facebook DLRM is accelerated with iMARS, which leads to 13.2× (57.8×) improvement in latency (energy) improvement.  <h2>publication_ref</h2>['b0', 'b1', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9', 'b8', 'b1', 'b10', 'b0']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>II. BACKGROUND<h2>text</h2>In this section, we review the basics of DNN-based Rec-Sys and related work on hardware accelerators for RecSys. Furthermore, we discuss the IMC circuits (TCAMs, GPCiMs, CMAs and crossbars) and technologies (i.e., CMOS, FeFETs) employed in iMARS.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. Recommendation Systems<h2>text</h2>RecSys are composed of a filtering and a ranking stage [2]. The filtering stage (Fig. 1(a)) aims to reduce the number of computations needed in the ranking stage by determining a set of candidate items (e.g. O(100)) from the entire item database (e.g. O(10 6 )). The ranking stage (Fig. 1(b)) aims to find the item with the highest score for a specific user from the candidate items. The filtering stage uses a DNN to characterize the user behavior as a single embedding vector. Based on this user behavior, the candidate items to recommend can be found by using the NNS on the item ET as shown in Fig. 1(a). The goal of the ranking stage is to evaluate each user-item pair and predict the score of each candidate item for a specific user. The score is defined as the click-through rate (CTR) and indicates the likelihood that an item will be clicked. Based on the scores, the top-k items (O (10)) are returned to the user (Fig. 1(b)).
Both DNN models in the filtering and ranking stages follow the configuration depicted in Fig. 1(c). DNNs are employed to generate the embedding vector representing the user behavior or predicting user-item pair scores. The models take advantage of both continuous (dense) and categorical (sparse) features.
Dense features can be directly processed by a DNN while sparse features are captured by large ETs with sparse lookup and pooling operations. These ET operations (lookups, NNS) contribute a significant portion of the run time in RecSys as shown in the operation breakdown of the MovieLens dataset using the YouTubeDNN RecSys [2] in Fig. 2.
Existing efforts on accelerating RecSys include field programmable gate array (FPGA)-based accelerators. E.g., Flee-tRec [12] and MicroRec [13] alleviate the memory-bandwidth bottleneck of the ETs in RecSys by using FPGAs with high bandwidth memory. Near-memory computing has also been considered to alleviate the memory bottleneck. RecNMP [4] uses a dual in-line memory module-based near-memory computing that can support sparse embedding models. Most of these existing hardware accelerators only focus on a single aspect of RecSys, i.e., ranking, filtering, DNNs or ETs. Though it is possible to simply cascade these accelerators to implement RecSys, such a simple-minded approach results in higher hardware cost due to duplicated components.<h2>publication_ref</h2>['b1', 'b9', 'b1', 'b11', 'b12', 'b3']<h2>figure_ref</h2>['fig_0', 'fig_0', 'fig_0', 'fig_0', 'fig_0', 'fig_1']<h2>table_ref</h2>[]<h2>heading</h2>B. In-Memory Computing Circuits<h2>text</h2>IMC circuits can alleviate the memory bottleneck and provide parallelism to RecSys operations. This section discusses different types of IMC circuits such as TCAMs, crossbars, GPCiMs and CMA.
TCAMs enable parallel searches based on the Hamming distance for a query against a large stored database in O(1) time [5]. Using the threshold-match mode of the TCAMs, we can retrieve the row entries in the array nearest to the query under the threshold distance by parallel searches. In a TCAM array with r rows and c columns, the TCAM cells in a row are serially connected through a common matchline. Each TCAM cell, s ij , performs an XOR operation between bit j of query Q and the bit j stored at s ij . Each matchline implements a logic AND of all the cells connected to it.
Crossbars are an IMC structure where every input is connected to every output through cross-points that consist of memory elements and selectors. Crossbars can efficiently implement matrix-vector multiplications, and are thus ideal accelerators for DNN models such as convolutional neural networks [14].
GPCiMs [7], [15] perform general-purpose Boolean logic and arithmetic operations inside RAM. GPCiMs can employ specialized memory cells based on emerging technologies to perform current-based operations inside a memory array (e.g., [7]). Alternatively, customized memory peripherals (such as sense amplifiers) can be designed to operate with two memory words that are simultaneously selected by row decoders. In GPCiMs that employ customized sensed amplifiers, the voltage (or current flow) through a column-connected bitline is sensed and compared to one (or multiple) reference(s) so the results of Boolean logic/arithmetic can be produced.
CMAs [8], [9], [15] combine multiple IMC functionalities in the same physical structure. For instance, CMAs can work as either TCAM or GPCiM units at distinct times. Note that conventional TCAMs perform row-wise sensing as matchlines are placed along the row direction. GPCiMs, on the other hand, require the voltage drop (or current flow) through the vertically-connected bitlines to be sensed and compared to one (or more) reference(s) in order to produce the results needed for general-purpose computation. Due to this difference in the TCAM and GPCiM arrays, combining them in a single hardware structure requires additional memory peripherals to achieve re-configurability [8], [9].
Storing RecSys ETs requires significant amount of memory and substantial communication between memory and processing units, which could be alleviated by IMC architectures. The use of CMAs based on emerging technologies can be beneficial for implementing IMC-based RecSys compared to standard CMOS CMAs [15] due to the increased density of memory cells and lower standby power (a result of the device's non-volatility).
FeFETs have been used in various IMC based circuits such as CAMs, GPCiMs and CMAs [16], [17]. Previous work has demonstrated the benefits of FeFET-based CMAs over other emerging technologies such as ReRAMs [8], [9]. FeFETs have similar structure as metal-oxide-semiconductor fieldeffect transistors (MOSFETs) used in standard CMOS silicon, except a layer of FE oxide is deposited in the transistor's gate stack. Because of this, FeFETs are compatible with the CMOS fabrication process [10] and large-scale FeFET memories have been demonstrated [10]. For these reasons, we employ a FeFET-based CMA design for accelerating RecSys.<h2>publication_ref</h2>['b4', 'b13', 'b6', 'b14', 'b6', 'b7', 'b8', 'b14', 'b7', 'b8', 'b14', 'b15', 'b16', 'b7', 'b8', 'b9', 'b9']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>III. IMC ACCELERATOR FOR RECOMMENDATION SYSTEMS<h2>text</h2>We propose iMARS, an architecture for RecSys that uses an IMC fabric to accelerate the DNN stack and ET related operations in both the filtering and ranking stage of RecSys. The RecSys ranking and filtering stages use sparse and dense features in determining which items to recommend to the user. The dense features are sent to the DNN stack while the sparse features are sent to ETs. For the DNN stack, crossbars can be readily leveraged. For ET related operations, we leverage CMAs and adder trees placed in the memory periphery. Since different ETs play different roles depending on the stage, how to organize these tables inside the IMC fabric must be considered carefully. We elaborate our proposed architecture in Sec. III-A and computation mapping in Sec. III-C.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. iMARS Architecture<h2>text</h2>The iMARS architecture, shown in Fig. 3(a), consists of two types of IMC arrays: Crossbar arrays for the DNN stack (bottom) and CMA arrays for the ETs (top). The implementation of the DNN and the ETs inside iMARS is described below.
1) Embedding Tables: Some sparse features are used in both the ranking and filtering stage and hence can share the same ETs. We employ two different ETs: user-item embedding table (UIET) and the item embedding table (ItET). The UIET holds user-item features used by the filtering and ranking stage. Some UIETs are exclusively used by the filtering or the ranking stage while some UIETs are shared by the two stages. The item characteristics are stored in the ItET. To conserve memory space, the ItET can be accessed by both the filtering and the ranking stage.
For the UIETs, ET lookups and pooling operations are done in the filtering and ranking stage, respectively. For the ItET, two primary operations are required: (i) lookups and pooling on the ET to transform a sparse input feature into a dense vector; (ii) the NNS on the ET to return the candidate item IDs in the filtering stage. The iMARS architecture implements the two types of ETs inside its IMC fabric. CMAs integrate reads, searches, and in-memory logic/arithmetic operations in a single array, which makes them a suitable choice for implementing UIETs and ItETs.
While the circuits inside a CMA are described in [9], we introduce the design of a novel, two-level hierarchy based on CMAs and adder trees to store and efficiently compute on the large number of items in the ETs as needed by the RecSys. The components that implement ETs inside iMARS (i.e., the CMAs and adder trees) are discussed below.
Hierarchical CMAs: To accomodate the large ETs, B banks of CMAs are deployed in iMARS. Fig. 3(b) depicts the structure of one CMA bank, which consists of M mats (labeled as Mat-1, Mat-2, ..., Mat-M ). Each mat is comprised of C CMAs that work independently as the IMC engines in iMARS for performing lookups, searches and additions. An individual CMA is depicted in Fig. 3(c). It employs CAM sense amplifiers (SAs) based on a preset threshold, as well as searchline (SL) drivers and priority encoders to perform threshold based NNS. We chose to implement threshold based matching in the CAM based on a reference current generated by a dummy 1T+1FeFET cell, which can be adjusted to compensate for process variations or to change the sensitivity of the Hamming distance in the NNS operation. The RAM SAs, wordline (WL) and bitline (BL) drivers are used during lookups. Pooling operations are performed with in-memory additions (through an accumulator placed next to the RAM SA). More details on the CMA structure can be found in [9]. FeFET-based CMAs [9] are utilized to implement the ETs. When compared to CMOS-based counterparts, FeFET-based CMAs have higher density (which helps to reduce the area footprint) and lower leakage power [8].
Adder trees: To support the accumulation of a large number of parameters, we develop a hierarchical, adder tree structure. Specifically, iMARS uses in-memory addition to sum, in a single memory array, embeddings comprised of 32 dimensions with int-8 quantization. To accumulate (sum up) the outputs of the CMAs for each mat, iMARS sums up C 256-bit (i.e., 32×8)-bit numbers leveraging a near-memory, 256-bit intramat adder tree placed in each mat. Different mats can perform intra-mat additions in parallel. Once the intra-mat additions are completed, their results are accumulated across the K mats to produce a single 256-bit result (one output per memory bank). iMARS supports the addition of four 256-bit inputs inside a near-memory "Intra-bank Adder Tree" in one shot (bottom right of Fig. 3(b)). In other words, we design an "Intra-bank Adder Tree" with a fan-in of 4, a design choice made as a compromise between area footprint of the iMARS banks and performance of the intra-bank addition. In cases where more inputs need to be accumulated (i.e., when K > 4), multiple rounds of addition are needed using the same "Intra-bank Adder Tree".
The aforementioned design parameters B, M and C largely impact the area, capacity and the performance of iMARS. First, area footprint increases proportionally to B, M and C. Larger B, M and C increase the capacity of iMARS for storing embeddings. High capacity enables iMARS to accomodate big workloads. However, a large C implies a large fan-in for the "Intra-mat Adder Tree" inside each mat, which leads to parasitic effects that increases the delay for aggregating the outputs of multiple CMAs. A large K, in turn, implies that more mats are connected to (and sharing) the same communication bus, which increases the overall latency of the RecSys (to be discussed in Sec. III-A3).
2) DNN Stack: The DNN stack requires matrix-vector multiplications which can be implemented using crossbar arrays in iMARS (Fig. 3(a)). Two dedicated crossbar banks are employed to execute the ranking and the filtering DNN stack composed of fully connected layers. Each crossbar bank contains multiple crossbar arrays (Fig. 3(d)) in order to accomodate the respective DNN model. These crossbar banks both hold the DNN Stack to obtain dense features and the DNN stack that returns a user embedding during the filtering stage or a user item-score for the ranking stage. Crossbar arrays can leverage the FeFET technology [18].
3) Communication inside iMARS: Data among the different hardware components of iMARS need to be communicated. To ensure such communication does not incur too much overhead, we carefully design communication channels inside iMARS. There are two types of communication in iMARS: (1) communication among the different functional blocks and (2) communication between the mats in each CMA bank. While (1) leverages the RecSys communication (RSC) bus, (2) occurs through the intra-bank communication (IBC) network. The RSC bus and the IBC network are depicted in Fig. 3(a) and (b), respectively.
The RSC bus enables the exchange of inputs/outputs through the different hardware blocks in iMARS. While data traffic between the different hardware blocks is essential to the system's overall functionality, the data traffic through the RSC bus is not as intense as the traffic inside the memory banks that store the ETs. Data communication on the IBC network and the RSC bus is serialized to minimize the wiring overhead (thus, reducing the area of iMARS). iMARS is designed for a RSC bus with 256-bit capacity. The IBC, on the other hand, supports the transmission of 128 bytes of data (i.e., four 256bit inputs) to be added up inside the intra-bank adder tree (bottom right of Fig. 3(b)) in one shot. Data communication on the IBC network is serialized when K >4 (the fan-in of the "Intra-bank Adder Tree"). The IBC network capacity and the number of "Intra-bank Adder Tree" fan-ins are design choices that must take into consideration the impact on area footprint, delay and energy. For instance, extremely wide buses may be impractical as they require too much area to be implemented. On the other hand, a narrow IBC would require many data fetches through the bus across K mats. The overhead of communication with the RSC bus/IBC network is accounted for in the results reported in Sec. IV.
Data traffic inside the RSC and the IBC is orchestrated by a controller circuit (indicated by the box labeled CTRL in Fig. 3(a)). The controller circuit consists of a clock generator and two counters that keep track of (i) the activated bank, and (ii) the mats inside the bank that are sending outputs for accumulation with the "Intra-bank Adder Tree" circuit. Data packets always travel through the IBC in a predetermined order, as defined by the counters (i.e., in Bank B, from Mat-1, Mat-2, ..., Mat-M in groups of four outputs, i.e., 128 bytes). The pre-defined communication pattern reduces conflicting accesses and eliminates the need for routers.<h2>publication_ref</h2>['b8', 'b8', 'b8', 'b7', 'b17']<h2>figure_ref</h2>['fig_2', 'fig_2', 'fig_2', 'fig_2', 'fig_2', 'fig_2', 'fig_2', 'fig_2', 'fig_2']<h2>table_ref</h2>[]<h2>heading</h2>B. Embedding Table Mapping<h2>text</h2>The iMARS architecture uses CMAs to store ETs., which have varying number of entries (typically 3-30,000 entries). Each row on the CMA represents an entry of an ET. Designing CMAs with varying sizes is not practical. We determined the optimal array-level CMA to be the size of 256 × 256 based on circuit-level simulations. Some of the ETs can fit in a single CMA and some require multiple CMAs. The number of CMAs needed to store an ET is n/R where n is the number of entries in the ET and R is the number of rows in the CMA. If n/R < C, we only need one mat, otherwise the number of mats needed to be activated is equal to n/(RC). Each sparse feature is mapped to a separate bank. Hence, the number of activated banks depends on the number of sparse features.
We quantize all ETs to 8-bit integer precision to reduce the memory requirement. We also replace the cosine-distance based NNS in the original filtering stage with the IMC-friendly Hamming-distance based NNS. To facilitate the Hamming distance search, we employ a locality-sensitive hashing (LSH) technique on the ItET [5]. Each row of the ItET includes the additional bits for storing the corresponding LSH values. Finally, a fixed-radius near neighbor search instead of top-k search is employed. The fixed-radius near neighbor search is amenable to the threshold-based match offered by the TCAM implementation, and reduces the total number of required operations. Algorithm-level evaluation will be presented in Sec. IV to study the effects of the adjustment on accuracy. We use a 256 LSH signature length which requires 2 CMAs to store a single entry.<h2>publication_ref</h2>['b4']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C. RecSys Operation Mapping<h2>text</h2>Given the iMARS architecture, careful mapping of the computation, how in Fig. 1 to iMARS is required in order to design the control sequence properly. The crossbars stored the trained weights of the DNN stack and the ItETs and UIETs store the trained ETs for the sparse feature vectors. Fig. 3 illustrates such a mapping where the labels (1a)-(2d*) indicates. We first discuss the operations in the filtering stage. (1a) The sparse features are sent to the corresponding ETs, i.e., UIETs and ItET for lookups and pooling. The embedding vectors of the features are obtained by looking up the stored ETs in the ItET CMAs and UIET CMAs using the RAM mode of the CMA. The retrieved embeddings are then aggregated (indicated by (1b*) in Fig. 3) by the in-memory adder, intramat and intra-bank adder trees. (1b) the dense features are sent to the pre-trained filtering sparse feature DNN stack which is implemented with the crossbar arrays. (1c) All features are then sent to the filtering DNN. The output of the filtering DNN is a user embedding vector (u i in Fig. 3). (1d) The user embedding vector is then sent to the ItET to retrieve the N nearest neighbors as candidate items inside the ItET CMAs. The indices of these retrieved (i.e., candidate) item embeddings are then stored in the item buffer (See (1d*) inf Fig. 3).
We now discuss the operations in the ranking stage. (2a) Each candidate item in the item buffer is then analyzed by the next steps with respect to the user's preferences. (2b) By using the item indices in the item buffer, their corresponding item embeddings are retrieved from the stored ET in the ItET and the ranking embeddings are retrieved in the stored ET in the ranking UIETs using the RAM mode. Note that some UIETs used in the filtering mode can be shared with the ranking stage. The item embeddings are pooled with the ranking embeddings either by concatenation or by an ADD operation using the in-memory adder, intra-bank and interbank adder trees. (See (2b*) in Fig. 3) The output forms a new set of embedding features. Together with the dense features are fed to the ranking DNN stack implemented in crossbars. (2c) the dense features are again obtained from the trained ranking DNN stack. (2d) The remaining crossbar arrays implement the ranking DNN with the pooled feature embeddings as input and return the click-through-rate (CTR) to the CTR buffer. The CTR buffer is a CMA that stores the CTR for each candidate item and the item index which are used for selecting the final top-k items. (2e) The CTR buffer then performs a topk operation using the threshold match mode of the CMA by searching a vector of all 1's (the maximum allowable CMA input).<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_0', 'fig_2', 'fig_2', 'fig_2', 'fig_2', 'fig_2']<h2>table_ref</h2>[]<h2>heading</h2>IV. EVALUATION<h2>text</h2>We evaluated the RecSys implementations with two RecSys instances: (1) YoutubeDNN model [2] on the MovieLens 1M dataset, which includes both the filtering and ranking stage; (2) DLRM model [1] targeting at the ranking stage on the Criteo Kaggle dataset. The configurations of the two RecSys shown in Table I were implemented on Nvidia RTX 1080 GPU. We used the tools Nvidia-smi and lineprofiler to obtain energy and latency, respectively. We only compared with the GPU evaluation as other accelerators use older RecSys models and datasets.
We dimension B, M and C based on the largest dataset used in our evaluation (i.e., the Criteo Kaggle) with the configuration shown in Table . I. Each ET have different number of entries. In this configuration, the maximum size of the ETs in the Criteo Kaggle is 30,000 entries. Since each CMA has 256 rows, 118 CMAs are required to store the embedding table. The number of arrays is rounded up to the nearest power-oftwo value, i.e., 128. We choose C=32, which corresponds to 4 mats (M =4) working in parallel inside each bank, to have a balance between storing small and large ETs. The outputs from the 4 mats are accumulated at the bank level. Finally, the Criteo Kaggle dataset has 26 sparse features for ranking. Hence, we dimension iMARS with 32 banks (B=32) to accomodate all these features. The other features are also mapped accordingly,  with some mats and CMAs deactivated in a bank according to the size of the ET. We use 26 activated banks, 104 activated mats and 2860 activated CMAs for the Criteo Kaggle dataset.
For the MovieLens dataset (also used in our evaluation), due to the much smaller number of rows per ET, we are still able to use the same architecture while keeping idle arrays deactivated. The MovieLens dataset uses 5 UIET for the filtering stage and 6 UIET for the ranking stage (Table I), 5 of which are shared between the filtering and ranking stages. ETs have a maximum of 6040 entries and a minimum of 3 entries. We use 7 active banks, 8 active mat and 54 active CMA in the MovieLens dataset.<h2>publication_ref</h2>['b1', 'b0']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_0', 'tab_0']<h2>heading</h2>A. Array-level Evaluation<h2>text</h2>We have designed the complete circuit of a 256×256 FeFET-based CMA, and simulated it in HSPICE by employing a Preisach based model for FeFETs [19] along with the CMOS Predictive Technology Model (PTM) from [20] with a 45nm technology node. In our evaluation, besides the memory cells inside each CMA, we also consider all the peripherals depicted in Fig. 3(c). The adder trees and communication network are implemented in Verilog and synthesized with Cadence Encounter RTL Compiler v14.10, with the NanGate 45nm open-cell library [21]. The crossbars are evaluated by the Neurosim tool [22] using a 45nm FeFET model. Table II summarizes the array-level figures-of-merit (FoM) for the different types of accesses supported by the CMA. Table II also includes the FoM for a 256×128 crossbar. These values are used for higher-level evaluations.<h2>publication_ref</h2>['b18', 'b19', 'b20', 'b21']<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>['tab_1']<h2>heading</h2>B. Accuracy Evaluation<h2>text</h2>We examined the algorithm-level performance (i.e., accuracy) of RecSys when using quantized data representation and when using different distance functions. We implemented a YoutubeDNN filtering model [2] on the MovieLens 1M dataset [11], where a FAISS-based distance search is used. We use the hit rate (HR), the # of hits (i.e., correct predictions) divided by the # of test users, as the accuracy metric. Three <h2>publication_ref</h2>['b1', 'b10']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C. Energy and Latency Evaluation<h2>text</h2>We estimated the energy and latency of iMARS based on the mapping and the simulated array-level FoM. As the latency and energy improvement of TCAM arrays and crossbars are well studied in previous work [5] [22], in this section we mainly focus on our findings on ET lookup operations, which is also a bottleneck of the RecSys. We compare the latency and energy of TCAM-based LSH search with the LSH search on GPU as well as the original cosine search on GPU. Finally we report the end-to-end system comparison.
1) ET lookup operation: Table III shows the latency and energy consumption of the ET lookup operation in the two RecSys instances as well as on GPU. All the data are obtained for one item input. The ET lookup operation in iMARS includes the multiple lookups of CMAs, the intra-mat addition and intra-bank addition. To estimate the latency and energy on iMARS, we consider the worst case that all lookups for one ET happen in the same array. Multiple lookups in one array requires multiple read, write and in-memory add operations, incurring higher latency and energy. We also include the latency/energy overhead of communication due to wiring and serialization with the RSC bus/IBC network. Thus, under the aforesaid worst case, the iMARS takes 0.24 µs latency and 6.88 µJ energy for a single input on the Criteo Kaggle dataset and achieves 61.83× latency and 47.9× energy improvement. For the MovieLens dataset, the iMARS achieves 43.1×/45.6× speedup and 516.05×/458.12× energy reduction over the GPU counterpart on the filtering/ranking stages.
From Table III, it can be seen that on both GPU and iMARS, the ranking stage takes more time and energy than the filtering stage for MovieLens because the ranking stage deploys one more ET than the filtering stage as the memory mapping shown in Table I. Also, the latency and energy for the MovieLens dataset is smaller than another dataset because of the relatively small ET size. These improvements are attributed to the fact that iMARS reduces the data movement between the processor and memory by using in-memory ET lookup. Also the adoption of the FeFET technology contributes to part of the improvement.
2) NNS operation: NN Search operations are needed in the filtering stage, and are realized by configuring CMAs to the CAM mode in iMARS. The utilization of the CAM search mode enables the NNS operation to be implemented in O(1) time instead of O(n). For the filtering stage on the MovieLens dataset with O(10 3 ) items, the search latency using the original cosine distance on the GPU is around 13.6 µs and it consumes 0.34 mJ for one input. With LSH search with 256 signature length, the GPU spends 6.97 µs and 0.15 mJ. The latency and energy improvement over the GPU counterpart (LSH search) is 3.8e4× and 2.8e4× as shown in Table II.
3) End-to-End: We compare the end-to-end improvements of iMARS over GPU in this section. For the GPU data, we only count DNN stack, ET lookup and NNS operation in the algorithm. For iMARS, the ET lookup operation and NNS operation are evaluated as we discussed before. The DNN stack is evaluated using Neurosim [22] (FoM shown in Table II), which brings around 2.69× latency improvement compared to the GPU counterpart.
For the ranking model on the Criteo Kaggle dataset, iMARS achieves 13.2× latency improvement and 57.8× energy improvement over GPU. For the filtering and ranking stage together, iMARS achieves 16.8× and 713× latency/energy end-to-end improvement on the MovieLens dataset. That is, it can achieve 22025 queries/second compared with the 1311 queries/second on the GPU. The end-to-end improvement is dominated by the ranking stage because each user only goes through the filtering stage once in iMARS. However, for each user, the CTR needs to be calculated for each candidate item during the ranking stage.<h2>publication_ref</h2>['b21']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_2', 'tab_2', 'tab_0', 'tab_1', 'tab_1']<h2>heading</h2>V. CONCLUSION<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2><h2>text</h2>We present iMARS, an IMC-based accelerator for recommendation systems. iMARS uses hierarchical IMC fabric consisting of both crossbars and CMAs to accelerate both the filtering and ranking stages of RecSys. We introduce an IMCfriendly embedding table organization and judicious computation mapping to maximize the benefit offered by IMC. iMARS achieves 22025 queries/second over 1311 queries/second on the GPU (16.8× speedup for the MovieLens dataset). Also, 713× end-to-end energy reduction compared with GPU is achieved by iMARS.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>Deep learning recommendation model for personalization and recommendation systems<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>M Naumov<h2>ref_id</h2>b1<h2>title</h2>Deep neural networks for youtube recommendations<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>P Covington<h2>ref_id</h2>b2<h2>title</h2>Compositional embeddings using complementary partitions for memory-efficient recommendation systems<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>H.-J M Shi<h2>ref_id</h2>b3<h2>title</h2>Recnmp: Accelerating personalized recommendation with near-memory processing<h2>journal</h2>IEEE<h2>year</h2>2020<h2>authors</h2>L Ke<h2>ref_id</h2>b4<h2>title</h2>Ferroelectric ternary content-addressable memory for oneshot learning<h2>journal</h2>Nature Electronics<h2>year</h2>2019<h2>authors</h2>K Ni<h2>ref_id</h2>b5<h2>title</h2>X-mann: A crossbar based architecture for memory augmented neural networks<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>A Ranjan<h2>ref_id</h2>b6<h2>title</h2>Computing in memory with fefets<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>D Reis<h2>ref_id</h2>b7<h2>title</h2>FeMAT: Exploring In-Memory Processing in Multifunctional FeFET-Based Memory Array<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>X Zhang<h2>ref_id</h2>b8<h2>title</h2>Attention-in-Memory for Few-Shot Learning with Configurable Ferroelectric FET Arrays<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>D Reis<h2>ref_id</h2>b9<h2>title</h2>A FeFET based super-low-power ultra-fast embedded NVM technology for 22nm FDSOI and beyond<h2>journal</h2>IEDM<h2>year</h2>2017<h2>authors</h2>S Dunkel<h2>ref_id</h2>b10<h2>title</h2>The movielens datasets: History and context<h2>journal</h2>Acm transactions on interactive intelligent systems (tiis)<h2>year</h2>2015<h2>authors</h2>F M Harper<h2>ref_id</h2>b11<h2>title</h2>Fleetrec: Large-scale recommendation inference on hybrid gpu-fpga clusters<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>W Jiang<h2>ref_id</h2>b12<h2>title</h2>Microrec: Efficient recommendation inference by hardware and data structure solutions<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>W Jiang<h2>ref_id</h2>b13<h2>title</h2>A convolutional neural network accelerator with in-situ analog arithmetic in crossbars<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>A Shafiee<h2>ref_id</h2>b14<h2>title</h2>A 28 nm configurable memory (tcam/bcam/sram) using push-rule 6t bit cell enabling logic-in-memory<h2>journal</h2>JSSC<h2>year</h2>2016<h2>authors</h2>S Jeloka<h2>ref_id</h2>b15<h2>title</h2>The impact of ferroelectric fets on digital and analog circuits and architectures<h2>journal</h2>IEEE Design & Test<h2>year</h2>2019<h2>authors</h2>X Chen<h2>ref_id</h2>b16<h2>title</h2>Fefet: A versatile cmos compatible device with gamechanging potential<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>S Beyer<h2>ref_id</h2>b17<h2>title</h2>Mlp+neurosimv3.0: Improving on-chip learning performance with device to algorithm optimizations<h2>journal</h2>Association for Computing Machinery<h2>year</h2>2019<h2>authors</h2>Y Luo<h2>ref_id</h2>b18<h2>title</h2>A circuit compatible accurate compact model for Ferroelectric FETs<h2>journal</h2>IEEE<h2>year</h2>2018<h2>authors</h2>K Ni<h2>ref_id</h2>b19<h2>title</h2>Predictive technology model<h2>journal</h2><h2>year</h2>2002<h2>authors</h2>Y Cao<h2>ref_id</h2>b20<h2>title</h2>Nangate 45nm open cell library<h2>journal</h2>EMEA<h2>year</h2>2008<h2>authors</h2>J Knudsen<h2>ref_id</h2>b21<h2>title</h2>Neurosim: A circuit-level macro model for benchmarking neuro-inspired architectures in online learning<h2>journal</h2>TCAD<h2>year</h2>2018<h2>authors</h2>P.-Y Chen<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Fig. 1 :1Fig. 1: Configuration of the (a) filtering and (b) ranking stages. (c) General DNN model used in the filtering and ranking stages.<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Fig. 2 :2Fig. 2: Operation breakdown of the filtering and ranking stages on the MovieLens dataset.<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Fig. 3 :3Fig. 3: (a) Proposed iMARS architecture. (b) CMA bank for item embedding table (ItET) and user-item embedding table (UIETs). (c) CMA structure, which is capable of switching between the CAM/RAM/GPCiM modes. (d) Crossbar array. Labels (1a), ..., (2d*) shows the computation flow of the ranking and filtering stages as discussed in Sec. III-C.<h2>figure_data</h2><h2>figure_label</h2>I<h2>figure_type</h2>table<h2>figure_id</h2>tab_0<h2>figure_caption</h2>RecSys configurations and memory mapping on iMARS<h2>figure_data</h2>MovielensCriteo KaggleModelYoutubeYoutubeDLRMStageFilteringRankingRankingDNN Network128-64-32128-1Bottom MLP 256-128-32Top MLP 256-64-1# UIET (Shared)5 (5)6 (5)26# ItET1/# Row per ET300028000# Bank726# Mat8104# CMA542860<h2>figure_label</h2>II<h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>Array-level evaluation of CMA, adder-trees and crossbars.<h2>figure_data</h2>ComponentOperationEnergy (pJ)Latency (ns)Write49.110.0256×256Read3.20.3CMAAddition108.08.1Search13.80.2Intra-mat adder tree256-bit Add137.014.7Intra-bank adder tree256-bit Add956.044.2256×128 CrossbarMatMul13.8225.0<h2>figure_label</h2>III<h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>ET operation comparison between the GPU and iMARS<h2>figure_data</h2>DatasetMovieLensKaggleStageFilteringRankingRankingGPU9.27µs9.60 µs14.97 µsLatencyiMARS0.21 µs0.21 µs0.24 µsSpeedup43.61×45.17×61.83×GPU203.97 µJ211.26 µJ329.34 µJEnergyiMARS0.40 µJ0.46 µJ6.88 µJReduction516.05×458.12×47.90×configurations are tested: (1) 32-bit floating-point (FP32) rep-resentation and cosine distance (2)8-bit Int and cosine distance(3) 8-bit Int and LSH-based Hamming distance and achieveHR to be 26.8% / 26.2% /20.8%, respectively. iMARS incursaround 5.4% accuracy degradation, which indicates that thedistance function plays an important role in the accuracy.However, since the filtering stage only provides a coarseselection of the candidate items, such accuracy loss is tolerableas the accuracy is retained by the ranking stage.<h1>formulas</h1>[]<h1>doi</h1><h1>title</h1>iHAS: Instance-wise Hierarchical Architecture Search for Deep Learning Recommendation Models<h1>authors</h1>Yakun Yu; Shi-Ang Qi; Jiuding Yang; Liyao Jiang; Di Niu<h1>pub_date</h1>2023-09-14<h1>abstract</h1>Current recommender systems employ large-sized embedding tables with uniform dimensions for all features, leading to overfitting, high computational cost, and suboptimal generalizing performance. Many techniques aim to solve this issue by feature selection or embedding dimension search. However, these techniques typically select a fixed subset of features or embedding dimensions for all instances and feed all instances into one recommender model without considering heterogeneity between items or users. This paper proposes a novel instance-wise Hierarchical Architecture Search framework, iHAS, which automates neural architecture search at the instance level. Specifically, iHAS incorporates three stages: searching, clustering, and retraining. The searching stage identifies optimal instance-wise embedding dimensions across different field features via carefully designed Bernoulli gates with stochastic selection and regularizers. After obtaining these dimensions, the clustering stage divides samples into distinct groups via a deterministic selection approach of Bernoulli gates. The retraining stage then constructs different recommender models, each one designed with optimal dimensions for the corresponding group. We conduct extensive experiments to evaluate the proposed iHAS on two public benchmark datasets from a real-world recommender system. The experimental results demonstrate the effectiveness of iHAS and its outstanding transferability to widely-used deep recommendation models.<h1>sections</h1><h2>heading</h2>INTRODUCTION<h2>text</h2>Recommender systems, which aim to predict the preference of users, have been widely deployed in various real-world scenarios, e.g., online advertising [3,27], social media [9], news apps [39], etc. Deep learning recommendation models (DLRMs) typically take a large amount of categorical (e.g., gender) or numerical (e.g., age) field features as input. These features are first encoded into highdimensional sparse one-hot vectors, which are later mapped into real-valued dense vectors via embedding tables. The recommender model then feeds these embeddings into a feature interaction layer which usually consists of deep neural network (DNN) [6] or factorization machine (FM) [8,14,26,34] to model user preferences for final prediction.
The embedding tables play a fundamental role in the recommendation system, as they dominate the majority of parameters. However, most existing methods construct their proposed recommender models with large-sized embedding tables and a uniform dimension size for all possible fields [8,10,29], which may lead to overfitting, high computational cost, and poor model generalization [15,25,33]. Therefore, the first objective for an optimal DLRM is to find optimal embedding dimensions for different fields and remove redundant dimensions. Performing embedding dimension search is also sufficient to include feature selection, i.e., the optimal dimension for a field feature could be zero, which means completely excluding this feature.
A common approach for embedding dimension search is to employ the l 0 norm on the dimensions to penalize the count of nonzero dimension entries. However, as the l 0 norm poses a computational challenge for gradient descent, researchers have attempted to substitute l 0 with a surrogate function, such as the l 1 norm for LASSO [32], yet achieving limited selection ability [35]. Recently, probabilistic approaches [18,33] suggest utilizing Bernoulli random variables (RVs) with Gumbel-Softmax approximations to identify the top 𝐾 features with the highest probabilities. Though these methods can be applied for dimension selection, we have empirically observed that the probabilities learned are often indistinguishable. Therefore, selecting the top 𝐾 features/dimensions based on these probabilities may inadvertently result in either the exclusion of critical features/dimensions or the inclusion of irrelevant ones.
Furthermore, prior approaches uniformly apply embedding dimension selection across all instances in the datasets, therefore disregarding the inherent variations between individual samples. This one-size-for-all approach can be inadequate in many scenarios, especially when dealing with highly heterogeneous populations where relevant features can significantly diverge across users or across items. For example, in a movie recommendation system, the feature "age" usually plays a crucial role in recommending Disney movies, thereby possibly necessitating a larger embedding dimension. Conversely, "age" is less relevant for comedy films, resulting in a smaller dimension size. Thus, it is evident that treating all instances identically may overlook these context-specific nuances. Intuitively speaking, when dimension selection is performed at the instance level, we can create neural architectures that are better suited to individual samples. Such an approach not only results in superior performance but also enables faster inference times by focusing on the most relevant dimensions of each sample.
In this paper, we propose an instance-wise Hierarchical Architecture Search framework, iHAS, which attempts to perform automatic architecture search on the instance level, using hierarchical training procedures for DLRMs. Specifically, iHAS includes three learning stages: searching, clustering, and retraining. The searching stage aims to find the optimal instance-wise embedding dimensions across different fields via a carefully designed "Bernoulli gate" with stochastic selection mode and a regularizer. After selecting instancewise embedding dimensions, we separate samples into different groups based on a novel deterministic selection approach in the clustering stage. The retraining stage trains different recommender models, with optimal dimensions tailored to different groups. During inference time, each test sample will first be assigned to a suitable group, where predictions are made by the corresponding recommender model. We summarize our major contributions as:
• We propose a hierarchical training framework that uses instance-wise "Bernoulli gates" to facilitate effective dimension search for each sample. • We apply a sparse and bi-polarization regularizer in the objective function to help the model learn distinguishable Bernoulli RVs, and use a threshold selector for downstream deterministic selection. • To balance the trade-off between the one-size-for-all and full-customization (which is not applicable with finite data size) strategies, we propose to divide samples into clusters and develop tailored recommender models for each cluster.
We empirically evaluate the performance of our framework on two large-scale benchmark datasets. Our experimental results indicate a notable superiority of our approach over various state-of-the-art baseline models on both datasets. Furthermore, the transferability analysis demonstrates our framework can be effectively transferred to diverse deep recommender models, thereby enhancing their performance. Additionally, our framework offers an efficiency advantage as it requires less inference time than competing baseline models.<h2>publication_ref</h2>['b2', 'b26', 'b8', 'b38', 'b5', 'b7', 'b13', 'b25', 'b33', 'b7', 'b9', 'b28', 'b14', 'b24', 'b32', 'b31', 'b34', 'b17', 'b32']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>RELATED WORK<h2>text</h2>This section introduces the main related works to our study, focusing on feature-based recommender models and AutoML approaches for recommendation systems.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Feature-based Recommender Models<h2>text</h2>Feature-based recommender models take sparse, high-dimensional features from users and items as input and transform them into low-dimensional representations to capture user preferences for improved recommendations. For example, Cheng et al. [4] propose Wide&Deep (W&D), a model composed of a linear module and a Multi-Layer Perceptron (MLP) layer to combine the benefits of memorization and generalization for recommender systems. Guo et al. [8] propose DeepFM that further integrates the power of factorization machines based on W&D to learn high-order feature interactions for recommendations. Recently, advanced neural networks, such as attention-based models [29], have been developed. However, these techniques apply a fixed embedding dimension for all features, which would downgrade the model performance and consume substantial computational resources.<h2>publication_ref</h2>['b3', 'b7', 'b28']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>AutoML for Recommendations<h2>text</h2>Automated Machine Learning (AutoML) has recently become a research hotspot due to its potential to automate the design process for recommender systems, minimizing human involvement. The research directions include feature selection [15,33], embedding dimension search [37,38], model architecture search [5], and other component search [30,31,36]. Feature selection involves selecting a subset of field features in recommendation systems. For example, AutoField [33] uses a simple controller based on differentiable architecture search [16] to select the top 𝐾 field features. AdaFS [15] enhances AutoField by modifying the controller to assign feature weights to fields for different samples. The objective of embedding dimension search is to find mixed embedding sizes for each field. For example, AutoEmb [38] finds the optimal dimension for each feature using differentiable search [16]. AutoDim [37] selects the best dimension for each field from a group of candidate dimensions in the same way as AutoEmb. Model architecture search explores various network architectures and selects the optimal one [5].
Our method is in alignment with embedding dimension search. All instances in the above methods share a uniform dimension size for each field. In contrast, our approach adaptively selects dimensions for each instance via the proposed Bernoulli gates, thereby considering the difference between individuals. Moreover, we introduce a polarization regularizer to overcome the shortcomings of the commonly-used top 𝐾 selection strategy. Furthermore, rather than processing all samples through a single model, we propose to divide samples into clusters and train different recommender models with optimal dimensions tailored to different clusters. These unique and innovative designs in our proposed method have been proven effective in terms of both performance enhancement and inference cost saving.<h2>publication_ref</h2>['b14', 'b32', 'b36', 'b37', 'b4', 'b29', 'b30', 'b35', 'b32', 'b15', 'b14', 'b37', 'b15', 'b36', 'b4']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>METHOD<h2>text</h2>This section introduces the technical specifications of the proposed iHAS framework, as visualized in Figure 1. We first provide a concise overview of the entire hierarchical training framework. Subsequently, the primary modules within our framework are described, as well as how to optimize them within each hierarchical stage.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_0']<h2>table_ref</h2>[]<h2>heading</h2>Overview<h2>text</h2>The methodology for the iHAS framework comprises three stages: searching, clustering, and retraining. It involves three principal modules: deep recommendation models (consisting of an embedding layer and an MLP component) for predicting user preferences, a Bernoulli gates layer responsible for dimension selection, and a K-means cluster algorithm that partitions the heterogeneous data.
In the searching stage, the key objective is to identify the optimal, instance-wise embedding dimensions across different fields, thus facilitating an accurate recommendation prediction. As shown in Figure 1, the categorical field features are directed to an embedding layer to generate embedding representations. These representations are then processed through the Bernoulli gates to produce embedding masks using a stochastic selection mode (see Section 3.3.1). Each embedding mask comprises a binary vector that serves as a gate on whether the corresponding dimension should be incorporated into the downstream architecture. Then the framework conducts an element-wise multiplication between a sample's embedding representations and embedding masks. This resultant masked embedding representation is then directed to the base MLP component to predict user preference.
In the clustering stage, the main objective is to utilize K-Means algorithm [28] to cluster samples into groups. This stage mostly mirrors the procedure used in the searching stage: using the embedding layer and Bernoulli gates (but using deterministic selection mode, see Section 3.3.2) to calculate the masked embedding representations. These masked embedding representations are then used to train a mini-batch K-Means cluster [28]. As shown in Figure 1, the K-means separates the red and yellow samples from the blue and green samples.
The retraining stage aims to develop cluster-customized DLRMs, considering the variation in dimension patterns across different clusters. In each cluster, we calculate the embedding masks (using deterministic selection mode) for all samples. The resultant masks are then averaged to obtain one vector, which is used to determine the final embedding dimensions of each DLRM.<h2>publication_ref</h2>['b27', 'b27']<h2>figure_ref</h2>['fig_0', 'fig_0']<h2>table_ref</h2>[]<h2>heading</h2>Deep Learning Recommender Models<h2>text</h2>In this subsection, we provide a brief introduction to the basic architecture of the DLRM. It typically comprises two primary components: an embedding layer and an MLP component.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Embedding Layer.<h2>text</h2>In classic DLRM, the embedding layer is commonly used to convert the categorical inputs into a dense vector of real numbers.
Let us denote the input of 𝑁 categorical field features for sample
𝑖 as 𝑿 𝑖 = [𝒙 𝑖,1 , • • • , 𝒙 𝑖,𝑛 , • • • , 𝒙 𝑖,𝑁 ],
where 𝒙 𝑖,𝑛 ∈ Z |𝑛 | represents the one-hot vector comprising sparse, high-dimensional binary values. The term |𝑛| denotes the number of unique values for 𝑛-th categorical field. For instance, a categorical field such as "gender" with unique valuesmale, female, and unknown -can be expressed through three-bit vectors [1, 0, 0], [0, 1, 0], and [0, 0, 1], respectively.
To process a numerical field feature, we will discretize it through custom-width binning, followed by applying a one-hot operation.
Then, the operation of the embedding layer can be represented as:
𝒆 𝑖,𝑛 = 𝒗 𝑛 𝒙 𝑖,𝑛 ,
where
𝒗 𝑛 ∈ R 𝑑 × |𝑛 |
is the embedding table of the 𝑛-th field, 𝑑 is the predefined embedding dimension (typically consistent across all fields), and 𝒆 𝑖,𝑛 is the low-dimensional embedding representation. Therefore, the final embedding of the input data 𝑿 𝑖 through 𝑁 embedding tables is
𝑬 𝑖 = [𝒆 𝑖,1 , • • • , 𝒆 𝑖,𝑛 , • • • , 𝒆 𝑖,𝑁 ].
Notably, the embedding dimension search techniques we discussed earlier in Section 2.2 (also the focus of this paper) aim at searching the optimal dimensions for embedding tables
𝑽 = [𝒗 1 , • • • , 𝒗 𝑛 , • • • , 𝒗 𝑁 ].
Specifically, our goal is to discover the optimal individual embedding dimension for each field, given the inherent diversity in the heterogeneous dataset. This could potentially enhance prediction performance.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>MLP Component.<h2>text</h2>The MLP component plays a crucial role in DLRMs, tasked with encoding embedding representations and predicting the recommendation. Empirically, it comprises multiple fully-connected (FC) layers (characterized by parameter 𝜽 ) and is also equipped with non-linear activation functions such as ReLU [1] or Sigmoid, thereby facilitating the nonlinear encoding process of these representations.
In the iHAS system, we will train three different DLRMs, as illustrated in Figure 1. Each DLRM consists of an embedding layer and an MLP component. These three models are named the base recommender model, recommender model 1, and recommender model 2, which are characterized by the parameter groups {𝑽, 𝜽 } 𝑏 , {𝑽, 𝜽 } 1 , and {𝑽, 𝜽 } 2 , respectively.<h2>publication_ref</h2>['b0']<h2>figure_ref</h2>['fig_0']<h2>table_ref</h2>[]<h2>heading</h2>Bernoulli Gates<h2>text</h2>Bernoulli gates operate as switches, facilitating the transmission of a sample's information from embedding tables to the downstream MLP component. Analogous to the l 0 norm, we hope these "switches" to be capable of fully opening or closing without compromising the information integrity (shrinking the embedding representation). Inspired by the approach presented in [18,35], we use Bernoulli gates to predict each sample's relevant dimensions given its embedding representation. The detailed process of the Bernoulli gates is graphically depicted in Figure 2.
The Bernoulli gates operate in two distinct modes: stochastic selection and deterministic selection. Under the stochastic selection mode, the gates operate as independent Bernoulli distributions, to independently "open" or "close" dimensions given the probabilities. The principle behind stochastic selection rests on the assumption that, given a sufficiently large number of training iterations, the gates will stochastically and comprehensively traverse all potential combinations of dimensions. This prompts the Bernoulli parameters to increase for beneficial dimensions and penalize unhelpful ones. Once the Bernoulli distributions (gates) have been fully explored, we capitalize on the learned distribution by deterministically opening the most advantageous dimensions in the deterministic selection mode. However, learned Bernoulli probabilities often exhibit heavy-tailedness, making it challenging to distinguish between important and unimportant dimensions. To mitigate this, we suggest employing a polarization regularizer and an automatic threshold searcher (both discussed in Section 3.3.3) inside Bernoulli gates.<h2>publication_ref</h2>['b17', 'b34']<h2>figure_ref</h2>['fig_1']<h2>table_ref</h2>[]<h2>heading</h2>Stochastic Selection.<h2>text</h2>In our previous discussion, we aim for Bernoulli gates to function as independent Bernoulli distributions in the stochastic selection mode during the searching stage. The first objective is to encode the embedding representations to the desired independent Bernoulli probabilities. To this end, we employ an FC layer (with parameter 𝒘) and a Sigmoid activation layer (𝜎) to project these embedding representations of the 𝑖-th sample onto Bernoulli probabilities (upper left of Figure 2), denoted by {𝑝 𝑖,𝑗 } 𝑁 * 𝑗=1 = 𝜎 (𝒘 𝑬 𝑖 ), where 𝑁 * = 𝑁 × 𝑑 is the total length of the embedding representations. This enables us to initiate a combinatorial search process over the space of Bernoulli probabilities and FC parameters. However, optimizing a loss function, which includes discrete RVs (Bernoulli distributions), incurs high variance [23]. To overcome this obstacle, we adopt Gumbel-Softmax [11] (aka Concrete distribution [21]), which offers a viable continuous approximation to the Bernoulli distribution, as visualized in Figure 2 (right).
Recall that Gumbel-Max [7,22] is an effective method for drawing samples from a Bernoulli distribution (or any type of discrete random variables), as long as we provide the class probabilities. For instance, if we use 𝑝 𝑖,𝑗 and 1 -𝑝 𝑖,𝑗 as the probabilities for selecting and not selecting the 𝑗-th embedding dimension for sample 𝑖, respectively, the Gumbel-Max trick to approximate Bernoulli distribution sampling can then be expressed as:
𝒛 𝑖,𝑗 = one hot (arg max (log 𝑝 𝑖,𝑗 + 𝐺 𝑖,𝑗 , log(1 -𝑝 𝑖,𝑗 ) + 𝐺 ′ 𝑖,𝑗 )),(1)
where 𝐺 𝑖,𝑗 and 𝐺 ′ 𝑖,𝑗 are i.i.d. samples drawn from a Gumbel distribution with the location at 0 and scale of 1, denoted as Gumbel(0, 1). However, both the "one hot()" and "arg max()" operations are nondifferentiable, making them intractable for gradient descent optimization. Therefore, the softmax function is used as a continuous, (2) As 𝜏 approaches 0, 𝒛 𝑖,𝑗 approximates the true binary vector, making the Gumbel-Softmax distribution become identical to the desired Bernoulli distribution. Then the final embedding masks, 𝒎 𝑖 , are created by concatenating the first bit of {𝒛 𝑖,𝑗 } 𝑁 * 𝑗=1 . However, our goal remains to produce true binary masks, which would effectively eliminate information from unimportant dimensions, as opposed to significantly shrinking them. The straightthrough (ST) Gumbel-Softmax [2,11] serves well in this context. In the ST variant, the operation from Equation 1 is implemented in the forward pass while the continuous approximation from Equation 2is used in the backward gradient descent. This approach enables sparse selection even when the temperature 𝜏 is high, while still allowing the gradient to propagate and update the parameters.<h2>publication_ref</h2>['b22', 'b10', 'b20', 'b6', 'b21', 'b1', 'b10']<h2>figure_ref</h2>['fig_1', 'fig_1']<h2>table_ref</h2>[]<h2>heading</h2>Deterministic Selection.<h2>text</h2>After training the Bernoulli probabilities (𝑝 𝑖,𝑗 ) during the searching phase, we utilize these probabilities to determine which dimensions will contribute to the accuracy of the recommendation predictions. However, 𝑝 𝑖,𝑗 are characterized by high variance and heavy-tailedness, as shown by the histogram in Figure 3 (left). These present two complications: (1) distinguishing important dimensions from unimportant ones becomes challenging; and (2) even the unimportant dimensions still possess a small probability of being selected. Moreover, masks created using Bernoulli gates introduce an element of randomness (Gumbel noise), which hinders their direct application during inference (where given the same data each time, consistent results should be generated).
To overcome these limitations, we propose a deterministic selection mode that directly selects the beneficial dimensions using the knowledge derived from the well-trained Bernoulli probabilities. This process is outlined in Figure 2 (bottom left). Firstly, we use the same FC layer and sigmoid layer to estimate the Bernoulli probabilities, 𝑝 𝑖,𝑗 , analogous to the first step in the stochastic selection mode.
Then, for each sample 𝑖, we search a threshold among the probabilities {𝑝 𝑖,𝑗 } 𝑁 * 𝑗=1 (see details in Section 3.3.3). We automatically adjust the gates to be open for probabilities exceeding this threshold and closed for those falling below it. The resulting embedding masks are utilized during the clustering and retraining phase (see Figure 1).<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2', 'fig_1', 'fig_0']<h2>table_ref</h2>[]<h2>heading</h2>Polarization and Automatic Threshold<h2>text</h2>Searcher. Let's consider an empirical optimization procedure with an l 0 regularization on the embedding masks during the searching stage:
R ( {𝑽, 𝜽 } 𝑏 , 𝒎 ) = E 𝑖 E 𝒎 𝑖 L( 𝑓 𝜽 𝑏 ( 𝑽 𝑏 • 𝑿 𝑖 ⊙ 𝒎 𝑖 ), 𝑦 𝑖 ) , (3) with regularizer R (𝒎) = E 𝑖 E 𝒎 𝑖 [ 𝜆 ∥𝒎 𝑖 ∥ 0 ] ,(4)
where L( 𝑗=1 𝑝 𝑖,𝑗 . Our experimentation, however, indicates that simply using this l 0 regularizer still experiences the heavy-tailedness and distinction difficulties, as discussed in Section 3.3.2. Inspired from Zhuang et al. [41], we incorporate a polarization regularizer into Equation 4:
R (𝒎) = ∑︁ 𝑖 𝑁 * ∑︁ 𝑗=1 𝜆 𝑝 𝑖,𝑗 -|𝑝 𝑖,𝑗 -p𝑖 |, with p𝑖 = 1 𝑁 * 𝑁 * ∑︁ 𝑗=1 𝑝 𝑖,𝑗 .(5)
The intuition of the second term (which is the polarization regularizer) is to maximally distance 𝑝 𝑖,𝑗 from their mean p𝑖 . Empirically, we have observed this polarization term effectively separates the probabilities of important and unimportant dimensions into two groups, thereby making them distinguishable (see Figure 3). Despite employing the regularizer as stated in Equation 5, a threshold searcher is still required to identify the threshold for 𝑝 𝑖,𝑗 . Currently, the histogram of 𝑝 𝑖,𝑗 trained with the polarization regularizer should have at least two peaks, with one of them located close to 0. Following the strategy from [41], we scan the histogram from left to right to identify the first saddle point, i.e., the bin that contains the local minimum (the red pin in Figure 2). The lower bound of this bin is subsequently set as the threshold.<h2>publication_ref</h2>['b40', 'b40']<h2>figure_ref</h2>['fig_2', 'fig_1']<h2>table_ref</h2>[]<h2>heading</h2>Clusters<h2>text</h2>Once obtaining a sample's unique embedding masks, our aim is to pass this sample through a recommender model tailored to its distinct dimension selection. This implies that, for every sample with its individual selected dimensions, there always exists a customized recommender model capable of handling this distinct input and generating a precise prediction. Nonetheless, training an extensive number of recommender models is impractical due to the constraint of finite data size. For example, with a obviously underestimated dimension, 𝑁 * = 100, the number of possible combinations of dimensions is 100 𝑘=0 100 𝑘 = 2 100 , meaning at least 2 100 distinct samples should be collected.
Consequently, we must strike a balance between using a one-sizefits-all recommender model (inadequate for highly heterogeneous samples) and using a fully-customized recommender model for each sample. Our resolution is to leverage clustering algorithms to segregate samples into separate groups. We anticipate that samples update {𝑽, 𝜽 } 1 according the objective from Eq. 3 20: end while 21: repeat lines 16-20 with D train 2 , D val 2 , and {𝑽, 𝜽 } 2 within the same group display similar patterns, which could be used to explore identical dimension patterns and identical recommender models. Furthermore, partitioning the data into clusters facilitates the training of small and manageable models for each cluster, thus accelerating the inference speed.
In this iHAS framework, we opt for the mini-batch K-Means clustering algorithm [28] to partition the samples. We chose this variant due to its computational efficiency in handling large datasets (comprised of tens of millions of samples), and its capacity to avoid getting stuck at local optima.<h2>publication_ref</h2>['b27']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Optimization Strategy<h2>text</h2>In the iHAS framework, we adopt a hierarchical training strategy to optimize different modules in different stages, inspired by [12,15,24,33], to tackle the encoding issue of jointly optimizing all modules. The detailed optimization strategy in each stage is illustrated in Algorithm 1. [35], the use of Gumbel-Softmax approximation for the discrete random variable suffers from high variance, which can lead to inconsistency in the set of selected dimensions. Inspired by [15], we first pretrain the base recommender model for a few epochs to obtain a tentative reliable embedding representation. During these pretrain epochs, the Bernoulli gates are always deterministically open (no matter what embedding representation it receives). Then, after we've obtained a tentative reliable embedding representation, we initialize the parameters for the Bernoulli gates and start the stochastic selection. Later we adopt the bi-level optimization strategy [12,24] to disjointly update the parameters 𝒘 in Bernoulli gates and the parameters {𝑽, 𝜽 } b in the base recommender model.<h2>publication_ref</h2>['b11', 'b14', 'b23', 'b32', 'b34', 'b14', 'b11', 'b23']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Searching Stage. As mentioned in<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Clustering Stage.<h2>text</h2>In the clustering stage, we can obtain the masked embedding representations using the embedding tables and Bernoulli gates which have been trained during the searching stage. Remember we use deterministic selection mode for Bernoulli gates to generate embedding masks in this clustering stage. For each sample, compute its Euclidean distance to the centroid using the masked embedding representations, then assign it to the nearest centroid (group), and later update the centroids.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Retraining Stage.<h2>text</h2>As to the retraining stage, we first divide all samples via the trained K-Means cluster. Then we find the majority dimensions for each group from their embedding masks using the deterministic mode. After that, we initialize the deep recommender model 1 and 2 by their corresponding optimal dimensions and train them separately using the samples of each cluster.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>EXPERIMENT<h2>text</h2>In this section, we conduct extensive experiments to evaluate our proposed framework. Specifically, the main research questions we care about are as follows:
• RQ1: How does iHAS perform compared with other mainstream selection methods? • RQ2: Can the proposed iHAS be successfully transferred to more powerful recommender models? • RQ3: How does each component contribute to the overall performance of the proposed iHAS? • RQ4: Does the proposed iHAS demonstrate efficiency when compared to baseline models? • RQ5: Does iHAS construct rational recommender model structures?<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Datasets<h2>text</h2>We conduct our experiments mainly on two commonly used public datasets, Avazu 1 and Criteo 2 , which are both large-scale realworld datasets and serve as benchmarks in click-through rate (CTR) prediction tasks. Table 1 presents the detailed statistics of both datasets. Each dataset has been randomly segmented into training/validation/testing sets based on the proportions of 80%, 10%, and 10%.
• Avazu dataset consists of 40 million users' click records on ads over 11 days. Each record contains 22 categorical field features. Following the general preprocessing steps [29,40],
1 https://www.kaggle.com/c/avazu-ctr-prediction/ 2 https://www.kaggle.com/c/criteo-display-ad-challenge/ we group fields of which frequency is less than ten as a single field "others". • Criteo dataset consists of 46 million users' click records on display ads. Each record contains 26 categorical fields and 13 numerical fields. we use the preprocessing method as Avazu for the low-frequency fields (less than ten) and transform each numerical field 𝑥 by 𝑙𝑜𝑔 2 (𝑥) if 𝑥 > 2.<h2>publication_ref</h2>['b28', 'b39']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_1']<h2>heading</h2>Evaluation Metrics<h2>text</h2>Following the previous works [17,25], we evaluate the performance of our method using two common metrics: AUC and Logloss. AUC refers to the area under the ROC curve, which means the probability that a model will rank a randomly selected positive instance higher than a randomly selected negative one. A higher AUC value indicates superior model performance. On the other hand, Logloss, aka binary cross-entropy loss, directly quantifies the model's performance, with a lower score denoting more accurate predictions. Note that a marginal 0.001-level improvement in AUC (increase) or Logloss (decrease) is perceived as a significant enhancement in model performance [19,33,37].<h2>publication_ref</h2>['b16', 'b24', 'b18', 'b32', 'b36']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Baseline Methods<h2>text</h2>We compare our proposed method with the following state-of-theart methods:
• PEP [17]: It adopts trainable thresholds to prune redundant embedding dimensions. • AutoField [33]: It utilizes neural architecture search techniques [16] to select important field features. • OptEmbed [20]: It trains a supernet with various selected embedding dimensions, then uses evolution search to find the optimal embedding dimensions based on the supernet. • AdaFS [15]: It assigns weights to different fields in a soft manner (AdaFS-soft) or masks unimportant fields in a hard manner (AdaFS-hard) via a novel controller network. • OptFS [19]: It simultaneously selects optimal field features and the optimal interactions between these features using "binary gates".<h2>publication_ref</h2>['b16', 'b32', 'b15', 'b19', 'b14', 'b18']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Implementation Details<h2>text</h2>We implement our method based on a public libraryfoot_0 that involves sixteen commonly-used DLRMs. As our framework is modelagnostic, it can be seamlessly integrated with any of these models, see Section 4.6. For the embedding layer, we set the initial embedding size of all fields as 16 in accordance with the previous works [15,33]. For the MLP component, we adopt two fully-connected layers of size (16,8) with the ReLU activation function. We use Adam optimizer [13] with an initial learning rate of 0.001, and weight decay of 1e-6. The batch size is set to 2048. We sample one validation batch every 100 training batches for bi-level optimization.
The temperature 𝜏 for ST Gumbel-Softmax is set to 0.1. The baseline models are implemented by the codes provided by their authors. For a fair comparison, we set the initial embedding dimension as 16 for all baselines. All the experiments are run on a single machine with an Nvidia RTX 3090 GPU.  First, our iHAS outperforms all the state-of-the-art baseline methods as it can achieve higher AUC and lower Logloss on both datasets, demonstrating the effectiveness of iHAS in deep recommendation systems. Specifically, iHAS outperforms the runner-ups by 0.0038 (AUC) and 0.0021 (Logloss) on the Avazu datasets, and by 0.0004 (AUC) and 0.0006 (Logloss) on the Criteo datasets.
Secondly, among all baselines, AdaFS-soft is the most effective model for the Avazu and Criteo datasets. However, it only shrinks the field features using a feature weighting layer and therefore does not completely eliminate the effect of unimportant fields. Although AdaFS-hard attempts to mask unimportant fields by uniformly keeping the top 𝐾 features, the trained feature weights may still exhibit a high variance pattern (remember the non-distinguishable probabilities in Figure 3, left panel). Therefore, this top 𝐾 selection manner may lead to selecting unimportant features or omitting the important feature in their final model, further compromising the model performance. Our polarization regularizer and threshold searcher can help with this issue, as detailed in Section 3.3.3 and evidenced by the empirical ablation study in Section 4.7.
Lastly, other baselines apply global feature/dimension selection across all samples, which fails to account for inherent variations among heterogeneous individuals, and consequently leads to suboptimal performance. Additionally, PEP mainly emphasizes on the model size, i.e., it stops searching once the embedding table reaches a predefined parameter size. This approach may result in a suboptimal embedding table due to overlooking the model performance. AutoFieldfoot_1 also employs the top 𝐾 selection manner, again leading to feature misselection and inferior model performance.<h2>publication_ref</h2>['b14', 'b32', 'b15', 'b7', 'b12']<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>[]<h2>heading</h2>Transferability Analysis (RQ2)<h2>text</h2>In this subsection, we explore the transferability of iHAS. Specifically, we freeze the parameters of the well-trained Bernoulli gates and utilize them to help train other popular deep recommendation models, including FM [26], W&D [4], and DeepFM [8].
Table 3 shows the experimental results on Avazu, where "original" refers to the corresponding model without any selection. We can observe that: (i) all the recommendation models have great improvement by adopting iHAS, which again demonstrates the importance of performing selection in the recommendations; (ii) The transferability of iHAS is better than the best baseline (by comparing the iHAS and AdaFS-soft in Table 3), which validates the effectiveness of our Bernoulli gates.
In summary, we conclude that iHAS has outstanding transferability across different recommendation models, which enables it to be leveraged in complicated real-world recommender systems.<h2>publication_ref</h2>['b25', 'b3', 'b7']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_4', 'tab_4']<h2>heading</h2>Ablation Study (RQ3)<h2>text</h2>In this subsection, we conduct the ablation study of key components in iHAS, as shown in Table 4. The Base model keeps all fields and the uniform embedding dimensions without any selections, and we derive four variants from iHAS: (i) iHAS-1: This variant is the model directly obtained in the searching stage, i.e., we remove the clustering and retraining stages; (ii) iHAS-2: This variant consists of a searching stage and a retraining stage. After we have the welltrained Bernoulli gates, we select dimensions across all samples to retrain one recommender model instead of separating samples into different clusters for retraining cluster-customized recommender models; (iii) iHAS-3: This variant is the standard iHAS without using the polarization regularizer described in Section 3. initialized vector of the dimension length, making every sample share the same probability for every dimension.
Based on the results in Table 4, we can find: (i) iHAS and its variants can increase the AUC and decrease the Logloss compared with the Base model, which indicates the necessity of performing selection on embedding dimensions for boosting model performance; (ii) iHAS performs better than iHAS-1, which indicates the necessity of the subsequent clustering and retraining stages; (iii) iHAS-1 outperforms iHAS-2, therefore, separating instances into different groups, i.e., the clustering stage, is beneficial for boosting the performance; (iv) Polarization is vital for acquiring better Bernoulli gates by comparing iHAS with iHAS-3; (v) Respecting the difference between instances can further boost the performance by comparing iHAS with iHAS-4.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_5', 'tab_5']<h2>heading</h2>Efficiency Analysis (RQ4)<h2>text</h2>In addition to model performance, efficiency is vital when deploying the recommendation model into online systems, especially inference efficiency. We report the inference time on the whole test set of iHAS and other baselines in Figure 4. We can find that iHAS achieves the least inference time. This is because iHAS feed different test data into its preferred recommender model of smaller size instead of feeding all test data into a single model which may lead to additional inference cost on some data. On the contrary, PEP requires the longest inference time because its embedding table is usually sparse and hardware-unfriendly.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_4']<h2>table_ref</h2>[]<h2>heading</h2>Case Study (RQ5)<h2>text</h2>In this subsection, we first use a case study to investigate the optimal embedding dimensions for each cluster from iHAS. We show the results on Avazu as an example and exclude all anonymous field features in Figure 5.
We can observe that: (i) Each field's optimal dimensions greatly vary from one to another (from 4 to 12), which highlights the necessity of dimension search in recommender systems; (ii) idrelated features, e.g., site_id and app_id, typically possess more dimensions. This aligns with human intuition as the id-related features are the core of recommender systems; (iii) Samples within different clusters tend to select different dimensions for each field, which validates our claim that different clusters present different  patterns and should be trained separately to enhance performance and reduce inference time in Section 3.4. Furthermore, we use four samples to illustrate the effectiveness of the iHAS framework consisting of group-customized recommender models. Figure 6 shows four samples grouped into two clusters (two in pink and two in cyan). Each cluster has its customized recommender model. We can find that the predictions are more correct (lower Logloss) if we feed the sample into its corresponding model. However, if feeding all of them together into one of the recommender models, we will receive some wrong predictions.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>CONCLUSION<h2>text</h2>This study proposes an instance-wise Hierarchical Architecture Search framework, iHAS, as an innovative solution to the challenges associated with identifying optimal embedding dimensions for DLRMs. iHAS employs a three-stage hierarchical training strategy including searching, clustering, and retraining. The searching stage aims to identify the optimal embedding dimensions for each sample across different fields. Subsequent stages of clustering and retraining provide a mechanism for gathering similar samples as clusters and training cluster-customized DLRMs based on the individual optimal dimensions, thereby enhancing recommendation predictions. We conduct extensive experiments on two large-scale datasets to authenticate the efficacy of the proposed framework. The results demonstrate that iHAS could boost the performance of deep recommendations while reducing inference costs. Additionally, iHAS exhibits outstanding transferability to popular DLRMs.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>Deep learning using rectified linear units (relu)<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Abien Fred; Agarap <h2>ref_id</h2>b1<h2>title</h2>Estimating or propagating gradients through stochastic neurons for conditional computation<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>Yoshua Bengio; Nicholas Léonard; Aaron Courville<h2>ref_id</h2>b2<h2>title</h2>Simple and scalable response prediction for display advertising<h2>journal</h2>ACM Transactions on Intelligent Systems and Technology (TIST)<h2>year</h2>2014<h2>authors</h2>Olivier Chapelle; Eren Manavoglu; Romer Rosales<h2>ref_id</h2>b3<h2>title</h2>Wide & deep learning for recommender systems<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>Heng-Tze Cheng; Levent Koc; Jeremiah Harmsen; Tal Shaked; Tushar Chandra; Hrishi Aradhye; Glen Anderson; Greg Corrado; Wei Chai; Mustafa Ispir<h2>ref_id</h2>b4<h2>title</h2>Towards Automatic Discovering of Deep Hybrid Network Architecture for Sequential Recommendation<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Mingyue Cheng; Zhiding Liu; Qi Liu; Shenyang Ge; Enhong Chen<h2>ref_id</h2>b5<h2>title</h2>Deep neural networks for youtube recommendations<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>Paul Covington; Jay Adams; Emre Sargin<h2>ref_id</h2>b6<h2>title</h2>Statistical theory of extreme values and some practical applications: a series of lectures<h2>journal</h2>US Government Printing Office<h2>year</h2>1954<h2>authors</h2>Emil Julius; Gumbel <h2>ref_id</h2>b7<h2>title</h2>DeepFM: a factorization-machine based neural network for CTR prediction<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Huifeng Guo; Ruiming Tang; Yunming Ye; Zhenguo Li; Xiuqiang He<h2>ref_id</h2>b8<h2>title</h2>Social media recommendation based on people and tags<h2>journal</h2><h2>year</h2>2010<h2>authors</h2>Ido Guy; Naama Zwerdling; Inbal Ronen; David Carmel; Erel Uziel<h2>ref_id</h2>b9<h2>title</h2>Neural factorization machines for sparse predictive analytics<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Xiangnan He; Tat-Seng Chua<h2>ref_id</h2>b10<h2>title</h2>Categorical Reparameterization with Gumbel-Softmax<h2>journal</h2><h2>year</h2><h2>authors</h2>Eric Jang; Shixiang Gu; Ben Poole<h2>ref_id</h2>b11<h2>title</h2>Have We Learned to Explain?: How Interpretability Methods Can Learn to Encode Predictions in their Interpretations<h2>journal</h2>PMLR<h2>year</h2>2021<h2>authors</h2>Neil Jethani; Mukund Sudarshan; Yindalon Aphinyanaphongs; Rajesh Ranganath<h2>ref_id</h2>b12<h2>title</h2>Adam: A method for stochastic optimization<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>P Diederik; Jimmy Kingma;  Ba<h2>ref_id</h2>b13<h2>title</h2>xdeepfm: Combining explicit and implicit feature interactions for recommender systems<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Jianxun Lian; Xiaohuan Zhou; Fuzheng Zhang; Zhongxia Chen; Xing Xie; Guangzhong Sun<h2>ref_id</h2>b14<h2>title</h2>AdaFS: Adaptive Feature Selection in Deep Recommender System<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Weilin Lin; Xiangyu Zhao; Yejing Wang; Tong Xu; Xian Wu<h2>ref_id</h2>b15<h2>title</h2>DARTS: Differentiable Architecture Search<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Hanxiao Liu; Karen Simonyan; Yiming Yang<h2>ref_id</h2>b16<h2>title</h2>Learnable Embedding sizes for Recommender Systems<h2>journal</h2><h2>year</h2><h2>authors</h2>Siyi Liu; Chen Gao; Yihong Chen; Depeng Jin; Yong Li<h2>ref_id</h2>b17<h2>title</h2>Learning Sparse Neural Networks through L_0 Regularization<h2>journal</h2><h2>year</h2><h2>authors</h2>Christos Louizos; Max Welling; Diederik P Kingma<h2>ref_id</h2>b18<h2>title</h2>Optimizing Feature Set for Click-Through Rate Prediction<h2>journal</h2><h2>year</h2>2023<h2>authors</h2>Fuyuan Lyu; Xing Tang; Dugang Liu; Liang Chen; Xiuqiang He; Xue Liu<h2>ref_id</h2>b19<h2>title</h2>OptEmbed: Learning Optimal Embedding Table for Clickthrough Rate Prediction<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Fuyuan Lyu; Xing Tang; Hong Zhu; Huifeng Guo; Yingxue Zhang; Ruiming Tang; Xue Liu<h2>ref_id</h2>b20<h2>title</h2>The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables<h2>journal</h2><h2>year</h2><h2>authors</h2>Andriy Chris J Maddison; Yee Whye Mnih;  Teh<h2>ref_id</h2>b21<h2>title</h2>A* sampling<h2>journal</h2>Advances in neural information processing systems<h2>year</h2>2014<h2>authors</h2>Chris J Maddison; Daniel Tarlow; Tom Minka<h2>ref_id</h2>b22<h2>title</h2>Variational inference for monte carlo objectives<h2>journal</h2>PMLR<h2>year</h2>2016<h2>authors</h2>Andriy Mnih; Danilo Rezende<h2>ref_id</h2>b23<h2>title</h2>Efficient neural architecture search via parameters sharing<h2>journal</h2>PMLR<h2>year</h2>2018<h2>authors</h2>Hieu Pham; Melody Guan; Barret Zoph; Quoc Le; Jeff Dean<h2>ref_id</h2>b24<h2>title</h2>Single-shot Embedding Dimension Search in Recommender System<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Liang Qu; Yonghong Ye; Ningzhi Tang; Lixin Zhang; Yuhui Shi; Hongzhi Yin<h2>ref_id</h2>b25<h2>title</h2>Factorization machines<h2>journal</h2>IEEE<h2>year</h2>2010<h2>authors</h2>Steffen Rendle<h2>ref_id</h2>b26<h2>title</h2>Predicting clicks: estimating the click-through rate for new ads<h2>journal</h2><h2>year</h2>2007<h2>authors</h2>Matthew Richardson; Ewa Dominowska; Robert Ragno<h2>ref_id</h2>b27<h2>title</h2>Web-scale k-means clustering<h2>journal</h2><h2>year</h2>2010<h2>authors</h2>David Sculley<h2>ref_id</h2>b28<h2>title</h2>Autoint: Automatic feature interaction learning via selfattentive neural networks<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Weiping Song; Chence Shi; Zhiping Xiao; Zhijian Duan; Yewen Xu; Ming Zhang; Jian Tang<h2>ref_id</h2>b29<h2>title</h2>Detecting beneficial feature interactions for recommender systems<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Yixin Su; Rui Zhang; Sarah Erfani; Zhenghua Xu<h2>ref_id</h2>b30<h2>title</h2>Detecting arbitrary order beneficial feature interactions for recommender systems<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Yixin Su; Yunxiang Zhao; Sarah Erfani; Junhao Gan; Rui Zhang<h2>ref_id</h2>b31<h2>title</h2>Regression shrinkage and selection via the lasso<h2>journal</h2>Journal of the Royal Statistical Society: Series B (Methodological)<h2>year</h2>1996<h2>authors</h2>Robert Tibshirani<h2>ref_id</h2>b32<h2>title</h2>Autofield: Automating feature selection in deep recommender systems<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Yejing Wang; Xiangyu Zhao; Tong Xu; Xian Wu<h2>ref_id</h2>b33<h2>title</h2>Attentional factorization machines: learning the weight of feature interactions via attention networks<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Jun Xiao; Xiangnan Hao Ye; Hanwang He; Fei Zhang; Tat-Seng Wu;  Chua<h2>ref_id</h2>b34<h2>title</h2>Feature selection using stochastic gates<h2>journal</h2>PMLR<h2>year</h2>2020<h2>authors</h2>Yutaro Yamada; Ofir Lindenbaum; Sahand Negahban; Yuval Kluger<h2>ref_id</h2>b35<h2>title</h2>Autoloss: Automated loss function search in recommendations<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Xiangyu Zhao; Haochen Liu; Wenqi Fan; Hui Liu; Jiliang Tang; Chong Wang<h2>ref_id</h2>b36<h2>title</h2>Autodim: Field-aware embedding dimension searchin recommender systems<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Xiangyu Zhao; Haochen Liu; Hui Liu; Jiliang Tang; Weiwei Guo; Jun Shi; Sida Wang; Huiji Gao; Bo Long<h2>ref_id</h2>b37<h2>title</h2>Autoemb: Automated embedding dimensionality search in streaming recommendations<h2>journal</h2>IEEE<h2>year</h2>2021<h2>authors</h2>Xiangyu Zhaok; Haochen Liu; Wenqi Fan; Hui Liu; Jiliang Tang; Chong Wang; Ming Chen; Xudong Zheng; Xiaobing Liu; Xiwang Yang<h2>ref_id</h2>b38<h2>title</h2>DRN: A deep reinforcement learning framework for news recommendation<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Guanjie Zheng; Fuzheng Zhang; Zihan Zheng; Yang Xiang; Nicholas Jing Yuan; Xing Xie; Zhenhui Li<h2>ref_id</h2>b39<h2>title</h2>Open benchmarking for click-through rate prediction<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Jieming Zhu; Jinyang Liu; Shuai Yang; Qi Zhang; Xiuqiang He<h2>ref_id</h2>b40<h2>title</h2>Neuron-level structured pruning using polarization regularizer<h2>journal</h2>Advances in neural information processing systems<h2>year</h2>2020<h2>authors</h2>Tao Zhuang; Zhixuan Zhang; Yuheng Huang; Xiaoyi Zeng; Kai Shuang; Xiang Li<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Figure 1 :1Figure 1: Overview of the three-stage training process in the iHAS framework. The four edge colors of the vectors (red, blue, yellow, and green) correspond to four different samples, which also correspond to the points in the clusters. The brightness level indicates the values of an element. " " represents the element-wise product operation.<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Figure 2 :2Figure 2: The detailed process of Bernoulli gates to generate embedding masks from the embedding representation. " " represents the element-wise summation operation.<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Figure 3 :3Figure 3: Histogram of the Bernoulli probabilities for a sample from Avazu dataset, trained (a) with and (b) without polarization regularizer. Note that y-axes use log scales, and within the same range, to facilitate better visual comparison.<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>Algorithm 11Optimization Strategy for iHAS Require: Training dataset D train , validation dataset D val Ensure: Bernoulli gates parameters 𝒘, K-Means with two clusters, base recommender model {𝑽, 𝜽 } 𝑏 , recommender model 1 {𝑽, 𝜽 } 1 , and recommender model 2 {𝑽, 𝜽 } 2 1: ### Searching ### 2: Pretrain {𝑽, 𝜽 } 𝑏 for 5 epoch using D train 3: while not converge on D val do 4: sample a mini-batch ∈ D train , get 𝒎 by stochastic selection 5: update {𝑽, 𝜽 } 𝑏 according the objective from Eq. 3 6: sample a mini-batch ∈ D val , get 𝒎 by stochastic selection 7: update 𝒘 according the objective from Eq. 3 and 5 8: end while 9: ### Clustering ### 10: while not converge on cluster centroids do 11: sample a mini-batch ∈ D train , get 𝒎 by deterministic selection, assign samples to their closest centroid 12: update the cluster centroids of K-means 13: end while 14: ### Retraining ### 15: Based on the K-Means cluster, split D train → D train 1 , D train 2 , and split D val → D val 1 , D val 2 16: find the optimal dimensions for D train 1 via Bernoulli gates, and initialize {𝑽, 𝜽 } 1 with the corresponding dimensions 17: while not converge on D val 1 do 18: sample a mini-batch from D train 1 19:<h2>figure_data</h2><h2>figure_label</h2>4<h2>figure_type</h2>figure<h2>figure_id</h2>fig_4<h2>figure_caption</h2>Figure 4 :4Figure 4: Inference time (in log scale) of iHAS and other baselines on the Avazu dataset.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_5<h2>figure_caption</h2>n e r _ p o s s i t e _ i d s i t e _ d o m a i n s i t e _ c a t a p p _ i d a p p _ d o m a i n a p p _ c a t d e v _ i d d e v _ i p d e v _ m o d e l d e v _ t y p d e v _ c o n n _ t y<h2>figure_data</h2><h2>figure_label</h2>56<h2>figure_type</h2>figure<h2>figure_id</h2>fig_6<h2>figure_caption</h2>Figure 5 :Figure 6 :56Figure 5: Case study of selected dimensions of each field for each DLRM in iHAS on the Avazu dataset.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_0<h2>figure_caption</h2>•, •) represents the binary cross-entropy (BCE) loss, 𝑓 𝜽 𝑏 (•) represents the base MLP component with parameters 𝜽 𝑏 , 𝑦 𝑖 is the ground truth label for the 𝑖-th sample, and 𝜆 is a balancing factor for the regularizer. Since we use Gumbel-Softmax to produce the embedding masks, 𝒎 𝑖 , the term E 𝑖 E 𝒎 𝑖 ∥𝒎 𝑖 ∥ effectively equals to the sum of Bernoulli probabilities, 𝑖 𝑁 *<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>The statistics of Avazu and Criteo datasets.<h2>figure_data</h2>Dataset #Instances #Fields #FeaturesAvazu 40,400,00022645,394Criteo 45,840,617391,086,810<h2>figure_label</h2>2<h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>Performance comparison between iHAS and baseline models.<h2>figure_data</h2>DatasetMetricMethodsPEPAutoField OptEmbed AdaFS-soft AdaFS-hard OptFS iHASAvazuAUC ↑ Logloss ↓ 0.3874 0.76650.7773 0.38130.7630 0.38940.7777 0.38120.7763 0.38210.7724 0.7815 0.3840 0.3791CriteoAUC ↑ Logloss ↓ 0.4507 0.80060.8029 0.44900.7962 0.45430.8039 0.44840.8031 0.45600.8015 0.8043 0.4504 0.44784.5 Overall Performance (RQ1)<h2>figure_label</h2>2<h2>figure_type</h2>table<h2>figure_id</h2>tab_3<h2>figure_caption</h2>compares the overall performance of our proposed iHAS and other baseline models on the Avazu and Criteo datasets. We summarize our observations below.<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>table<h2>figure_id</h2>tab_4<h2>figure_caption</h2>Transferability of iHAS on the Avazu dataset.<h2>figure_data</h2>ModelMetricTransfer TypeOriginal AdaFS-soft iHASFMAUC ↑ Logloss ↓0.7766 0.38150.7799 0.37970.7826 0.3793W&DAUC ↑ Logloss ↓0.7772 0.38150.7790 0.38020.7797 0.3800DeepFMAUC ↑ Logloss ↓0.7806 0.37950.7817 0.37860.7840 0.3784<h2>figure_label</h2>4<h2>figure_type</h2>table<h2>figure_id</h2>tab_5<h2>figure_caption</h2>Ablation study on the Avazu datasets.<h2>figure_data</h2>MetricMethodsBase iHAS-1 iHAS-2 iHAS-3 iHAS-4 iHASAUC ↑ 0.7765 0.7772 0.7767 0.7801 0.7768 0.7815Logloss ↓ 0.3818 0.3813 0.3816 0.3800 0.3817 0.3791<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>𝑖 as 𝑿 𝑖 = [𝒙 𝑖,1 , • • • , 𝒙 𝑖,𝑛 , • • • , 𝒙 𝑖,𝑁 ],<h2>formula_coordinates</h2>[3.0, 53.44, 679.09, 129.99, 8.44]<h2>formula_id</h2>formula_1<h2>formula_text</h2>𝒆 𝑖,𝑛 = 𝒗 𝑛 𝒙 𝑖,𝑛 ,<h2>formula_coordinates</h2>[3.0, 413.42, 169.88, 49.27, 8.25]<h2>formula_id</h2>formula_2<h2>formula_text</h2>𝒗 𝑛 ∈ R 𝑑 × |𝑛 |<h2>formula_coordinates</h2>[3.0, 341.96, 195.99, 43.42, 10.64]<h2>formula_id</h2>formula_3<h2>formula_text</h2>𝑬 𝑖 = [𝒆 𝑖,1 , • • • , 𝒆 𝑖,𝑛 , • • • , 𝒆 𝑖,𝑁 ].<h2>formula_coordinates</h2>[3.0, 392.44, 241.89, 107.84, 8.44]<h2>formula_id</h2>formula_4<h2>formula_text</h2>𝑽 = [𝒗 1 , • • • , 𝒗 𝑛 , • • • , 𝒗 𝑁 ].<h2>formula_coordinates</h2>[3.0, 318.49, 274.95, 239.71, 20.15]<h2>formula_id</h2>formula_5<h2>formula_text</h2>𝒛 𝑖,𝑗 = one hot (arg max (log 𝑝 𝑖,𝑗 + 𝐺 𝑖,𝑗 , log(1 -𝑝 𝑖,𝑗 ) + 𝐺 ′ 𝑖,𝑗 )),(1)<h2>formula_coordinates</h2>[4.0, 322.82, 622.89, 235.92, 11.69]<h2>formula_id</h2>formula_6<h2>formula_text</h2>R ( {𝑽, 𝜽 } 𝑏 , 𝒎 ) = E 𝑖 E 𝒎 𝑖 L( 𝑓 𝜽 𝑏 ( 𝑽 𝑏 • 𝑿 𝑖 ⊙ 𝒎 𝑖 ), 𝑦 𝑖 ) , (3) with regularizer R (𝒎) = E 𝑖 E 𝒎 𝑖 [ 𝜆 ∥𝒎 𝑖 ∥ 0 ] ,(4)<h2>formula_coordinates</h2>[6.0, 65.03, 187.74, 229.55, 23.26]<h2>formula_id</h2>formula_7<h2>formula_text</h2>R (𝒎) = ∑︁ 𝑖 𝑁 * ∑︁ 𝑗=1 𝜆 𝑝 𝑖,𝑗 -|𝑝 𝑖,𝑗 -p𝑖 |, with p𝑖 = 1 𝑁 * 𝑁 * ∑︁ 𝑗=1 𝑝 𝑖,𝑗 .(5)<h2>formula_coordinates</h2>[6.0, 61.06, 332.44, 233.52, 28.1]<h1>doi</h1>10.1145/3583780.3614925<h1>title</h1>iDriveSense: Dynamic Route Planning Involving Roads Quality Information<h1>authors</h1>Amr S El-Wakeel; Aboelmagd Noureldin; Hossam S Hassanein; Nizar Zorba<h1>pub_date</h1><h1>abstract</h1>Owing to the expeditious growth in the information and communication technologies, smart cities have raised the expectations in terms of efficient functioning and management. One key aspect of residents' daily comfort is assured through affording reliable traffic management and route planning. Comprehensively, the majority of the present trip planning applications and service providers are enabling their trip planning recommendations relying on shortest paths and/or fastest routes. However, such suggestions may discount drivers' preferences with respect to safe and less disturbing trips. Road anomalies such as cracks, potholes, and manholes induce risky driving scenarios and can lead to vehicles damages and costly repairs. Accordingly, in this paper, we propose a crowdsensing based dynamic route planning system. Leveraging both the vehicle motion sensors and the inertial sensors within the smart devices, road surface types and anomalies have been detected and categorized. In addition, the monitored events are geo-referenced utilizing GPS receivers on both vehicles and smart devices. Consequently, road segments assessments are conducted using fuzzy system models based on aspects such as the number of anomalies and their severity levels in each road segment. Afterward, another fuzzy model is adopted to recommend the best trip routes based on the road segments quality in each potential route. Extensive road experiments are held to build and show the potential of the proposed system.<h1>sections</h1><h2>heading</h2>I. INTRODUCTION<h2>text</h2>Smart Cities, by 2024, are predicted to generate $2.3 trillion according to CISCO [1]. Meanwhile, there are various smart applications and services present in multiple sectors spanning environment, health, waste management and transportation [2,3]. Nevertheless, further insights, evaluations, and improvements are necessary for granting adequate performance of smart cities. Mainly, smart transportation and traffic management are highly needed as one can say they broadly influence almost every aspect of the smart cities operation on daily bases [4]. In particular, trip route planning receives great interest particularly in big and crowded cities [4,5]. Principally, trip planning applications and service providers afford route recommendations based on relatively shorter paths, traffic congestion and even with up to date construction works [6].
Consequently, some of the route planning key players as Google have adopted online dynamic routing driven by live traffic network information. For example, Google maps provide an online suggestion for vehicle re-routing when roads are experiencing instantaneous traffic congestion based on many factors such as untraditional mobility behavior or accidents. On the other hand, corwdsensed based trip planning application Waze [7] relies on lively sensed traffic situations which shared with the users' intervention. APOLO [8] system was introduced to overcome the network overload introduced in many of the traffic management systems because of information exchange between vehicles and servers. This system proposed a centralized traffic monitoring system that works on both online and offline bases. In the offline stage, mobility patterns are conducted by historical data processing while in the online stage vehicles are re-routed away from the congested routes. The results showed travel time reduction of 17 % along with a speed increase of 6% compared to present approaches.
In addition, various efforts in the literature provided suggestions to enable shorter and faster routing for land vehicles. In [9], an adaptive routing approach was introduced and dealt with route planning as a probabilistic dynamic problem. In their algorithms, they aimed to reduce the predicted en route trip time while considering broadcasted traffic information, onboard based traffic state measurement, and historical traffic patterns. Moreover, in [10] personal behavior based trip planning was presented to contribute a solution for traffic congestion problem. The authors assumed and discussed that the driving preferences changes from a driver to another could be handled in a way to create drivers' profiles which are used in their route planning leading to less traffic congestion. Furthermore, in [6] an extended version of [10] specified three significant aspects of personal based route planning. These significant considerations are the road safety regarding the presence of snow or black ice, traffic speed and congestion level. The contributions of these factors are assessed based on fuzzy inference engine while the overall optimum routing was enabled by an optimization problem. In [11] a dynamic route planning system was proposed to include future traffic hazards in vehicle routing. This system contained three components which are real-time data streamed from the vehicles plus data collected by automatic traffic loops sensors and the third component used both data sources to predict future traffic conditions through Spatio-Temporal random field process.
In order to assure relaxing trips, in [12] a system was introduced to reduce the routes distances along with providing suggestions for routes with high-quality sceneries. A memetic algorithm was used to provide skyline scenic trip planning while maintaining low travel distances. On the other hand, to ensure drivers and travelers safety, a system was proposed in [13] to enable route planning while discarding routes that encounter high crime rate. Based on crime data provided by Chicago and Philadelphia a risk model was introduced for the cities urban networks.
The highlighted literature showed significant efforts in enriching efficient route planning regarding suggesting shortest paths, fewer traffic routes and considering personal preferences as well. However, road quality information is the crucial aspect that enables drivers safe and comfort trips was not considered in the most of route planning systems [14]. Deteriorated road surface conditions can lead to vehicle damage and dangerous driving scenarios that result in drivers' frustration and stress [14,15]. Consequently, existing land vehicles are considered mobile sensor hubs with various sensing and communications capabilities [16]. Vehicle motion sensors in the land vehicles along with the inertial sensors embedded in the drivers' smart devices enabled adequate detection for various road surface types and anomalies. Thanks to both GPS receivers and inertial sensors the detected anomalies are robustly geo-referenced [17,18].
In this paper, we present iDriveSense a corwdsensed based Road Information Services (RIS) system. In this system, we leverage the sensing capabilities of the land vehicles and drivers' smart devices to generate detailed data sets of the road surface types and anomalies with different severity levels. Also, these datasets are used as an input to a cloud-based Fuzzy Inference System (FIS) utilized for road segment assessments. Moreover, iDriveSense provides independent route planning or through evaluating potential routes suggested by trip planning service providers like Google Maps. Route suggestions and evaluations are enabled to the drivers through another FIS.<h2>publication_ref</h2>['b0', 'b1', 'b2', 'b3', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9', 'b5', 'b9', 'b10', 'b11', 'b12', 'b13', 'b13', 'b14', 'b15', 'b16', 'b17']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>II. SYSTEM STRUCTURE<h2>text</h2>In this section, we present the system configuration used to build iDriveSense. As mentioned earlier, in this system land vehicles are considered as mobile crowdsensing nodes. As shown in fig. 1, detailed and descriptive datasets of road surface conditions are sent to a cloud RIS. Accordingly, road segments assessment and route recommendations are provided through cascaded FIS.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. FIS<h2>text</h2>Basically, fuzzy logic is intended to deal with real-world applications through framework able to deal with ambiguity and inaccuracy [19]. In fuzzy logic, quantified rules or statements are adopted to avoid firm true or false decisions. Accordingly, fuzzy logic sets grant objects values that range from 0 to 1 through graded memberships. Therefore, FIS maps sets of given inputs to outputs with the aid of fuzzy logic. Fig. 1 iDriveSense system architecture Fig. 2 FIS system Structure.
In addition, FIS dynamic performance is modeled by sets of linguistic descriptive rules that are set according to the system designer prior knowledge [19]. For example, the fuzzy rules of a multiple-input-single-output (MISO) fuzzy system are given by
R1: if (a) is X1 and (b) is Y1, then (c) is Z1; R2: if (a) is X2 and (b) is Y2, then (c) is Z2;(1)
……….
Rn: if (a) is Xn and (b) is Yn, then (c) is Zn;
As a, b and c are linguistic variables representing two inputs process state variables and one output variable. While, Xi and Yi are linguistic values of the linguistic a, b in the universe of discourse U and V with i = 1, 2, … , n. The linguistics values Zi of the linguistic variable c in the universe of discourse W in case of Mamdani FIS [20].
Fundamentally, as shown in fig. 2, four components together represent the FIS. The fuzzy rules which can be called "IF-THEN" are built according to the prior knowledge of the required system. Also, the input domain crisp values U are outlined with fuzzy sets defined in the same universe of disclosure by the aid the fuzzification stage. On the other hand, an inverted operation is carried by the defuzzification stage to map the crisp values of the output domain V with the predefined fuzzy sets. Further details on FIS structure and derivations can be found in [21].<h2>publication_ref</h2>['b18', 'b18', 'b19', 'b20']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>B. Road Segment Assesment FIS<h2>text</h2>For road segment assessments, inputs from the road surface types and conditions data sets are used to compute three inputs for the FIS. The first input is the related to the total number of road anomalies in a given road segment 𝑆. For each segment, a normalized percentage of road anomalies 𝑅𝐴 is computed simple through dividing the total number of anomalies over 𝑆 to reflect the density of the anomalies in particular segment. Thus this input is mapped to three membership functions which are defined as low, moderate and high. As shown in fig. 3 we adopted sigmoidal membership function for both low and high functions. A sigmoidal function is a mapping on input vector 𝑎, and can be represented by:
𝑓(𝑎, 𝑚, 𝑛) = 1 1+exp(-𝑚(𝑎-𝑛)) (2)
Where the sigmoidal membership functions innately open to the right or left according to the sign of the parameter 𝑚 and 𝑛 is a control parameter. The product of two sigmoidal functions is used in the moderate function and is given by: 𝑓 𝑘 (𝑎) = Given that 𝑘 = 1,2 and the parameters 𝑚 1 and 𝑚 2 command the left and right curves slopes and these two parameters have to be positive and negative, respectively. While 𝑛 1 and 𝑛 2 control the left and right curves points of inflection.
The second input is representing the effect of the anomalies severity level on the assessment of a road segment. As the road segments with equal lengths and have the same density of anomalies should not receive the same assessment decision if they experience different types of anomalies with different levels of severity. Accordingly, the average percentage of anomalies severity level in each segment is calculated and normalized concerning the segment length and presented by mild and severe sigmoid membership functions. Lastly, the third input is to distinguish road segments of single and double lanes. This input was chosen to represent the significance of the road segment wideness on its quality assessment. The road segments with multi-lanes allow the driver to maneuver before the anomalies easily while this is difficult to occur in single road segments and it can lead to dangerous scenarios within the two ways road segments. The third input is also mapped through two sigmoid wide and narrow membership functions. In this FIS, the fuzzification of the inputs is mapped by 11 Mamdani fuzzy rules. The road assessment FIS is then defuzzified to enable three output levels of road segment quality. They are classified into Good, Moderate and poor segments. <h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_0']<h2>table_ref</h2>[]<h2>heading</h2>C. Route Suggestion FIS<h2>text</h2>Regarding the route suggestions, a cascaded FIS is utilized as shown in fig. 1 to provide route recommendation and evaluation. In this FIS, as shown in fig. 4, there are three inputs adopted to decide the route recommendation. The first input is the average quality of the segments in a potential route. This input is controlled by three membership functions namely poor, moderate and good which reflects one aspect of the route evaluation. We adopted two sigmoid functions for the poor and good membership functions while we used the product of two sigmoidal functions for the moderate one. It is worth mentioning that the primary concern in iDrivesense route planning is in providing high road quality routes. However, high traffic routes and long paths should be avoided as well whenever is possible. Therefore, the second and the third inputs are described by the route travel time and route distance, respectively. The second one is divided by two sigmoid membership functions named slower and faster. On the other hand, the third input is also described by two sigmoid membership functions called longer and shorter. In the route recommendation FIS, the fuzzification of the three inputs is controlled by 12 fuzzy rules. While the defuzzification of this cascaded FIS provides three output levels of route recommendations. They are divided into (not suggested, marginally suggested and suggested).<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_1']<h2>table_ref</h2>[]<h2>heading</h2>III. RESULTS AND DISCUSSION<h2>text</h2>In order to assess the performance of the proposed system, we conducted extensive experiments in Kingston, ON, Canada. These experiments were held adopting multiple vehicles and included various motion sensors and smart devices. These road experiments involved numerous roads in heavy traffic downtown core, urban and neighborhoods residential areas to assure the variety of road segments quality and routing approaches. Accordingly, to show the performance of the iDriveSense system in route recommendation considering the road quality information. We consider a real trip request as shown in fig. 5. In this trip, the driver requires route planning to travel from point A to point B while requesting a stable and safe drive as the highest priority. According to Google maps, as shown in fig. 6, there are two recommended routes. The first one reaches point B in 5 minutes, and it is 1.3 Km regarding route distance. On the other hand, the second recommended route travel time is 7 minutes with a distance of 1.4 Km. Thus according to Google maps suggestions which are mainly provided based on less trip time and shortest route distances, Route 1 is recommended as shown figure.6.
Consequently, as requested by the driver, the safe and high road quality has the highest priority in the trip satisfaction. For this regard, iDriveSense examined the quality of the road segments in the potential routes as listed Table 1 and shown in fig. 7. With the aid of the first FIS system described in Sec. II., the road segments quality of Route 1 and Route 2 were assessed. The first route (suggested by Google Maps) has nine road segments.   In this route, there are eight segments assessed as poor quality ones, and there is only one segment assessed as a moderate one. On the other hand, the second recommended route (Route 2) consists of 10 segments. Utilizing the first FIS, 5 of the road segments within this route are evaluated as good ones, and there were other four assessed as moderate while only one is considered a poor road segment. As per fig. 7, the assessed road segments of Route 1 and Route 2 are highlighted with different colors to indicate different levels of quality.
Afterward, the route suggestion FIS was adopted to recommend the route with high road segments quality. In this cascaded FIS, the output of the first FIS along with the trip time and route distance in each route is used to set the routes recommendation levels. Given the predefined fuzzy rules, the three inputs and the required priority to the route with high road segments quality, iDrivesense contrary to Google Maps has recommended Route 2 as shown in fig. 8  Comprehensively, the presented results show the significance of considering road information quality in dynamic route planning. As the requests for safety and comfort trips have introduced new metrics in route suggestions. Thus iDrivesense showed high capabilities in providing dynamic, safe and comfortable trips. However, roads quality are subject to change due to the effects of traffic and harsh weather. To sustain reliable dynamic route planning, continuous road segments assessments are enabled by iDrivesense.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2', 'fig_3', 'fig_4', 'fig_4']<h2>table_ref</h2>[]<h2>heading</h2>IV. CONCLUSION<h2>text</h2>Future smart cities are required to consider numerous aspects to meet the expectations of their residents. Smooth and safe vehicle routing come on the top of the resident's demands due to their implications for their comfort and productivity on a daily bases. However, the popular route planning systems and service providers are not considering the road quality information while providing their trip planning services. In this paper, we presented "iDriveSense" a crowdsensing based system to enable such challenging demand. Our system benefits from the sensing capabilities of the vehicle motion sensors and the inertial sensors and GPS receivers to monitor road surface conditions. Accordingly, provides a cloud-based dynamic route planning services. The system was successfully able to operate independently or cooperatively with route planning service providers as Google Maps. The system can adequately asses the quality of road segments considering various aspects that affect the drivers' comfort and safety enabling efficient dynamic route planning while maintaining reasonable trip times and route distances. .<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>Digital Cities: Building the New Public Infrastructure (white paper)<h2>journal</h2>CISCO<h2>year</h2>2018-04<h2>authors</h2>J Barbier; K Delaney; N France<h2>ref_id</h2>b1<h2>title</h2>Enabling Communication Technologies for Smart Cities<h2>journal</h2>IEEE Commun. Mag<h2>year</h2>2017-05<h2>authors</h2>I Yaqoob; I Hashem; Y Mehmood; A Gani; S Mokhtar; S Guizani<h2>ref_id</h2>b2<h2>title</h2>Soft Sensing in Smart Cities: Handling 3Vs Using Recommender Systems, Machine Intelligence, and Data Analytics<h2>journal</h2>IEEE Communications Magazine<h2>year</h2>2018-02<h2>authors</h2>H Habibzadeh; A Boggio-Dandry; Z Qin; T Soyata; B Kantarci; H T Mouftah<h2>ref_id</h2>b3<h2>title</h2>A Communications-Oriented Perspective on Traffic Management Systems for Smart Cities: Challenges and Innovative Approaches<h2>journal</h2>IEEE Communications Surveys & Tutorials<h2>year</h2><h2>authors</h2>S Djahel; R Doolan; G M Muntean; J Murphy<h2>ref_id</h2>b4<h2>title</h2>Smart Mobility Trends: Open Data and Other Tools<h2>journal</h2>IEEE Intelligent Transportation Systems Magazine<h2>year</h2>2018<h2>authors</h2>F R Soriano; J J Samper-Zapater; J J Martinez-Dura; R V Cirilo-Gimeno; J Martinez Plume<h2>ref_id</h2>b5<h2>title</h2>Traveler Centric Trip Planning: A Behavioral-Driven System<h2>journal</h2>IEEE Transactions on Intelligent Transportation Systems<h2>year</h2>2016-06<h2>authors</h2>H M Amar; N M Drawil; O A Basir<h2>ref_id</h2>b6<h2>title</h2>WAZE Outsmarting Traffic, Together<h2>journal</h2><h2>year</h2><h2>authors</h2> Waze Mobile<h2>ref_id</h2>b7<h2>title</h2>APOLO: A Mobility Pattern Analysis Approach to Improve Urban Mobility<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>A T Akabane; R L Gomes; R W Pazzi; E R M Madeira; L A Villas<h2>ref_id</h2>b8<h2>title</h2>Adaptive vehicle navigation with en route stochastic traffic information<h2>journal</h2>IEEE Trans. Intell. Transp. Syst<h2>year</h2>2014-10<h2>authors</h2>L Xiao; H K Lo<h2>ref_id</h2>b9<h2>title</h2>A Solution to the Congestion Problem: Profiles Driven Trip Planning<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>H M Amar; O A Basir<h2>ref_id</h2>b10<h2>title</h2>Dynamic Route Planning with Real-Time Traffic Predictions<h2>journal</h2>Information Systems<h2>year</h2>2017<h2>authors</h2>T Liebig; N Piatkowski; C Bockermann; K Morik<h2>ref_id</h2>b11<h2>title</h2>MA-SSR: A Memetic Algorithm for Skyline Scenic Routes Planning Leveraging Heterogeneous User-Generated Digital Footprints<h2>journal</h2>IEEE Transactions on Vehicular Technology<h2>year</h2>2017-07<h2>authors</h2>C Chen<h2>ref_id</h2>b12<h2>title</h2>Urban navigation beyond shortest route: The case of safe paths<h2>journal</h2>Information Systems<h2>year</h2>2016<h2>authors</h2>E Galburn; K Pelechrinis; E Terzi<h2>ref_id</h2>b13<h2>title</h2>Facilitating safe vehicle routing in smart cities<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>A E M Taha<h2>ref_id</h2>b14<h2>title</h2>Road Test Experiments and Statistical Analysis for Real-Time Monitoring of Road Surface Conditions<h2>journal</h2><h2>year</h2>2017-12<h2>authors</h2>A S El-Wakeel; A Osman; A Noureldin; H S Hassanein<h2>ref_id</h2>b15<h2>title</h2>Vehicle as a resource (VaaR)<h2>journal</h2>IEEE Network<h2>year</h2>2015-02<h2>authors</h2>S Abdelhamid; H S Hassanein; G Takahara<h2>ref_id</h2>b16<h2>title</h2>Towards a Practical Crowdsensing System for Road Surface Conditions Monitoring<h2>journal</h2>IEEE Internet Things J<h2>year</h2>2018<h2>authors</h2>A S El-Wakeel; J Li; A Noureldin; H S Hassanein; N Zorba<h2>ref_id</h2>b17<h2>title</h2>Utilization of Wavelet Packet Sensor De-noising for Accurate Positioning in Intelligent Road Services<h2>journal</h2><h2>year</h2>2018-06<h2>authors</h2>A S El-Wakeel; A Noureldin; H S Hassanein; N Zorba<h2>ref_id</h2>b18<h2>title</h2>Adaptive Fuzzy Prediction of Low-Cost Inertial-Based Positioning Errors<h2>journal</h2>IEEE Transactions on Fuzzy Systems<h2>year</h2>2007-06<h2>authors</h2>W Abdel-Hamid; A Noureldin; N El-Sheimy<h2>ref_id</h2>b19<h2>title</h2>Application of fuzzy algorithm for control of simple dynamic plant<h2>journal</h2><h2>year</h2>1974<h2>authors</h2>M Mamdani<h2>ref_id</h2>b20<h2>title</h2>Fuzzy set methods for local modeling and identification<h2>journal</h2>Taylor & Francis<h2>year</h2>1997<h2>authors</h2>R Babuška; H B Verbruggen<h1>figures</h1><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Fig. 331 1+exp(-𝑚 𝑘 (𝑎-𝑛 𝑘 )) Membership functions utilized in road segments assessment: a) percentage of anomalies (low, moderate, and high), b) average level of severity (low and high) and c) lanes (narrow and wide)<h2>figure_data</h2><h2>figure_label</h2>4<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Fig. 44Membership functions utilized in route suggestion: a) average segments quality (poor, moderate, and good), b) route time (slow and fast) and c) route length (long and short)<h2>figure_data</h2><h2>figure_label</h2>5<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Fig. 55Fig.5 Route planning request from point A to point B (Top view).<h2>figure_data</h2><h2>figure_label</h2>6<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>Fig 6 .6Fig 6. Route suggestions provided by Google Maps.<h2>figure_data</h2><h2>figure_label</h2>7<h2>figure_type</h2>figure<h2>figure_id</h2>fig_4<h2>figure_caption</h2>Fig 7 .7Fig 7. Road segments assessment by iDriveSense for the routes suggested by Google Maps.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_5<h2>figure_caption</h2>Fig8.Fig8. Route suggestion provided by iDriveSense.<h2>figure_data</h2><h2>figure_label</h2>I<h2>figure_type</h2>table<h2>figure_id</h2>tab_0<h2>figure_caption</h2><h2>figure_data</h2>ROADROUTE 1ROUTE 2SEGMENTS1PoorModerate2PoorModerate3PoorGood4PoorGood5PoorGood6PoorGood7PoorGood8ModerateModerate9PoorModerate10NAPoor<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>R1: if (a) is X1 and (b) is Y1, then (c) is Z1; R2: if (a) is X2 and (b) is Y2, then (c) is Z2;(1)<h2>formula_coordinates</h2>[2.0, 345.31, 474.95, 212.15, 25.88]<h2>formula_id</h2>formula_1<h2>formula_text</h2>𝑓(𝑎, 𝑚, 𝑛) = 1 1+exp(-𝑚(𝑎-𝑛)) (2)<h2>formula_coordinates</h2>[3.0, 104.3, 223.57, 182.12, 17.88]<h1>doi</h1><h1>title</h1>iFixR: Bug Report driven Program Repair<h1>authors</h1>Anil Koyuncu; Kui Liu; Tegawendé F Bissyandé; Dongsun Kim; Martin Monperrus; Jacques Klein; Yves Le Traon; Mar- Tin Monperrus;  Ca Munawar Hafiz<h1>pub_date</h1>2019-07-12<h1>abstract</h1>Issue tracking systems are commonly used in modern software development for collecting feedback from users and developers. An ultimate automation target of software maintenance is then the systematization of patch generation for user-reported bugs. Although this ambition is aligned with the momentum of automated program repair, the literature has, so far, mostly focused on generate-andvalidate setups where fault localization and patch generation are driven by a well-defined test suite. On the one hand, however, the common (yet strong) assumption on the existence of relevant test cases does not hold in practice for most development settings: many bugs are reported without the available test suite being able to reveal them. On the other hand, for many projects, the number of bug reports generally outstrips the resources available to triage them. Towards increasing the adoption of patch generation tools by practitioners, we investigate a new repair pipeline, iFixR, driven by bug reports: (1) bug reports are fed to an IR-based fault localizer; (2) patches are generated from fix patterns and validated via regression testing; (3) a prioritized list of generated patches is proposed to developers. We evaluate iFixR on the Defects4J dataset, which we enriched (i.e., faults are linked to bug reports) and carefullyreorganized (i.e., the timeline of test-cases is naturally split). iFixR generates genuine/plausible patches for 21/44 Defects4J faults with its IR-based fault localizer. iFixR accurately places a genuine/plausible patch among its top-5 recommendation for 8/13 of these faults (without using future test cases in generation-and-validation).<h1>sections</h1><h2>heading</h2>INTRODUCTION<h2>text</h2>Automated program repair (APR) has gained incredible momentum in the last decade. Since the seminal work by Weimer et al. [83] who relied on genetic programming to evolve program variants until one variant is found to satisfy the functional constraints of a test suite, the community has been interested in test-based techniques to repair programs without specifications. Thus, various approaches [12,13,18,19,25,26,32,33,38,45,48,49,52,53,55,62,65,83,84,92,93] have been proposed in the literature aiming at reducing manual debugging efforts through automatically generating patches. Beyond fixing syntactic errors, i.e., cases where the code violates some programming language specifications [16], the current challenges lie in fixing semantic bugs, i.e., cases where implementation of program behavior deviates from developer's intention [22,61].
Ten years ago, the work of Weimer et al. [83] was explicitly motivated by the fact that, despite significant advances in specification mining (e.g., [40]), formal specifications are rarely available. Thus, test suites represented an affordable approximation to program specifications. Unfortunately, the assumption that test cases are readily available still does not hold in practice [7,27,68]. Therefore, while current test-based APR approaches would be suitable in a test-driven development setting [6], their adoption by practitioners faces a simple reality: developers majoritarily (1) write few tests [27], (2) write tests after the source code [7], and (3) write tests to validate that bugs are indeed fixed and will not reoccur [23].
Although APR bots [79] can come in handy in a continuous integration environment, the reality is that bug reports remain the main source of the stream of bugs that developers struggle to handle daily [5]. Bugs are indeed reported in natural language, where users tentatively describe the execution scenario that was being carried out and the unexpected outcome (e.g., crash stack traces). Such bug reports constitute an essential artifact within a software development cycle and can become an overwhelming concern for maintainers. For example, as early as in 2005, a triager of the Mozilla project was reported in [5, page 363] to have commented that: "Everyday, almost 300 bugs appear that need triaging. This is far too much for only the Mozilla programmers to handle." However, very few studies [9,43] have undertaken to automate patch generation based on bug reports. To the best of our knowledge, Liu et al. [43] proposed the most advanced study in this direction. Unfortunately, their R2Fix approach carries several caveats: as illustrated in Figure 1, it focuses on perfect bug reports [43,  The trailing zero (`\0') will be written to state [4] which is out of bound.
linux/net/mac80211/debugfs_sta.c: -strcpy(state, "off "); + strcpy(state, "off");
(a) Linux Kernel Bug Report (b) Patch to Fix the Bug<h2>publication_ref</h2>['b82', 'b11', 'b12', 'b17', 'b18', 'b24', 'b25', 'b31', 'b32', 'b37', 'b44', 'b47', 'b48', 'b51', 'b52', 'b54', 'b61', 'b64', 'b82', 'b83', 'b91', 'b92', 'b15', 'b21', 'b60', 'b82', 'b39', 'b6', 'b26', 'b67', 'b5', 'b26', 'b1', 'b6', 'b22', 'b78', 'b4', 'b8', 'b42', 'b42']<h2>figure_ref</h2>['fig_1']<h2>table_ref</h2>[]<h2>heading</h2>Remove the space character<h2>text</h2>Figure 1. Converting a bug report to a patch. "-" denotes a line to be deleted; "+" denotes a line to be added; and " " is one space character.
developers forget to check if the array is long enough to hold the content before the assignment; or was the bug caused by more complex reasons? The developers then need to check out the buggy version, modify the buggy code to fix the bug, and generate the patch that can be applied to the shared source code repository. The result of this challenging and time-consuming process by developers for bug 11975 is the patch in Figure 1(b). The patch deletes the line that writes 5 bytes to buffer state (denoted by -strcpy(state, "off ");), and adds a new line to write only 4 bytes to state (+ strcpy(state, "off");), which fixes the overflow bug.
Developers often need to fix more bugs than their time and resources allow [6]. Although developers spend almost half of their time fixing bugs [21], bugs take years to be fixed on average [9], [19].
Therefore, support to make it easier and faster for developers to fix bugs is in high demand. The capability to automatically generate patches (e.g., Figure 1(b)) from bug reports (e.g., Figure 1(a)) could: (1) save programmers' time and effort in diagnosing bugs and generating patches, allowing developers to fix more bugs or focus on other development tasks; and (2) shorten the bug-fixing time, thus improve software reliability and security.<h2>publication_ref</h2>['b5', 'b20', 'b8', 'b18', 'b0']<h2>figure_ref</h2>['fig_1', 'fig_1', 'fig_1', 'fig_1']<h2>table_ref</h2>[]<h2>heading</h2>A. Ideal Goal Versus Realistic Goal<h2>text</h2>Ideally, we want to automatically generate patches for all bug reports. Realistically, it is impossible. We found that only 16.7-33.5% of bug reports in the Linux kernel, Mozilla, and Apache bug databases are fixed. This is because, many bug reports are invalid, unreproducible, incomplete, etc. Even among bugs that can be fixed, some are too complex to be fixed automatically because they require redesign of the algorithm, addition of new features, etc.  (1) which explicitly include localization information, (2) where the symptom (e.g., buffer overrun) is explicitly indicated by the reporter, and ( 3) which are about one of the following three simple bug types: Buffer overflow, Null Pointer dereference or memory leak. R2Fix runs a straightforward classification to identify the bug category and uses a match and transform engine (e.g., Coccinelle [66]) to generate patches. As the authors admitted, their target space represents <1% of bug reports in their dataset. Furthermore, it should be noted that, given the limited scope of the changes implemented in its fix patterns, R2Fix does not need to run tests for verifying that the generated patches do not break any functionality.
This paper. We propose to investigate the feasibility of a program repair system driven by bug reports, thus we replace classical spectrum-based fault localization with Information Retrieval (IR)-based fault localization. Eventually, we propose iFixR, a new program repair workflow which considers a practical repair setup by imitating the fundamental steps of manual debugging. iFixR works under the following constraint:
When a bug report is submitted to the issue tracking system, a relevant test case reproducing the bug may not be readily available.
Therefore, iFixR is leveraged in this study to assess to what extent an APR pipeline is feasible under the practical constraint of limited test suites. iFixR uses bug reports written in natural language as the main input. Eventually, we make the following contributions:
• We present the architecture of a program repair system adapted to the constraints of maintainers dealing with user-reported bugs.
In particular, iFixR replaces classical spectrum-based fault localization with Information Retrieval (IR)-based fault localization. • We propose a strategy to prioritize patches for recommendation to developers. Indeed, given that we assume only the presence of regression test cases to validate patch candidates, many of these patches may fail on the future test cases that are relevant to the reported bugs. We order patches to present correct patches first. • We assess and discuss the performance of iFixR on the Defects4J benchmark to compare with the state-of-the-art APR tools. To that end, we provide a refined Defects4J benchmark for APR targeting bug reports. Bugs are carefully linked with the corresponding bug reports, and for each bug we are able to dissociate future test cases that were introduced after the relevant fixes. Overall, experimental results show that there are promising research directions to further investigate towards the integration of automatic patch generation in actual software development cycles. In particular, our findings suggest that IR-based fault localization errors lead less to overfitting patches than spectrum-based fault localization errors. Furthermore, iFixR offers provides comparable results to most state-of-the-art APR tools, although it is run under the constraint that post-fix knowledge (i.e., future test cases) is not available. Finally, iFixR's prioritization strategy tends to place more correct/plausible patches on top of the recommendation list.<h2>publication_ref</h2>['b1', 'b65']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>MOTIVATION<h2>text</h2>We motivate our work by revisiting two essential steps in APR:
(1) During fault localization, relevant program entities are identified as suspicious locations that must be changed. Commonly, stateof-the-art APR tools leverage spectrum-based fault localization (SBFL) [12,25,32,33,39,50,52,53,62,65,83,92,93], which uses execution coverage information of passing and failing test cases to predict buggy statements. We dissect the construction of the Defects4J dataset to highlight the practical challenges of fault localization for user-reported bugs. (2) Once a patch candidate is generated, the patch validation step ensures that it is actually relevant for repairing the program. Currently, widespread test-based APR techniques use test suites as the repair oracle. This however is challenged by the incompleteness of test suites, and may further not be inline with developer requirements/expectations in the repair process.<h2>publication_ref</h2>['b11', 'b24', 'b31', 'b32', 'b38', 'b49', 'b51', 'b52', 'b61', 'b64', 'b82', 'b91', 'b92']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Fault Localization Challenges<h2>text</h2>Defects4J is a manual curated dataset widely used in the APR literature [12,18,73,84,90,91]. Since Defects4J was not initially built for APR, the real order of precedence between the bug report, the patch and the test case is being overlooked by the dataset users. Indeed, Defects4J offers a user-friendly way of checking out buggy versions of programs with all relevant test cases for readily benchmarking test-based systems. We propose to carefully examine the actual bug fix commits associated with Defects4J bugs and study how the test suite is evolved. Table 1 provides detailed information. Overall, for 96%(=381/395) bugs, the relevant test cases are actually future data with respect to the bug discovery process. This finding suggests that, in practice, even the fault localization may be challenged in the case of user-reported bugs, given the lack of relevant test cases. The statistics listed in Table 2 indeed shows that if future test cases are dropped, no test case is failing when executing buggy program versions for 365 (i.e., 92%) bugs. In the APR literature, fault localization is generally performed using the GZoltar [11] testing framework and a SBFL formula [88] (e.g., Ochiai [2]). To support our discussions, we attempt to perform fault localization without the future test cases to evaluate the performance gap. Experimental results (see details forward in Table 6 of Section 5) expectedly reveal that the majority of the Defects4J bugs (i.e., 375/395) cannot be localized by SBFL at the time the bug is reported by users.
It is necessary to investigate alternate fault localization approaches that build on bug report information since relevant test cases are often unavailable when users report bugs. Step 0
Step 1
Step 2
Step 3
Step 4
Step 5
Iterative folding<h2>publication_ref</h2>['b11', 'b17', 'b72', 'b83', 'b89', 'b90', 'b10', 'b87', 'b1']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_1', 'tab_2', 'tab_9']<h2>heading</h2>Patch Candidates<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Patch Generation Patch Validation<h2>text</h2>Regression Testing  <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Fix Pattern Matching<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Patch Validation in Practice<h2>text</h2>The repair community has started to reflect on the acceptability [26,63] and correctness [76,91] of the patches generated by APR tools. Notably, various studies [10,34,69,76,94] have raised concerns about overfitting patches: a typical APR tool that uses a test suite as the correctness criterion can produce a patched program that actually overfits the test-suite (i.e., the patch makes the program pass all test cases but does not actually repair it). Recently, new research directions [89,97] are being explored in the automation of test case generation for APR to overcome the overfitting issue. Nevertheless, so far they have had minimal positive impact due to the oracle problem [98] in automatic test generation. At the same time, the software industry takes a more systematic approach for patch validation by developers. For instance, in the open-source community, the Linux development project has integrated a patch generation engine to automate collateral evolutions that are validated by maintainers [29,66]. In proprietary settings, Facebook has recently reported on their Getafix [75] tool, which automatically suggests fixes to their developers. Similarly, Ubisoft developed Clever [64] to detect risky commits at commit-time using patterns of programming mistakes from the code history.
Patch recommendation for validation by developers is acceptable in the software development communities. It may thus be worthwhile to focus on tractable techniques for recommending patches in the road to fully automated program repair.<h2>publication_ref</h2>['b25', 'b62', 'b75', 'b90', 'b9', 'b33', 'b68', 'b75', 'b93', 'b88', 'b96', 'b97', 'b28', 'b65', 'b74', 'b63']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>THE IFIXR APPROACH<h2>text</h2>Figure 2 overviews the workflow of the proposed iFixR approach. Given a defective program, we consider the following issues:
(1) Where is the bug? We take as input the bug report in natural language submitted by the program user. We rely on the information in this report to localize the bug positions. (2) How should we change the code? We apply fix patterns that are recurrently found in real-world bug fixes. Fix patterns are selected following the structure of the abstract syntax tree representing the code entity of the identified suspicious code. (3) Which patches are valid? We make no assumptions on the availability of positive test cases [83] that encode functionality requirements at the time the bug is discovered. Nevertheless, we leverage existing test cases to ensure, at least, that the patch does not regress the program. (4) Which patches do we recommend first? In the absence of a complete test suite, we cannot guarantee that all patches that pass regression tests will fix the bug. We rely on heuristics to re-prioritize the validated patches in order to increase the probability of placing a correct patch on top of the list.<h2>publication_ref</h2>['b82']<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>[]<h2>heading</h2>Input: Bug reports<h2>text</h2>Issue tracking systems (e.g., Jira) are widely used by software development communities in the open source and commercial realms. Although they can be used by developers to keep track of the bugs that they encounter and the features to be implemented, issue tracking systems allow for user participation as a communication channel for collecting feedback on software executions in production. Table 3 illustrates a typical bug report when a user of the LANG library code has encountered an issue while using the NumberUtils API. A description of erroneous behavior is provided. Occasionally, the user may include in the bug description some information on how to reproduce the bug. Oftentimes, users simply insert code snippets or dump the execution stack traces.
In this study, among our dataset of 162 bug reports, we note that only 27 (i.e., ∼17%) are reported by users who are also developersfoot_0 contributing to the projects. 15 (i.e., ∼9%) bugs are reported and again fixed by the same project contributors. These percentages suggest that, for the majority of cases, the bug reports are indeed genuinely submitted by users of the software who require project developers' attention.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Table 3: Example bug report (Defects4J Lang-7).<h2>text</h2>Issue No.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>LANG-822 Summary<h2>text</h2>NumberUtils#createNumber -bad behaviour for leading "-" Description NumberUtils#createNumber checks for a leading "-" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal.
Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.
Given the buggy program version and a bug report, iFixR must unfold the workflow for precisely identifying (at the statement level) the buggy code locations. We remind the reader that, in this step, future test cases cannot be relied upon. We consider that if such test cases could have triggered the bug, a continuous integration system would have helped developers deal with the bug before the software is shipped towards users.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Fault Localization w/o Test Cases<h2>text</h2>To identify buggy code locations within the source code of a program, we resort to Information Retrieval (IR)-based fault localization (IRFL) [67,80]. The general objective is to leverage potential similarity between the terms used in a bug report and the source code to identify relevant buggy code locations. The literature includes a large body of work on IRFL [57,72,81,85,87,96,99] where researchers systematically extract tokens from a given bug report to formulate a query to be matched in a search space of documents formed by the collections of source code files and indexed through tokens extracted from source code. IRFL approaches then rank the documents based on a probability of relevance (often measured as a similarity score). Highly ranked files are predicted to be the ones that are likely to contain the buggy code.
Despite recurring interest in the literature, with numerous approaches continuously claiming new performance improvements over the state-of-the-art, we are not aware of any adoption in program repair research or practice. We postulate that one of the reasons is that IRFL techniques have so far focused on file-level localization, which is too coarse-grained (in comparison to spectrumbased fault localization output). Recently, Locus [85] and BLIA [96] are state-of-the-art techniques which narrow down localization, respectively to the code change or the method level. Nevertheless, to the best of our knowledge, no IRFL technique has been proposed in the literature for statement-level localization.
In this work, we develop an algorithm to rank suspicious statements based on the output (i.e., files) of a state-of-the-art IRFL tool, thus yielding a fine-grained IR-based fault localizer which will then be readily integrated into a concrete patch generation pipeline.<h2>publication_ref</h2>['b66', 'b79', 'b56', 'b71', 'b80', 'b84', 'b86', 'b95', 'b98', 'b84', 'b95']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Ranking Suspicious Files.<h2>text</h2>We leverage an existing IRFL tool. Given that expensive extractions of tokens from a large corpus of bug reports is often necessary to tune IRFL tools [41], we selected a tool for which the authors provide datasets and pre-processed data. We use the D&C [30] as the specific implementation of file-level IRFL available online [1] , which is a machine learning-based IRFL tool using a similarity matrix of 70-dimension feature vectors (7 features from bug reports and 10 features from source code files): D&C uses multiple classifier models that are trained each for specific groups of bug reports. Given a bug report, the different predictions of the different classifiers are merged to yield a single list of suspicious code files. Our execution of D&C (Line 2 in Algorithm 1) is tractable given that we only need to preprocess those bug reports that we must localize. Trained classifiers are already available. We ensure that no data leakage is induced (i.e., the classifiers are not trained with bug reports that we want to localize in this work).<h2>publication_ref</h2>['b40', 'b29', 'b0']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Ranking Suspicious<h2>text</h2>Statements. Patch generation requires fine-grained information on code entities that must be changed. For iFixR, we propose to produce a standard output, as for spectrumbased fault localization, to facilitate integration and reuse of stateof-the-art patch generation techniques. To start, we build on the conclusions on a recent large-scale study [47] of bug fixes to limit the search space of suspicious locations to the statements that are more error-prone. After investigating in detail the abstract syntax tree (AST)-based code differences of over 16 000 real-world patches from Java projects, Liu et al. [47] reported that the following specific AST statement nodes were significantly more prone to be faulty than others: IfStatements, ExpressionStatements, FieldDeclarations, ReturnStatements and VariableDeclara-tionStatements. Lines 7-17 in Algorithm 1 detail the process to produce a ranked list of suspicious statements.
Algorithm 1 describes the process of our fault localization approach used in iFixR. Top k files are selected among the returned list of suspicious files of the IRFL along with their computed suspiciousness scores. Then each file is parsed to retain only the relevant error-prone statements from which textual tokens are extracted. The summary and descriptions of the bug report are also analyzed  return Sscor e (lexically) to collect all its tokens. Due to the specific nature of stack traces and other code elements which may appear in the bug report, we use regular expressions to detect stack traces and code elements to improve the tokenization process, which is based on punctuations, camel case splitting (e.g., findNumber splits into find, number) as well as snake case splitting (e.g., find_number splits into find, number). Stop word removalfoot_1 is then applied before performing stemming (using the PorterStemmer [24]) on all tokens to create homogeneity with the term's root (i.e., by conflating variants of the same term). Each bag of tokens (for the bug report, and for each statement) is then eventually used to build a feature vector. We use cosine similarity among the vectors to rank the file statements that are relevant to the bug report.
Given that we considered k files, the statements of each having their own similarity score with respect to the bug report, we weight these scores with the suspiciousness score of the associated file. Eventually, we sort the statements using the weighted scores and produce a ranked list of code locations (i.e., statements in files) to be recommended as candidate fault locations.<h2>publication_ref</h2>['b46', 'b46', 'b23']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Fix Pattern-based Patch Generation<h2>text</h2>A common, and reliable, strategy in automatic program repair is to generate concrete patches based on fix patterns [26] (also referred to as fix templates [51] or program transformation schemas [18]). Several APR systems [14,18,26,31,46,48,49,51,60,73] in the literature implement this strategy by using diverse sets of fix patterns obtained either via manual generation or automatic mining of bug fix datasets. In this work, we consider the pioneer PAR system by Kim et al. [26]. Concretely, we build on kPAR [48], an open-source Java implementation of PAR in which we included a diverse set of fix patterns collected from the literature. Table 4 provides an enumeration of fix patterns used in this work. For more implementation details, we refer the reader to our replication package. All tools and data are released as open source to the community to foster further research into these directions. As illustrated in Figure 3, a fix pattern encodes the recipe of change actions that should be applied to mutate a code element.  For a given reported bug, once our fault localizer yields its list of suspicious statements, iFixR iteratively attempts to select fix patterns for each statement. The selection of fix patterns is conducted in a naïve way based on the context information of each suspicious statement (i.e., all nodes in its abstract syntax tree, AST). Specifically, iFixR parses the code and traverses each node of the suspicious statement AST from its first child node to its last leaf node in a breadth-first strategy (i.e, left-to-right and top-to-bottom). If a node matches the context a fix pattern (i.e., same AST node types), the fix pattern will be applied to generate patch candidates by mutating the matched code entity following the recipe in the fix pattern. Whether the node matches a fix pattern or not, iFixR keeps traversing its children nodes and searches fix patterns for them to generate patch candidates successively. This process is iteratively performed until leaf nodes are encountered.
Consider the example of bug Math-75 illustrated in Figure 4. iFixR parses the buggy statement (i.e., statement at line 302 in the file Frequency.java) into an AST as illustrated by Figure 5. First, iFixR matches a fix pattern that can mutate the expression in the return statement with other expression(s) returning data of type double. It further selects fix patterns for the direct child node (i.e., method invocation: getCumPct((Comparable<?> v))) of the return statement. This method invocation can be matched against fix patterns with two contexts: method name and parameter(s). With the breadth-first strategy, iFixR assigns a fix pattern, calling another method with the same parameters (cf. PAR [26, page 804]), to mutate the method name, and then selects fix patterns to mutate the parameter. Furthermore, iFixR will match fix patterns for the type and variable of the cast expression respectively and successively. <h2>publication_ref</h2>['b25', 'b50', 'b17', 'b13', 'b17', 'b25', 'b30', 'b45', 'b47', 'b48', 'b50', 'b59', 'b72', 'b25', 'b47']<h2>figure_ref</h2>['fig_5']<h2>table_ref</h2>['tab_6']<h2>heading</h2>Patch Validation with Regression Testing<h2>text</h2>For every reported bug, fault localization followed by pattern matching and code mutation will yield a set of patch candidates. In a typical test-based APR system, these patch candidates must let the program pass all test cases (including some positive test cases [83], which encode the actual functional requirements relevant to the bug). Thus, the patch candidates set is actively pruned to remove all patches that do not meet these requirements. In our work, in accordance with our investigation findings that such test cases may
ReturnStatement "raw_code" MethodInvocation "raw_code" MethodName "getCumPct" CastExpression "raw_code"
Type "Comparable<?>" VariableName "v" ① ② ③ ④ ⑤
*"raw_code" denotes the corresponding source code at the related node position.
Figure 5: AST of bug Math-75 source code statement. not be available at the time the bug is reported (cf. Section 2), we assume that iFixR cannot reason about future test cases to select patch candidates.
Instead, we rely only on past test cases, which were available in the code base, when the bug is reported. Such test cases are leveraged to perform regression testing [95], which will ensure that, at least, the selected patches do not obstruct the behavior of the existing, unchanged part of the software, which is already explicitly encoded by developers in their current test suite.<h2>publication_ref</h2>['b82', 'b94']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Output: Patch Recommendation List<h2>text</h2>Eventually, iFixR produces a ranked recommendation list of patch suggestions for developers. Until now, the order of patches is influenced mainly by two steps in the workflow:
(1) localization: our statement-level IRFL yields a ranked list of statements to modify in priority. (2) pattern matching: the AST node of the buggy code entity is broken down into its children and iteratively navigated in a breadth-first manner to successively produce candidate patches. Eventually, the produced list of patches has an order, which carries the biases of fault localization [48], and is noised by the pre-set breadth-first strategy for matching fix patterns. We thus design an ordering process with a function 3 , f r cmd : 2 P → P k , as follows:
f r cmd (patches) = (pri type • pri susp • pri chanдe )(patches) (1)
where pri * are three heuristics-based prioritization functions used in iFixR. f r cmd takes a set of patches validated via regression testing (cf. Section 3.4) and produces an ordered sequence of patches (f r cmd (patches) = seq r cmd ∈ P k ). We propose the following heuristics to re-prioritize the patch candidates:
(1) [Minimal changes]: we favor patches that minimize the differences between the patched program and the buggy program. To that end, patches are ordered following their AST edit script sizes. Formally, we define pri chanдe : 2
P → P n where n = |patches |, pri chanдe (patches) = [p i , p i+1 , p i+2 , • • • ] and holds ∀p ∈ patches, C chanдe (p i ) ≤ C chanдe (p i+1 ).
Here, C chanдe (p) is a function that counts the number of deleted and inserted AST nodes by the change actions of p.
(2) [Fault localization suspiciousness]: when two patch candidates have equal edit script sizes, the tie is broken by using the suspiciousness scores (of the associated statements) yielded during IR-based fault localization. Thus, when C chanдe (p i ) == C chanдe (p i+1 ), pri susp re-orders the two patch candidates. We define pri susp :
P n → P n such that pri susp (seq chanдe ) = [• • • , p i , p i+1 , • • • ] holds S susp (p i ) ≥ S susp (p i+1 )
, where seq chanдe is the result of pri chanдe and S susp returns a suspicious score of the statement that a given patch p i changes. (3) [Affected code elements]: after a manual analysis of fix patterns and the performance of associated APR in the literature, we empirically found that some change actions are irrelevant to bug fixing. Thus, for the corresponding pre-defined patterns, iFixR systematically under-prioritizes their generated patches against any other patches, although among themselves the ranking obtained so far (through pri chanдe and pri susp ) is preserved for those under-prioritized patches. These are patches generated by (i) mutating a literal expression, (ii) mutating a variable into a method invocation or a final static variable, or (iii) inserting a method invocation without parameter. This prioritization, is defined by pri type : P n → P k , which returns a sequence of top k ordered patches (k ≤ n = |patches |). To define this prioritization function, we assign natural numbers j 1 , j 2 , j 3 , j 4 ∈ N to each patch generation types (i.e., j 1 ←(i), j 2 ←(ii), and j 3 ←(iii), respectively) and (j 4 ←) everything else, which strictly hold j 4 > j 1 , j 4 > j 2 , j 4 > j 3 . This prioritization function takes the result of pri susp and returns another sequence
[p i , p i+1 , p i+2 , • • • ] that holds ∀p i , D type (p i ) ≥ D type (p i+1
). D type is defined as D type : 2 P → {j 1 , j 2 , j 3 , j 4 } and determines how a patch p i has been generated as defined above. From the ordered sequence, the function returns the leftmost (i.e., top) k patches as a result.<h2>publication_ref</h2>['b47']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>EXPERIMENTAL SETUP<h2>text</h2>We now provide details on the experiments that we carry out to assess the iFixR patch generation pipeline for user-reported bugs. Notably, we discuss the dataset and benchmark, some implementation details before enumerating the research questions.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Dataset & Benchmark<h2>text</h2>To evaluate iFixR we propose to rely on the Defects4J [21] dataset which is widely used as a benchmark in the Java APR literature. Nevertheless, given that Defects4J does not provide direct links to the bug reports that are associated with the benchmark bugs, we must undertake a fairly accurate bug linking task [78]. Furthermore, to realistically evaluate iFixR, we must reorganize the dataset test suites to accurately simulate the context at the time the bug report is submitted by users.<h2>publication_ref</h2>['b20', 'b77']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Bug linking.<h2>text</h2>To identify the bug report describing a given bug in the Defects4J dataset we focus on recovering the links between the bug fix commits and bug reports from the issue tracking system. Unfortunately, projects Joda-Time, JFreeChart and Closure have migrated their source code repositories and issue tracking systems into GitHub without a proper reassignment of bug report identifiers. Therefore, for these projects, bug IDs referred to in the commit logs are ambiguous (for some bugs this may match with the GitHub issue tracking numbering, while in others, it refers to the original issue tracker). To avoid introducing noise in our validation data, we simply drop these projects. For the remaining projects (Lang and Math), we leverage the bug linking strategies implemented in the Jira issue tracking software. We use a similar approach to Fischer et al. [15] and Thomas et al. [78] to link to commits to corresponding bug reports. Concretely, we crawled the bug reports related to each project and assessed the links with a two-step search strategy: (i)
we check commit logs to identify bug report IDs and associate the corresponding changes as bug fix changes; then (ii) we check for bug reports that are indeed considered as such (i.e., tagged as "BUG") and are further marked as resolved (i.e., with tags "RESOLVED" or "FIXED"), and completed (i.e., with status "CLOSED"). Eventually, our evaluation dataset includes 156 faults (i.e., De-fects4J bugs). Actually, for the considered projects, Defects4J enumerates 171 bugs associated with 162 bug reports: 15 bugs are indeed left out because either (1) the corresponding bug reports are not in the desired status in the bug tracking system, which may lead to noisy data, or (2) there is ambiguity in the buggy program version (e.g., some fixed files appear to be missing in the repository at the time of bug reporting).<h2>publication_ref</h2>['b14', 'b77']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Test suite reorganization.<h2>text</h2>We ensure that the benchmark separates past test cases (i.e., regression test cases) from future test cases (i.e., test cases that encode functional requirements specified after the bug is reported). This timeline split is necessary to simulate the snapshot of the repository at the time the bug is reported. As highlighted in Section 2, for over 90% cases of bugs in the De-fects4J benchmark, the test cases relevant to the defective behavior was actually provided along the bug fixing patches. We have thus manually split the commits to identify test cases that should be considered as future test cases for each bug report.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Implementation Choices<h2>text</h2>During implementation, we have made the following parameter choices in the iFixR workflow:
• IR fault localization considers the top 50 (i.e., k = 50 in Algorithm 1) suspicious files for each bug report, in order to search for buggy code locations. • For patch recommendation experiments, we limit the search space to the top 20 suspected buggy statements yielded by the fine-grained IR-based fault localization. • For comparison experiments, we implement spectrum-based fault localization using the GZoltar testing framework with the Ochiai ranking strategy. Unless otherwise indicated, GZoltar version 0.1.1 is used (as it is widely adopted in the literature, by Astor [59], ACS [92], ssFix [90] and CapGen [84] among others).<h2>publication_ref</h2>['b58', 'b91', 'b89', 'b83']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Research Questions<h2>text</h2>The assessment objective is to assess the feasibility of automating the generation of patches for user-reported bugs, while investigating the foreseen bottlenecks as well as the research directions that the community must embrace to realize this long-standing endeavor. To that end, we focus on the following research questions associated with the different steps in the iFixR workflow.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>• RQ1 [Fault localization] :<h2>text</h2>To what extent does IR-based fault localization provide reliable results for an APR scenario? In particular, we investigate the performance differences when comparing our fine-grained IRFL implementation against the classical spectrumbased localization. <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>ASSESSMENT RESULTS<h2>text</h2>In this section, we present the results of the investigations for the previously-enumerated research questions.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>RQ1: [Fault Localization]<h2>text</h2>Fault localization being the first step in program repair, we evaluate the performance of the IR-based fault localization developed within iFixR. As recently thoroughly studied by Liu et al. [48], an APR tool should not be expected to fix a bug that current fault localization systems fail to localize. Nevertheless, with iFixR, we must demonstrate that our fine-grained IRFL offers comparable performance with SBFL tools used in the APR literature.
Table 5 provides performance measurements on the localization of bugs. SBFL is performed based on two different versions of the GZoltar testing framework, but always based on the Ochiai ranking metric. Finally, because fault localization tools output a ranked list of suspicious statements, results are provided in terms of whether the correct location is placed under the top-k suspected statements. In this work, following the practice in the literature [48,56], we consider that a bug is localized if any buggy statement is localized. Overall, the results show that our IRFL implementation is strictly comparable to the common implementation of spectrum-based fault localization when applied on the Defects4J bug dataset. Note that the comparison is conducted for 171 bugs of Math and Lang, given that these are the projects for which the bug linking can be reliably performed for applying the IRFL. Although performance results are similar, we remind the reader that SBFL is applied by considering future test cases. To highlight a practical interest of IRFL, we compute for each bug localizable in the top-10, the elapsed time between the bug report date and the date the relevant test case is submitted for this bug. Based on the distribution shown in Figure 6, on mean average, IRFL could reduce this time by 26 days. Finally, to stress the importance of future test cases for spectrumbased fault localization, we consider all Defects4J bugs and compute localization performance with and without future test cases.
Results listed in Table 6 confirms that in most bug cases, the localization is impossible: Only 10 bugs (out of 395) can be localized among the top-10 suspicious statements of SBFL at the time the bug is reported. In comparison, our IRFL locates 72 bugs under the same conditions of having no relevant test cases to trigger the bugs. Fine-grained IR-based fault localization in iFixR is as accurate as Spectrum-based fault localization in localizing Defects4J bugs. Additionally, it does not have the constraint of requiring test cases that may not be available when the bug is reported.<h2>publication_ref</h2>['b47', 'b47', 'b55']<h2>figure_ref</h2>['fig_6']<h2>table_ref</h2>['tab_8', 'tab_9']<h2>heading</h2>RQ2: [Overfitting]<h2>text</h2>Patch generation attempts to mutate suspected buggy code with suitable fix patterns. Aside from having adequate patterns or not (which is out of the scope of our study), a common challenge of APR lies in the effective selection of buggy statements. In typical test-based APR, test cases drive the selection of these statements. The incompleteness of test suites is however currently suspected to often lead to overfitting of generated patches [94].
We perform patch generation experiments to investigate the impact of localization bias. We compare our IRFL implementation against commonly-used SBFL implementations in the literature of test-based APR. We recall that the patch validation step in these experiments makes no assumptions about future test cases (i.e., all test cases are leveraged as in classical APR pipeline). For each bug, depending on the rank of the buggy statements in the suspicious statements yielded the fault localization system (either IRFL or SBFL), the patch generation can produce more or less relevant patches. Table 7 details the repair performance in relation to the position of buggy statements in the output of fault localization. Results are provided in terms of numbers of plausible and correct [69] patches that can be found by considering top-k statements returned by the fault localizer. x is the number of bugs for which a correct patch is generated; y is the number of bugs for which a plausible patch is generated.
Overall, we find that IRFL and SBFL localization information lead to similar repair performance in terms of the number of fixed bugs plausibly/correctly. Actually IRFL-supported APR outperforms SBFL-supported APR on the Lang project bugs and vice-versa for Math project bugs: overall, 6 bugs that are fixed using IRFL output, cannot be fixed using SBFL output (although assuming the availability of the bug triggering test cases to run the SBFL tool).
We investigate the cases of plausible patches in both localization scenarios to characterize the reasons why these patches appear to only be overfitting the test suites. Table 8 details the overfitting reasons for the two scenarios. Table 8: Dissection of reasons why patches are plausible * but not correct.<h2>publication_ref</h2>['b93', 'b68']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_10']<h2>heading</h2>Localization Error<h2>text</h2>Pattern Prioritization Lack of Fix ingredients w/ IRFL 6 1 16 w/ SBFL 15 1 10 * A plausible patch passes all test cases, but may not be semantically equivalent to developer patch (i.e., correct). We consider a plausible patch to be overfitted to the test suite (1) Among the 23(= 44 -21) plausible patches that are generated based on IRFL identified code locations and that are not found to be correct, 6 are found to be caused by fault localization errors: these bugs are plausibly fixed by mutating irrelevantlysuspicious statements that are placed before the actual buggy statements in the fault localization output list. This phenomenon has been recently investigated in the literature as the problem of fault localization bias [48]. Nevertheless, we note that patches generated based on SBFL identified code locations suffer more of fault localization bias: 15 of the 26 (= 50-24) plausible patches are concerned by this issue. (2) Pattern prioritization failures may also lead to plausible patches: while a correct patch could have been generated using a specific pattern at a lower node in the AST, another pattern (leading to an only plausible patch) was first found to be matching the statement during the iterative search of matching nodes (cf. Section 3.3). ( 3) Finally, we note that both configurations yield plausible patches due to the lack of suitable patterns or due to a failed search for the adequate donor code (i.e., fix ingredient [44]).
Experiments with the Defects4J dataset suggest that code locations provided by IR-based fault localization lead less to overfitted patches than the code locations suggested by Spectrum-based fault localization: cf. "Localization error" column in Table 8.<h2>publication_ref</h2>['b47', 'b43']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>RQ3: [Patch Ordering]<h2>text</h2>While the previous experiment focused on patch generation, our final experiment assesses the complete pipeline of iFixR as it was imagined for meeting the constraints that developers can face in practice: future test cases, i.e., those which encode the functionality requirements that are not met by the buggy programs, may not be available at the time the bug is reported. We thus discard the future test cases of the Defects4J dataset and generate patches that must be recommended to developers. The evaluation protocol thus consists in assessing to what extent correct/plausible patches are placed in the top of the recommendation list.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>5.<h2>text</h2>3.1 Overall performance. Table 9 details the performance of the patch recommendation by iFixR: we present the number of bugs for which a correct/plausible patch is generated and presented among the top-k of the list of recommended patches. In the absence of future test cases to drive the patch validation process, we use heuristics (cf. Section 4.2) to re-prioritize the patch candidates towards ensuring that patches which are recommended first will eventually be correct (or at least plausible when relevant test cases are implemented). We present results both for the case where we do not re-prioritize and the case where we re-prioritize.
Recall that, given that the re-organized benchmark separately includes the future test cases, we can leverage them to systematize the assessment of patch plausibility. The correctness (also referred to as correctness [69]) of patches, however, is still decided manually by comparing against the actual bug fix provided by developers and available in the benchmark. Overall, we note that iFixR performance is promising as it manages, for 13 bugs, to present a plausible patch among its top-5 recommended patches per bug. Among those plausible patches, 8 are eventually found to be correct.  [19] which uses sophisticated techniques to improve the fault localization accuracy and search for fix ingredients. It should be noted that in this case, our prioritization strategy is not applied to the generated patches. iFixR opt represents the reference performance for our experiment which assesses the prioritization. Table 10 provides the comparison matrix. Information on stateof-the-art results are excerpted from their respective publications.
iFixR offers a reasonable performance in patch recommendation when we consider the number of Defects4J bugs that are successfully patched among the top-5 (in a scenario where we assume not having relevant test cases to validate the patch candidates). Performance results are even comparable to many state-of-the-art test-based APR tools in the literature.<h2>publication_ref</h2>['b68', 'b18']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_11', 'tab_12']<h2>heading</h2>5.3.3<h2>text</h2>Properties of iFixR's patches. In Table 11, we characterize the correct and plausible patches recommended by iFixR top5 . Overall, update and insert changes have been successful; most patches affect a single statement, and impact precisely an expression entity within a statement.   <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_13']<h2>heading</h2>5.3.4<h2>text</h2>Diversity of iFixR's fixed bugs. Finally, in Table 12 we dissect the nature of the bugs for which iFixR t op5 is able to recommend a correct or a plausible patch. Priority information about the bug report is collected from the issue tracking systems, while the root cause is inferred by analyzing the bug reports and fixes. Overall, we note that 9 out of the 13 bugs have been marked as Major issues. 12 different bug types (i.e., root causes) are addressed. In contrast, R2Fix [43] only focused on 3 simple bug types.<h2>publication_ref</h2>['b42']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_14']<h2>heading</h2>DISCUSSION<h2>text</h2>This study presents the conclusions of our investigation into the feasibility of generating patches automatically from bug reports. We set strong constraints on the absence of test cases, which are used in test-based APR to approximate what the program is actually supposed to do and when the repair is completed [83]. Our experiments on the widely-used Defects4J bugs eventually show that patch generation without bug-triggering test cases is promising.
Manually looking at the details of failures and success in generating patches with iFixR, several insights can be drawn: Test cases can be buggy: During manual analysis of results, we noted that iFixR actually fails to generate correct patches for three bugs (namely, Math-5, Math-59 and Math-65) because even the test cases were buggy. Figure 7 illustrates the example of bug Math-5 where its patch also updated the relevant test case. This example supports our endeavor, given that users would find and report bugs for which the appropriate test cases were never properly written.
// Patched Source Code: ---a/src/main/java/org/apache/commons/math3/complex/Complex.java +++ b/src/main/java/org/apache/commons/math3/complex/Complex.java @@ -304,3 +304,3 @@ public class Complex if (real == 0.0 && imaginary == 0.  Bug reports deserve more interest: With iFixR, we have shown that bug reports could be handled automatically for a variety of bugs. This is an opportunity for issue trackers to add a recommendation layer to the bug triaging process by integrating patch generation techniques. There are, however, several directions to further investigation, among which: (1) help users write proper bug reports; and (2) re-investigate IRFL techniques at a finer-grained level that is suitable for APR. Prioritization techniques must be investigated: In the absence of complete test suites for validating every single patch candidate, a recommendation system must ensure that patches presented first to the developers are the most likely to be plausible and even correct. There are thus two directions of research that are promising: (1) ensure that fix patterns are properly prioritized to generate good patches and be able to early-stop for not exploding the search space; and (2) ensure that candidate patches are effectively re-prioritized. These investigations must start with a thorough dissection of plausible patches for a deep understanding of plausibility factors. More sophisticated approaches to triaging and selecting fix ingredients are necessary: In its current form, iFixR implements a naïve approach to patch generation, ensuring that the performance is tractable. However, the literature already includes novel APR techniques that implement strategies for selecting donor code and filters patterns. Integrating such techniques into iFixR may lead to performance improvement. More comprehensive benchmarks are needed: Due to bug linking challenges, our experiments were only performed on half of the Defects4J benchmark. To drive strong research in patch generation for user-reported bugs, the community must build larger and reliable benchmarks, potentially even linking several artifacts of continuous integration (i.e, build logs, past execution traces, etc.). In the future, we plan to investigate the dataset of Bugs.jar [71]. Automatic test generation techniques could be used as a supplement: Our study tries to cope radically with the incompleteness of test suites. In the future, however, we could investigate the use of automatic test generation techniques to supplement the regression test cases during patch validation.
Threats to external validity: The bug reports used in this study may be of low quality (i.e., wrong links for corresponding bugs). We reduced this threat by focusing only on bugs from the Lang and Math projects, which kept a single issue tracking system. We also manually verified the links between the bug reports and the Defects4J bugs. Table 13 characterizes the bug reports of our dataset following the criteria enumerated by Zimmermann et al. [100] in their study of "what makes a good bug report". Notably, as illustrated by the distribution of comments in Figure 8, we note that the bug reports have been actively discussed before being resolved. This suggests that they are not trivial cases (cf. [17] on measuring bug report significance). Code-related terms such as package/class names found in the summary and description, in addition to stack traces and code blocks, as separate features referred to as hints. Another threat to external validity relates to the diversity of the fix patterns used in this study. iFixR currently may not implement a reasonable number of relevant fix patterns. We minimize this threat by surveying the literature and considering patterns from several pattern-based APR. Threats to internal validity: Our implementation of fine-grained IRFL carries some threats: during the search of buggy statements, we considered top-50 suspicious buggy files from the file-level IRFL tool, to limit the search space. Different threshold values may lead to different results. We also considered only 5 statement types as more bug-prone. This second threat is minimized by the empirical evidence provided by Liu et al. [47].
Additionally, another internal threat is in our patch generation steps: iFixR only searches for donor code from the local code files, which contain the buggy statement. The adequate fix ingredient may however be located elsewhere. Threats to construct validity: In this study, we assumed that patch construction and test case creation are two separated tasks for developers. This may not be the case in practice. The threat is however mitigated given that, in any case, we have shown that the test cases are often unavailable when the bug is reported.<h2>publication_ref</h2>['b82', 'b0', 'b70', 'b99', 'b16', 'b46']<h2>figure_ref</h2>['fig_8', 'fig_9']<h2>table_ref</h2>['tab_16']<h2>heading</h2>RELATED WORK<h2>text</h2>Fault Localization. As stated in a recent study [48], fault localization is a critical task affecting the effectiveness of automated program repair. Several techniques have been proposed [67,80,88] and they use different information such as spectrum [4], text [85], slice [58], and statistics [42]. The first two types of techniques are widely studies in the community. SBFL techniques [3,20] are widely adopted in APR pipelines since they identify bug positions at a fine-grained level (i.e., statements). However, they have limitations on localizing buggy locations since it highly relies on the test suite [48]. Information retrieval based fault localization (IRFL) [41] leverages textual information in a bug report. It is mainly used to help developers narrow down suspected buggy files in the absence of relevant test cases. For the purpose of our study, we have proposed an algorithm for further localizing the faulty code entities at the statement level. Patch Generation. Patch generation is another key process of APR pipeline, which is, in other words, a task searching for another shape of a program (i.e., a patch) in the space of all possible programs [37,54]. To improve repair performance, many APR systems have been explored to address the search space problem by using different information and approaches: stochastic mutation [38,83], synthesis [53,92,93], pattern [14,18,19,26,32,36,49,51,52,73], contract [12,82], symbolic execution [65], learning [8,16,55,70,77,86], and donor code searching [25,62]. In this paper, patch generation is implemented with fix patterns presented in the literature since it may make the generated patches more robust [74]. Patch Validation. The ultimate goal of APR systems is to automatically generate a correct patch that can actually resolve the program defects rather than satisfying minimal functional constraints. At the beginning, patch correctness is evaluated by passing all test cases [26,36,83]. However, these patches could be overfitting [34,69] and even worse than the bug [76]. Since then, APR systems are evaluated with the precision of generating correct patches [19,49,84,92]. Recently, researchers explore automated frameworks that can identify patch correctness for APR systems automatically [35,91]. In this paper, our approach validates generated patches with regression test suites since fail-inducing test cases are readily available for most of bugs as described in Section 2.<h2>publication_ref</h2>['b47', 'b66', 'b79', 'b87', 'b3', 'b84', 'b57', 'b41', 'b2', 'b19', 'b47', 'b40', 'b36', 'b53', 'b37', 'b82', 'b52', 'b91', 'b92', 'b13', 'b17', 'b18', 'b25', 'b31', 'b35', 'b48', 'b50', 'b51', 'b72', 'b11', 'b81', 'b64', 'b7', 'b15', 'b54', 'b69', 'b76', 'b85', 'b24', 'b61', 'b73', 'b25', 'b35', 'b82', 'b33', 'b68', 'b75', 'b18', 'b48', 'b83', 'b91', 'b34', 'b90']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>CONCLUSION<h2>text</h2>In this study, we have investigated the feasibility of automating patch generation from bug reports. To that end, we implemented iFixR, an APR pipeline variant adapted to the constraints of test cases unavailability when users report bugs. The proposed system revisits the fundamental steps, notably fault localization, patch generation and patch validation, which are all tightly-dependent to the positive test cases [83] in a test-based APR system.
Without making any assumptions on the availability of test cases, we demonstrate, after re-organizing the Defects4J benchmark, that iFixR can generate and recommend priority correct (and more plausible) patches for a diverse set of user-reported bugs. The repair performance of iFixR is even found to be comparable to that of the majority of test-based APR systems on the Defects4J dataset.
We open source iFixR's code and release all data of this study to facilitate replication and encourage further research in this direction which is promising for practical adoption in the software development community: https://github.com/SerVal-DTF/iFixR <h2>publication_ref</h2>['b82']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>D&C<h2>journal</h2><h2>year</h2>2019<h2>authors</h2><h2>ref_id</h2>b1<h2>title</h2>On the accuracy of spectrum-based fault localization<h2>journal</h2>IEEE<h2>year</h2>2007<h2>authors</h2>Rui Abreu; J C Arjan; Peter Van Gemund;  Zoeteweij<h2>ref_id</h2>b2<h2>title</h2>A practical evaluation of spectrum-based fault localization<h2>journal</h2>JSS<h2>year</h2>2009<h2>authors</h2>Rui Abreu; Peter Zoeteweij; Rob Golsteijn; Arjan Jc Van Gemund<h2>ref_id</h2>b3<h2>title</h2>Spectrum-based multiple fault localization<h2>journal</h2><h2>year</h2>2009<h2>authors</h2>Rui Abreu; Peter Zoeteweij; Arjan Jc Van Gemund<h2>ref_id</h2>b4<h2>title</h2>Who should fix this bug?<h2>journal</h2><h2>year</h2>2006<h2>authors</h2>John Anvik; Lyndon Hiew; Gail C Murphy<h2>ref_id</h2>b5<h2>title</h2>Test-driven development: by example<h2>journal</h2>Addison-Wesley Professional<h2>year</h2>2003<h2>authors</h2>Kent Beck<h2>ref_id</h2>b6<h2>title</h2>When, how, and why developers (do not) test in their IDEs<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Moritz Beller; Georgios Gousios; Annibale Panichella; Andy Zaidman<h2>ref_id</h2>b7<h2>title</h2>Neuro-symbolic program corrector for introductory programming assignments<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Sahil Bhatia; Pushmeet Kohli; Rishabh Singh<h2>ref_id</h2>b8<h2>title</h2>Harvesting Fix Hints in the History of Bugs<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>F Tegawendé;  Bissyandé<h2>ref_id</h2>b9<h2>title</h2>Where is the bug and how is it fixed? an experiment with practitioners<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Marcel Böhme; Sudipta Ezekiel O Soremekun; Emamurho Chattopadhyay; Andreas Ugherughe;  Zeller<h2>ref_id</h2>b10<h2>title</h2>Gzoltar: an eclipse plug-in for testing and debugging<h2>journal</h2><h2>year</h2>2012<h2>authors</h2>José Campos; André Riboira; Alexandre Perez; Rui Abreu<h2>ref_id</h2>b11<h2>title</h2>Contract-based program repair without the contracts<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Liushan Chen; Yu Pei; Carlo A Furia<h2>ref_id</h2>b12<h2>title</h2>Program transformations to fix C integers<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>Zack Coker; Munawar Hafiz<h2>ref_id</h2>b13<h2>title</h2>Dynamic patch generation for null pointer exceptions using metaprogramming<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Thomas Durieux; Benoit Cornu; Lionel Seinturier; Martin Monperrus<h2>ref_id</h2>b14<h2>title</h2>Populating a release history database from version control and bug tracking systems<h2>journal</h2><h2>year</h2>2003<h2>authors</h2>Michael Fischer; Martin Pinzger; Harald Gall<h2>ref_id</h2>b15<h2>title</h2>DeepFix: Fixing Common C Language Errors by Deep Learning<h2>journal</h2>AAAI Press<h2>year</h2>2017<h2>authors</h2>Rahul Gupta; Soham Pal; Aditya Kanade; Shirish Shevade<h2>ref_id</h2>b16<h2>title</h2>Modeling bug report quality<h2>journal</h2><h2>year</h2>2007<h2>authors</h2>Pieter Hooimeijer; Westley Weimer<h2>ref_id</h2>b17<h2>title</h2>Towards practical program repair with on-demand candidate generation<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Jinru Hua; Mengshi Zhang; Kaiyuan Wang; Sarfraz Khurshid<h2>ref_id</h2>b18<h2>title</h2>Shaping Program Repair Space with Existing Patches and Similar Code<h2>journal</h2>ACM<h2>year</h2>2018<h2>authors</h2>Jiajun Jiang; Yingfei Xiong; Hongyu Zhang; Qing Gao; Xiangqun Chen<h2>ref_id</h2>b19<h2>title</h2>Empirical evaluation of the tarantula automatic fault-localization technique<h2>journal</h2><h2>year</h2>2005<h2>authors</h2>A James; Mary Jones; Harrold Jean<h2>ref_id</h2>b20<h2>title</h2>Defects4J: A database of existing faults to enable controlled testing studies for Java programs<h2>journal</h2>ACM<h2>year</h2>2014<h2>authors</h2>René Just; Darioush Jalali; Michael D Ernst<h2>ref_id</h2>b21<h2>title</h2>Comparing developer-provided to user-provided tests for fault localization and automated program repair<h2>journal</h2>ACM<h2>year</h2>2018<h2>authors</h2>René Just; Chris Parnin; Ian Drosos; Michael D Ernst<h2>ref_id</h2>b22<h2>title</h2>Guest editors' introduction: Software testing practices in industry<h2>journal</h2>IEEE Software<h2>year</h2>2006<h2>authors</h2>Natalia Juristo; Juzgado ; Ana María Moreno; Wolfgang Strigel<h2>ref_id</h2>b23<h2>title</h2>Information retrieval with porter stemmer: a new version for English<h2>journal</h2>Springer<h2>year</h2>2013<h2>authors</h2>Wahiba Ben; Abdessalem Karaa; Nidhal Gribâa<h2>ref_id</h2>b24<h2>title</h2>Repairing programs with semantic code search (t)<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Yalin Ke; Kathryn T Stolee; Claire Le Goues; Yuriy Brun<h2>ref_id</h2>b25<h2>title</h2>Automatic patch generation learned from human-written patches<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>Dongsun Kim; Jaechang Nam; Jaewoo Song; Sunghun Kim<h2>ref_id</h2>b26<h2>title</h2>An empirical study of adoption of software testing in open source projects<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>Pavneet Singh Kochhar; F Tegawendé; David Bissyandé; Lingxiao Lo;  Jiang<h2>ref_id</h2>b27<h2>title</h2>Elements of the Theory of Functions and Functional Analysis (dover books on mathematics<h2>journal</h2>Dover Publications<h2>year</h2>1999<h2>authors</h2>A N Kolmogorov; S V Fomin<h2>ref_id</h2>b28<h2>title</h2>Impact of tool support in patch construction<h2>journal</h2>ACM<h2>year</h2>2017<h2>authors</h2>Anil Koyuncu; F Tegawendé; Dongsun Bissyandé; Jacques Kim; Martin Klein; Yves Monperrus;  Le Traon<h2>ref_id</h2>b29<h2>title</h2>D&C: A Divide-and-Conquer Approach to IR-based Bug Localization<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Anil Koyuncu; F Tegawendé; Dongsun Bissyandé; Kui Kim; Jacques Liu; Martin Klein; Yves Monperrus;  Le Traon<h2>ref_id</h2>b30<h2>title</h2>FixMiner: Mining Relevant Fix Patterns for Automated Program Repair<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Anil Koyuncu; Kui Liu; Tegawendé F Bissyandé; Dongsun Kim; Jacques Klein; Martin Monperrus; Yves Le Traon<h2>ref_id</h2>b31<h2>title</h2>S3: syntax-and semantic-guided repair synthesis via programming by examples<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Xuan-Bach D Le; Duc-Hiep Chu; David Lo; Claire Le Goues; Willem Visser<h2>ref_id</h2>b32<h2>title</h2>Enhancing automated program repair with deductive verification<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>Xuan-Bach D Le; Quang Loc Le; David Lo; Claire Le Goues<h2>ref_id</h2>b33<h2>title</h2>Overfitting in semantics-based automated program repair<h2>journal</h2>EMSE Journal<h2>year</h2>2018<h2>authors</h2>Xuan Bach; D Le; Ferdian Thung; David Lo; Claire Le Goues<h2>ref_id</h2>b34<h2>title</h2>On Reliability of Patch Correctness Assessment<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>D Xuan-Bach; Lingfeng Le; David Bao; Xin Lo; Shanping Xia;  Li<h2>ref_id</h2>b35<h2>title</h2>History Driven Program Repair<h2>journal</h2>IEEE<h2>year</h2>2016<h2>authors</h2>D Xuan-Bach; David Le; Claire Le Lo;  Goues<h2>ref_id</h2>b36<h2>title</h2>A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each<h2>journal</h2><h2>year</h2>2012<h2>authors</h2>Claire Le Goues; Michael Dewey-Vogt; Stephanie Forrest; Westley Weimer<h2>ref_id</h2>b37<h2>title</h2>GenProg: A generic method for automatic software repair<h2>journal</h2>TSE<h2>year</h2>2012<h2>authors</h2>Claire Le Goues; Thanhvu Nguyen; Stephanie Forrest; Westley Weimer<h2>ref_id</h2>b38<h2>title</h2>GenProg: A Generic Method for Automatic Software Repair<h2>journal</h2>TSE<h2>year</h2>2012<h2>authors</h2>Claire Le Goues; Thanhvu Nguyen; Stephanie Forrest; Westley Weimer<h2>ref_id</h2>b39<h2>title</h2>Specification mining with few false positives<h2>journal</h2>Springer<h2>year</h2>2009<h2>authors</h2>Claire Le; Goues ; Westley Weimer<h2>ref_id</h2>b40<h2>title</h2>Bench4bl: reproducibility study on the performance of ir-based bug localization<h2>journal</h2>ACM<h2>year</h2>2018<h2>authors</h2>Jaekwon Lee; Dongsun Kim; F Tegawendé; Woosung Bissyandé; Yves Jung;  Le Traon<h2>ref_id</h2>b41<h2>title</h2>Scalable statistical bug isolation<h2>journal</h2><h2>year</h2>2005<h2>authors</h2>Ben Liblit; Mayur Naik; Alice X Zheng; Alex Aiken; Michael I Jordan<h2>ref_id</h2>b42<h2>title</h2>R2Fix: Automatically generating bug fixes from bug reports<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>Chen Liu; Jinqiu Yang; Lin Tan; Munawar Hafiz<h2>ref_id</h2>b43<h2>title</h2>LSRepair: Live Search of Fix Ingredients for Automated Program Repair<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Kui Liu; Koyuncu Anil; Kisub Kim; Dongsun Kim; Tegawendé F Bissyandé<h2>ref_id</h2>b44<h2>title</h2>Learning to Spot and Refactor Inconsistent Method Names<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Kui Liu; Dongsun Kim; Tegawendé F Bissyandé; Taeyoung Kim; Kisub Kim; Anil Koyuncu; Suntae Kim; Yves Le Traon<h2>ref_id</h2>b45<h2>title</h2>Mining fix patterns for findbugs violations<h2>journal</h2>TSE<h2>year</h2>2018<h2>authors</h2>Kui Liu; Dongsun Kim; F Tegawendé; Shin Bissyandé; Yves Yoo;  Le Traon<h2>ref_id</h2>b46<h2>title</h2>A closer look at real-world patches<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Kui Liu; Dongsun Kim; Anil Koyuncu; Li Li; Tegawendé F Bissyandé; Yves Le Traon<h2>ref_id</h2>b47<h2>title</h2>You Cannot Fix What You Cannot Find! An Investigation of Fault Localization Bias in Benchmarking Automated Program Repair Systems<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Kui Liu; Anil Koyuncu; F Tegawendé; Dongsun Bissyandé; Jacques Kim; Yves Klein;  Le Traon<h2>ref_id</h2>b48<h2>title</h2>AVATAR: Fixing Semantic Bugs with Fix Patterns of Static Analysis Violations<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Kui Liu; Anil Koyuncu; Dongsun Kim; Tegawendé F Bissyandé<h2>ref_id</h2>b49<h2>title</h2>TBar : Revisiting Template-based Automated Program Repair<h2>journal</h2>ACM<h2>year</h2>2019<h2>authors</h2>Kui Liu; Anil Koyuncu; Dongsun Kim; Tegawendé F Bissyandé<h2>ref_id</h2>b50<h2>title</h2>Mining stackoverflow for program repair<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Xuliang Liu; Hao Zhong<h2>ref_id</h2>b51<h2>title</h2>Automatic inference of code transforms for patch generation<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Fan Long; Peter Amidon; Martin Rinard<h2>ref_id</h2>b52<h2>title</h2>Staged program repair with condition synthesis<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Fan Long; Martin Rinard<h2>ref_id</h2>b53<h2>title</h2>An analysis of the search spaces for generate and validate patch generation systems<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>Fan Long; Martin Rinard<h2>ref_id</h2>b54<h2>title</h2>Automatic patch generation by learning correct code<h2>journal</h2>ACM<h2>year</h2>2016<h2>authors</h2>Fan Long; Martin Rinard<h2>ref_id</h2>b55<h2>title</h2>Are Faults Localizable?<h2>journal</h2><h2>year</h2>2012<h2>authors</h2>Lucia Lucia; Ferdian Thung; David Lo; Lingxiao Jiang<h2>ref_id</h2>b56<h2>title</h2>Bug localization using latent Dirichlet allocation<h2>journal</h2>IST<h2>year</h2>2010<h2>authors</h2>Nicholas A Stacy K Lukins; Letha H Kraft;  Etzkorn<h2>ref_id</h2>b57<h2>title</h2>Slice-based statistical fault localization<h2>journal</h2>JSS<h2>year</h2>2014<h2>authors</h2>Xiaoguang Mao; Yan Lei; Ziying Dai; Yuhua Qi; Chengsong Wang<h2>ref_id</h2>b58<h2>title</h2>Astor: A program repair library for java<h2>journal</h2>ACM<h2>year</h2>2016<h2>authors</h2>Matias Martinez; Martin Monperrus<h2>ref_id</h2>b59<h2>title</h2>Ultra-Large Repair Search Space with Automatically Mined Templates: The Cardumen Mode of Astor<h2>journal</h2>Springer<h2>year</h2>2018<h2>authors</h2>Matias Martinez; Martin Monperrus<h2>ref_id</h2>b60<h2>title</h2>Semantic Program Repair Using a Reference Implementation<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Sergey Mechtaev; Manh-Dung Nguyen; Yannic Noller; Lars Grunske; Abhik Roychoudhury<h2>ref_id</h2>b61<h2>title</h2>Directfix: Looking for simple program repairs<h2>journal</h2>IEEE Press<h2>year</h2>2015<h2>authors</h2>Sergey Mechtaev; Jooyong Yi; Abhik Roychoudhury<h2>ref_id</h2>b62<h2>title</h2>A critical review of automatic patch generation learned from human-written patches: essay on the problem statement and the evaluation of automatic software repair<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>Martin Monperrus<h2>ref_id</h2>b63<h2>title</h2>CLEVER: combining code metrics with clone detection for just-in-time fault prevention and resolution in large industrial projects<h2>journal</h2>ACM<h2>year</h2>2018<h2>authors</h2>Mathieu Nayrolles; Abdelwahab Hamou-Lhadj<h2>ref_id</h2>b64<h2>title</h2>SemFix: program repair via semantic analysis<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>Hoang Duong; Thien Nguyen; Dawei Qi; Abhik Roychoudhury; Satish Chandra<h2>ref_id</h2>b65<h2>title</h2>Documenting and automating collateral evolutions in Linux device drivers<h2>journal</h2>ACM<h2>year</h2>2008<h2>authors</h2>Yoann Padioleau; Julia Lawall; René Rydhof Hansen; Gilles Muller<h2>ref_id</h2>b66<h2>title</h2>Are automated debugging techniques actually helping programmers?<h2>journal</h2>ACM<h2>year</h2>2011<h2>authors</h2>Chris Parnin; Alessandro Orso<h2>ref_id</h2>b67<h2>title</h2>How Effectively Is Defective Code Actually Tested?: An Analysis of JUnit Tests in Seven Open Source Systems<h2>journal</h2>ACM<h2>year</h2>2018<h2>authors</h2>Jean Petrić; Tracy Hall; David Bowes<h2>ref_id</h2>b68<h2>title</h2>An analysis of patch plausibility and correctness for generate-and-validate patch generation systems<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Zichao Qi; Fan Long; Sara Achour; Martin Rinard<h2>ref_id</h2>b69<h2>title</h2>Learning syntactic program transformations from examples<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Reudismam Rolim; Gustavo Soares; Loris D 'antoni; Oleksandr Polozov; Sumit Gulwani; Rohit Gheyi; Ryo Suzuki; Björn Hartmann<h2>ref_id</h2>b70<h2>title</h2>Bugs.jar: a large-scale, diverse dataset of real-world java bugs<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Ripon Saha; Yingjun Lyu; Wing Lam; Hiroaki Yoshida; Mukul Prasad<h2>ref_id</h2>b71<h2>title</h2>Improving bug localization using structured information retrieval<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>K Ripon; Matthew Saha; Sarfraz Lease; Dewayne E Khurshid;  Perry<h2>ref_id</h2>b72<h2>title</h2>ELIXIR: Effective object-oriented program repair<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>K Ripon; Yingjun Saha; Hiroaki Lyu; Mukul R Yoshida;  Prasad<h2>ref_id</h2>b73<h2>title</h2>Software mutational robustness<h2>journal</h2>Genetic Programming and Evolvable Machines<h2>year</h2>2014<h2>authors</h2>Eric Schulte; Zachary P Fry; Ethan Fast; Westley Weimer; Stephanie Forrest<h2>ref_id</h2>b74<h2>title</h2>Getafix: Learning to fix bugs automatically<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Andrew Scott; Johannes Bader; Satish Chandra<h2>ref_id</h2>b75<h2>title</h2>Is the cure worse than the disease? Overfitting in automated program repair<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>K Edward; Earl T Smith; Claire Le Barr; Yuriy Goues;  Brun<h2>ref_id</h2>b76<h2>title</h2>Using a probabilistic model to predict bug fixes<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Mauricio Soto; Claire Le; Goues <h2>ref_id</h2>b77<h2>title</h2>The impact of classifier configuration and classifier combination on bug localization<h2>journal</h2>TSE<h2>year</h2>2013<h2>authors</h2>Meiyappan Stephen W Thomas; Dorothea Nagappan; Ahmed E Blostein;  Hassan<h2>ref_id</h2>b78<h2>title</h2>How to design a program repair bot?: insights from the repairnator project<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Simon Urli; Zhongxing Yu; Lionel Seinturier; Martin Monperrus<h2>ref_id</h2>b79<h2>title</h2>Evaluating the usefulness of IR-based fault localization techniques<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Qianqian Wang; Chris Parnin; Alessandro Orso<h2>ref_id</h2>b80<h2>title</h2>Version History, Similar Report, and Structure: Putting Them Together for Improved Bug Localization<h2>journal</h2>ACM<h2>year</h2>2014<h2>authors</h2>Shaowei Wang; David Lo<h2>ref_id</h2>b81<h2>title</h2>Automated fixing of programs with contracts<h2>journal</h2>ACM<h2>year</h2>2010<h2>authors</h2>Yi Wei; Yu Pei; Carlo A Furia; Lucas S Silva; Stefan Buchholz; Bertrand Meyer; Andreas Zeller<h2>ref_id</h2>b82<h2>title</h2>Automatically finding patches using genetic programming<h2>journal</h2><h2>year</h2>2009<h2>authors</h2>Westley Weimer; Thanhvu Nguyen; Claire Le Goues; Stephanie Forrest<h2>ref_id</h2>b83<h2>title</h2>Context-Aware Patch Generation for Better Automated Program Repair<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Ming Wen; Junjie Chen; Rongxin Wu; Dan Hao; Shing-Chi Cheung<h2>ref_id</h2>b84<h2>title</h2>Locus: Locating bugs from software changes<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>Ming Wen; Rongxin Wu; Shing-Chi Cheung<h2>ref_id</h2>b85<h2>title</h2>Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities<h2>journal</h2>IEEE<h2>year</h2>2019<h2>authors</h2>Martin White; Michele Tufano; Matias Martinez; Martin Monperrus; Denys Poshyvanyk<h2>ref_id</h2>b86<h2>title</h2>Boosting Bug-Report-Oriented Fault Localization with Segmentation and Stack-Trace Analysis<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>Chu-Pan Wong; Yingfei Xiong; Hongyu Zhang; Dan Hao; Lu Zhang; Hong Mei<h2>ref_id</h2>b87<h2>title</h2>A survey on software fault localization<h2>journal</h2>TSE<h2>year</h2>2016<h2>authors</h2>Eric Wong; Ruizhi Gao; Yihao Li; Rui Abreu; Franz Wotawa<h2>ref_id</h2>b88<h2>title</h2>Identifying test-suite-overfitted patches through test case generation<h2>journal</h2>ACM<h2>year</h2>2017<h2>authors</h2>Qi Xin; Steven P Reiss<h2>ref_id</h2>b89<h2>title</h2>Leveraging syntax-related code for automated program repair<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Qi Xin; Steven P Reiss<h2>ref_id</h2>b90<h2>title</h2>Identifying patch correctness in test-based program repair<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Yingfei Xiong; Xinyuan Liu; Muhan Zeng; Lu Zhang; Gang Huang<h2>ref_id</h2>b91<h2>title</h2>Precise condition synthesis for program repair<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Yingfei Xiong; Jie Wang; Runfa Yan; Jiachen Zhang; Shi Han; Gang Huang; Lu Zhang<h2>ref_id</h2>b92<h2>title</h2>Nopol: Automatic repair of conditional statement bugs in java programs<h2>journal</h2>TSE<h2>year</h2>2017<h2>authors</h2>Jifeng Xuan; Matias Martinez; Favio Demarco; Maxime Clement; Sebastian Lamelas Marcote; Thomas Durieux; Daniel Le Berre; Martin Monperrus<h2>ref_id</h2>b93<h2>title</h2>Better test cases for better automated program repair<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Jinqiu Yang; Alexey Zhikhartsev; Yuefei Liu; Lin Tan<h2>ref_id</h2>b94<h2>title</h2>Regression testing minimization, selection and prioritization: a survey<h2>journal</h2>STVR<h2>year</h2>2012<h2>authors</h2>Shin Yoo; Mark Harman<h2>ref_id</h2>b95<h2>title</h2>Improved bug localization based on code change histories and bug reports<h2>journal</h2>IST<h2>year</h2>2017<h2>authors</h2>Klaus Changsun Youm; June Ahn; Eunseok Lee<h2>ref_id</h2>b96<h2>title</h2>Test case generation for program repair: A study of feasibility and effectiveness<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Zhongxing Yu; Matias Martinez; Benjamin Danglot; Thomas Durieux; Martin Monperrus<h2>ref_id</h2>b97<h2>title</h2>Alleviating patch overfitting with automatic test generation: a study of feasibility and effectiveness for the Nopol repair system<h2>journal</h2>EMSE Journal<h2>year</h2>2018<h2>authors</h2>Zhongxing Yu; Matias Martinez; Benjamin Danglot; Thomas Durieux; Martin Monperrus<h2>ref_id</h2>b98<h2>title</h2>Where should the bugs be fixed? more accurate information retrieval-based bug localization based on bug reports<h2>journal</h2><h2>year</h2>2012<h2>authors</h2>Jian Zhou; Hongyu Zhang; David Lo<h2>ref_id</h2>b99<h2>title</h2>What makes a good bug report<h2>journal</h2>TSE<h2>year</h2>2010<h2>authors</h2>Thomas Zimmermann; Rahul Premraj; Nicolas Bettenburg; Sascha Just; Adrian Schroter; Cathrin Weiss<h1>figures</h1><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>282<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Figure 1 :1Figure 1: Example of Linux bug report addressed by R2Fix.(1) which explicitly include localization information,(2) where the symptom (e.g., buffer overrun) is explicitly indicated by the reporter, and (3) which are about one of the following three simple bug types: Buffer overflow, Null Pointer dereference or memory leak. R2Fix runs a straightforward classification to identify the bug category and uses a match and transform engine (e.g., Coccinelle[66]) to generate patches. As the authors admitted, their target space represents <1% of bug reports in their dataset. Furthermore, it should be noted that, given the limited scope of the changes implemented in its fix patterns, R2Fix does not need to run tests for verifying that the generated patches do not break any functionality.This paper. We propose to investigate the feasibility of a program repair system driven by bug reports, thus we replace classical spectrum-based fault localization with Information Retrieval (IR)-based fault localization. Eventually, we propose iFixR, a new program repair workflow which considers a practical repair setup by imitating the fundamental steps of manual debugging. iFixR works under the following constraint:<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Figure 2 :2Figure 2: The iFixR Program Repair Workflow.<h2>figure_data</h2><h2>figure_label</h2>123<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>Algorithm 1 : 2 F 3 F123Statement-level IR-based Fault Localization. Input :br : a bug report Input : irT ool : IRFL tool Output : Sscor e : Suspicious Statements with weight scores 1 Function main (br ,irT ool ) ← fileLocalizations (irT ool ,br ) ← selectTop (F ,k )4 c b ← bagOfTokens (br ) /* c b : Bag of Tokens of bug report */ 5 c ′ b ← preprocess (c b ) /* tokenization,stopword removal, stemming */ 6 v b ← tfIdfVectorizer(c ′ b ) /* v b<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_4<h2>figure_caption</h2>iFixR: Bug Report driven Program Repair ESEC/FSE '19, August 26-30, 2019, Tallinn, Estonia<h2>figure_data</h2><h2>figure_label</h2>4<h2>figure_type</h2>figure<h2>figure_id</h2>fig_5<h2>figure_caption</h2>FileFigure 4 :4Figure 4: Buggy code of Defects4J bug Math-75.<h2>figure_data</h2><h2>figure_label</h2>6<h2>figure_type</h2>figure<h2>figure_id</h2>fig_6<h2>figure_caption</h2>Figure 6 :6Figure 6: Distribution of elapsed time (in days) between bug report submission and test case attachment.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_7<h2>figure_caption</h2>iFixR: Bug Report driven Program Repair ESEC/FSE '19, August 26-30, 2019, Tallinn, Estonia<h2>figure_data</h2><h2>figure_label</h2>7<h2>figure_type</h2>figure<h2>figure_id</h2>fig_8<h2>figure_caption</h2>Figure 7 :7Figure 7: Patched source code and test case of fixing Math-5.<h2>figure_data</h2><h2>figure_label</h2>8<h2>figure_type</h2>figure<h2>figure_id</h2>fig_9<h2>figure_caption</h2>Figure 8 :8Figure 8: Distribution of # of comments per bug report.<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>Test case changes in fix commits of Defects4J bugs.<h2>figure_data</h2>Test case related commits# bugsCommit does not alter test cases14Commit is inserting new test case(s) and updating previous test case(s)62Commit is updating previous test case(s) (without inserting new test cases)76Commit is inserting new test case(s) (without updating previous test cases)243<h2>figure_label</h2>2<h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2><h2>figure_data</h2>Failing test cases# bugsFailing test cases exist (and no future test cases are committed)14Failing test cases exist (but future test cases update the test scenarios)9Failing test cases exist (but they are fewer when considering future test cases)4Failing test cases exist (but they differ from future test cases which trigger the bug)3No failing test case exists (i.e., only future test cases trigger the bug)365<h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_5<h2>figure_caption</h2>: Bug report Feature Vector */<h2>figure_data</h2>7for f in F do8S ← parse(f )/* S : List of statements */9for s in S do10cs ← bagOfTokens (s )/* cs : Bag of Tokens of statements */11 12c ′ s ← preprocess (cs ) vs ← tfIdfVectorizer(c ′ s )/* vs : Statements Feature Vector */13/* Cosine similarity between bug report and statement*/14simcos ← similarity cosine (v b ,vs )15wscor e ← simcos × f .score;/* score: Suspicious Value */18<h2>figure_label</h2>4<h2>figure_type</h2>table<h2>figure_id</h2>tab_6<h2>figure_caption</h2>Fix patterns implemented in iFixR.<h2>figure_data</h2>Pattern descriptionused by  *Pattern descriptionused by  *Insert Cast CheckerGenesisMutate Literal ExpressionSimFixInsert Null Pointer CheckerNPEFixMutate Method InvocationELIXIRInsert Range CheckerSOFixMutate OperatorjMutRepairInsert Missed StatementHDRepairMutate Return StatementSketchFixMutate Conditional ExpressionssFixMutate VariableCapGenMutate Data TypeAVATARMove Statement(s)PARRemove Statement(s)FixMiner*  We mention only one example tool even when several tools implement it.+ if (exp instanceof T) {...(T) exp...; ......+ }Figure 3: Illustration of "Insert Cast Checker" fix pattern.<h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_7<h2>figure_caption</h2>What is the effectiveness of iFixR's patch ordering strategy? In particular, we investigate the overall workflow of iFixR, by re-simulating the real-world cases of software maintenance cycle when a bug is reported: future test cases are not available for patch validation.<h2>figure_data</h2>iFixR: Bug Report driven Program RepairESEC/FSE '19, August 26-30, 2019, Tallinn, Estonia• RQ3 [Patch ordering] :<h2>figure_label</h2>5<h2>figure_type</h2>table<h2>figure_id</h2>tab_8<h2>figure_caption</h2>Fault localization results: IRFL (IR-based) vs. SBFL (Spectrum-based) on Defects4J (Math and Lang) bugs.<h2>figure_data</h2>(171 bugs)Top-1Top-10Top-50Top-100Top-200AllIRFL2572102117121139SBFLGZ v 1 GZ v 226 2375 79106 119110 135114 150120 156† GZ v 1 and GZ v 2 refer to GZoltar 0.1.1 and 1.6.0 respectively, which are widely used in APRsystems for Java programs.<h2>figure_label</h2>6<h2>figure_type</h2>table<h2>figure_id</h2>tab_9<h2>figure_caption</h2>Fault localization performance.<h2>figure_data</h2>GZoltar + Ochiai (395 bugs)Top-1Top-10Top-50Top-100Top-200Allwithout future tests51017171920with future tests45140198214239263<h2>figure_label</h2>7<h2>figure_type</h2>table<h2>figure_id</h2>tab_10<h2>figure_caption</h2>IRFL vs. SBFL impacts on the number of generated correct/plausible patches for Defects4J bugs.<h2>figure_data</h2>LangMathTotalIRFL Top-11/43/44/8SBFL Top-11/46/87/12IRFL Top-53/67/1410/20SBFL Top-52/711/1713/24IRFL Top-104/99/1713/26SBFL Top-104/1116/2720/38IRFL Top-207/129/1816/30SBFL Top-204/1118/3022/41IRFL Top-507/1510/2217/37SBFL Top-504/1319/3423/47IRFL Top-1008/1810/2318/41SBFL Top-1005/1419/3624/50IRFL All11/1910/2521/44SBFL All5/1419/3624/50*  We indicate x/y numbers of patches:<h2>figure_label</h2>9<h2>figure_type</h2>table<h2>figure_id</h2>tab_11<h2>figure_caption</h2>Overall performance of iFixR for patch recommendation on the Defects4J benchmark. * x/y: x is the number of bugs for which a correct patch is generated; y is the number of bugs for which a plausible patch is generated.5.3.2 Comparison with the state-of-the-art test-based APR systems.To objectively position the performance of iFixR (which does not require future test cases to localize bugs, generate patches and present a sorted recommendation list of patches), we count the number of bugs for which iFixR can propose a correct/plausible patch. We consider three scenarios with iFixR:(1) [iFixR top5 ] -developers will be provided with only top 5 recommended patches which have been validated only with regression tests: in this case, iFixR outperforms about half of the state-of-the-art in terms of numbers bugs fixed with both plausible or correct patches.<h2>figure_data</h2>Recommendation rankTop-1Top-5Top-10Top-20Allwithout patch re-prioritization3/34/56/106/1013/27with patch re-prioritization3/48/139/1410/1513/27(2) [iFixR all ] -developers are presented with all (i.e., not onlytop-5) generated patches validated with regression tests: in thiscase, only four (out of sixteen) state-of-the-art APR techniquesoutperform iFixR.(3) [iFixR opt ] -developers are presented with all generated patcheswhich have been validated with augmented test suites (i.e., opti-mistically with future test cases): with this configuration, iFixRoutperforms all state-of-the-art, except SimFix<h2>figure_label</h2>10<h2>figure_type</h2>table<h2>figure_id</h2>tab_12<h2>figure_caption</h2>iFixR vs state-of-the-art APR tools. /y: x is the number of bugs for which a correct patch is generated; y is the number of bugs for which a plausible patch is generated. iFixRopt : the version of iFixR where available test cases are relevant to the bugs. iFixR al l : all recommended patches are considered. iFixR t op5 : only top 5 recommended patches are considered.<h2>figure_data</h2>APR toolLang  *Math  *Total  *jGenProg [59]0/05/185/18jKali [59]0/01/141/14jMutRepair [59]0/12/112/12HDRepair [36]2/64/76/13Nopol [93]3/71/214/28ACS [92]3/412/1615/20ELIXIR [73]8/1212/1920/31JAID [12]1/81/82/16ssFix [90]5/1210/2615/38CapGen [84]5/512/1617/21SketchFix [18]3/47/810/12FixMiner [31]2/312/1414/17LSRepair [44]8/147/1415/28SimFix [19]9/1314/2623/39kPAR [48]1/87/188/26AVATAR [49]5/116/1311/24iFixRopt11/1910/2521/44iFixR al l6/117/1613/27iFixR t op53/75/68/13*  x<h2>figure_label</h2>11<h2>figure_type</h2>table<h2>figure_id</h2>tab_13<h2>figure_caption</h2>Change properties of iFixR's correct patches. * x/y -→ for x bugs the patches are correct, while for y bugs they are plausible.<h2>figure_data</h2>Change action#bugs  *Impacted statement(s)#bugs  *Granularity#bugs  *Update5/7Single-statement8/12Statement1/2Insert3/5Multiple-statement0/1Expression7/11Delete0/1<h2>figure_label</h2>12<h2>figure_type</h2>table<h2>figure_id</h2>tab_14<h2>figure_caption</h2>Dissection of bugs successfully fixed by iFixR.<h2>figure_data</h2>Patch TypeDefect4J Bug IDIssue IDRoot CausePriorityGL-6LANG-857String index out of bounds exceptionMinorGL-24LANG-664Wrong behavior due missing conditionMajorGL-57LANG-304Null pointer exceptionMajorGM-15MATH-904Double precision floating point format errorMajorGM-34MATH-779Missing "read only access" to internal listMajorGM-35MATH-776Range checkMajorGM-57MATH-546Wrong variable type truncates double valueMinorGM-75MATH-329Method signature mismatchMinorPL-13LANG-788Serialization error in primitive typesMajorPL-21LANG-677Wrong Date Format in comparisonMajorPL-45LANG-419Range checkMinorPL-58LANG-300Number formatting errorMajorPM-2MATH-1021Integer overflowMajor"G" denotes correct patch and "P" means plausible patch.<h2>figure_label</h2>13<h2>figure_type</h2>table<h2>figure_id</h2>tab_16<h2>figure_caption</h2>Dissection of bug reports related to Defects4J bugs.<h2>figure_data</h2>Proj.Unique Bug Reportsw/ Patch AttachedAverage Commentsw/ Stack Tracesw/ Hintsw/ Code BlocksLang62114.5346231Math100235.1559251<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>Type "Comparable<?>" VariableName "v" ① ② ③ ④ ⑤<h2>formula_coordinates</h2>[5.0, 322.43, 88.75, 227.95, 57.08]<h2>formula_id</h2>formula_1<h2>formula_text</h2>f r cmd (patches) = (pri type • pri susp • pri chanдe )(patches) (1)<h2>formula_coordinates</h2>[5.0, 327.42, 440.38, 230.78, 10.4]<h2>formula_id</h2>formula_2<h2>formula_text</h2>P → P n where n = |patches |, pri chanдe (patches) = [p i , p i+1 , p i+2 , • • • ] and holds ∀p ∈ patches, C chanдe (p i ) ≤ C chanдe (p i+1 ).<h2>formula_coordinates</h2>[5.0, 331.12, 543.86, 227.08, 34.67]<h2>formula_id</h2>formula_3<h2>formula_text</h2>P n → P n such that pri susp (seq chanдe ) = [• • • , p i , p i+1 , • • • ] holds S susp (p i ) ≥ S susp (p i+1 )<h2>formula_coordinates</h2>[5.0, 331.45, 654.38, 226.75, 22.91]<h2>formula_id</h2>formula_4<h2>formula_text</h2>[p i , p i+1 , p i+2 , • • • ] that holds ∀p i , D type (p i ) ≥ D type (p i+1<h2>formula_coordinates</h2>[6.0, 67.3, 295.68, 226.75, 20.74]<h1>doi</h1>10.1145/3338906.3338935<h1>title</h1>iQUANT: Interactive Quantitative Investment Using Sparse Regression Factors<h1>authors</h1>Xuanwu Yue; Qiao Gu; Deyun Wang; Huamin Qu; Yong Wang<h1>pub_date</h1>2021-04-23<h1>abstract</h1>The model-based investing using financial factors is evolving as a principal method for quantitative investment. The main challenge lies in the selection of effective factors towards excess market returns. Existing approaches, either hand-picking factors or applying feature selection algorithms, do not orchestrate both human knowledge and computational power. This paper presents iQUANT, an interactive quantitative investment system that assists equity traders to quickly spot promising financial factors from initial recommendations suggested by algorithmic models, and conduct a joint refinement of factors and stocks for investment portfolio composition. We work closely with professional traders to assemble empirical characteristics of "good" factors and propose effective visualization designs to illustrate the collective performance of financial factors, stock portfolios, and their interactions. We evaluate iQUANT through a formal user study, two case studies, and expert interviews, using a real stock market dataset consisting of 3000 stocks × 6000 days × 56 factors.<h1>sections</h1><h2>heading</h2>Introduction<h2>text</h2>Quantitative investment using algorithmic trading has accounted for over 85% of transactions in US stock markets since 2012 [GK13]. There is a growing demand for machine learning models that can describe and explain the price change of an individual or a portfolio of stocks in the market. Factor investing is one of the most widely used strategies in quantitative investment. In a factor investing paradigm [Tsi18, FF92, FF04], a selective collection of financial variables called factors (e.g., a company's fundamental data and its growth measures) are used as input predictors and stock returns are set as target outcomes to build predictive models. With these models, traders forecast future stock returns regularly in order to construct time-sensitive portfolios composed of a small set of high-return securities. The returns of these securities are expected to beat the average market return, which is known as the excess return. Recent global surveys show that the factor investing paradigm is employed by 90% of institutional investors to manage at least part of their portfolio [MAHG16]. The percentage of assets traded with factor investing is over 30% in size and still fast-growing.
Despite the prevalence of factor investing, selecting an effective set of factors for market investment is a difficult task. There can be hundreds of candidate factors or more available for predictive mod-els. Currently, most quantitative traders manually select factors by comparing their corresponding portfolio returns in a standard backtesting. Such a trial-and-error approach is indirect, less intuitive, and often requires an exhaustive search in the vast combinatorial factor space. To compute returns from a set of factors, a predictive model and stock portfolio need to be constructed, whose interactions with the selected factors are not fully understood and utilized in the current approach. Traders must also speculate an initial factor selection for evaluation. On the other hand, feature selection methods in machine learning research [GE03] have been applied to this domain, but not as an end-to-end approach like many other fields. Algorithm-selected factors are mostly used as a reference or starting point of the manual selection process, as these factors (especially in black-box models) are not trusted by traders until they can be explained and evaluated. There are currently few tools that can combine human intelligence (e.g., prior knowledge of a factor's utility) and computational power (e.g., assessing stability and sensitivity of a factor) for factor selection in stock markets. This work is committed to three essential requirements of interactive factor investing. First, traders need an intuitive and comprehensive way to assess the performance of numerous factors with respect to excess returns. Second, high-performance factors need to be selected and refined together with high-return stocks in the construction of an investment portfolio. Third, portfolios need to be evaluated and compared in the industry-standard backtesting for their returns in both past and future outlook. To meet these requirements, we present iQUANT (interactive Quantitative investment Using spArse regressioN facTors), a visual analytics system that seamlessly integrates human and algorithmic factor selection for effective factor investing in stock markets. iQUANT enjoys the best of both worlds from human intelligence by a heuristic search in the combinatorial factor space and from computational power by summarizing and presenting key performance metrics of factors and stocks in close interactions. The contributions of this work can be summarized as follows.
• We explore the use of sparse regression models as the initial feature selection method for factor investing. Sparse regression models are preferred as they optimize the procedure between factor selection and stock prediction in the same process. Performance metrics of factors are computed, including not only their feature weights, but also the factor contribution that describes its importance to the prediction. • We introduce elaborate visualization designs that comprehensively illustrate the performance of factors in the regression model to predict the prices of candidate stocks. Our designs are based on expert studies about empirical characteristics of valuable factors, including stability and sensitivity. • On top of the visualization interface, we propose an interaction framework for factor investing that allows traders to jointly select high-performance factors and high-return stocks in an integrated model-building and portfolio-construction process. By our framework, traders can iteratively refine the factor selection and stock portfolio based on visual evidence shown in the interface. The industry-standard backtesting is also supported. • We evaluate iQUANT in a formal user study comparing with baseline alternatives. Case studies with domain experts on targeted user tasks are conducted using a real stock market dataset.
Expert feedback about iQUANT is also reported and discussed.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Related Work<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Financial Data Visualization<h2>text</h2>Financial data, such as stock/fund prices and economic indicators, has been widely used by traders and investors to guide their investment decisions. Currently, few studies have been done in integrating visualization into the factor investing process. Some existing studies [CHL * 03,RSE09,SLFE11] apply visualizations to help manage investment portfolios. FundExplorer [CHL * 03] introduces a distorted treemap to visualize both the composition of a fund's portfolio and the remaining stocks. Their design facilitates the convenient retrieval of complementary funds to achieve portfolio diversification. FinVis [RSE09] employs visualization to help non-experts understand the expected returns, risks and the changing context of multiple portfolios. PortfolioCompare [SLFE11] displays the variability and correlation of a portfolio's risks and returns by scatterplots and distribution charts. Their designs can assist users in making better investments according to their risk preferences. None of these visualization tools is integrated with predictive and feature selection models for factor investing.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Interactive Feature Selection<h2>text</h2>Feature selection is a key step in machine learning. Prior studies have focused on algorithmic feature selections [LCW * 17], whose mechanism and reasoning are often opaque to users. Inspired by prior research on predictive model visualization [LXL * 18, PKWQ20,ZWLC19,MQB19,EASD * 19], interactive feature selections have been widely studied. These studies allow users to visually explore the feature space by novel representations and interactive interfaces. Seo and Shneiderman [JS04] proposed a rankby-feature framework for the exploration of multidimensional data. Data features are ranked and visualized by scatter plots, bar charts, etc. Johansson and Johansson [JJ09] utilized parallel coordinates to visualize data features, with the order determined by user-defined metrics.
On the other hand, there are some works considering the relationship between features and predictive models. For example, SmartStripes [MBD *  11]  Existing studies summarize the feature dependency or performance by hybrid quality measures (e.g., feature ranking), but the temporal evolution of features are rarely considered. In factor investing, the temporal dynamics of financial factors (features) and their influence on the prediction model are crucial, which makes existing work not suitable for our scenario.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>System Overview<h2>text</h2>In this section, the technical background of interactive factor investing is introduced, on which key user requirements are summarized.
To meet these requirements, four user tasks are defined which are achieved by a pipeline design in our system.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Background<h2>text</h2>Historically, factor investing originated from the Capital Asset Pricing Model (CAPM) proposed by Sharpe [Sha64] and Linter [Lin65], which uses a single factor of a stock's sensitivity to market returns to explain stock returns. Then, researchers and practitioners continued to discover new factors related to stock returns. Notably, Fama and French [FF92] proposed the famous three-factor model which considers a size factor (large v.s. small capitalization stocks) and a value factor (high v.s. low book-to-market ratios) in addition to the market sensitivity factor. Later, many factors explaining stock returns were discovered, including a company's recent fundamental data [Asn97], the long-term income growth of companies [PLdSSV98, DS97], etc.
In a typical quantitative investment scenario for stock markets, traders hold a portfolio composed of a collection of individual stocks appropriately mixed in value. The portfolio is adjusted in regular time intervals, which is also known as the trading cycle. Depending on the style of traders and funds, the trading cycle can range from one day/week (e.g., private equity) to several months (e.g., public funds). By the factor investing paradigm, at the beginning of each trading cycle, traders pick a few factors based on their experience, domain knowledge, market status and factor returns. Quantitative models are then built up over historical market data, which try to establish linkages between factors and stock returns. Both machine learning and statistical models have been applied in this stage [CG20]. From model-predicted stock returns, traders further apply a trading strategy to determine how to adjust their investment assets, i.e., when and which stocks to buy/sell. Because it is extremely hard to forecast how the portfolio adjustment decision performs in the upcoming trading cycle, a standard backtesting helps to evaluate its returns on historical data. The model consistently earns excess returns to the market, or at least in recent cycles, is considered good in the context of quantitative investment.<h2>publication_ref</h2>['b15', 'b0']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Task Characterization<h2>text</h2>The factor investing paradigm introduced above seems straightforward. However, due to the closedness of the quantitative investment community, we only came to this understanding after pilot studies with expert traders in several mutual funds, both public and private ones. In the interviews, almost all of them mentioned a key difficulty in the factor investing practice, the selection of valuable factors. While during their long-term jobs, a number of factor combinations proven in the past have been accumulated, they never stop finding and evaluating new factors because stock fluctuations in the future can hardly be entirely the same as the past. In a sense, the ability to discover new factors is the central competence of quantitative traders and accounts for most of their performance difference.
As we learned from experts in the interview, the current factor selection process is mostly done manually. A major user pain point lies in that the evaluation of selected factors are indirect, less intuitive, and often requires an exhaustive search in the vast combinatorial factor space to identify the best factor collection. In the standard backtesting of quantitative models, to be able to compute model returns, the learning/statistical algorithm and the focused stock need to be determined in addition to the factor selection. Importantly, these model returns are often the result of interactions among factor, model and stock selections. Users often require a tedious trialand-error comparison process to determine factors. As such a process is done in every trading cycle, efficiency is also a critical consideration not to be neglected. Meanwhile, automatic factor selections by machine learning algorithms (e.g., feature selection) have been incorporated in a trader's job. Due to explainability concerns, these algorithm-picked factors are only used as a starting point or reference for manual factor selection until traders can understand their working mechanism. To this end, interactive factor investing systems that can integrate feature selection, model building, and human intelligence in heuristic factor search, as well as visually explain selected factors are of great demand. Below we summarize key user requirements collected from our expert studies.
R1. Direct and intuitive factor evaluation. As there are a large number of candidate factors, for a given pool of stocks, the system should recommend an initial factor selection for users to start with (R1.1). Importance measures of these factors should be computed in the context of quantitative models to account for the interaction between factors and models. These factor importance and the performance of underlying stock/model need to be presented comprehensively to allow intuitive factor evaluation (R1.2).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R2. Interaction support for factor and portfolio selection.<h2>text</h2>Based on evaluations of factor and stock performance, the system should support appropriate interaction to help construct stock portfolios to invest. This requires interactions to jointly select and refine factors and stocks, as well as a mechanism to help users understand.
R3. Integrated model building and backtesting. In the factor selection and evaluation, the system should automatically train quantitative models to predict stock returns based on selected factors and stocks, algorithmically or jointly (R3.1). In factor investing, backtesting of these models should be performed to compare portfolio returns and guide investment decisions (R3.2).
To meet the requirements of expert traders, we target to build a visual analytics system that can support the following key tasks.
T1. Selection of an initial pool of stocks and factors. Traders can start from all the stocks in the market for their investment analysis. More frequently, s/he focuses on a few sectors (per funder's requirements) and picked several stocks from each sector to construct a balanced initial stock portfolio. Our system provides an interface to allow users to manually determine the initial stock portfolio from the list of stocks organized by sectors. Initial factor selections are automatically recommended based on the stock portfolio. (R1.1) T2. Evaluation of factors through visualization. Our system illustrates the performance of individual factors in a comprehensive visualization interface. Users can visually identify valuable factors by examining key dimensions such as stability and sensitivity. The stock returns of quantitative models are also displayed. (R1.2) T3. Interactive joint factor refinement and portfolio adjustment. Traders select factors and stocks to construct investment portfolios in an interactive and iterative manner. Upon each refinement of factors or stock portfolio, the stock return of the resulting model is displayed immediately, which provides visual evidence for user's heuristic factor search. (R2) T4. Evaluation and comparison of portfolio returns. To compute portfolio returns, quantitative models are automatically built up over selected factors (R3.1). Traders compare the temporal dynamics of returns from alternative portfolios and select the best portfolio in a standard backtesting (R3.2).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>System Design<h2>text</h2>We propose iQUANT, a visual analytics system supporint all the four user tasks defined above. iQUANT features a pipelined design as shown in Figure 1. In the first stage, the system takes three types of stock market data. The first type is stock transaction data, notably the stock daily opening/closing prices. The second type is market information, including but not limited to financial statements (total assets, debts, cash flows, etc.), market news, and financial reports. The third and most critical part of data are factors computed from stock transactions and market information, which is used in our system for user selection and model building. In this application, we apply data from a major stock market consisting of 3,000 stocks during 30 years (1990-2018). The data is collected from both external sources (e.g., open APIs for transaction data) or by internal computations (e.g., financial factors). The underlying technique of our system can be easily extended to support other stock markets.
In the second stage of Figure 1, we integrate a sparse regression model to carry out factor selection and return prediction simultaneously. The performance of selected factors and predictive models are visualized in iQUANT . The result of interactive factor investing is evaluated in a standard backtesting view in the final stage.
The iQUANT system is a web-based full-stack application. All data are stored in MongoDB. The front-ends use Vue.js and D3.js libraries. The sparse regression model is time-consuming. However, it only takes ∼20 seconds in the back-end to compute a sector (dozen) of stocks and respond to the web browser. All performance is measured in a laptop requesting a four-CPU cloud server. We expect to upgrade the server to further reduce computation time.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_1', 'fig_1']<h2>table_ref</h2>[]<h2>heading</h2>Predictive Analytics<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Sparse Regression Model<h2>text</h2>The interactive factor investing approach takes two steps to make final decisions on portfolio adjustment. First, the collection of financial factors are used to build models on the pool of candidate stocks for investment. Second, traders evaluate the predicted stock returns by the model and decide on the portfolio adjustment. In iQUANT, the second step is achieved by user interactions with the interface to understand the model output, and using existing trading strategies/algorithms (beyond the scope of this paper). In this part, we describe a suite of sparse regression models applied in the first step to jointly recommend factors (i.e. feature selection) and predict stock returns (i.e., model optimization). The sparse regression model is a popularly-used one in industry. Users can replace it with other statistics/machine learning models with similar output format. We include the sparse regression model to demonstrate the workflow of iQUANT.
Consider the model for a stock ϒ during a time period of Γ = [T + 1, T + L], where L is the length of Γ and any time t ∈ Γ indicates the tth trading day. Γ is further partitioned into N trading 
+ L + 1] compose a matrix X = (x 1 , • • • , x T+L+1 ) where x i = (x i1 , • • • , x iF ) . The modeling of stock ϒ in Γ takes N steps, corresponding to the trading cycles of τ 1 , • • • , τ N ,
respectively. Consider the ith trading cycle τ i , a sub-vector of actual stock returns is used as outcomes, denoted by y (i) = (y (i-1)D+2 , • • • , y (i-1)D+T +1 ). A sub-factor-matrix is used as the design matrix (predictors), denoted by X (i) = (x (i-1)D+1 , • • • , x (i-1)D+T ). We fit a sparse regression model:
y (i) = w (i) X (i) + b (i) + ε (i) (1)
Subject to the objective function:
Minimize Lasso NLL = NLL + 1 2 λ [α||w (n) || 1 + (1 -α)||w (n) || 2 2 ]
(2) where w (i) = (w
(i) 1 , • • • , w (i) F ) denotes the weight vector of financial factors in the fitted model, b (i) is the bias, ε (i) is the error term, ||•|| 1 (|| • || 2 ) is L 1 (L 2
) norm of the vector. D is set to 21 (average number of monthly trading days). The best weight vector is computed by minimizing the negative log likelihood (NLL) of a linear regression term plus L 1 and L 2 penalty terms of the weight vector. The penalty terms shrink the weight of unimportant factors to zero. On each trading cycle, the model is applied to predict daily stock returns.
Note that λ and α in Eq. (2) control the degree of model sparsity and weights between L 1 and L 2 terms. In this work, we provide three choices of modeling: α = 1 (without the L 2 term), the famous Lasso model [Tib96] with a fixed λ ; LassoCV model using 10fold cross-validation to determine λ of the Lasso model The sparse regression model achieves feature selection (L 1 and L 2 penalty terms) and model optimization (regression term) simultaneously. This is extremely helpful to alleviate the overfitting effect in the factor investing case when the number of predictors (F = 56 in our implementation) is comparable to the number of training samples (T = 200). Though we apply a basic linear regression for stock prediction, the sparse models with L 1 and L 2 penalties can be easily extended to support more sophisticated models such as nonlinear models using customized kernel functions.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Financial Factors<h2>text</h2>For factor computation, we adopt the result of a latest white paper on the target stock market [Tsi18], which recommends six types of 56 factors with each factor characterizes a key aspect of a stock:
• Transaction friction: 17 factors delineating overall status of a company (e.g., firm size by its market value, age of a firm) and transaction statistics (e.g., volatility, risks measured by stock returns compared with the market return); • Momentum: 5 factors computing daily stock returns in the recent 6/12 months, the change of momentum, and specially defined momentums; • Value: 8 factors including the famous book-to-market ratio proposed by Fama and French [FF92], the asset-to-market ratio, and other company performance related ratios; • Growth: 11 factors describing the growth of a company's asset, debt, market value, sales, profit, tax, etc; • Profitability: 8 factors quantifying return on equity, return on asset, and other factors related to company profits; • Liquidity: 7 factors characterizing the liquidity performance of a company, including current ratio, quick ratio, cash flow to debt ratio, etc.
For the above factors, the ranges of their values differ significantly from each other. Also, their values can change dramatically along with the stock market. Since all the factor values are normalized initially, the relative importance of each factor can be assessed, which can facilitate the factor analysis. The sparse regression model in iQUANT outputs three measures to delineate the importance of each factor in the prediction. Take the jth factor in the ith trading cycle (e.g., daily, monthly or quarterly) as an example, the three measures are as follows:
• Factor weight learned in the model, denoted by w (i) j ;
• Factor value changing in a daily basis, denoted by X (i) ; • Contribution of factors to the prediction of daily stock returns, denoted by
∑ T +iD k=T +(i-1)D+1 w (i) j x k j .
By collecting the feedback from traders in our pilot study, we further extracted three metrics that are widely-used in industry to delineate the utilization of each factor. These two metrics are visualized in iQUANT to aid the interactive factor selection and stock portfolio construction process.
• Sensitivity: a metric that indicates whether the utilization of a factor can be replaced by other factors. The sensitivity is computed by (ξ i jξ i ) + where ξ i j denotes the prediction error by removing the jth factor from sparse regression models, ξ i denotes the prediction error of the original model; • Stability: a good factor for the predictive model should have stable contributions in a long period of time before confidently applied. Quantitatively, it is measured by the number of times the factor's contribution to a stock's prediction flips from negative to positive or from positive to negative.<h2>publication_ref</h2>['b15']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Visualization<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Control Panel<h2>text</h2>The control panel in Figure 2(a) allows users to select an initial pool of stocks for analysis (T1). S/he can input the stock code to select an individual stock or choose an entire sector of stocks from a sector list. All the selected stocks will appear in the bottom part of the control panel, aggregated by sectors. Next, users need to specify a time period and a model for factor investing. Finally, users click the "Draw" button to visualize factors related to the selected stocks.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>[]<h2>heading</h2>Factor View<h2>text</h2>Factor view (Figure 2 To meet the above design requirements, we propose a customized visual design to integrate time-varying factor importance metrics in a single factor view. As shown in Figure 2(b), the x-axis of the view indicates a timeline divided into multiple time units (months by default). Each factor is visualized as a series of filled circles in these time units. On each circle, the color hue encodes factor type, the color opacity encodes the factor sensitivity to the model, and the circle radius represents the aggregated factor importance to all the selected stocks. Three importance metrics can be displayed by the checkboxes in the top row of the factor view: weight, value, and contribution (default). All the circles on the same time unit are vertically aligned by a fixed order of their corresponding factors to facilitate factor tracking and comparison along the timeline. As each factor can have a positive or negative contribution in different time units, we depict positive and negative factors as circles above and below the x-axis respectively.
User interactions are designed in the factor view to help identify useful factors for modeling. When a user hovers a factor circle, all the circles representing the same factor are linked by a curved line to reveal its stability over time. (Figure 2(b)). Also, a tooltip will be displayed, showing the detailed information of the factor circle such as factor type, name, sensitivity and importance metric. The user can interactively filter factors to reduce visual complexity.
Alternative designs of the factor view have been considered. The line chart is the most intuitive form to visualize factor importance over time. When we have as many as 56 factors, line charts suffer from severe visual clutter. Stacked flow chart [BW08] is another possible visual design. A quick overview of factor composition can be provided, but it is not good at displaying the temporal dynamics of individual factors (especially for unimportant factors). For the circle layout, we also experimented with circle packing in which positive and negative factors are placed separately. The circle packing leads to more compact visualization (Figure 3(a)). However, our domain experts complained that the design made it difficult to track individual factors across the timeline. The relative position of the same factor circle varied a lot across different time units. In an improved layered circle packing (Figure 3(b)), which is similar to a matrix design, the same type of factors is aligned horizontally. It facilitates tracking of the same factor, but the space usage is inefficient and the connecting lines on factors lead to heavy crossings. The current design is a good trade-off between space usage and effectiveness compared with other alternative designs.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2', 'fig_2', 'fig_2', 'fig_4', 'fig_4']<h2>table_ref</h2>[]<h2>heading</h2>Stock View<h2>text</h2>Stock view (Figure 2(c)) splits the aggregated factor time series in the factor view according to their importance to individual stocks. This helps to evaluate the factor performance among different stocks and sectors (T2). The visual design is challenging, as the space for each stock is limited and there could be many factors relevant to a stock. On each factor, there are also multiple attributes to be visualized, e.g., the magnitude and polarity of importance (contribution), the model bias, and the trend of the stock price.
In our design, the visualization of each stock follows a similar layout to the display in the factor view. As shown in Figure 2(c), the x-axis is a timeline divided into units. At each time unit, a stock bar is drawn (the yellow bar of Figure 6 (d)), whose height represents the ratio of stock price change in this time unit and width represents the rate of prediction error on the price change by models. A red (green) bar indicates a rise (drop) of the stock price. On top of and below each stock bar, there are two factor glyphs which summarize the factors having a positive and negative contribution to the price change in the predictive model.
The factor glyph at the top and bottom in Figure 6 (d) has a circular shape, and is composed of one inner circle and six angular sectors in the outside. The radius of the inner circle encodes the model bias. Each outer sector represents one type of factors, with the size of each sector indicating the sum of all factor's importance in this type. On each factor glyph, the top-5 factors with the highest importance in all factor types are displayed. When more than one factor from the same type belongs to the top-5, a Voronoi diagram is drawn to pack these factors in the same angular sector. Each region in the Voronoi diagram corresponds to one factor, with the region size indicating its importance. The sum of all the other factor's importance out of the top-5 category is represented by the grey region close to the glyph center. When users hover a Voronoi region of one factor, a line will be drawn to connect all the Voronoi regions of this factor over time. Meanwhile, a tooltip showing the factor type, name and importance will also appear. When the factor does not belong to the top-5 category, the line will be linked to zero position on the x-axis.
Alternative designs such as line chart and streamgraph [HHN00] are commonly used for visualizing temporal evolution of variables. In our scenario, the top-5 factors can be quite different in separate time units, making line charts or streamgraph discontinuous over time and hard to track by users. Commodity designs such as stacked bar charts and pie charts are also considered. Nevertheless, the stacked bar chart is not scalable to support dozens of factors in the same view. The pie chart can clearly show the percentage of each factor in importance, but again the relative position of the same factor can vary a lot in different units, making it hard to compare the evolution of the same factor. iQUANT is designed for financial factor selection, which makes it inevitable to consider much domain knowledge and incorporate different factors in our design. This can increase the design complexity and it takes some time for user to learn, but most of our users can be well prepared after a training session of about 10 minutes as indicated in Section 6.3, which we would argue is a reasonable learning effort.<h2>publication_ref</h2>['b19']<h2>figure_ref</h2>['fig_2', 'fig_2', 'fig_7', 'fig_7']<h2>table_ref</h2>[]<h2>heading</h2>Factor List<h2>text</h2>Factor list (Figure 2(d)) aggregates factor data together to depict the overall factor performance (T2). The six types of factors are listed in rows, with factors in one type uniformly distributed as factor bars in the same row. Each factor bar the positive and negative contribution of the factor in the entire time period, using the height of the bar above and below the x-axis respectively. Users can manually select factors by clicking factor bars, then the stock prediction models are re-built using the selected factors after triggering the "Draw" button. The factor performance in the stock view is also updated for iterative factor/stock selection. (T3)<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>[]<h2>heading</h2>Stock Return View<h2>text</h2>Stock return view (Figure 2(e)) is organized by sectors. In each expandable sector box, the investment returns of all stocks in that sector are calculated by backtesting and displayed in multiple grey line charts. An additional blue line indicates the market return. Users could select promising stocks in the stock view and examine their returns by the lines highlighted in red. The stocks could be selected in the return view and added into a portfolio table. A "Backtest" button will trigger the backtesting computation by the selected factors (factor list) and selected stocks (portfolio table).<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>[]<h2>heading</h2>Backtesting View<h2>text</h2>Backtesting view (Figure 2(f)) evaluates the investment performance of the selected portfolio (T4). Backtesting calculates the portfolio returns using historical stock data. The blue line indicates the average return in the market and the grey one indicates the portfolio return. A dotted line after the selected time period represents the predicted return in future. Comparison among multiple portfolios is enabled by adding more portfolio from the stock return view.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>[]<h2>heading</h2>Multiple Views Interaction<h2>text</h2>iQUANT supports interactions among multiple views for comprehensive evaluation of factor performance (T2) and interactive selection of stocks and factors (T3).
Selection of factors: Users can select/de-select factors by clicking factor circles in the factor view, Voronoi regions in the stock view, and factor bars in the factor list. Each change to the factor selection will trigger updates in all the views simultaneously.
Selection of stocks: An initial collection of stocks can be selected in the control panel. Users can further compose an investment portfolio by clicking the stock name in the stock view or the return curve in the stock return view. This helps to interactively and iteratively refine stock selection according to their returns or the performance of relevant factors.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>User Experiment<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Design<h2>text</h2>We conducted a formal user study to compare the effectiveness of iQUANT visualization design with a baseline factor visualization using commodity multiple line charts (See the supplementary material for more details of the baseline factor visualization used). Since there is no efficient tool that can fulfill the tasks mentioned in Task Characterization, we chose to compare iQUANT with the widelyused line charts in the stock market. In both designs, the time series of financial factors and stock prices were displayed. Users were required to complete multiple tasks involving the selection of factors and stocks for investment, which corresponded well with T1∼T3 in Task Characterization. In total, 14 expert quantitative traders were recruited as subjects, who were paid for their valuable time. We applied a between-subject design that subjects were randomly assigned to two groups of equal size. The seven subjects in each group will complete all the tasks with only one visualization, iQUANT or baseline. The experiment was composed of two sessions. In the first training session, subjects were presented with a tutorial and allowed a trial usage of iQUANT to be tested. In the subsequent formal test session, each subject took seven tasks. To alleviate the influence of financial data on the result and increase statistical power, we presented two stock data. Each subject was required to complete all the tasks with both data using the tested visualization. The full documents of the experiment design are provided in Appendix A. The user tasks can be categorized into four groups.
Q1/Q2 (factor selection by contribution): Among all the factors, which factors have the highest positive/negative contribution to the return of a particular stock during a given time period? Q3 (factor selection by stability): Among all the factors, which factors have the highest unstable contribution to the return of a particular stock during a given time period? Q4/Q5 (factor selection by contribution to a sector of stocks): Among all the factors, which factors have the highest positive/negative contribution to the return of all stocks in a particular sector during a given time period? Q6/Q7 (stock selection by factor contribution): Among all the stocks in a given sector, whose return receive the highest positive/negative contribution from a selected factor during a given time period?
For each question, the subject was required to provide 1 ∼ 2 answers. Because of the between-subject design, subjects were not asked for their subjective rating on each visualization (no within-subject comparison). Instead, we collected their verbal feedback on the pros and cons of each design.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Result and Analysis<h2>text</h2>We compute the accuracy of each user's response to a question by comparing them with top-2 answers. The overall accuracy is measured by the number of correct answers to each question. In Figure 4, we depict the accuracy and completion time of four groups of seven questions. Because both accuracy and time do not follow normal distributions, we apply the Mann-Whitney test to analyze.
On task accuracy (Figure 4(a)), the result reveals that, for factor selection by contribution (Q1/Q2), by stability (Q3), and stock selection by factor contribution (Q6/Q7), the accuracy of iQUANT is significantly higher than the baseline design: U = 324.0, p = .024 for Q1/Q2; U = 30.0, p = .001 for Q3; U = 207.0, p < .001 for Q6/Q7. The average task accuracies are 1.14±0.14 (iQUANT) and 0.96±0.07 (baseline) for Q1/Q2; 1.29±0.27 (iQUANT) and 0.43±0.3 (baseline) for Q3; 1.36±0.19 (iQUANT) and 0.82±0.15 (baseline) for Q6/Q7. For factor selection in a sector (Q4/Q5), the accuracy difference between the iQUANT and baseline is not significant (U = 364.0, p = .15), though the average accuracy of iQUANT (1.0±0.0) is higher than the baseline (0.93±0.1).
On task completion time (Figure 4(b)), the result reveals that, for factor selection by stability (Q3), the completion time using iQUANT is significantly shorter than the baseline design (U = 30.5, p = .002). The average completion times are 27.2s±3.8s (iQUANT) and 50.9s±12.8s (baseline). For factor selection by contribution (Q1/Q2), in a sector of stocks (Q4/Q5), and stock selection by factor contribution (Q6/Q7), the completion time using iQUANT is not significantly shorter than the baseline design: U = 359.0, p = .58 for Q1/Q2; U = 367.0, p = .68 for Q4/Q5; U = 342.0, p = .41 for Q6/Q7. The average completion times are 17.1s±3.4s (iQUANT) and 16.2±3.6s (baseline) for Q1/Q2; 15.9s±3.4s (iQUANT) and 15.9s±4.5s (baseline) for Q4/Q5; 20.4s±3.6s (iQUANT) and 29.3s±9.0s (baseline) for Q6/Q7.
The study result indicates that, on most tasks, iQUANT leads to higher task accuracy than the baseline design. The only exception happens on the factor selection task for a sector of stocks (Q4/Q5). It is found that in the stock data used for these two questions, the factor having the highest positive/negative contribution is quite separated from all the other factors. Hence, only the top-1 factor is used as the correct answer, and in both designs, subjects could easily identify this factor (a 100% accuracy for iQUANT). For similarly easy tasks (Q1/Q2 and Q6/Q7), though the baseline design in most case helps to identify one (but only one) top factor, the accuracy of iQUANT is significantly higher because subjects could quickly discover more top factors in comparison to the baseline design. For the hard task of selecting factors by stability (Q3), the advantage of iQUANT in accuracy becomes the highest. Meanwhile, on completion time, iQUANT is favored only in the hard task of Q3, for which the iQUANT design can better support temporal comparison and analytics of factor contribution. For the other tasks, the completion time is similar in both iQUANT and baseline design.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_5', 'fig_5', 'fig_5']<h2>table_ref</h2>[]<h2>heading</h2>Expert Feedback<h2>text</h2>In the user experiment, experts also provided valuable feedback about several aspects of iQUANT . On positive aspects, some experts stressed that iQUANT presented an effective way to visually compare salient factors among different sectors of the market. In practice, the factor constituent and dynamics are truly different across sectors, which is of great importance to factor investing. As supported by the study result, experts also found iQUANT to be useful in analyzing the temporal dimension of factors, as well as the relationship between factors and stocks, which are exactly our design goal. Overall, subjects reported the iQUANT interface to be user-friendly, as our design was similar to their familiar and popular tools. Some experts even commented that our design could be part of standard stock market analysis tools (e.g., the work from Bloomberg [SB13]).
On the other hand, it was suggested that the interface could incorporate more statistical information according to the industry standard (e.g., the change of stock holdings in the backtesting view). The Voronoi map for top-5 factors could be reduced to only show the top-3 or less, in order to better reveal the most salient signal for the stock market. More detailed visual encoding could be applied to distinguish the factors of the same type, which are now depicted in the same color. Notably, we thought the complexity of the interface could raise learning issues. During the study, most subjects could be well prepared after a 10-minute training session, as indicated by the high task accuracy in applying iQUANT.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Case Study<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Selection of Investment Time<h2>text</h2>We first invited a fund manager to select the best investment time for individual stock with iQUANT. He looked at the year of 2016.2-2017.1 when the market index rose by 15.7%. From a list of candidates, the manager examined each stock until a feasible investment time for excess return is identified.
The manager reported stock in the Computer sector (code: 002351.SZ) for which the composite index of the whole sector rose by 2.5%. In his analysis, the Lasso model was selected. As shown in Figure 5(a1), the initial factor view displays a summary of important factors recommended by the Lasso model. In the factor list view (Figure 5(a2)), the manager identified several useful factors having large contribution to the stock return. He hovered these factors one by one to examine their performance in the given period. The AM factor (asset-to-market ratio) was chosen because its contributions are stable in the first six months: all negative to the rise of stock prices. There are two months when the factor sen- sitivity is high, as indicated by opaque colors. By this one-factor model, the investment return (red curve in Figure 5(a3)) surpasses the stock price change (blue curve) by 26.5% at the end of the period. A maximal return of 11% can also be forecast for the next three months, as shown by the grey dotted curve in the backtesting view of Figure 5(a4). Notably, the manager suggested applying the model from June when the AM factor has been stable for four months and enjoys a high sensitivity in the latest month (May). In case this strategy was adopted, an excess return of 39.1% could be earned in June when the overall market only rose by 1.6%.
To optimize the investment for the whole year, the manager explored other factors that can characterize the price change in the second half of the year. He found that the volumed factor was complementary to AM, as the volumed factor contributed to the model mainly from July (Figure 5(b1)). By applying a AM+volumed twofactor model, a return of 81.8% can be obtained by the end of the period, which is 14.5% more than the one-factor model (Figure 5(b3)). Notably, the predicted return for the next three months increases to 40.9% with the new model (Figure 5(b4)).<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_6', 'fig_6', 'fig_6', 'fig_6', 'fig_6', 'fig_6']<h2>table_ref</h2>[]<h2>heading</h2>Selection of Stock Portfolio<h2>text</h2>In the second scenario, we invited the fund manager to select a portfolio of stocks to invest from an initial stock pool in a given time period. The sector-based division was adopted. The manager analyzed every candidate sector, and combined the selected stocks in each sector into a final portfolio for investment. The year of 2015 was considered when the market experienced a spike with the maximal index climbing as high as 60% in the first six months. The yearly growth was 9.4% after the downturn in the second half of the year. It is thus an appropriate scenario to test whether our tool can improve factor investing even in rapidly changing markets.
The manager first studied the Chemical sector which had a similar trend to the market in 2015. The factor view in Figure 2(b) visualizes an initial collection of factors using the ELNetApp model. It can be seen in the factor view that many factors are quite stable over time. Different from the study on individual stocks, for portfolio construction, the manager would like to select both valuable factors and a set of stocks whose selected factors behave in a similarly stable and effective way. Our tool provides support with an interactive selection process. Users can click on each important factor in the factor list (Figure 2(d)) and scroll in the stock view (Figure 2(c)) to identify candidate stocks. By this process, the manager found factor lagretn, which had a moderate overall contribution to stock returns of the sector (Figure 2(d)) and the contribution was stable on several individual stocks. As shown in Figure 6(a), the selected stocks share similar patterns on lagretn. When the stock price started to rise again from September (notice red/green bars in Figure 2(c)), the contribution of lagretn reversed to negative and stayed for few months, as shown by the connecting lines in Figure 6(a). It is found in Figure 6(b) that five stocks (red lines) sharing this pattern on lagretn receive excess returns above the sector average (blue line). The prediction performance in Figure 6(c) (grey dotted line) also surpasses the market average (blue dotted line).
The manager proceeded to interactively examine all the five selected stocks. In the factor list view, he checked again all the candidate factors and found the factor std dvol, which had a favorable pattern as the factor lagretn on all the selected stocks. After applying the two factors together (Figure 2(d)), the stock returns were re-computed and updated in Figure 2(e). One stock appeared to receive less return after adding the second factor, while all the other four stocks remained superior to the sector average. In the backtesting view (Figure 2(f)), the final portfolio of two factors and four stocks (red lines) turned out to be better than the single-factor portfolio (grey lines) in both the backtest performance in 2015 (solid lines) and an outlook in the first three months of 2016 (dotted lines).
In another study, we asked the manager to work in the Household Appliances sector in the same year of 2015. By repeating a similar process, he was able to build a two-factor model of size+retnmax. Three stocks having stable contributions were selected. The portfolio returns in both 2015 and 2016 (outlook) are better than the market average. We omit screenshots due to the space limit.
By comparing the visual analysis result in the above two sectors, it is noticed that the Appliance sector is almost exclusively influenced by the transaction factors, while the Chemical sector is influenced by transaction factors and other types of factors such as momentum, value, and growth (Figure 2(d)). We hypothesize that because of their relationship, the Chemical sector is not only affected by transactions in the stock market but also linked to the price of crude oil, which is independently traded. <h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2', 'fig_2', 'fig_2', 'fig_2', 'fig_7', 'fig_2', 'fig_7', 'fig_7', 'fig_7', 'fig_2', 'fig_2', 'fig_2', 'fig_2']<h2>table_ref</h2>[]<h2>heading</h2>Discussion<h2>text</h2>The sparse regression model and its flexibility. In our system, users routinely start from an initial set of factors recommended by the build-in sparse regression model, which potentially limits the flexibility of factor investing by iQUANT . This issue is alleviated from three directions: in the factor list view (Figure 2(d)), users can manually select any set of factors as the starting point of analysis; the sparsity parameter of the regression model (λ ) can be tuned in the backend to offer more or less initial factors; finally, the iQUANT system is fully compatible with any other sparse models or a combination of feature selection and prediction models.
The joint human-model approach and its advantage. We have mentioned in previous sections, the model-only approach in factor selection suffers from the interpretability issue. Traders need to understand the factor before applied in practice. Also, a human is better in a heuristic search of the best combination within the vast factor space. More importantly, in the financial domain, there is no model that works in the long term. It will be critical to have human stand by models, and would be better to early adjust accordingly.
The visual design and its learning curve. Notice that stock traders may not have a strong background in visualization. Thus, we tried to strike a balance between the intuitiveness and expressiveness of visualization. On one hand, classical designs such as bar charts and line charts are employed in iQUANT , due to their intuitiveness and users' familiarity. On the other hand, our designs are tailored to the target users. For example in the factor view, green/red are used to indicate the decline/rise of stock prices, according to the convention of our target market. The effectiveness of iQUANT design is confirmed by the user study result and expert feedback. Most subjects found iQUANT easy to learn and understand.
The user study and its design choices. We evaluated iQUANT through a formal user study focusing on the performance of factor/stock selection. The study would have accounted for more stages of iQUANT by looking at the final stock returns, or the accuracy in predicting. We stayed in the scope of evaluating factors for two reasons. First, the stock return by an interactive factor investing system is determined in complex interactions among factor/stock selection, predictive model, and trading strategy. A higher stock return in the study may or may not be directly correlated with a better factor selection design. Second, there is currently no com-mercial tool that supports end-to-end interactive factor investing. We compare iQUANT with a baseline design, which is not natively integrated with prediction models and trading strategies.
The deployment of iQUANT and its extensibility. The investment industry is highly closed as the winning secret can be too valuable to disclose. This closedness prohibits a more practical approach to directly optimize deployed tools. Instead, we developed a homegrown interactive factor investing pipeline and an end-to-end system. Because of the complexity of quantitative investment, we mainly focus on improving the factor selection in the pipeline. We demonstrated the effectiveness of iQUANT by using the 20-year real stock market data. However, our system can also be deployed to take in real-time data. As a next step, we plan to customize and integrate iQUANT into the production system of our industrial collaborators, in the hope of making real impacts on the industry.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>[]<h2>heading</h2>Conclusion<h2>text</h2>This paper presents iQUANT, a visual analytics system that seamlessly integrates algorithmic and user-steered feature selection methods for interactive factor investing. On the analytics side, we explore the use of sparse regression models that jointly optimizing factor selection and stock prediction processes. On the visualization side, we propose multiple coordinated visual designs to comprehensively illustrate importance metrics of each factor, including its positive/negative contribution to models, the factor stability and sensitivity. On top of the prediction model and visualization design, iQUANT develops an interactive framework that encloses factor refinement and portfolio construction in the same visual analytics loop. With iQUANT , traders can optimize their investment strategy and evaluate them in industry-standard backtesting. We demonstrate the value of iQUANT through one formal user study and two case studies, using a real stock market dataset recording the stock data in the past 30 years. On the selection of investment time and stock portfolio, our domain experts achieved excess returns from both individual stock and sector-based portfolios.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>The interaction of value and momentum strategies<h2>journal</h2>Financial Analysts Journal<h2>year</h2>1997<h2>authors</h2>C S Asness<h2>ref_id</h2>b1<h2>title</h2>ifeed: Interactive feature extraction for engineering design<h2>journal</h2>American Society of Mechanical Engineers<h2>year</h2>2016<h2>authors</h2>H Bang; D Selva<h2>ref_id</h2>b2<h2>title</h2>Stacked graphs -geometry & aesthetics<h2>journal</h2>IEEE Transactions on Visualization and Computer Graphics<h2>year</h2>2008<h2>authors</h2>Byron L Wattenberg; M <h2>ref_id</h2>b3<h2>title</h2>Machine Learning for Factor Investing: R Version<h2>journal</h2>CRC Press<h2>year</h2>2020<h2>authors</h2>G Coqueret; T Guida<h2>ref_id</h2>b4<h2>title</h2><h2>journal</h2><h2>year</h2><h2>authors</h2> Cgk<h2>ref_id</h2>b5<h2>title</h2>Wirevis: Visualization of categorical, time-varying data from financial transactions<h2>journal</h2><h2>year</h2>2007-10<h2>authors</h2> Chang R; M Ghoniem;  Kosara R; W Ribarsky; J Yang; E Suma; C Ziemkiewicz; D Kern; A Sudjianto<h2>ref_id</h2>b6<h2>title</h2><h2>journal</h2><h2>year</h2><h2>authors</h2> Chl<h2>ref_id</h2>b7<h2>title</h2>Fundexplorer: Supporting the diversification of mutual fund portfolios using context treemaps<h2>journal</h2><h2>year</h2>2003-10<h2>authors</h2>C Csallner; M Handte; Stasko J Lehmann O<h2>ref_id</h2>b8<h2>title</h2>Featureminer: A tool for interactive feature selection<h2>journal</h2>ACM<h2>year</h2>2016<h2>authors</h2>K Cheng; J Li; H Liu<h2>ref_id</h2>b9<h2>title</h2>Nasdaq velocity and forces: An interactive visualization of activity and change<h2>journal</h2>Journal of Universal Computer Science<h2>year</h2>2008<h2>authors</h2>H T Dao; A L Bazinet;  Berthier R; B Shneiderman<h2>ref_id</h2>b10<h2>title</h2>Financevis.net-a visual survey of financial data visualizations<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>M Dumas; M J Mcguffin; V L Lemieux<h2>ref_id</h2>b11<h2>title</h2>Returns to contrarian investment strategies: Tests of naive expectations hypotheses<h2>journal</h2>Journal of Financial Economics<h2>year</h2>1997<h2>authors</h2>P M Dechow;  G Sloan R<h2>ref_id</h2>b12<h2>title</h2><h2>journal</h2><h2>year</h2><h2>authors</h2> Dvvh<h2>ref_id</h2>b13<h2>title</h2>Regressionexplorer: Interactive exploration of logistic regression models with subgroup analysis<h2>journal</h2>IEEE Transactions on Visualization and Computer Graphics<h2>year</h2>2019<h2>authors</h2>D Dingen; M Van't Veer; P Houthuizen; E H J Me-Strom; E H Korsten; A R Bouwman;  Van Wijk J<h2>ref_id</h2>b14<h2>title</h2>Visual analytics for topic model optimization based on user-steerable speculative execution<h2>journal</h2>IEEE Transactions on Visualization and Computer Graphics<h2>year</h2>2019<h2>authors</h2> El-Assady M; F Sperrle; O Deussen; D Keim; C Collins<h2>ref_id</h2>b15<h2>title</h2>The cross-section of expected stock returns<h2>journal</h2>The Journal of Finance<h2>year</h2>1992<h2>authors</h2>E F Fama; K R French<h2>ref_id</h2>b16<h2>title</h2>The capital asset pricing model: Theory and evidence<h2>journal</h2>Journal of Economic Perspectives<h2>year</h2>2004<h2>authors</h2>E F Fama; K R French<h2>ref_id</h2>b17<h2>title</h2>An introduction to variable and feature selection<h2>journal</h2>The Journal of Machine Learning Research<h2>year</h2>2003<h2>authors</h2>I Guyon; A Elisseeff<h2>ref_id</h2>b18<h2>title</h2>Multi-Asset Risk Modeling: Techniques for a Global Economy in an Electronic and Algorithmic Trading Era<h2>journal</h2>Academic Press<h2>year</h2>2013<h2>authors</h2>M Glantz;  Kissell R<h2>ref_id</h2>b19<h2>title</h2>Themeriver: visualizing theme changes over time<h2>journal</h2><h2>year</h2>2000-10<h2>authors</h2>S Havre; B Hetzler; L Nowell<h2>ref_id</h2>b20<h2>title</h2>Interactive dimensionality reduction through user-defined combinations of quality metrics<h2>journal</h2>IEEE Transactions on Visualization and Computer Graphics<h2>year</h2>2009<h2>authors</h2>S Johansson; J Johansson<h2>ref_id</h2>b21<h2>title</h2>A rank-by-feature framework for unsupervised multidimensional data exploration using low dimensional projections<h2>journal</h2><h2>year</h2>2004-10<h2>authors</h2> Seo;  Shneiderman B<h2>ref_id</h2>b22<h2>title</h2>A survey on visual analysis approaches for financial data<h2>journal</h2>Computer Graphics Forum<h2>year</h2>2016<h2>authors</h2>S Ko; I Cho; S Afzal; C Yau; Chae J Malik; A Beck; K Jang; Y Ribarsky; W Ebert; D S <h2>ref_id</h2>b23<h2>title</h2>A spectral visualization system for analyzing financial time series data<h2>journal</h2>Eurographics Association<h2>year</h2>2006<h2>authors</h2>D A Keim; T Nietzschmann; N Schelwies; J Schnei-Dewind; T Schreck; H Ziegler<h2>ref_id</h2>b24<h2>title</h2>Infuse: Interactive feature selection for predictive modeling of high dimensional data<h2>journal</h2>IEEE Transactions on Visualization and Computer Graphics<h2>year</h2>2014<h2>authors</h2>J Krause; A Perer; E Bertini<h2>ref_id</h2>b25<h2>title</h2>Interacting with predictions: Visual inspection of black-box machine learning models<h2>journal</h2>ACM<h2>year</h2>2016<h2>authors</h2>J Krause; A Perer; K Ng<h2>ref_id</h2>b26<h2>title</h2><h2>journal</h2><h2>year</h2><h2>authors</h2> Lcw<h2>ref_id</h2>b27<h2>title</h2>Feature selection: A data perspective<h2>journal</h2>ACM Computing Surveys<h2>year</h2>2017<h2>authors</h2>Li J Cheng; K Wang; S Morstatter; F P Trevino R; J Tang; H Liu<h2>ref_id</h2>b28<h2>title</h2>The valuation of risk assets and the selection of risky investments in stock portfolios and capital budgets<h2>journal</h2>The Review of Economics and Statistics<h2>year</h2>1965<h2>authors</h2>J Lintner<h2>ref_id</h2>b29<h2>title</h2>Taxthemis: Interactive mining and exploration of suspicious tax evasion groups<h2>journal</h2>IEEE Transactions on Visualization and Computer Graphics<h2>year</h2>2020<h2>authors</h2>Y Lin; K Wong; Y Wang;  Zhang R; B Dong; H Qu; Q Zheng<h2>ref_id</h2>b30<h2>title</h2><h2>journal</h2><h2>year</h2><h2>authors</h2> Lxl<h2>ref_id</h2>b31<h2>title</h2>Visual diagnosis of tree boosting methods<h2>journal</h2>IEEE Transactions on Visualization and Computer Graphics<h2>year</h2>2018<h2>authors</h2>S Liu; J Xiao; J Liu; X Wang; J Wu; J Zhu<h2>ref_id</h2>b32<h2>title</h2>The rise of factor investing<h2>journal</h2>BlackRock<h2>year</h2>2016<h2>authors</h2>M Mccombe; A Ang; K Hogan; Y Gelfand<h2>ref_id</h2>b33<h2>title</h2><h2>journal</h2><h2>year</h2><h2>authors</h2> Mbd<h2>ref_id</h2>b34<h2>title</h2>Guiding feature subset selection with an interactive visualization<h2>journal</h2><h2>year</h2>2011-10<h2>authors</h2>May T Bannach; A Davey; J Ruppert; T Kohlhammer; J <h2>ref_id</h2>b35<h2>title</h2>Rulematrix: visualizing and understanding classifiers with rules<h2>journal</h2>IEEE Transactions on Visualization and Computer Graphics<h2>year</h2>2019<h2>authors</h2>Ming Y Qu; H Bertini; E <h2>ref_id</h2>b36<h2>title</h2>Finding trading patterns in stock market data<h2>journal</h2>IEEE Computer Graphics and Applications<h2>year</h2>2004<h2>authors</h2>K V Nesbitt; S Barrass<h2>ref_id</h2>b37<h2>title</h2>Rankbooster: Visual © 2021 The Author(s) Computer Graphics Forum<h2>journal</h2>The Eurographics Association and John Wiley & Sons Ltd<h2>year</h2>2021<h2>authors</h2>A Puri; B K Ku; Y Wang; H Qu<h2>ref_id</h2>b38<h2>title</h2>iQUANT: Interactive Quantitative Investment Using Sparse Regression Factors analysis of ranking predictions<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Xuanwu Yue<h2>ref_id</h2>b39<h2>title</h2>Law and finance<h2>journal</h2>Journal of Political Economy<h2>year</h2>1998<h2>authors</h2> L Porta R;  Lopez-De; F Silanes; A Shleifer; R W Vishny<h2>ref_id</h2>b40<h2>title</h2>Visualizing financial data<h2>journal</h2>Wiley Online Library<h2>year</h2>2016<h2>authors</h2>J Rodriguez; P Kaczmarek; D Depew<h2>ref_id</h2>b41<h2>title</h2>Finvis: Applied visual analytics for personal financial planning<h2>journal</h2><h2>year</h2>2009-10<h2>authors</h2>Rudolph S Savikhin; A Ebert; D S <h2>ref_id</h2>b42<h2>title</h2>Financial visualization case study: Correlating financial timeseries and discrete events to support investment decisions<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>E Sorenson;  Brath R<h2>ref_id</h2>b43<h2>title</h2>Visual cluster analysis of trajectory data with interactive kohonen maps<h2>journal</h2><h2>year</h2>2008-10<h2>authors</h2>T Schreck; J Bernard; T Tekusova; J Kohlhammer<h2>ref_id</h2>b44<h2>title</h2>Capital asset prices: A theory of market equilibrium under conditions of risk<h2>journal</h2>The Journal of Finance<h2>year</h2>1964<h2>authors</h2>W F Sharpe<h2>ref_id</h2>b45<h2>title</h2>Visualization of stock market charts<h2>journal</h2><h2>year</h2>2003<h2>authors</h2>K Šimuni<h2>ref_id</h2>b46<h2>title</h2>An experimental study of financial portfolio selection with visual analytics for decision support<h2>journal</h2><h2>year</h2>2011<h2>authors</h2>A Savikhin; H C Lam; B Fisher; D S Ebert<h2>ref_id</h2>b47<h2>title</h2>Trajectory-based visual analysis of large financial time series data<h2>journal</h2>ACM SIGKDD Explorations Newsletter<h2>year</h2>2007<h2>authors</h2>T Schreck; T Tekušová; J Kohlhammer; D Fellner<h2>ref_id</h2>b48<h2>title</h2>Business information visualization<h2>journal</h2>Communications of the Association for Information Systems<h2>year</h2>1999<h2>authors</h2>D P Tegarden<h2>ref_id</h2>b49<h2>title</h2>Regression shrinkage and selection via the lasso<h2>journal</h2>Journal of the Royal Statistical Society: Series B (Methodological)<h2>year</h2>1996<h2>authors</h2> Tibshirani R<h2>ref_id</h2>b50<h2>title</h2>Tradao: A visual analytics system for trading algorithm optimization<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>K W Tsang; H Li; F M Lam; Y Mu; Y Wang; H Qu<h2>ref_id</h2>b51<h2>title</h2>Visualizing time-dependent data in multivariate hierarchic plots -design and evaluation of an economic application<h2>journal</h2><h2>year</h2>2008-07<h2>authors</h2>T Tekusova; T Schreck<h2>ref_id</h2>b52<h2>title</h2>Lasso with cross-validation for genomic selection<h2>journal</h2>Genetics Research<h2>year</h2>2009<h2>authors</h2>M G Usai; M E Goddard; B J Hayes<h2>ref_id</h2>b53<h2>title</h2>Cluster and calendar based visualization of time series data<h2>journal</h2><h2>year</h2>1999-10<h2>authors</h2>J J Van Wijk;  R Van Selow E<h2>ref_id</h2>b54<h2>title</h2><h2>journal</h2><h2>year</h2><h2>authors</h2> Ybl<h2>ref_id</h2>b55<h2>title</h2>sportfolio: Stratified visual analysis of stock portfolios<h2>journal</h2>IEEE Transactions on Visualization and Computer Graphics<h2>year</h2>2019<h2>authors</h2>X Yue; J Bai; Q Liu; Y Tang; A Puri; K Li; H Qu<h2>ref_id</h2>b56<h2>title</h2><h2>journal</h2><h2>year</h2><h2>authors</h2> Ysz<h2>ref_id</h2>b57<h2>title</h2>Bitextract: Interactive visualization for extracting bitcoin exchange intelligence<h2>journal</h2>IEEE Transactions on Visualization and Computer Graphics<h2>year</h2>2019<h2>authors</h2>X Yue; X Shu; X Zhu; X Du; Z Yu; D Papadopoulos; S Liu<h2>ref_id</h2>b58<h2>title</h2>Regularization and variable selection via the elastic net<h2>journal</h2>Journal of the Royal Statistical Society. Series B (Statistical Methodology)<h2>year</h2>2005<h2>authors</h2>H Zou; T Hastie<h2>ref_id</h2>b59<h2>title</h2>Visual market sector analysis for financial time series data<h2>journal</h2><h2>year</h2>2010-10<h2>authors</h2>H Ziegler; Jenny M Gruse; T Keim; D A <h2>ref_id</h2>b60<h2>title</h2>Relevance driven visualization of financial performance measures<h2>journal</h2><h2>year</h2>2007<h2>authors</h2>H Ziegler; T Nietzschmann; D A Keim<h2>ref_id</h2>b61<h2>title</h2>iforest: Interpreting random forests via visual analytics<h2>journal</h2>IEEE Transactions on Visualization and Computer Graphics<h2>year</h2>2019<h2>authors</h2>X Zhao; Y Wu; D L Lee; W Cui<h1>figures</h1><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>combines interactive feature selection by allowing users to explore the dependencies between features and algorithms. INFUSE [KPB14] integrates feature selection with prediction models and a glyph-based design to inform users about the feature performance with respect to the prediction performance. iFEED [BS16] and FeatureMiner [CLL16] visualize the feature performance by line charts, bar charts and scatter plots. Prospector [KPN16] applies partial dependence diagnostics to analyze the correlation among features, data values, and prediction results. Re-gressionExplorer [DvVH * 19] combines feature space visualization of regression models with a dynamic subgroup of data analysis.<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Figure 1 :1Figure 1: iQUANT system pipeline composed of two modules: the sparse regression model for feature selection and prediction, and the visualization of factors and stocks for factor selection and portfolio construction.<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Figure 2 :2Figure 2: Interactive selection of four stocks from the Chemical sector in 2015 using a two-factor model (lagretn+std dvol): (a) control panel; (b) factor view; (c) stock view by sectors; (d) factor list for selection; (e) stock return view for portfolio construction; (f) backtesting view for evaluation of returns.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>(b)) is designed to provide traders with a quick overview of financial factors relevant to the selected stocks and time period. These factors are initially recommended by sparse models in iQUANT (T1). The model measures the importance of each factor by its weight, value, or contribution to the prediction of stock returns. By default, traders first look at the contribution measure which covers both its current value and the overall weight. The contribution can be either positive or negative depending on the relationship between the factor and stock returns. According to our expert study, they also consider several other metrics in the factor evaluation and selection process, including the temporal stability of factors and their sensitivity in the model. (T2)<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_4<h2>figure_caption</h2>Figure 3 :3Figure 3: Alternative designs of the circle layout in the factor view: (a) circle packing; (b) layered circle packing.<h2>figure_data</h2><h2>figure_label</h2>4<h2>figure_type</h2>figure<h2>figure_id</h2>fig_5<h2>figure_caption</h2>Figure 4 :4Figure 4: User experiment results: (a) the number of correct answers provided by users in two visualization designs; (b) the task completion time.<h2>figure_data</h2><h2>figure_label</h2>5<h2>figure_type</h2>figure<h2>figure_id</h2>fig_6<h2>figure_caption</h2>XuanwuFigure 5 :5Figure 5: The selection of investment time during 2016.2-2017.1 on 002351.SZ by: (a) a single-factor model (AM); (b) a two-factor model (AM+volumed). The manager switched from a single-factor model to a two-factor model to optimize his investment strategy.<h2>figure_data</h2><h2>figure_label</h2>6<h2>figure_type</h2>figure<h2>figure_id</h2>fig_7<h2>figure_caption</h2>Figure 6 :6Figure 6: Portfolio construction from the Chemical sector using the single factor (lagretn) in 2015: (a) lagretn factor contributed similarly to theses three stocks; (b) selected high return stocks for portfolio; (c) backtesting results from the portfolio; (d) a detailed illustration of the Voronoi-based glyph.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2><h2>figure_caption</h2><h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_0<h2>figure_caption</h2>Meanwhile, versatile visualizations beyond classical charts have also been developed to cope with the complexity of modern financial data, e.g., Growth Matrix [KNS * 06, ZNK07], wedge charts [DBBS08], and 3D visualizations [TS08, NB04].<h2>figure_data</h2>07] appliedmultiple coordinated views with enhanced line charts and matrixdesigns to illustrate the time-varying wire transactions in banking.Ziegler et al. [ZJGK10] designed color-coded bars to display rela-tive changes in asset prices over time. Sorenson and Brath [SB13]<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>+ L + 1] compose a matrix X = (x 1 , • • • , x T+L+1 ) where x i = (x i1 , • • • , x iF ) . The modeling of stock ϒ in Γ takes N steps, corresponding to the trading cycles of τ 1 , • • • , τ N ,<h2>formula_coordinates</h2>[4.0, 312.1, 370.67, 239.1, 31.19]<h2>formula_id</h2>formula_1<h2>formula_text</h2>y (i) = w (i) X (i) + b (i) + ε (i) (1)<h2>formula_coordinates</h2>[4.0, 382.59, 461.65, 168.61, 10.59]<h2>formula_id</h2>formula_2<h2>formula_text</h2>Minimize Lasso NLL = NLL + 1 2 λ [α||w (n) || 1 + (1 -α)||w (n) || 2 2 ]<h2>formula_coordinates</h2>[4.0, 313.28, 495.37, 236.75, 19.99]<h2>formula_id</h2>formula_3<h2>formula_text</h2>(i) 1 , • • • , w (i) F ) denotes the weight vector of financial factors in the fitted model, b (i) is the bias, ε (i) is the error term, ||•|| 1 (|| • || 2 ) is L 1 (L 2<h2>formula_coordinates</h2>[4.0, 312.1, 523.58, 239.1, 36.13]<h2>formula_id</h2>formula_4<h2>formula_text</h2>∑ T +iD k=T +(i-1)D+1 w (i) j x k j .<h2>formula_coordinates</h2>[5.0, 363.15, 520.25, 83.82, 14.52]<h1>doi</h1>10.2469/faj.v53.n2.2069<h1>title</h1>ITRACE: AN IMPLICIT TRUST INFERENCE METHOD FOR TRUST-AWARE COLLABORATIVE FILTERING<h1>authors</h1>Xu He; Bin Liu; Ke-Jia Chen<h1>pub_date</h1>2017-08-15<h1>abstract</h1>The growth of Internet commerce has stimulated the use of collaborative filtering (CF) algorithms as recommender systems. A CF algorithm recommends items of interest to the target user by leveraging the votes given by other similar users. In a standard CF framework, it is assumed that the credibility of every voting user is exactly the same with respect to the target user. This assumption is not satisfied and may lead to misleading recommendations in practice. A natural countermeasure is to design a trust-aware CF algorithm, which can take account of the difference in the credibilities of the voting users when performing CF. To this end, this paper presents a trust inference approach, which can predict the implicit trust of the target user on every voting user from a sparse explicit trust matrix. Then an improved CF algorithm termed iTrace is proposed, which employs both the explicit and the predicted implicit trust to provide recommendations. An empirical evaluation on a public dataset demonstrates that the proposed algorithm provides a significant improvement in recommendation quality in terms of mean absolute error.<h1>sections</h1><h2>heading</h2>INTRODUCTION<h2>text</h2>With the massive growth of the internet and the emergence of electronic commerce over the last decades, recommender system (RecSys) has become an indispensable technique to mitigate the problem of information overload for users. The aim of RecSys is to provide target users with high quality, personalized recommendations, and to help them find items ⋆ Correspondence author: Bin Liu (Email: bins@ieee.org). This work was partly supported by the National Natural Science Foundation (NSF) of China under grant No. 61571238, China Postdoctoral Science Foundation under grant Nos. 2015M580455 and 2016T90483, the Six Talents Peak Foundation of Jiangsu Province under grant No. XYDXXJS-CXTD-006 and the Scientific and Technological Support Project (Society) of Jiangsu Province under grant No. BE2016776.
(e.g., books, movies, news, music, etc.) of interest from a plethora of available choices [1].
Collaborative filtering (CF) seems to be one of the most well-known and commonly used techniques to build a Rec-Sys [2][3][4]. The underlying idea of CF is that users with similar preferences in the past are likely to favor the same items (e.g., books, movies, news, music, etc.) in the future. The CF method is easy to implement. A typical CF method predicts the rating value user u gives to item i as follows [2]:
r u,i = ru + v∈U w(u, v)(r v,i -rv ) v∈U |w(u, v)| ,(1)
where U denotes a set of K neighbors of u who rated item i (also called u ′ s voting users in what follows), ru the average rating of user u for all the items rated by u, and w(u, v) the weight assigned to user v ′ s vote when she recommends items to u. In a standard CF framework, the weight w(u, v) is set as a similarity measure between users u and v, denoted by sim(u, v), and the neighbors of u are those most similar to u who co-rated item i with u. In order to compute the similarity between users, a variety of similarity measures have been proposed, such as Pearson correlation, cosine vector similarity, Spearman correlation, entropy-based uncertainty, and meansquare difference. It is reported that Pearson correlation performs better than the others [5,6]. The Pearson correlation coefficient is used here, defined as follows [3,7] sim
(u, v) = i∈I (r u,i -ru )(r v,i -rv ) i∈I (r u,i -ru ) 2 i∈I (r v,i -rv ) 2 , (2
) where I denotes the set of items that users u and v have corated.
In practical applications, users in general only rate a small portion of items, but accurate recommendations are expected for the cold users who rate only a few items. This raises two inherent obstacles to obtain satisfactory recommending quality, namely data sparsity and cold start [8][9][10][11]. In principle, this is caused by the lack of sufficient and reliable elements in U and/or I to calculate Eqns.( 1) and (2). A possible solution to get around this is to incorporate trust relationships into the CF framework, resulting in the trust based or trust-aware CF (TaCF) [8][9][10][12][13][14]. The underlying intuition supporting the working of trust-aware recommender systems (TaRS) is that users often accept advice from trustworthy friends in real life on topics they are not expert in. So it is reasonable to expect that considering trust relationship among users may bring in benefits in generating recommendations. Furthermore, trust can be propagated over a network of users, hence TaRS can overcome the data sparsity and cold start problems, from which traditional CF methods suffer, at least in concept.
A practical issue to be considered when designing a TaCF algorithm is that the explicit trust information is usually much more sparse than the users' ratings. A trust propagation model along with an effective implicit trust inference method is desirable to overcome the above limitation. To this end, this paper presents an applicable implicit trust inference method, based on which an improved CF algorithm termed iTrace (i.e., Implicit TRust-Aware Collaborative filtEring) is proposed.<h2>publication_ref</h2>['b0', 'b1', 'b2', 'b3', 'b1', 'b4', 'b5', 'b2', 'b6', 'b7', 'b8', 'b9', 'b10', 'b1', 'b7', 'b8', 'b9', 'b11', 'b12', 'b13']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>THE PROPOSED ITRACE ALGORITHM<h2>text</h2>In this section, we present the iTrace algorithm followed by an analysis of its connections to existent related work.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Algorithm Design<h2>text</h2>An architecture of the iTrace algorithm is shown in Fig. 1. The inputs include an N × N explicit trust matrix and an N × M rating matrix, denoted in what follows by T e and R, respectively. N and M denote the numbers of the users and of the items, respectively. In contrast with a standard CF algorithm, iTrace leverages much more information except user similarity for prediction of user rating. Such additional information is represented by a trust matrix, denoted by T in what follows, which is estimated by an implicit trust inference module that takes T e as input. The details on the implicit trust inference module are presented in Sec.2.2. A working flow of the iTrace algorithm for predicting user u's rating value on item i is summarized as follows.
1. Calculate the similarity metrics between u and the other users who rated i using Eqn.(2).
2. Select the top K users who are most similar to u as u's voting users.
3. Estimate the trust of u on every voting user, using the implicit trust inference method presented in Sec. 2.2.
4. Predict user u's rating value on item i as follows
r u,i = ru + v∈U f (sim(u, v), t(u, v))(r v,i -rv ) v∈U |f (sim(u, v), t(u, v))| ,(3)
where t(u, v) is the estimated trust of u on v, obtained from Step 3. The function f plays a role of integrating user similarity and trust in rating prediction.
We consider two different forms of the function f in our algorithm. The first one, termed incremental weighting (IW) here, is specified as follows
f (sim(u, v), t(u, v)) = sim(u, v) t(u, v) j∈U sim(u, j) t(u, j) .(4)
The standard CF framework corresponds to a special case in which t(u, i) = t(u, j) for ∀i, j ∈ U . The other form of f under consideration, termed linear weighting (LW) here, is
f (sim(u, v), t(u, v)) = αsim(u, v) j∈U sim(u, j) + (1 -α) t(u, v) j∈U t(u, j) ,(5)
where 0 ≤ α ≤ 1 denotes the linear weighting coefficient. The standard CF algorithm then corresponds to the case in which α = 1. Through the analysis on Eqns.( 4) and ( 5), we see that the standard CF framework totally neglects the impact of the user trust. We show that in this paper, by taking into account of user trust, the iTrace algorithm can provide more accurate recommendations compared with the standard CF method. Through evaluation on a public dataset, we also demonstrate that Eqn.( 4) is preferable to Eqn. (5) in Sec.3.
The design of the implicit trust inference procedure, which is involved at Step 3 as shown above, creates a difference between the proposed iTrace algorithm and the other existing TaCF methods. The connections to related work in the literature are presented in Sec.2.3. We describe in detail the implicit trust inference procedure in the next subsection.<h2>publication_ref</h2>['b4']<h2>figure_ref</h2>['fig_0']<h2>table_ref</h2>[]<h2>heading</h2>Implicit Trust Inference<h2>text</h2>This module takes as input an N × N sparse explicit trust matrix T e , and exploits trust propagation in order to predict, Fig. 2: 4 typical example cases under consideration for predicting user u's trust on user v. A solid line with an arrow pointing from i to j is associated with the event that t e (i, j) takes value 1. A dotted line with an arrow pointing from i to j indicates a missing value of t e (i, j) and that there exists an implicit trust of i on j, which can be inferred from T e . The lightning symbol in the 4th sub-figure indicates a cut-off of the trust information flow.
for every user, how much she could trust every other user. In this way, it outputs an estimated trust matrix T , the (i, j)th cell t(i, j) (if present) of which represents how much the ith user trusts the jth user. The input matrix T e has a very limited number of cells that take value 1 and all the other cells are empty with missing values. If the (i, j)th cell of T e , denoted by t e (i, j), takes value 1, it means that user i has expressed a trust statement that she trusts user j. It is worth noting that, in practical applications, the available trust data, represented as matrix T e here, would always be very sparse. This is so because no user can reasonably interact with every other user and then issue a trust statement about them.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Four categories of trust information flow patterns<h2>text</h2>We categorize the patterns of the trust information flow from user u to v into 4 complementary classes. For each class, we show a typical example in Fig. 2. The 1st sub-figure corresponds to the case in which t e (u, v) = 1. If t e (i, j) = 1, then j is called an explicit trustee of i. The 2nd sub-figure exemplifies the case in which v is not an explicit trustee of u but u and v have common explicit trustee(s). The 3rd sub-figure exemplifies the case in which v is not an explicit trustee of u, u and v have no common explicit trustee but there is at least one trust propagation path from u to v. A trust propagation path from i to j is defined by a series of user pairs {p m , p m+1 }, m = 1, . . . , M -1, M ∈ N, which satisfies p 1 = i, p M = j, and t(p m , p m+1 ) > 0, ∀m ∈ {1, . . . , M -1}. This model indicates that, if user i trusts another user k to some extent and user k trusts user j to some extent, then there will be a trust propagation path from i to j. Note that this model conforms to the transitivity property of the concept of trust [9,15,16]. The 4th sub-figure is associated with the case that there is no trust propagation path from u to v.<h2>publication_ref</h2>['b8', 'b14', 'b15']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Trust inference procedures<h2>text</h2>First we initialize T to be a zero matrix. Given a pair of users, u and v, we first determine which one of the 4 categories presented above the pattern of the trust information flow from u to v belongs to. If it belongs to the 1st category, set t(u, v) = 1. If it belongs to the 2nd category, we estimate t(u, v) as follows
t(u, v) = |S(u) S(v)| |S(u) S(v)| ,(6)
where S(i) denotes the set of i's explicit trustees. For the case shown in the 2nd sub-figure of Fig. 2, we then have t(u, v) = 2/3. Now we focus on the case in which the pattern of the trust information flow from u to v belongs to the 3rd category. We consider every user pair {i, j} that belongs to any one of the aforementioned two categories and estimate t(i, j) correspondingly. Then we exploit trust propagation over the trust network defined by T to estimate t for user pairs associated with the 3rd category. We treat the trust matrix T as a weighted directed graph G, in which the nodes denote the users and the weight of an edge denotes the trust of the starting vertex on the end vertex. For a pair of users, say u and v, there may be multiple paths originating from u and ending at v, as exemplified in the 3rd sub-figure of Fig. 2. To compute t(u, v), we first build up a reciprocal trust matrix Tr , whose (i, j)th cell tr (i, j) = 1/ t(i, j) if t(i, j) > 0; otherwise, set tr (i, j) = ∞. We treat the reciprocal trust matrix as a weighted directed graph G r . An exemplary show of the transformation from G to G r is presented in Fig. 3. Then we consider a shortest path problem [17], which aims to find the shortest path from u to v, denoted by SP r (u, v), in G r . The Dijkstra's algorithm [18] is employed here to find SP r (u, v).
Then we set t(u, v) as follows
t(u, v) = 1 M × L(SP r (u, v)) ,(7)
where L(•) and M denote the length of a path and the number of nodes except the starting node included in the shortest path, respectively. For the sake of clarity, consider the case shown in the right graph of Fig. 3, for which the shortest path from u
to v, SP r (u, v), is u → k → v, L(SP r (u, v)) = tr (u, k) + tr (k, v) = 4, M = 2 and thus t(u, v) = 1/8.
If the pattern of the trust information flow from u to v does not belong to any of the above mentioned categories, it then must belong to the 4th category, for which we set t(u, v) = 0. It is worth noting that, given T , the computation of t(u, v) for all cases included in the 1st, 3rd and 4th categories can be unified by a single formula as follows
t(u, v) = 1 M × M-1 m=1 1 t(pm,pm+1) , (8
)
where  8), we can infer that, provided all the other conditions are the same, the bigger the value of M is or the smaller the value of t(p m , p m+1 ) is, the smaller the value of t(u, v) will be, and vice versa. The above effect is consistent with our intuitive understanding of the property of transitivity in the trust type relationships between a pair of users.
p 1 = u, p M = v and p 1 → p 2 → . . . → p M is the shortest path in G r from u to v.<h2>publication_ref</h2>['b16', 'b17']<h2>figure_ref</h2>['fig_1', 'fig_1']<h2>table_ref</h2>[]<h2>heading</h2>Connections to related work<h2>text</h2>The iTrace algorithm proposed here finds connections to several existent TaCF methods in the literature. The algorithm architecture of iTrace falls within a generic TaCF framework presented in [9], while the implicit inference procedure of iTrace presented here is unique. In a variety of existent TaCF methods [16,[19][20][21][22], the trust score is derived from the user rating data. To this regard, trust inference and the computation of user similarity are performed based on exactly the same information source. In contrast with the aforementioned work, the iTrace algorithm employs not only the user rating data but also data other than user rating, namely the explicit user trust data. Since the explicit trust data and the rating data are processed independently, the iTrace has the advantage of making full use of two complementary views in rating prediction. Furthermore, since the trust inference procedure can be performed offline prior to calculation of user similarity, the computation time of the iTrace for online rating predictions is similar to traditional CF methods. The iTrace algorithm also finds connections to our previous work on trust modeling in the context of wireless sensor networks [23][24][25]. Although the same term trust is used, its physical meaning is different. In iTrace, the term trust represents a classical social relationship among users, while in [23][24][25], it is an artificially designed concept related to abnormal sensory behaviors caused by sensor faults.
To our knowledge, the most similar work to our algorithm is a trust based CF method presented in [26], which has come to our attention only recently. In contrast with [26], we provide a new and more efficient way for readers to understand the shortest path based formulation of the trust inference problem by identifying four categories of trust information flow patterns and unifying three of them by a single formula, namely Eqn. (8). In addition, we consider two different ways, namely IW and LW as specified by Eqns.( 4) and ( 5), respectively, for fusion of trust and similarity; while in [26], only one way, i.e., LW, is considered. Further, the iTrace algorithm leverages the property of trust value attenuation in the trust propagation process by adding a penalization item M to the denominator of Eqn.( 7), while the method in [26] does not take into account of such attenuation effect. Finally, through a public open dataset, we demonstrate that our iTrace algorithm outperforms the method proposed in [26].<h2>publication_ref</h2>['b8', 'b15', 'b18', 'b19', 'b20', 'b21', 'b22', 'b23', 'b24', 'b22', 'b23', 'b24', 'b25', 'b25', 'b7', 'b25', 'b25', 'b25']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>PERFORMANCE EVALUATION<h2>text</h2>In this section, we present experimental results, which show that the proposed iTrace algorithm outperforms existing competitor methods. We conducted empirical performance evaluations on the public dataset Filmtrust [27].<h2>publication_ref</h2>['b26']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>About the dataset<h2>text</h2>The Filmtrust dataset consists of a N ×M rating matrix R and an N × N explicit trust matrix T e , associated with N = 1508 users and M = 2071 movie items. The (i, j)th cell of R is filled with user i's rating on item j if it exists; otherwise it is empty. A total number of 35416 ratings are included in R, whose values are between 0.5 and 4; and the empty cells of R are missing values to be predicted. The matrix T e is sparse in that only 1642 cells of it are filled with value 1 associated with a set of trust statements and the other cells are empty, corresponding to missing values to be predicted.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Experiment setting<h2>text</h2>The comparison methods include the traditional CF algorithm, which uses the Pearson correlation as the similarity measure, an explicit trust based TaCF (called E-TaCF for short in what follows) and a Dijkstra's algorithm based TaCF (termed D-TaCF for short in what follows) proposed in [26]. The traditional CF is included here as a baseline for performance comparison. The E-TaCF algorithm can be regarded as a simplified version of iTrace that discards the whole implicit trust inference procedure, namely, it sets T straightforward to be T e prior to the calculation of Eqn. (3). We consider two types of E-TaCF, E-TaCF-I and E-TaCF-II, corresponding to the usage of Eqn.(4) and of Eqn. (5), respectively, for fusion of similarity and trust. The missing values in T e are filled with 0 when performing E-TaCF. This E-TaCF algorithm is included here in order to demonstrate the value of the proposed implicit trust procedure. The D-TaCF is involved here as it is the most similar method in the literature to our iTrace algorithm. For iTrace and E-TaCF, we consider the IW and LW weighting mechanisms both, corresponding to Eqns.( 4) and ( 5), respectively, and the aim is to investigate which one is better for use. Apart from Eqn. (7), we also considered another way to calculate t(u, v) by
t(u, v) = 1/L(SP r (u, v)). (9
)
The purpose is to demonstrate that taking account of the attenuation feature of trust via Eqn.( 7) is beneficial for improving accuracy in recommendations. To summarize, we considered in total 4 types of iTrace as shown in Table 1. <h2>publication_ref</h2>['b25', 'b2', 'b4', 'b6']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_1']<h2>heading</h2>Experiment results<h2>text</h2>In our experiment, the number of voting users K takes values in {5, 10, 15, 20, 25, 30, 35, 40, 45}, and for every K value, a cross validation type test for every comparison method is performed. We partition the sample of rating data into two complementary subsets, perform the user similarity analysis on one subset, which occupies 80% of the whole dataset, hiding the other 20% ratings and trying to predict them. The predicted rating is then compared with the real rating and the difference (in absolute value) is the prediction error. The mean absolute error (MAE) is adopted as the performance measure.
To reduce variability, we perform 5 rounds of the above operations using different partitions for each algorithm, and the prediction results are averaged over the rounds. For the sake of fairness in comparison, we try different α values and then select the optimal value 0.3 for use for iTrace-III and iTrace-IV.
The comparison result, in terms of averaged MAE per rating prediction, is presented in Fig. 4. It is shown that trust based methods outperform the traditional CF significantly and that the iTrace-I algorithm beats all the other competitors. It also indicates that Eqn.( 4) is preferable to Eqn. (5) and Eqn.( 7) is preferable to Eqn. (9) for use in implementing iTrace. Note that, since the implicit trust inference procedure is performed offline, the computation time of iTrace for online rating prediction is similar to the traditional CF method. <h2>publication_ref</h2>['b4', 'b8']<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>[]<h2>heading</h2>CONCLUSIONS<h2>text</h2>In this paper, we proposed an improved CF algorithm termed iTrace, which is featured by an embedded powerful implicit trust inference method. This method can estimate the implicit trust relationship between a pair of users based on available but very limited explicit trust information among users. The result from an extensive experiment on a public dataset demonstrates the superiority of our algorithm to existent competitors. Future work lies in using the proposed technique to analyze more datasets. How to model and employ more social interactions among users and the temporal dynamics in the users' rating behaviors to improve CF is also a promising topic for future investigation.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>Recommender systems<h2>journal</h2>Communications of the ACM<h2>year</h2>1997<h2>authors</h2>P Resnick; H R Varian<h2>ref_id</h2>b1<h2>title</h2>Item-based collaborative filtering recommendation algorithms<h2>journal</h2><h2>year</h2>2001<h2>authors</h2>B Sarwar; G Karypis; J Konstan; J Riedl<h2>ref_id</h2>b2<h2>title</h2>Evaluating collaborative filtering recommender systems<h2>journal</h2>ACM Transactions on Information Systems<h2>year</h2>2004<h2>authors</h2>J L Herlocker; J A Konstan; L G Terveen; J T Riedl<h2>ref_id</h2>b3<h2>title</h2>Advances in collaborative filtering<h2>journal</h2>Springer<h2>year</h2>2015<h2>authors</h2>Y Koren; R Bell<h2>ref_id</h2>b4<h2>title</h2>An algorithmic framework for performing collaborative filtering<h2>journal</h2>ACM<h2>year</h2>1999<h2>authors</h2>J L Herlocker; J A Konstan; A Borchers; J Riedl<h2>ref_id</h2>b5<h2>title</h2>Empirical analysis of predictive algorithms for collaborative filtering<h2>journal</h2>Morgan Kaufmann Publishers Inc<h2>year</h2>1998<h2>authors</h2>J S Breese; D Heckerman; C Kadie<h2>ref_id</h2>b6<h2>title</h2>Unifying user-based and item-based collaborative filtering approaches by similarity fusion<h2>journal</h2>ACM<h2>year</h2>2006<h2>authors</h2>J Wang; A P De; M J Vries;  Reinders<h2>ref_id</h2>b7<h2>title</h2>Merging trust in collaborative filtering to alleviate data sparsity and cold start<h2>journal</h2>Knowledge-Based Systems<h2>year</h2>2014<h2>authors</h2>G Guo; J Zhang; D Thalmann<h2>ref_id</h2>b8<h2>title</h2>Trust-aware collaborative filtering for recommender systems<h2>journal</h2>Springer<h2>year</h2>2008<h2>authors</h2>P Massa; P Avesani<h2>ref_id</h2>b9<h2>title</h2>Trust-aware recommender systems<h2>journal</h2>ACM<h2>year</h2>2007<h2>authors</h2><h2>ref_id</h2>b10<h2>title</h2>Recommender systems handbook<h2>journal</h2>Springer<h2>year</h2>2015<h2>authors</h2>F Ricci; L Rokach; B Shapira; P B Kantor<h2>ref_id</h2>b11<h2>title</h2>Trust in recommender systems<h2>journal</h2>ACM<h2>year</h2>2005<h2>authors</h2>J O'donovan; B Smyth<h2>ref_id</h2>b12<h2>title</h2>Improving recommendation accuracy by clustering social networks with trust<h2>journal</h2>Recommender Systems & the Social Web<h2>year</h2>2009<h2>authors</h2>T Dubois; J Golbeck; J Kleint; A Srinivasan<h2>ref_id</h2>b13<h2>title</h2>Improving recommendation accuracy by combining trust communities and collaborative filtering<h2>journal</h2>ACM<h2>year</h2>2014<h2>authors</h2>X Ma; H Lu; Z Gan<h2>ref_id</h2>b14<h2>title</h2>Trust and transitivity: how trust-transfer works<h2>journal</h2><h2>year</h2>2012<h2>authors</h2>R Falcone; C Castelfranchi<h2>ref_id</h2>b15<h2>title</h2>Alleviating the sparsity problem of collaborative filtering using trust inferences<h2>journal</h2>Trust Management<h2>year</h2>2005<h2>authors</h2>M Papagelis; D Plexousakis; T Kutsuras<h2>ref_id</h2>b16<h2>title</h2>Faster algorithms for the shortest path problem<h2>journal</h2>Journal of the ACM<h2>year</h2>1990<h2>authors</h2>R K Ahuja; K Mehlhorn; J Orlin; R E Tarjan<h2>ref_id</h2>b17<h2>title</h2>Dijkstras shortest path algorithm<h2>journal</h2>Journal of Formalized Mathematics<h2>year</h2>2003<h2>authors</h2>J.-C Chen<h2>ref_id</h2>b18<h2>title</h2>Supporting trust in virtual communities<h2>journal</h2>IEEE<h2>year</h2>2000<h2>authors</h2>A Abdul-Rahman; S Hailes<h2>ref_id</h2>b19<h2>title</h2>Trust-based collaborative filtering<h2>journal</h2>Springer<h2>year</h2>2008<h2>authors</h2>N Lathia; S Hailes; L Capra<h2>ref_id</h2>b20<h2>title</h2>Using trust in collaborative filtering recommendation<h2>journal</h2>Springer<h2>year</h2>2007<h2>authors</h2>C.-S Hwang; Y.-P Chen<h2>ref_id</h2>b21<h2>title</h2>A model of trust derivation from evidence for use in recommendation systems<h2>journal</h2><h2>year</h2>2004<h2>authors</h2>G Pitsilis; L F Marshall<h2>ref_id</h2>b22<h2>title</h2>Toward reliable data analysis for internet of things by bayesian dynamic modeling and computation<h2>journal</h2>IEEE<h2>year</h2>2015<h2>authors</h2>B Liu; Z Xu; J Chen; G Yang<h2>ref_id</h2>b23<h2>title</h2>Online fault-tolerant dynamic event region detection in sensor networks via trust model<h2>journal</h2>IEEE<h2>year</h2>2017<h2>authors</h2>J Wang; B Liu<h2>ref_id</h2>b24<h2>title</h2>State space model based trust evaluation over wireless sensor networks: An iterative particle filter approach<h2>journal</h2>The Journal of Engineering<h2>year</h2>2017<h2>authors</h2>B Liu; S Cheng<h2>ref_id</h2>b25<h2>title</h2>Trust-based collaborative filtering algorithm in social network<h2>journal</h2>IEEE<h2>year</h2>2016<h2>authors</h2>X Chen; Y Guo; Y Yang; Z Mi<h2>ref_id</h2>b26<h2>title</h2>Filmtrust: Movie recommendations using trust in web-based social networks<h2>journal</h2><h2>year</h2>2006<h2>authors</h2>J Golbeck; J Hendler<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Fig. 1 :1Fig. 1: Architecture of the iTrace algorithm. The inputs of the algorithm include an N × N explicit trust matrix and an N × M rating matrix. N and M denote the number of the users and of the items, respectively.<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Fig. 3 :3Fig. 3: An example show of the transformation from a weighted directed graph (the left panel) to its reciprocal counterpart graph (the right panel). The edge weight in the right graph is the reciprocal of the weight in the left graph.<h2>figure_data</h2><h2>figure_label</h2>4<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Fig. 4 :4Fig. 4: Averaged MAE per rating prediction.<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>4 types of the iTrace algorithm under consideration<h2>figure_data</h2>iTrace-I iTrace-II iTrace-III iTrace-IVchoice for fEqn.(4)Eqn.(4)Eqn.(5)Eqn.(5)choice for t(u, v)Eqn.(7)Eqn.(9)Eqn.(7)Eqn.(9)<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>r u,i = ru + v∈U w(u, v)(r v,i -rv ) v∈U |w(u, v)| ,(1)<h2>formula_coordinates</h2>[1.0, 358.2, 364.29, 200.84, 26.05]<h2>formula_id</h2>formula_1<h2>formula_text</h2>(u, v) = i∈I (r u,i -ru )(r v,i -rv ) i∈I (r u,i -ru ) 2 i∈I (r v,i -rv ) 2 , (2<h2>formula_coordinates</h2>[1.0, 334.98, 571.65, 220.15, 27.01]<h2>formula_id</h2>formula_2<h2>formula_text</h2>r u,i = ru + v∈U f (sim(u, v), t(u, v))(r v,i -rv ) v∈U |f (sim(u, v), t(u, v))| ,(3)<h2>formula_coordinates</h2>[2.0, 344.76, 242.73, 214.28, 37.96]<h2>formula_id</h2>formula_3<h2>formula_text</h2>f (sim(u, v), t(u, v)) = sim(u, v) t(u, v) j∈U sim(u, j) t(u, j) .(4)<h2>formula_coordinates</h2>[2.0, 341.28, 368.85, 217.76, 28.09]<h2>formula_id</h2>formula_4<h2>formula_text</h2>f (sim(u, v), t(u, v)) = αsim(u, v) j∈U sim(u, j) + (1 -α) t(u, v) j∈U t(u, j) ,(5)<h2>formula_coordinates</h2>[2.0, 318.0, 449.01, 241.04, 37.96]<h2>formula_id</h2>formula_5<h2>formula_text</h2>t(u, v) = |S(u) S(v)| |S(u) S(v)| ,(6)<h2>formula_coordinates</h2>[3.0, 386.28, 278.97, 172.76, 23.64]<h2>formula_id</h2>formula_6<h2>formula_text</h2>t(u, v) = 1 M × L(SP r (u, v)) ,(7)<h2>formula_coordinates</h2>[3.0, 375.72, 587.85, 183.32, 23.89]<h2>formula_id</h2>formula_7<h2>formula_text</h2>to v, SP r (u, v), is u → k → v, L(SP r (u, v)) = tr (u, k) + tr (k, v) = 4, M = 2 and thus t(u, v) = 1/8.<h2>formula_coordinates</h2>[3.0, 315.24, 664.05, 243.78, 23.76]<h2>formula_id</h2>formula_8<h2>formula_text</h2>t(u, v) = 1 M × M-1 m=1 1 t(pm,pm+1) , (8<h2>formula_coordinates</h2>[4.0, 104.88, 282.21, 189.49, 27.57]<h2>formula_id</h2>formula_9<h2>formula_text</h2>)<h2>formula_coordinates</h2>[4.0, 294.37, 289.67, 3.91, 8.91]<h2>formula_id</h2>formula_10<h2>formula_text</h2>p 1 = u, p M = v and p 1 → p 2 → . . . → p M is the shortest path in G r from u to v.<h2>formula_coordinates</h2>[4.0, 54.48, 321.45, 243.75, 22.33]<h2>formula_id</h2>formula_11<h2>formula_text</h2>t(u, v) = 1/L(SP r (u, v)). (9<h2>formula_coordinates</h2>[5.0, 122.52, 236.13, 171.85, 12.25]<h2>formula_id</h2>formula_12<h2>formula_text</h2>)<h2>formula_coordinates</h2>[5.0, 294.37, 238.78, 3.91, 8.91]<h1>doi</h1><h1>title</h1>IEEE Transactions on Information Theory 2015 k-Connectivity in Random Key Graphs with Unreliable Links<h1>authors</h1>Jun Zhao; Osman Yagan; Virgil Gligor<h1>pub_date</h1>2019-11-02<h1>abstract</h1>Random key graphs form a class of random intersection graphs and are naturally induced by the random key predistribution scheme of Eschenauer and Gligor for securing wireless sensor network (WSN) communications. Random key graphs have received much interest recently, owing in part to their wide applicability in various domains including recommender systems, social networks, secure sensor networks, clustering and classification analysis, and cryptanalysis to name a few. In this paper, we study connectivity properties of random key graphs in the presence of unreliable links. Unreliability of the edges are captured by independent Bernoulli random variables, rendering edges of the graph to be on or off independently from each other. The resulting model is an intersection of a random key graph and an Erdős-Rényi graph, and is expected to be useful in capturing various real-world networks; e.g., with secure WSN applications in mind, link unreliability can be attributed to harsh environmental conditions severely impairing transmissions. We present conditions on how to scale this model's parameters so that i) the minimum node degree in the graph is at least k, and ii) the graph is k-connected, both with high probability as the number of nodes becomes large. The results are given in the form of zeroone laws with critical thresholds identified and shown to coincide for both graph properties. These findings improve the previous results by Rybarczyk on the k-connectivity of random key graphs (with reliable links), as well as the zero-one laws by Yagan on the 1-connectivity of random key graphs with unreliable links.<h1>sections</h1><h2>heading</h2>I. INTRODUCTION<h2>text</h2>Random key graphs have received significant interest recently with applications spanning key predistribution in secure wireless sensor networks (WSNs) [2], [5], [8], [9], [13], social networks [7], [18], [41], recommender systems [27], clustering and classification analysis [4], [19], cryptanalysis of hash functions [3], circuit design [35], and the modeling of epidemics [1] and "small-world" networks [38]. They belong to a larger class of random graphs known as random intersection graphs [2]- [7], [10], [14], [28], [34], [35]; in fact, they are referred to as uniform random intersection graphs by some authors [2], [3], [6], [28], [32]- [34], [45], [46].
To fix the terminology, we will describe random key graphs in the context of secure WSNs, where they have originated from. Security is expected to be a key challenge in resource constrained sensor networks. A widely accepted solution for securing WSN communications is the random predistribution of cryptographic keys to sensor nodes, and utilization of symmetric-key encryption modes [17], [21], [31] to ensure message secrecy and authenticity. Among various key predistribution algorithms proposed to date, the original scheme by Eschenauer and Gligor (EG) [13] is still the most widely recognized one. According to the EG scheme, each of the n sensors is assigned K n distinct keys that are selected uniformly at random from a key pool of size P n . Two sensors can then securely communicate over an existing communication link if they have at least one key in common; i.e., if they share a common key. This notion of adjacency defines the random key graph, hereafter denoted by G(n, K n , P n ). For generality, K n and P n are assumed to scale with the number of nodes n, with the natural condition 1 ≤ K n ≤ P n always imposed.
In this paper, we study connectivity properties of random key graphs in the presence of unreliable links. Unreliability of the edges are captured by independent Bernoulli random variables, rendering each edge of G(n; K n , P n ) to be on (with probability p n ) or off (with probability 1p n ) independently from all other edges. Put differently, we consider an Erdős-Rényi (ER) graph G(n; p n ) [11] on the same set of n vertices, with edges appearing between any pair of vertices independently with probability p n . A random key graph with unreliable links thus corresponds to the intersection of a random key graph and an ER graph. Hereafter, we denote this graph by G on = G(n; K n , p n ) ∩ G(n; p n ); see Section III for precise definitions.
Just like the random key graph, the G on model can be used in various applications, particularly when links are expected to be unreliable. For example, in a secure WSN application, links might be unreliable due to wireless media of the communication, or due to physical obstacles and altering environmental conditions severally impairing the transmission. We refer the reader to [39] and [41] for two other applications of G on : i) secure connectivity of WSNs under an on-off channel model, and ii) large scale, distributed publish-subscribe services in online social networks, respectively.
The main goal of this paper to study k-connectivity of G on . A network (or graph) is said to be k-connected if for each pair of nodes there exist at least k mutually disjoint paths connecting them. An equivalent definition of k-connectivity is that a network is k-connected if the network remains connected despite the failure of any (k -1) nodes [29]; a network is said to be simply connected if it is 1-connected. kconnectivity is a fundamental graph property and is important for various applications of random key graphs. For example, in a WSN application where sensor nodes operate autonomously and physically unprotected, k-connectivity provides communication security against an adversary that is able to compromise up to k -1 links by launching a sensor capture attack [8]; i.e., two sensors can communicate securely as long as at least one of the k disjoint paths connecting them consists of links that are not compromised by the adversary. Also, k-connectivity improves resiliency against network disconnection due to battery depletion, in both normal mode of operation and under battery-depletion attacks [26]. Furthermore, it enables flexible communication-load balancing across multiple paths so that network energy consumption is distributed without penalizing any access path [15].
Our main contributions are zero-one laws for two related graph properties for G on : i) the minimum node degree being at least k, and ii) k-connectivity. Namely, we present conditions on how to scale the model parameters K n , P n , p n such that these properties hold with probability approaching to one and zero, respectively, as the number of nodes n becomes large. Our main results also imply a zero-one law for k-connectivity in random key graph G(n, K n , P n ) (see Corollary 2), and the established result is shown to improve that given previously by Rybarczyk [32]; see Section IV-D for details. Moreover, for the 1-connectivity of G on , we provide a stronger form of the zero-one law as compared to that given by Yagan [37]; see Section IV-D.
We organize the rest of the paper as follows: In Section II, we survey the relevant results from the literature, while in Section III we give a detailed description of the system model G on . The main results of the paper are presented (see Theorem 1) in Section IV, with a detailed discussion and comparisons with the existing results given in Section IV-D; also, in Section IV-E we provide numerical results that confirm Theorem 1. The basic ideas that pave the way in establishing Theorem 1 are given in Section V. Sections VI through VIII are devoted to establishing the zero-law part of Theorem 1, whereas the one-law of Theorem 1 is established in Sections IX through XIII. The paper is concluded in Section XIV, and some of the technical details are given in Appendix A-C.<h2>publication_ref</h2>['b1', 'b4', 'b7', 'b8', 'b12', 'b6', 'b17', 'b40', 'b26', 'b3', 'b18', 'b2', 'b34', 'b0', 'b37', 'b1', 'b6', 'b9', 'b13', 'b27', 'b33', 'b34', 'b1', 'b2', 'b5', 'b27', 'b31', 'b33', 'b44', 'b45', 'b16', 'b20', 'b30', 'b12', 'b10', 'b38', 'b40', 'b28', 'b7', 'b25', 'b14', 'b31', 'b36']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>II. RELATED WORK<h2>text</h2>Erdős and Rényi [11] and Gilbert [16] introduces the random graph G(n, p), which is defined on n nodes and there exists an edge between any two nodes with probability p independently of all other edges. The probability p can also be a function of n, in which case we refer to it as p n . Throughout the paper, we refer to the random graph G(n, p n ) as an Erdős-Rényi (ER) graph following the convention in the literature.
Erdős and Rényi [11] prove that when p n is ln n+αn n , graph G(n, p n ) is asymptotically almost surely 1 (a.a.s.) connected (resp., not connected) if lim n→∞ α n = +∞ (resp., lim n→∞ α n = -∞). In later work [12], they further explore k-connectivity [30] in G(n, p n ) and show that if p n = 1 We say that an event takes place asymptotically almost surely if its probability approaches to 1 as n → ∞. Also, we use "resp." as a shorthand for "respectively".
ln n+(k-1) ln ln n+αn n
, G(n, p n ) is a.a.s. k-connected (resp., not k-connected) if lim n→∞ α n = +∞ (resp., lim n→∞ α n = -∞).
Previous work [2], [32], [39] investigates the zero-one law for connectivity in random key graph G(n, K n , P n ), where P n and K n are the key pool size and the key ring size, respectively. Blackburn and Gerke [2] prove that if K n ≥ 2 and P n = ⌊n ξ ⌋, where ξ is a positive constant, G(n, K n , P n ) is a.a.s. connected (resp., not connected) if lim inf n→+∞
K 2 n n Pn ln n > 1 (resp., lim sup n→+∞ K 2 n n
Pn ln n < 1). Yagan and Makowski [39] demonstrate that iffoot_0 K n ≥ 2, P n = Ω(n) and
K 2 n Pn = ln n+αn n , then G(n, K n , P n ) is a.a.s. connected (resp., not connected) if lim n→∞ α n = +∞ (resp., lim n→∞ α n = -∞).
Rybarczyk [32] obtains a stronger result without requiring P n = Ω(n). In particular, she derives the asymptotically exact probability of connectivity in G(n, K n , P n ) as follows: under K n ≥ 2, if the sequence α n defined through
K 2 n Pn = ln n+αn n has a limit α * ∈ [-∞, ∞], then the probability of G(n, K n , P n ) being connected approaches to e -e -α *
as n → ∞. This asymptotically exact probability result is stronger than a zeroone law since the latter can be obtained by setting α * as ∞ and -∞ in the former. Rybarczyk also establishes [33, Remark 1, p. 5] a zero-one law for k-connectivity in G(n, K n , P n ) by showing the similarity between G(n, K n , P n ) and a random intersection graph [5] via a coupling argument. Specifically, she proves that if P n = Θ(n ξ ) for some ξ > 1 and
K 2 n Pn = ln n+(k-1) ln ln n+αn n , then the G(n, K n , P n ) is a.a.s. k-connected (resp., not k-connected) if lim n→∞ α n = +∞ (resp., lim n→∞ α n = -∞).
Recently Yagan [37] gives a zero-one law for connectivity (i.e., 1-connectivity) in graph G(n, K n , P n )∩G(n, p n ), which is the intersection of random key graph G(n, K n , P n ) and random graph G(n, p n ), and clearly is equivalent to our key graph G on ; see Section III. Specifically, he proves that if
K n ≥ 2, P n = Ω(n) and p n • 1 - ( Pn -Kn Kn ) ( Pn Kn )
∼ c ln n n hold, and lim n→∞ (p n ln n) exists, then graph G(n, K n , P n ) ∩ G(n, p n ) is asymptotically almost surely connected (resp., not connected) if c > 1 (resp., c < 1). A comparison of our results with the related work is given in Section IV-D.
After the submission of this paper, we have derived the asymptotically exact probability of k-connectivity in G(n, K n , P n ) [45] (resp., G on [44]). Based on the proofs in this paper, we show i) that [45] under P n = Ω(n), if the sequence α n defined through
K 2 n Pn = ln n+(k-1) ln ln n+αn n has a limit α * ∈ [-∞, ∞], then the probability of G(n, K n , P n ) being k-connected converges to e -e -α *
(k-1)! as n → ∞, and ii) that [44] under P n = Ω(n) and Kn Pn = o(1), if the sequence
α n defined through p n • 1 - ( Pn -Kn Kn ) ( Pn Kn ) = ln n+(k-1) ln ln n+αn n has a limit α * ∈ [-∞, ∞],
then the probability of G on being k-connected converges to e -e -α * (k-1)! as n → ∞.<h2>publication_ref</h2>['b10', 'b15', 'b10', 'b11', 'b29', 'b1', 'b31', 'b38', 'b1', 'b38', 'b31', 'b4', 'b36', 'b44', 'b43', 'b44', 'b43']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>III. SYSTEM MODEL G on<h2>text</h2>Consider a vertex set V = {v 1 , v 2 , . . . , v n }. Each node v i ∈ V is assigned a key ring S i that consists of K n distinct keys selected uniformly at random from a key pool P of size P n . The random key graph G(n, K n , P n ) is defined on the vertex set V such that two distinct nodes v i and v j are adjacent, denoted K ij , if their key rings have at least one key in common; i.e.,
K ij = [S i ∩ S j = ∅].
For distinct nodes v x and v y , we let S xy denote the intersection of their key rings S x and S y ; i.e., S xy = S x ∩ S y .
Our main interest is to study random key graphs whose links are unreliable. In particular, we assume that each link is on with probability p n , or off with probability 1p n , independently from any other link. Namely, with C ij denoting the event that link between v i and v j is on, {C ij , 1 ≤ i < j ≤ n} are mutually independent such that
P [C ij ] = p n , 1 ≤ i < j ≤ n.(1)
This unreliable link model can be represented [11] by an Erdős-Rényi (ER) graph G(n, p n ) on the vertices V such that there exists an edge between nodes v i and v j if the link between them is on; i.e., if the event C ij takes place.
Finally, the graph G on (n, K n , P n , p n ) is defined on the vertices V such that two distinct nodes v i and v j have an edge in between, denoted E ij , if the events K ij and C ij take place at the same time. In other words, we have
E ij = K ij ∩ C ij , 1 ≤ i < j ≤ n(2)
so that
G on (n, K n , P n , p n ) = G(n, K n , P n ) ∩ G(n, p n ).(3)
Throughout, we simplify the notation by writing G on instead of G on (n, K n , P n , p n ). Thus, our main model G on is an intersection of a random key graph and an ER graph. Throughout, we let p s (K n , P n ) be the probability that the key rings of two distinct nodes share at least one key and let p e (K n , P n , p n ) be the probability that there exists a link between two distinct nodes in G on . For simplicity, we write p s (K n , P n ) as p s and write p e (K n , P n , p n ) as p e . Then for any two distinct nodes v i and v j , we have
p s := P[K ij ].(4)
It is easy to derive p s in terms of K n and P n as shown in previous work [2], [32], [39]. In fact, we have
p s = P[S i ∩ S j = ∅] =    1 - ( Pn -Kn Kn ) ( Pn Kn ) , if P n ≥ 2K n , 1 if P n < 2K n .(5)
Given (2), the independence of the events C ij and K ij gives
p e := P[E ij ] = P[C ij ] • P[K ij ] = p n • p s(6)
from ( 1) and (4). Substituting ( 5) into (6), we obtain
p e = p n • 1 - Pn-Kn Kn Pn Kn if P n ≥ 2K n .(7)<h2>publication_ref</h2>['b10', 'b1', 'b31', 'b38', 'b1']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>IV. MAIN RESULTS AND DISCUSSION<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. The Main Result<h2>text</h2>The main result of this paper, given below, establishes zero-one laws for k-connectivity and for the property that the minimum node degree is no less than k in graph G on . Throughout this paper, k is a positive integer and does not scale with n. Also, we let N (resp., N 0 ) stand for the set of all non-negative (resp., positive) integers.
We refer to any pair of mappings K, P : N 0 → N 0 as a scaling as long as it satisfies the natural conditions
K n ≤ P n , n = 1, 2, . . . .(8)
Similarly, any mapping p : N 0 → (0, 1) defines a scaling.
Theorem 1. Consider scalings K, P : N 0 → N 0 , p : N 0 → (0, 1) such that K n ≥ 2 for all n sufficiently large. We define a sequence α : N 0 → R such that for any n ∈ N 0 , we have
p e = ln n + (k -1) ln ln n + α n n .(9)
The properties (a) and (b) below hold. (a) If
K 2 n Pn = o(1)
and either there exists ǫ > 0 such that p e n > ǫ holds for all n sufficiently large or lim n→∞ p e n = 0, then
lim n→∞ P [G on is k-connected ] = 0 if lim n→∞ α n = -∞,(10)
and (12) and
lim n→∞ P Minimum node degree of G on is no less than k = 0 if lim n→∞ α n = -∞. (11
) (b) If P n = Ω(n) and Kn Pn = o(1), then lim n→∞ P [G on is k-connected ] = 1 if lim n→∞ α n = ∞,
lim n→∞ P Minimum node degree of G on is no less than k = 1 if lim n→∞ α n = ∞.(13)
Note that if we combine (10) and (12), we obtain the zeroone law for k-connectivity in G on , whereas combining (11) and ( 13) leads to the zero-one law for the minimum node degree. Therefore, Theorem 1 presents the zero-one laws of k-connectivity and the minimum node degree in graph G on . We also see from (9) that the critical scaling for both properties is given by p e = ln n+(k-1) ln ln n n . The sequence α n : N 0 → R defined through (9) therefore measures by how much the probability p e deviates from the critical scaling.
In case (b) of Theorem 1, the conditions P n = Ω(n) and Kn Pn = o(1) indicate that the size of the key pool P n should grow at least linearly with the number of sensor nodes in the network, and should grow unboundedly with the size of each key ring. These conditions are enforced here merely for technical reasons, but they hold trivially in practical wireless sensor network applications [8], [9], [13]. Again, the condition 1) enforced for the zero-law in Theorem 1 is not a stringent one since the P n is expected to be several orders of magnitude larger than K n . Finally, the condition that either p e n > ǫ > 0 for all n large or lim n→∞ p e n = 0 is imposed to avoid degenerate situations. In most cases of interest it holds that p e n > ǫ > 0 as otherwise the graph G on becomes trivially disconnected. To see this, notice that p e n is an upper-bound on the expected degree of a node and that the expected number of edges in the graph is less than p e n 2 ; yet, a connected graph on n nodes must have at least n -1 edges.
K 2 n Pn = o(<h2>publication_ref</h2>['b11', 'b9', 'b11', 'b10', 'b8', 'b8', 'b7', 'b8', 'b12']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>B. Results with an approximation of probability p s<h2>text</h2>An analog of Theorem 1 can be given with a simpler form of the scaling (9); i.e., with p s replaced by the more easily expressed quantity K 2 n /P n , and hence with p e = p n K 2 n /P n . In fact, in the case of random key graph G(n, K n , P n ) it is a common practice [2], [32], [39] to replace p s by
K 2 n
Pn , owing to the fact [39] that
p s ∼ K 2 n P n if K 2 n P n = o(1).(14)
However, when random key graph G(n, K n , P n ) is intersected with an ER graph G(n, p n ) (as in the case of G on ) the simplification does not occur naturally (even under ( 14)), and as seen below, simpler forms of the zero-one laws are obtained at the expense of extra conditions enforced on the parameters K n and P n .
Corollary 1. Consider a positive integer k, and scalings K, P : N 0 → N 0 , p : N 0 → (0, 1) such that K n ≥ 2 for all n sufficiently large. We define a sequence α : N 0 → R such that for any n ∈ N 0 , we have
p n • K 2 n P n = ln n + (k -1) ln ln n + α n n . (15
)
The properties (a) and (b) below hold. (a) If
K 2 n Pn = O( 1 ln n ) and lim n→∞ (ln n + (k -1) ln ln n + α n ) = ∞, then lim n→∞ P [G on is k-connected ] = 0 if lim n→∞ α n = -∞,(16)
and
lim n→∞ P Minimum node degree of G on is no less than k = 0 if lim n→∞ α n = -∞. (17
) (b) If P n = Ω(n) and K 2 n Pn = O( 1 ln n ), then lim n→∞ P [G on is k-connected ] = 1 if lim n→∞ α n = ∞, (18
)
and
lim n→∞ P Minimum node degree of G on is no less than k = 1 if lim n→∞ α n = ∞.(19)
A proof of Corollary 1 can be found in Section IV-F. Note that the condition
K 2 n Pn = O( 1 ln n ) enforced in Corollary 1 implies both Kn Pn = o(1)
and
K 2 n Pn = o(1)
, and thus it is a stronger condition than those enforced in Theorem 1.<h2>publication_ref</h2>['b1', 'b31', 'b38', 'b38']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C. A Zero-One Law for k-Connectivity in Random Key Graphs<h2>text</h2>We now provide a useful corollary of Theorem 1 that gives a zero-one law for k-connectivity in the random key graph G(n, K n , P n ). As discussed in Section IV-D below, this result improves the one given implicitly by Rybarczyk [33].
Corollary 2. Consider a positive integer k, and scalings K, P : N 0 → N 0 such that K n ≥ 2 for all n sufficiently large. With α : N 0 → R given by
K 2 n P n = ln n + (k -1) ln ln n + α n n , n = 1, 2, . . . ,(20)
the following two properties hold. (a) If either there exists an ǫ > 0 such that n
K 2 n
Pn > ǫ for all n sufficiently large, or lim n→∞ n K 2 n Pn = 0, then we have
lim n→∞ P [G(n, K n , P n ) is k-connected ] = 0 if lim n→∞ α n = -∞. (b) If P n = Ω(n), then we have lim n→∞ P [G(n, K n , P n ) is k-connected ] = 1 if lim n→∞ α n = ∞.
A proof of Corollary 2 can be found in Section IV-G.<h2>publication_ref</h2>['b32']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>D. Discussion and Comparison with Related Results<h2>text</h2>As already noted in the literature [2], [11], [12], [32], [33], [39], Erdős-Rényi graph G(n, p n ) and random key graph G(n, K n , P n ) have similar k-connectivity properties when they are matched through their link probabilities; i.e. when p n = p s with p s as defined in (5). In particular, Erdős and Rényi [12] showed that if p n = ln n+(k-1) ln ln n+αn n , then G(n, p n ) is asymptotically almost surely k-connected (resp., not k-connected) if lim n→∞ α n = +∞ (resp., lim n→∞ α n = -∞). Similarly, Rybarczyk [33] has shown under some extra conditions (i.e.,
P n = Θ(n ξ ) with ξ > 1) that if p s = ln n+(k-1) ln ln n+αn n , then G(n, K n , P n ) is almost surely k- connected (resp., not k-connected) if lim n→∞ α n = +∞ (resp., lim n→∞ α n = -∞).
The analogy between these two results could be exploited to conjecture similar k-connectivity results for our system model G on . To see this, recall from (3) that
G on = G(n, K n , P n ) ∩ G(n, p n ).(21)
Since G(n, K n , P n ) and G(n, p s ) have similar k-connectivity properties, it would seem intuitive to replace G(n, K n , P n ) with G(n, p s ) in the above equation (21). Then, using we would automatically obtain Theorem 1 via the aforementioned results of Erdős and Rényi [12]. Unfortunately, such heuristic approaches can not be taken for granted as G(n, K n , P n ) = G(n, p s ) in general. For instance, the two graphs are shown [38], [40] to exhibit quite different characteristics in terms of properties including clustering coefficient, number of triangles, etc. To this end, Theorem 1 formally validates the above intuition for the k-connectivity property, and it is worth mentioning that we establish Theorem 1 with a direct proof that does not rely on coupling arguments between random key graph and ER graph.
G on ≃ G(n, p s ) ∩ G(n, p n ) = G(n, p n p s ) = G(n, p e ),
We now compare our results with those of Rybarczyk [33] for the k-connectivity of random key graph G(n, K n , P n ). As already noted, Rybarczyk [33, Remark 1, p. 5] has established an analog of Corollary 2, but under assumptions much stronger than ours. In particular, her result requires that P n = Θ(n ξ ) where ξ > 1. In comparison, Corollary 2 established here enforces only that P n ≥ Ω(n), which is clearly a much weaker condition than P n = Θ(n ξ ) with ξ > 1. More importantly, our condition P n ≥ Ω(n) requires (from (20)) only that K n = Ω( √ ln n) for the one-law to hold; i.e., for G on to be k-connected. However, the condition P n = Θ(n ξ ) with ξ > 1 enforced in [33] requires the key ring sizes to satisfy K n = Ω( √ n ξ-1 ln n) with ξ -1 > 0. This condition not only constitutes a much stronger requirement than K n = Ω( √ ln n), but it also renders the k-connectivity result given in [33] not applicable in the context of WSNs. This is because K n controls the number of keys kept in each sensor's memory, and should be very small [13] due to limited memory and computational capability of sensor nodes; in general K n = O(ln n) is accepted [9] as a reasonable bound on the key ring sizes.
Finally, we compare Theorem 1 with the zero-one law given by Yagan [37] for the 1-connectivity of G on . As mentioned in Section II above, he shows that if
p e ∼ c ln n n = ln n + (c -1) ln n n(22)
then G on is a.a.s. connected if c > 1, and it is a.a.s. not connected if c < 1. This was done under the additional 
G o n is k-connected p = 0.2 k = 4 k = 6 k = 8 k = 10
Fig. 2. Empirical probability that Gon(n, K, P, p) is k-connected for k = 4, 6, 8, and 10. We take n = 2000, P = 10, 000 and p = 0.2. Vertical dashed lines stand for the critical threshold of k-connectivity asserted by Theorem 1.
conditions that P n = Ω(n) (required only for the one-law) and that lim n→∞ p n ln n exists (required only for the zerolaw). On the other hand, Theorem 1 given here establishes (by setting k = 1) that, if
p e = ln n + α n n(23)
then G on is a.a.s. connected if lim n→∞ α n = ∞, and it is a.a.s. not connected if lim n→∞ α n = -∞. This result relies on the extra conditions P n = Ω(n) and Kn Pn = o(1) for the one-law and on
K 2 n Pn = o(1)
for the zero-law. Comparing ( 22) and ( 23), we see that our 1-connectivity result for G on is somewhat more fine-grained than Yagan's [37]. This is because, a deviation of α n = ±Ω(ln n) is required to get the zero-one law in the form (22), whereas in our formulation (23), it suffices to have an unbounded deviation; e.g., even α n = ± ln ln • • • ln n will do. Put differently, we cover the case of c = 1 in (22) (i.e., the case when p e ∼ ln n n ) and show that G on could be almost surely connected or not connected, depending on the limit of α n ; in fact, if (22) holds with c > 1, we see from Theorem 1 that G on is not only 1connected but also k-connected for any k = 1, 2, . . .. However, it is worth noting that the additional conditions assumed in [37] are weaker than those we enforce in Theorem 1 for k = 1.<h2>publication_ref</h2>['b1', 'b10', 'b11', 'b31', 'b32', 'b38', 'b4', 'b11', 'b32', 'b20', 'b11', 'b37', 'b39', 'b32', 'b32', 'b32', 'b12', 'b8', 'b36', 'b36', 'b21', 'b22', 'b21', 'b36']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>E. Numerical Results<h2>text</h2>We now present numerical results to check the validity of Theorem 1, particularly in the non-asymptotic regime. In all experiments, we fix the number of nodes at n = 2000 and the size of the key pool at P = 10, 000. For Figure 1, we consider several different probabilities of links being on; specifically, we have p = 0.2, 0.4, 0.6, 0.8, while varying the parameter K from 5 to 23; recall that K stands for the number of keys per node. For Figure 2, we fix p = 0.2 and vary K from 16 to 29. For each parameter pair (K, p), we generate 200 independent samples of the graph G on (n, K, P, p) and count the number of times (out of a possible 200) that the obtained graphs i) have minimum node degree no less than k and ii) are k-connected, for k = 1, 2, . . .. Dividing the counts by 200, we obtain the (empirical) probabilities for the events of interest. In all cases, we observe that G on is k-connected whenever its minimum node degree is no less than k, yielding the same empirical probability for both events. This confirms the asymptotic equivalence of the properties of k-connectivity and the minimum node degree being no less than k in G on as stated in Theorem 1.
Figure 1 plots the empirical probability of 2-connectivity in G on versus K for different p values, while Figure 2 depicts the empirical probability of k-connectivity in G on versus K for different k. For each curve, we also show the critical threshold of k-connectivity asserted by Theorem 1 (viz. ( 9)) by a vertical dashed line. Namely, the vertical dashed lines stand for the minimum integer value of K that satisfies
p e = p • 1 - P -K K P K > ln n + ln ln n n .(24)
Even with n = 2000, the threshold behavior in the probability of k-connectivity is evident; it transitions from zero to one with K varying very slightly from a certain value that is close to the analytical prediction obtained from (24). Hence, we conclude that the experimentally observed thresholds of k-connectivity are in good agreement with our theoretical results.<h2>publication_ref</h2>['b23']<h2>figure_ref</h2>['fig_0', 'fig_0']<h2>table_ref</h2>[]<h2>heading</h2>F. A proof of Corollary 1<h2>text</h2>Consider p n , K n and P n as in the statement of Corollary 1 such that (15) holds. As explained above, conditions Kn Pn = o(1) and
K 2 n Pn = o(1) both hold.
The proof is based on Theorem 1. Namely, we will show that if the sequence α ′ : N 0 → R is defined such that
p e = ln n + (k -1) ln ln n + α ′ n n(25)
for any n ∈ N 0 , then it holds that
α ′ n = α n ± O(1)(26)
under the enforced assumptions. In view of lim n→∞ (ln n + (k -1) ln ln n+ α n ) = ∞ and (26), we get lim n→∞ p e n = ∞ from (25). Thus, for any ǫ > 0, we have p e n > ǫ for all n sufficiently large. Hence, all the conditions enforced by Theorem 1 are met, and under ( 25) and ( 26), Corollary 1 follows from Theorem 1 since
lim n→∞ α ′ n = ±∞ if lim n→∞ α n = ±∞.
We now establish (26). First, as seen by the analysis given in Section V-B below, we can introduce the extra condition α n = o(ln n) in proving part (b) of Corollary 1; i.e., in proving the one-law under the condition lim n→∞ α n = ∞. This yields (15). Also, in the case lim n→∞ α n = -∞, we have α n < 0 for all n sufficiently large so that
p n K 2 n Pn = O( ln n n ) under
p n K 2 n Pn = O( ln n n )
. Now, in order to establish (26), we observe from part (a) of Lemma 8foot_1 that
p s = K 2 n P n ± O K 4 n P 2 n .(27)
Then, from (27) and the fact that p e = p s p n , we get
p e = p n • K 2 n P n ± p n • K 2 n P n • O K 2 n P n .(28)
Substituting (15),
p n K 2 n Pn = O( ln n n ) and K 2 n Pn = O 1 ln n into (28), we find p e = ln n + (k -1) ln ln n + α n ± O(1) n . (29
)
Comparing the above relation with (25), the desired conclusion (26) follows.<h2>publication_ref</h2>['b24', 'b25', 'b14', 'b25', 'b26', 'b14', 'b24']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>G. A proof of Corollary 2<h2>text</h2>We first establish the zero-law. Pick K n , P n such that (20) holds with lim n→∞ α n = -∞. It is clear that we have α n < 0 for all n sufficiently large so that
K 2 n Pn = O( ln n n ) = o(1)
. In view of ( 27) we thus get
p s = ln n + (k -1) ln ln n + α n ± o(1) n , n = 1, 2, . . . Let p n = 1 for all n. In this case, graph G on becomes equivalent to G(n, K n , P n ) with p e = ln n + (k -1) ln ln n + α n ± o(1) n , n = 1, 2, . . .(30)
From ( 30) and ( 20), we have p e n = n
K 2 n Pn ± o(1) so that i) if there exists an ǫ > 0 such that n K 2 n
Pn > ǫ, then there exists an ǫ ′ > 0 such that p e n > ǫ ′ for all n sufficiently large and ii) if lim n→∞ n K 2 n Pn = 0, then lim n→∞ p e n = 0. Thus, all the conditions enforced by part (a) of Theorem 1 are satisfied for the given K n , P n and p n . Comparing (30) with (9), we get lim n→∞ α n ± o(1) = -∞ and the zero law
lim n→∞ P [G(n, K n , P n ) is k-connected ] = 0 follows from (10) of Theorem 1.
We now establish the one-law. Pick K n , P n such that (20) holds with lim n→∞ α n = +∞, P n = Ω(n) and K n ≥ 2 for all n sufficiently large. In view of [39, Lemma 6.1], there exists Kn , Pn such that Kn ≥ 2 for all n sufficiently large,
Kn ≤ K n and Pn = P n , n = 1, 2, . . . ,and
K2 n Pn = ln n + (k -1) ln ln n + αn n , n = 1, 2, . . . ,(31)
with αn = O(ln n) and lim n→∞ αn = ∞. By an easy coupling argument, it is easy to check that
P G(n, Kn , Pn ) is k-connected ≤ P [G(n, K n , P n ) is k-connected ] .
Therefore, the one-law proof will be completed upon showing
lim n→∞ P G(n, Kn , Pn ) is k-connected = 1.
Under (31) we have V. BASIC IDEAS FOR PROVING THEOREM 1<h2>publication_ref</h2>['b29', 'b8', 'b30']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. k-Connectivity vs. Minimum Node Degree<h2>text</h2>It is easy to see that if a graph G is k-connected, then the minimum node degree of G is at least k [29]. Therefore, we have
[G is k-connected ] ⊆
Minimum node degree of G is no less than k and the inequality
P [G is k-connected ] ≤ P Minimum node degree of G is no less than k follows immediately.
It is now clear that ( 11) implies ( 10) and ( 12) implies ( 13). Thus, in order to prove Theorem 1, we only need to show (11) under the conditions of case (a), and ( 12) under the conditions of case (b).<h2>publication_ref</h2>['b28', 'b10']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>B. Confining α n<h2>text</h2>As seen in Section V-A, Theorem 1 will follow if we show (11) and (12) under the appropriate conditions. In this subsection, we show that the extra condition α n = o(ln n) can be introduced in the proof of (12). Namely, we will show that part (b) of Theorem 1 under
α n = o(ln n) ⇒ part (b) of Theorem 1(32)
We write G on as G on (n, K n , P n , p n ) and remember that given K n , P n and p n , one can determine α n from (9); just use (7).
Assume that part (b) of Theorem 1 holds under the extra condition α n = o(ln n). The desired result (32) will follow if we establish 
lim n→∞ P G(n, Kn , Pn , pn ) is k-connected = 1(33
and that we have
P[G on (n, Kn , Pn , pn ) is k-connected ] ≥ P[G on (n, Kn , Pn , pn ) is k-connected ].(38)
Notice that Kn , Pn and pn satisfy all the conditions enforced by part (b) of Theorem 1 together with the extra condition αn = o(ln n). Thus, we get
lim n→∞ P[G on (n, Kn , Pn , pn ) is k-connected ] = 1(39)
by the initial assumption, and (33) follows immediately from (38) and (39). Therefore, given any Kn , Pn and pn as stated above, if we can show the existence of Kn , Pn and pn that satisfy ( 35)-( 38), then the desired conclusion (32) will follow. We now establish the existence of Kn , Pn and pn that satisfy ( 35)- (38). Let Pn = Pn and Kn = Kn so that ( 35) is satisfied automatically. Let αn = min { αn , ln ln n}. Hence, we have αn ≤ αn , αn = o(ln n) and lim n→∞ αn = +∞ so that ( 37) is also satisfied. The remaining parameter pn will be defined through
pn •   1 - Pn-Kn Kn Pn Kn   = ln n + (k -1) ln ln n + αn n (40
)
so that pe = pn • 1 -
( Pn -Kn Kn ) ( Pn Kn )
satisfies (36). Thus, it remains to establish (38).
Comparing (40) with (34), it follows that pn ≤ pn since Kn = Kn , Pn = Pn and αn ≤ αn . Consider graphs G on (n, Kn , Pn , pn ), G on (n, Kn , Pn , pn ) that have the same number of nodes n, the same key ring size Kn and the same key pool size Pn , but have different probabilities pn and pn for a link to be on. We will show that there exists a coupling such that G on (n, Kn , Pn , pn ) is a spanning subgraph of G on (n, Kn , Pn , pn ) so that, as shown by Rybarczyk [ 
for any monotone increasingfoot_2 graph property P. The properties of being k-connected and having a minimum node degree of at least k can easily be seen to be monotone increasing graph properties. Therefore, (38) will follow immediately (with Kn = Kn and Pn = Pn ) if (41) holds. We now give the coupling argument that leads to (41). As seen from ( 3), G on is the intersection of a random key graph G(n, K n , P n ) and an Erdős-Rényi graph G(n, p n ). Using graph coupling, we use the same random key graph G(n, Kn , Pn ) to help construct both G on (n, Kn , Pn , pn ) and G on (n, Kn , Pn , pn ). Then we have
G on (n, Kn , Pn , pn ) = G(n, Kn , Pn ) ∩ G(n, pn )(42)
G on (n, Kn , Pn , pn ) = G(n, Kn , Pn ) ∩ G(n, pn ).(43)
Since pn ≤ pn , we couple G(n, pn ) and G(n, pn ) in the following manner. Pick independent Erdős-Rényi graphs G(n, pn /p n ) and G(n, pn ) on the same vertex set. It is clear that the intersection G(n, pn /p n ) ∩ G(n, pn ) will still be an Erdős-Rényi graph (due to independence) with an edge probability given by pn • pn pn = pn . In other words, we have G(n, pn /p n ) ∩ G(n, pn ) = G(n, pn ). Consequently, under this coupling, G(n, pn ) is a spanning subgraph of G(n, pn ). Then from ( 42) and ( 43), G on (n, Kn , Pn , pn ) is a spanning subgraph of G on (n, Kn , Pn , pn ) and ( 41) follows.<h2>publication_ref</h2>['b10', 'b11', 'b11', 'b6', 'b31', 'b37', 'b38', 'b37', 'b35', 'b37', 'b39', 'b33', 'b37', 'b40', 'b40']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C. The Method of First and Second Moments<h2>text</h2>The following fact is based on the method of the first and second moments and will be useful in deriving zero-one laws for the minimum node degree of a graph. We use E[•] to denote the expectation operator.
Fact 1. For any graph G with n nodes, let X ℓ be the number of nodes having degree ℓ in G, where ℓ = 0, 1, . . . , n -1; and let δ be the minimum node degree of G. Then the following three properties hold for any positive integer k.
(a) For any non-negative integer ℓ, if E[X ℓ ] = o(1), then
lim n→∞ P [δ = ℓ] = 0. (44
)
(b) If (44) holds for ℓ = 0, 1, . . . , k -1, then
lim n→∞ P[δ ≥ k] = 1. (c) If E X ℓ 2 ∼ E X ℓ 2 and E X ℓ → +∞ as n →
∞ hold for some ℓ = 0, 1, . . . , k -1, then
lim n→∞ P[δ ≥ k] = 0.
A proof of Fact 1 is given in Appendix B-A.<h2>publication_ref</h2>['b43']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>D. Useful Notation for Graph G on<h2>text</h2>We collect in this section some notation that will be used throughout. For any event A, we let A be the complement of A. Also, for sets S a and S b , the relative complement of S a in S b is given by S a \ S b .
In graph G on , for each node v i ∈ V, we define N i as the set of neighbors of node v i . For any two distinct nodes v x and v y , there are (n-2) nodes other than v x and v y in graph G on . These (n -2) nodes can be split into the four sets N xy , N xy , N xy and N x y as follows. Let N xy be the set of nodes that are neighbors of both v x and v y ; i.e., N xy = N x ∩ N y . Let N xy denote the set of nodes in V \ {v x , v y } that are neighbors of v x , but are not neighbors of v y . Similarly, N xy is defined as the set of nodes in V \ {v x , v y } that are not neighbors of v x , but are neighbors of v y . Finally, N x y is the set of nodes in V \ {v x , v y } that are not connected to either v x or v y .
For any three distinct nodes v x , v y and v j , recalling that E xj (resp., E yj ) is the event that there exists a link between nodes v x (resp., v y ) and v j , we define
E xj∩yj := E xj ∩ E yj , E xj∩yj := E xj ∩ E yj , E xj∩yj := E xj ∩ E yj , and E xj∩yj := E xj ∩ E yj .
In graph G on , for any non-negative integer ℓ, let X ℓ be the number of nodes having degree ℓ; let D x,ℓ be the event that node v x has degree ℓ. We define δ as the minimum node degree of graph G on , and define κ as the connectivity of graph G on . The connectivity of a graph is defined as the minimum number of nodes whose deletion renders the graph disconnected; thus, a graph is k-connected if and only if its connectivity is at least k. Finally, a graph is said to be simply connected if its connectivity is at least 1, i.e., if it is 1-connected.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>VI. ESTABLISHING (11) (THE ZERO-LAW FOR THE MINIMUM NODE DEGREE IN G on )<h2>text</h2>Our main goal in this section is to establish (11) under the following conditions: From property (c) of Fact 1, we see that the proof will be completed if we demonstrate the following two results under the conditions ( 45) and ( 46):
lim n→∞ E X ℓ = +∞,(47)
and
E X ℓ 2 ∼ E X ℓ 2 . (48
)
for some ℓ = 0, 1, . . . , k -1.
The first step in establishing (47) and ( 48) is to compute the moments E [X ℓ ] and E (X ℓ ) 2 . This step is taken in the next Lemma. Recall that in graph G on , X ℓ stands for the number of nodes with degree ℓ for each ℓ = 0, 1, . . .. Also, D x,ℓ is the event that node v x has degree ℓ for each x = 1, 2, . . . , n.
Lemma 1. In G on , for any non-negative integer ℓ and any two distinct nodes v x and v y , we have
E X ℓ = nP [D x,ℓ ] ,(49)
E X ℓ 2 = nP [D x,ℓ ] + n(n -1)P [D x,ℓ D y,ℓ ] . (50
)
Lemma 1 follows from the exchangeability of the indicator random variables {1[D i,ℓ ]; i = 1, . . . , n} upon writing
X ℓ = n i=1 1[D i,ℓ ].
Interested reader is referred to the full version [41] for details.
In view of (49), we will obtain (47) once we show that
lim n→+∞ (nP [D x,ℓ ]) = +∞.(51)
under (45) and (46). Also, from (49) and (50), we get
E X ℓ 2 E X ℓ 2 = 1 nP [D x,ℓ ] + n -1 n • P [D x,ℓ D y,ℓ ] P [D x,ℓ ] 2 . (52
)
Thus, (48) will follow upon showing (51) and
P [D x,ℓ D y,ℓ ] ∼ P [D x,ℓ ] 2(53)
for some ℓ = 0, 1, . . . , k -1 under ( 45) and (46). We establish (51) and (53) from of the following two results.<h2>publication_ref</h2>['b10', 'b40', 'b44', 'b45', 'b45']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Lemma 2.<h2>text</h2>If p e = o 1 √ n , then for any non-negative integer constant ℓ and any node v x ,
P [D x,ℓ ] ∼ (ℓ!) -1 (p e n) ℓ e -pen .(54)
A proof of Lemma 2 is given in Appendix C-A. (a) If there exist an ǫ > 0 such that p e n > ǫ for all n sufficiently large, then for any non-negative integer constant ℓ and any two distinct nodes v x and v y , we have
P [D x,ℓ ∩ D y,ℓ ] ∼ (ℓ!) -2 (p e n) 2ℓ e -2pen . (55
)
(b) For any two distinct nodes v x and v y , we have
P [D x,0 ∩ D y,0 ] ∼ e -2pen .(56)
Proof.
Recalling that E xy is the event that nodes v x and v y are adjacent, we have
P [D x,ℓ ∩ D y,ℓ ] = P[D x,ℓ ∩ D y,ℓ ∩ E xy ] + P[D x,ℓ ∩ D y,ℓ ∩ E xy ].(57)
Thus, Lemma 3 will follow from the following two results.
Proposition 1. Let p s = o(1), K n ≥ 2 for all n sufficiently large and p e = ln n+(k-1) ln ln n+αn n with lim n→∞ α n = -∞. Then, the following two properties hold.
(a) If there exist an ǫ > 0 such that p e n > ǫ for all n sufficiently large, then for any non-negative integer constant ℓ, we have
P[D x,ℓ ∩ D y,ℓ ∩ E xy ] ∼ (ℓ!) -2 (p e n) 2ℓ e -2pen . (58
) (b) We have P[D x,0 ∩ D y,0 ∩ E xy ] ∼ e -2pen .(59)
Proposition 2. Let p s = o(1), K n ≥ 2 for all n sufficiently large and p e = ln n+(k-1) ln ln n+αn n with lim n→∞ α n = -∞. If there exists an ǫ > 0 such that p e n > ǫ for all n sufficiently large, then for any positive integer constant ℓ, we have
P[D x,ℓ ∩ D y,ℓ ∩ E xy ] = o P[D x,ℓ ∩ D y,ℓ ∩ E xy ] . (60)
Propositions 1 and 2 are established in Section VII and Section VIII, respectively. Now, we complete the proof of Lemma 3. Under the condition p e n > ǫ > 0, (55) follows from (58) and (60) in view of (57). For ℓ = 0, we obtain (56) by using (59) in (57) and noting that P[D x,0 ∩D y,0 ∩E xy ] = 0 always holds; it is not possible for nodes v x and v y to have degree zero and yet to have an edge in between.
We now complete the proof of ( 51) and ( 53) under ( 45) and (46). First, in view of ( 9) and the condition lim n→∞ α n = -∞, we obtain p e ≤ ln n+(k-1) ln ln n n for all n sufficiently large. Thus, p e = o 1
√ n , and we use Lemma 2 to get
nP [D x,ℓ ] ∼ n • (ℓ!) -1 (p e n) ℓ e -pen(61)
for each ℓ = 0, 1, . . .. The proof will be given in two steps. First, in the case where there exists an ǫ > 0 such that p e n > ǫ for all n sufficiently large, we will establish (51) and ( 53) for ℓ = k -1. Next, for the case where lim n→∞ p e n = 0, we will show that (51) and (53) hold for ℓ = 0. Assume now that p e n > ǫ > 0 for all n sufficiently large. Substituting ( 9) into (61) with ℓ = k -1, we get
nP [D x,k-1 ](62)
∼ n • [(k -1)!] -1 (p e n) k-1 e -ln n-(k-1) ln ln n-αn = [(k -1)!] -1 × (ln n + (k -1) ln ln n + α n ) k-1 e -(k-1) ln ln n-αn . Let f n (k; α n ) := (ln n + (k -1) ln ln n + α n ) k-1 e -(k-1) ln ln n-αn ,
and observe that we have ln n + (k -1) ln ln n + α n ≥ ǫ for all n sufficiently large since p e n > ǫ. On that range, fix n, pick 0 < γ < 1 and consider the cases α n ≤ -(1γ) ln n and α n > -(1γ) ln n. In the former case, we have
f n (k; α n ) ≥ ǫ • e -(k-1) ln ln n+(1-γ) ln n ,
whereas in the latter we obtain
f n (k; α n ) ≥ (γ ln n) k-1 e -(k-1) ln ln n-αn = γ k-1 e -αn .
Thus, for all n sufficiently large, we have
f n (k; α n ) ≥ min ǫ • e -(k-1) ln ln n+(1-γ) ln n , γ k-1 e -αn .
It is now easy to see that lim n→∞ f n (k; α n ) = ∞ since 0 < γ < 1 and lim n→∞ α n = -∞. Substituting this into (62), we obtain (51) with ℓ = k -1. In addition, from (54) of Lemma 2, and (55) of Lemma 3, it is clear that (53) follows with ℓ = k -1. As mentioned already, (51) and ( 53) imply ( 47) and (48) in view of Lemma 1, and the zero-law ( 11) is now established for the case when p e n > ǫ > 0.
We now turn to the case where lim n→∞ p e n = p ⋆ e = 0. This time, we let ℓ = 0 in (61) and obtain
nP [D x,0 ] ∼ ne -pen ∼ n.
We clearly have (51) for ℓ = 0. Also, from (54) of Lemma 2 with ℓ = 0, and (56) of Lemma 3, we obtain (53) for ℓ = 0. Having obtained (51) and ( 53) for ℓ = 0, we get (47) and ( 48) and the zero-law ( 11) is now established from Fact 1 (c).<h2>publication_ref</h2>['b45']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>VII. A PROOF OF PROPOSITION 1<h2>text</h2>We start by noting that D x,ℓ ∩ D y,ℓ ∩ E xy stands for the event that nodes v x and v y both have ℓ neighbors but are not neighbors with each other. To compute its probability, we specify all the possible cardinalities of sets N xy , N xy and N xy , defined in Section V-D. To this end, we define the series of events A h in the following manner
A h = [|N xy | = h] [|N xy | = ℓ -h] [|N xy | = ℓ -h] (63)
for each h = 0, 1, . . . , ℓ; here, |S| denotes the cardinality of the discrete set S.
It is now a simple matter to check that
D x,ℓ ∩ D y,ℓ ∩ E xy = ℓ h=0 A h ∩ E xy . (64
)
for each ℓ = 0, 1, . . .. Using (64) and the fact that the events A h (h = 0, 1, . . . , ℓ) are mutually exclusive, we obtain
P D x,ℓ ∩ D y,ℓ ∩ E xy = ℓ h=0 P A h ∩ E xy . (65
)
We begin computing the right hand side (R.H.S.) of (65) by evaluating E xy . From (2), we have
E xy = K xy ∩ C xy . Hence E xy = K xy ∪ C xy = K xy ∪ (K xy ∩ C xy ).(66)
Also, by definition we have
K xy = Kn u=1 (|S xy | = u).(67)
For each u = 1, 2, . . . , K n , we define event X u as follows:
X u = (|S xy | = u) ∩ C xy(68)
Applying (67) to (66) and using (68), we obtain
E xy = K xy ∪ Kn u=1 (|S xy | = u) ∩ C xy = K xy ∪ Kn u=1 X u .(69)
From (69) and the fact that the events K xy , X 1 , X 2 , . . . , X Kn are mutually disjoint, we obtain
P A h ∩ E xy = P A h ∩ K xy + Kn u=1 P [A h ∩ X u ] .(70)
Substituting (70) into (65), we get
P D x,ℓ ∩ D y,ℓ ∩ E xy = ℓ h=0 P A h ∩ K xy + ℓ h=0 Kn u=1 P [A h ∩ X u ] .(71)
Proposition 1 will follow from the next two results.
Proposition 1.1. Let ℓ be a non-negative integer constant. If p s = o(1), p e = ln n+(k-1) ln ln n+αn n with lim n→∞ α n = -∞, then
ℓ h=0 P A h ∩ K xy ∼ (ℓ!) -2 (p e n) 2ℓ e -2pen . (72
)
Proposition 1.2. Let ℓ be a non-negative integer constant. Consider p s = o(1), K n ≥ 2 for all n sufficiently large and p e = ln n+(k-1) ln ln n+αn n with lim n→∞ α n = -∞. Then, the following two properties hold.
(a) If there exists an ǫ > 0 such that p e n > ǫ for all n sufficiently large, then we have
ℓ h=0 Kn u=1 P [A h ∩ X u ] = o ℓ h=0 P A h ∩ K xy . (73
) (b) We have Kn u=1 P [A 0 ∩ X u ] = o P A 0 ∩ K xy .(74)
In order to see why Proposition 1 follows from Propositions 1.1 and 1.2, consider p s and p e as stated in Proposition 1. Then from Propositions 1.1 and 1.2, (72) and (73) hold. Substituting (72) and ( 73) into (71), we get (58). Also, using (72) with ℓ = 0 we get P A 0 ∩ K xy ∼ e -2pen . Using this and (74) in (71) with ℓ = 0, we obtain (59) and Proposition 1 is then established. The rest of this section is devoted to establishing Propositions 1.1 and 1.2. We will establish Proposition 2 in the next Section VIII, and this will complete the proof of Lemma 3 and thus the zero-law (11).<h2>publication_ref</h2>['b10']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. A Proof of Proposition 1.1<h2>text</h2>Given
P[K xy ] = 1 -p s → 1 as n → ∞, it is clear that ℓ h=0 P A h ∩ K xy ∼ ℓ h=0 P A h | K xy(75)
The next result evaluates a generalization of P A h | K xy . In addition to the proof of Proposition 1.1 here, the proofs of Propositions 1.2 and 2.1 also use Lemma 4.
Lemma 4. Let m 1 , m 2 and m 3 be non-negative integer constants. We define event F as follows.
F := [|N xy | = m 1 ] [|N xy | = m 2 ] [|N xy | = m 3 ] . (76
)
Then given u in {0, 1, . . . , K n } and p e = ln n+(k-1) ln ln n+αn n with lim n→∞ α n = -∞, we have
P [F | (|S xy | = u)] ∼ n m1+m2+m3 m 1 !m 2 !m 3 ! • e -2pen+ pepn u Kn n × {P[E xj∩yj | (|S xy | = u)]} m1 × {P[E xj∩yj | (|S xy | = u)]} m2 × {P[E xj∩yj | (|S xy | = u)]} m3 (77)
with j distinct from x and y.
A proof of Lemma 4 is given in Appendix C-B.
Given the definition of A h in (63) and K xy ⇔ (|S xy | = 0), we let m 1 = h, m 2 = m 3 = ℓh and u = 0 in Lemma 4 in order to compute P A h | K xy . We get
P A h | K xy ∼ n 2ℓ-h h![(ℓ -h)!] 2 • e -2pen • P[E xj∩yj | K xy ] h × {P[E xj∩yj | K xy ]} ℓ-h {P[E xj∩yj | K xy ]} ℓ-h . (78)
In order to compute the R.H.S. of (78), we evaluate the following three terms in turn:
P[E xj∩yj | K xy ], P[E xj∩yj | K xy ], and P[E xj∩yj | K xy ].
For the first term
P[E xj∩yj | K xy ], we use E xj = K xj ∩ C xj and E yj = K yj ∩ C yj to obtain P[E xj∩yj | K xy ] = P[(C xj ∩ C yj ) ∩ (K xj ∩ K yj ) | K xy ]. = p n 2 • P[K xj ∩ K yj | K xy ](79)
Applying Lemma 9 (Appendix A-B) to (79) and using the definition p e = p n p s , we get
P[E xj∩yj | K xy ] ≤ p e 2 . (80
)
We now evaluate the second term
P[E xj∩yj | K xy ]. It is clear that E xj is independent of K xy . Hence, P[E xj | K xy ] = p e .(81)
Since p e = ln n+(k-1) ln ln n+αn n with lim n→∞ α n = -∞, we have p e = o 1 √ n . Together with (80), (81) this yields
P[E xj∩yj | K xy ] = P[E xj | K xy ] -P[E xj∩yj | K xy ] = p e -O p e 2 ∼ p e .(82)
Similarly, for the third term P[E xj∩yj | K xy ], we have
P[E xj∩yj | K xy ] ∼ p e .(83)
Now we compute the R.H.S. of (78). Substituting (82) and (83) into R.H.S. of (78), given constant ℓ, we obtain
P A h | K xy ∼ n 2ℓ-h h![(ℓ -h)!] 2 • e -2pen • P[E xj∩yj | K xy ] h • p 2(ℓ-h) e . (84
)
for each h = 0, 1, . . . , ℓ. Thus, for h = 0, we have
P A 0 | K xy ∼ (ℓ!) -2 (p e n) 2ℓ e -2pen .(85)
For h = 1, 2, . . . , ℓ, we use (80) and (84) to get
P A h | K xy P A 0 | K xy ∼ n -h (ℓ!) 2 h![(ℓ -h)!] 2 P[E xj∩yj | K xy ] h p -2h e ≤ n -h (ℓ!) 2 h![(ℓ -h)!] 2 = o(1).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Thus, we have<h2>text</h2>P A h | K xy = o P A 0 | K xy , h = 1, 2, . . . , ℓ. (86)
Applying ( 85) and ( 86) to (75), we obtain the desired conclusion (72) (for Propostion 1.1) since ℓ is constant.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>B. A Proof of Proposition 1.2<h2>text</h2>Notice that (74) can be obtained from (73) by setting ℓ = 0. Thus, in the discussion given below, we will establish (73) for each ℓ = 0, 1, . . . under p e n = Ω(1), and show that this extra condition is not needed if ℓ = 0.
We start by finding an upper bound on the left hand side (L.H.S.) of (73). Given the definition of X u in (68), we obtain
P [A h ∩ X u ] ≤ P [A h ∩ (|S xy | = u)] .
Then, we have
ℓ h=0 Kn u=1 P [A h ∩ X u ] ≤ ℓ h=0 Kn u=1 P [A h ∩ (|S xy | = u)] = Kn u=1 P[|S xy | = u] • ℓ h=0 P [A h | (|S xy | = u)] . (87)
To compute the R.H.S. of (87), we first use Lemma 10 to get
P[|S xy | = u] ≤ 1 u! K 2 n P n -K n u .(88)
Next, we compute
P [A h | (|S xy | = u)]
. Given (63), we let m 1 = h and m 2 = m 3 = ℓh in Lemma 4 and obtain
P [A h | (|S xy | = u)] ∼ n 2ℓ-h h![(ℓ -h)!] 2 • e -2pen+ pepnu Kn n × {P[E xj∩yj | (|S xy | = u)]} h × {P[E xj∩yj | (|S xy | = u)]} ℓ-h × {P[E xj∩yj | (|S xy | = u)]} ℓ-h . (89
)
From E xj = C xj ∩ K xj and E yj = C yj ∩ K yj , it is clear that E xj and E yj are independent of (|S xy | = u). This leads
P[E xj∩yj | (|S xy | = u)] ≤ P[E xj | (|S xy | = u)] = p e (90) P[E xj∩yj | (|S xy | = u)] ≤ P[E xj | (|S xy | = u)] = p e (91) P[E xj∩yj | (|S xy | = u)] ≤ P[E yj | (|S xy | = u)] = p e . (92)
Applying (90), ( 91) and ( 92) to (89), we obtain
P [A h | (|S xy | = u)] ≤ 2n 2ℓ-h • e -2pen+ pepnnu Kn • (p e ) 2ℓ-h = 2e -2pen+ pepn nu Kn (p e n) 2ℓ-h (93)
for all n sufficiently large.
Applying (93) to (87), we derive for all n sufficiently large From ( 72) and (97), we have R.H.S. of (94)
= ℓ h=0 P A h ∩ K xy • O((ℓ!) 2 ) • Kn u=1 K 2 n P n -K n • e pnpen Kn u . (98
)
If we show that
K 2 n P n -K n • e pn Kn •pen = o(1),(99)
then we obtain
Kn u=1 K 2 n P n -K n • e pn pen Kn u ≤ K 2 n Pn-Kn • e pn Kn •pen 1 - K 2 n Pn-Kn • e pn Kn •pen = o(1),(100)
leading to (73) given ( 98) and the fact that ℓ is constant. Now we prove (99). Given p e = ln n+(k-1) ln ln n+αn n with lim n→∞ α n = -∞ we have p e ≤ 3 2 • ln n n for all sufficiently large n. Recalling also that K n ≥ 2, we get . We now obtain
K 2 n P n -K n ∼ K 2 n P n ∼ p s .
Then,
K 2 n
Pn-Kn ≤ 2p s for all n sufficiently large. Hence, on the same range, we see from (101) that
K 2 n P n -K n • e pn Kn •pen ≤ 2p s • e 3 4 pn ln n . (102
)
In order to evaluate the R.H.S. of (102), we define
F (n) = 2p s • e 3 4 pn ln n . (103
)
With p n p s = p e ≤ 3 2 • ln n n for all n sufficiently large, we note that
p s ≤ 3 2 ln n np n . (104
)
Now, fix n large enough such that (102) and (104) hold. We consider the cases p n ≤ 1 ln n and p n > 1 ln n , separately. In the former case, we have F (n) ≤ 2p s e 3/4 immediately from (103). In the latter case we use the bound (104) to get
F (n) ≤ 3 ln n np n e 3 4 pn ln n < 3 (ln n) 2 n • n 3/4
upon noting that p n ≤ 1. Combining the two bounds, we have
F (n) ≤ max 2p s e 3/4 , 3n -1/4 (ln n) 2 (105)
for all n sufficiently large. Letting n grow large and recalling that p s = o(1) we obtain lim n→∞ F (n) = 0. This establishes (99) in view of (102), and (95) follows from ( 98) and (100) for constant ℓ. From ( 94) and (95), we finally establish the desired conclusion (73). Note that (74) also follows since the extra condition p e n = Ω( 1) is used only once in obtaining (96) which holds trivially for ℓ = 0. The proof of Proposition 1.2 is thus completed.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>VIII. A PROOF OF PROPOSITION 2<h2>text</h2>Given (71) and Proposition 1.2 (property (a)), it is clear that Proposition 2 will follow if we show for each ℓ = 1, 2 . . . that
P[D x,ℓ ∩ D y,ℓ ∩ E xy ] = o ℓ h=0 P A h ∩ K xy . (106)
In order to establish (106), we evaluate P[D x,ℓ ∩D y,ℓ ∩E xy ] proceeding similarly as in the proof of Proposition 1. To this end, we define the series of events B h in the following manner
B h = (|N xy | = h) (|N xy | = ℓ -h -1) (|N xy | = ℓ -h -1) . (107
)
for each h = 0, 1, . . . , ℓ -1. An analog of (64) follows immediately for any positive integer ℓ.
D x,ℓ ∩ D y,ℓ ∩ E xy = ℓ-1 h=0 (B h ∩ E xy ) . (108
)
The minus one term on ℓ is due to the fact that x and y are adjacent on event E xy ; there can be at most ℓ -1 nodes that are neighbors of both x and y on D x,ℓ ∩ D y,ℓ ∩ E xy . Given (108) and mutually exclusive events B h (h = 0, 1, . . . , ℓ -1), we obtain
P[D x,ℓ ∩ D y,ℓ ∩ E xy ] = ℓ-1 h=0 P [B h ∩ E xy ] .(109)
We will establish Proposition 2 by obtaining the following result which evaluates the R.H.S. of (109). 
P [B h ∩ E xy ] = o ℓ h=0 P A h ∩ K xy . (110
)
In order to see why Proposition 2 follows from Proposition 2.1, observe that (110) establishes (106) with the help of (109). As noted before, this establishes Proposition 2. 
P [B h ∩ E xy ] = P B h ∩ Kn u=1 Y u = Kn u=1 P [B h ∩ Y u ] .(111)
Given
Y u = [(|S xy | = u) ∩ C xy ],
we obtain
P [B h ∩ Y u ] ≤ P [B h ∩ (|S xy | = u)] .(112)
Applying ( 112) to (111), it follows that
ℓ-1 h=0 P [B h ∩ E xy ] ≤ ℓ-1 h=0 Kn u=1 P [B h ∩ (|S xy | = u)] = Kn u=1 P[|S xy | = u] • ℓ-1 h=0 P [B h | (|S xy | = u)] .(113)
R.H.S. of ( 113) is similar to the R.H.S. of (87), whence it will be computed in a similar manner. We first calculate
P [B h | (|S xy | = u)]. Given the definition of B h in (107), we let m 1 = h and m 2 = m 3 = ℓ -h -1 in Lemma 4 to obtain P [B h | (|S xy | = u)] ∼ n 2ℓ-h-2 h![(ℓ -h -1)!] 2 • e -2pen+ pepnu Kn n × {P[E xj∩yj | (|S xy | = u)]} h × {P[E xj∩yj | (|S xy | = u)]} ℓ-h-1 × {P[E xj∩yj | (|S xy | = u)]} ℓ-h-1 . (114
)
Substituting ( 90), ( 91) and ( 92) into (114), we obtain
P [B h | (|S xy | = u)] ≤ 2e -2pen+ pepnnu Kn (p e n) 2ℓ-h-2 .
(115) for all n sufficiently large.
Returning to the evaluation of the R.H.S. of (113), we apply (115) to ( 113) and obtain for all n sufficiently large,
ℓ-1 h=0 P [B h ∩ E xy ] ≤ Kn u=1 P[|S xy | = u] • 2e -2pen+ pn u Kn •pen • ℓ h=0 (p e n) 2ℓ-h-2 = (p e n) -2 × R.H.S. of (94). (116
)
From p e n = Ω(1), it follows that
ℓ-1 h=0 P [B h ∩ E xy ] = O (R.H.S. of (94)) .(117)
Given ( 95) and (117), we obtain (110) and this completes the proof of Proposition 2.
Having established Propositions 1 and 2, we prove Lemma 3, and the zero-law (11) follows as explained in Section VI.<h2>publication_ref</h2>['b10']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>IX. ESTABLISHING (12) (THE ONE-LAW<h2>text</h2>FOR k-CONNECTIVITY IN G on )
As shown in Section V-B, we can enforce the extra condition α n = o(ln n) in establishing (12) (i.e., the one-law for kconnectivity in G on ). Therefore, we will establish (12) under the following conditions: (9), K n ≥ 2 for all n sufficiently large , P n = Ω(n), (118)
K n P n = o(1), lim n→∞ α n = +∞ and α n = o(ln n).(119)
In graph G on , consider scalings K, P : N 0 → N 0 and p : N 0 → (0, 1) as in Theorem 1. We find it useful to define a sequence β ℓ,n : N × N 0 → R through the relation
p e = ln n + ℓ ln ln n + β ℓ,n n(120)
for each n ∈ N 0 and each ℓ ∈ N. (120) follows by just setting
β ℓ,n := np e -ln n -ℓ ln ln n. (121
)
The one-law (12) will follow from the next key result. Recall that, as defined in Section V-D, κ is the connectivity of the graph G on , namely the minimum number nodes whose deletion makes it disconnected.
Lemma 5. Let ℓ be a non-negative constant integer. If K n ≥ 2 for any sufficiently large n, P n = Ω(n), Kn Pn = o(1), and (120) holds with β ℓ,n = o(ln n) and lim n→∞ β ℓ,n = +∞, then
lim n→∞ P [κ = ℓ] = 0. (122
)
We now explain why the one-law (12) follows from Lemma 5. Consider p n , K n and P n such that (118) and (119) hold. Comparing ( 9) and (120), we get
β ℓ,n = (k -1 -ℓ) ln ln n + α n . (123
)
Since α n = o(ln n) and lim n→∞ α n = +∞, we have for each ℓ = 0, 1, . . . , k -1 that
lim n→∞ β ℓ,n = +∞ and β ℓ,n = o(ln n).(124)
Given (124), we use Lemma 5 and obtain
lim n→∞ P [κ = ℓ] = 0, ℓ = 0, 1, . . . , k -1.
For any constant k, this implies lim n→∞ P [κ ≥ k] = 1, or equivalently
lim n→∞ P [G on is k-connected ] = 1.
This completes the proof of the one-law (12). The remaining part of this section is devoted to the proof of Lemma 5. Proof. We present the steps of proving Lemma 5 below. First, by a crude bounding argument, we get
P [κ = ℓ] ≤ P [(κ = ℓ) ∩ (δ > ℓ)] + P [δ ≤ ℓ] ,
where δ is the minimum node degree of graph G on , as defined in Section V-D. We will prove Lemma 5 by establishing the following two results under the enforced assumptions:
lim n→∞ P [δ ≤ ℓ] = 0 if lim n→∞ β ℓ,n = +∞,(125)
and
lim n→∞ P [κ = ℓ ∩ δ > ℓ] = 0 if lim n→∞ β ℓ,n = +∞.(126)
We first establish (125). First, from ℓ ln ln n = o(ln n), β ℓ,n = o(ln n) and p e = ln n+ℓ ln ln n+β ℓ,n n
, it is clear that p e ∼ ln n n . Then p e = o 1 √ n . Thus, from Lemmas 1 and 2, we get
E X ℓ = nP [D x,ℓ ] ∼ n • (ℓ!) -1 (p e n) ℓ e -pen .(127)
Substituting p e ∼ ln n n and (120) into (127), we get
E X ℓ ∼ n (ℓ!) -1 (ln n) ℓ e -ln n-ℓ ln ln n-β ℓ,n = (ℓ!) -1 e -β ℓ,n .
In view of the fact that lim n→∞ β ℓ,n = +∞, we thus obtain E X ℓ = o(1). Then from property (a) of Fact 1 (Section V-C), we get
lim n→∞ P[δ = ℓ] = 0.(128)
As seen from ( 121), β ℓ,n is decreasing in ℓ. Thus, we have
lim n→∞ β ℓ ⋆ ,n = +∞ for each ℓ ⋆ = 0, 1, . . . , ℓ. It is also im- mediate from (121) that β ℓ ⋆ ,n = o(ln n) since β ℓ,n = o(ln n).
Therefore, using the same arguments that lead to (128), we obtain
lim n→∞ P[δ = ℓ ⋆ ] = 0, ℓ ⋆ = 0, 1, . . . , ℓ,
and (125) follows immediately. As ( 125) is established, it remains to prove (126) in order to complete the proof of Lemma 5. The basic idea in establishing ( 126) is to find a sufficiently tight upper bound on the probability P [κ = ℓ ∩ δ > ℓ] and then to show that this bound tends to zero as n goes to +∞. This approach is similar to the one used for proving the one-law for k-connectivity in Erdős-Rényi graphs [12], as well as to the approach used by Yagan [37] to establish the one-law for connectivity in the graph G on .
We start by obtaining the needed upper bound. Let N denote the collection of all non-empty subsets of {v 1 , . . . , v n }. We define
N * = {T | T ∈ N , |T | ≥ 2} and K T = ∪ vi∈T S i .
For the reasons that will later become apparent we find it useful to introduce the event E(J ) in the following manner:
E(J ) = T ∈N * |K T | ≤ J |T | ,(129)
where J = [J 2 , J 3 , . . . , J n ] is an (n -1)-dimensional integer valued array. Let
r n := min P n K n , n 2 . (130
)
We define J i as follows:
J i = max{⌊(1 + ε)K n ⌋ , ⌊λK n i⌋} i = 2, . . . , r n , ⌊µP n ⌋ i = r n + 1, . . . , n.(131)
for some arbitrary constant 0 < ε < 1 and constants λ, µ in (0, 1 2 ) that will be specified later; see (134)-(135) below. By a crude bounding argument we now get
P [(κ = ℓ) ∩ (δ > ℓ)] ≤ P [E(J )] + P (κ = ℓ) ∩ (δ > ℓ) ∩ E(J ) . (132)
Hence, a proof of (126) consists of establishing the following two propositions.
Proposition 3. Let ℓ be a non-negative constant integer. If (120) holds with β ℓ,n > 0, K n ≥ 2 and P n ≥ σn for some σ > 0 for all n sufficiently large and Kn Pn = o(1), then
lim n→∞ P [E(J )] = 0,(133)
where J = [J 2 , J 3 , . . . , J n ] is as specified in (131) with arbitrary ε in (0, 1), constant λ in (0, 1 2 ) is selected small enough to ensure
max 2λσ, λ e 2 σ λ 1-2λ < 1,(134)
and constant µ in (0, 1 2 ) is selected so that
max 2 √ µ e µ µ σ , √ µ e µ µ < 1.(135)
A proof of Proposition 3 is given in Section X below. Note that for any σ > 0, lim λ↓0 λ e 2 σ λ 1-2λ = 0 so that the condition (134) can always be met by suitably selecting constant λ > 0 small enough. Also, we have lim µ↓0 e µ µ = 1, whence
lim µ↓0 √ µ e µ µ
= 0, and (135) can be made to hold for any constant σ > 0 by taking µ > 0 sufficiently small. Finally, we remark that the condition P n ≥ σn for some σ > 0 is equivalent to having P n = Ω(n). Proposition 4. Let ℓ be a non-negative constant integer. If K n ≥ 2 and P n ≥ σn for some σ > 0 for all n sufficiently large, Kn Pn = o(1), and (120) holds with β ℓ,n = o(ln n) and lim n→∞ β ℓ,n = +∞, then
lim n→∞ P (κ = ℓ) ∩ (δ > ℓ) ∩ E(J ) = 0,
where J = [J 2 , J 3 , . . . , J n ] is as specified in (131) with arbitrary ε in (0, 1), constant µ in (0, 1  2 ) selected small enough to ensure (135) and constant λ ∈ (0, 1  2 ) selected such that it satisfies (134).
A proof of Proposition 4 is given in Section XI below.
Using Proposition 3 and Proposition 4 (with the same constants ε, λ, µ) in (132), we obtain the desired conclusion (126). The proof of Lemma 5 is now completed.<h2>publication_ref</h2>['b11', 'b11', 'b11', 'b36']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>X. A PROOF OF PROPOSITION 3<h2>text</h2>We begin by finding an upper bound on the probability P [E(J )]. To this end, we define
Y i = ⌊λK n i⌋ i = 2, . . . , r n , ⌊µP n ⌋ i = r n + 1, . . . , n.(136)
From ( 131) and (136), we get
J i = max{⌊(1 + ε)K n ⌋ , Y i } i = 2, . . . , r n , Y i i = r n + 1, . . . , n.(137)
We also define
N -:= {T | T ∈ N , 2 ≤ |T | ≤ r n },
and
N + := {T | T ∈ N , |T | > r n }.
Using the definition (129) and the fact that J i = Y i for i = r n + 1, r n + 2, . . . , n, we get
E(J ) =   T ∈N- |K T | ≤ J |T |   ∪   T ∈N+ |K T | ≤ Y |T |   .(138)
Given
J i = max{⌊(1 + ε)K n ⌋, Y i } for i = 2, 3, . . . , r n , we have   T ∈N- |K T | ≤ J |T |   (139) =   T ∈N- [|K T | ≤ (1 + ε)K n ]   ∪   T ∈N- |K T | ≤ Y |T |   .
From ( 138), (139) and the fact that N * = N -∪ N + , we obtain
E(J ) (140) =   T ∈N- [|K T | ≤ (1 + ε)K n ]   ∪ T ∈N * |K T | ≤ Y |T | .
It is easy to check by direct inspection that
T ∈N- [|K T | ≤ (1 + ε)K n ] = T ∈Nn,2 [|K T | ≤ (1 + ε)K n ]
(141) where N n,2 denotes the collection of all subsets of {v 1 , . . . , v n } with exactly two elements. With Y = [Y 2 , Y 3 , . . . , Y n ] and
E(Y ) = T ∈N * |K T | ≤ Y |T |(142)
it is also easy to see that
E(J ) =   T ∈Nn,2 [|K T | ≤ (1 + ε)K n ]   ∪ E(Y ).
upon using (141) and (142) in (140). Using a standard union bound, we now get
P [E(J )] ≤ P [E(Y )] + T ∈Nn,2 P [|K T | ≤ (1 + ε)K n ] .
It was shown in [37, Proposition 7.2] that given P n = Ω(n) and lim n→∞ K n = ∞, we have
P [E(Y )] = o(1).(143)
Noting that lim n→∞ K n = ∞ holds in view of Lemma 7 and P n = Ω(n) by assumption, we conclude that (143) holds under the assumptions enforced in Proposition 3.
In order to compute T ∈Nn,2
[|K T | ≤ (1 + ε)K n ],
we use exchangeability and the fact that
|N n,2 | = n 2 . With K 1,2 = S 1 ∪ S 2 , we find P [E(J )] ≤ o(1) + n 2 P [K 1,2 ≤ ⌊(1 + ε)K n ⌋] .(144)
Then, from (144), the desired conclusion (133) (for Proposition 3) will follow if we show that
n 2 P [K 1,2 ≤ ⌊(1 + ε)K n ⌋] = o(1).(145)
This will also be established by means of the bounds given in [36]. To this end, it was shown [36,Proposition 7.4.11, under the condition Kn Pn = o(1) that
P [K 1,2 ≤ ⌊(1 + ε)K n ⌋] ≤ Γ(ε) K n P n Kn(1-ε)
, with Γ(ε) := (1 + ε)e 1+ε 1-ε . Using this bound, we now obtain
n 2 P [K 1,2 ≤ ⌊(1 + ε)K n ⌋] ≤ Γ(ε)n 2 (1-ε)Kn K n P n Kn(1-ε) . (146
)
Given P n ≥ σn and Kn Pn = o(1), there exist a sequence w n satisfying lim n→+∞ w n = ∞ such that for all n sufficiently large, we have
P n ≥ max{σn, K n w n }.
As noted before, it also holds that lim n→∞ K n = ∞ in view of Lemma 7. It is now easy to see that
n 2 Kn (1-ε) K n P n ≤ min n -1+ 2 Kn (1-ε) K n σ , e 2 ln n Kn (1-ε) w n ≤ max n -1 2 ln n σ , e 2 (1-ε)
w n for all n sufficiently large to ensure that K n ≥ 4/(1ε). The last inequality follows by considering the cases K n ≥ ln n and K n < ln n separately for each n on the given range. It follows that
lim n→∞ Γ(ε)n 2 Kn (1-ε) K n P n = 0,
and the desired conclusion (145) follows from (146). Proposition 3 is now established.<h2>publication_ref</h2>['b35', 'b35']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>XI. A PROOF OF PROPOSITION 4<h2>text</h2>We start by introducing some notation. For any non-empty subset U of nodes, i.e., U ⊆ {v 1 , . . . , v n }, we define the graph G on (U ) (with vertex set U ) as the subgraph of G on restricted to the nodes in U . If all nodes in U are deleted from G on , the remaining graph is given by G on (U c ) on the vertices U c = {v 1 , . . . , v n } \ U . Let N U c denote the collection of all nonempty subsets of {v 1 , . . . , v n } \ U . We say that a subset T in N U c is isolated in G on (U c ) if there are no edges (in G on ) between the nodes in T and the nodes in U c \ T . This is characterized by
E ij , v i ∈ T, v j ∈ U c \ T.
With each non-empty subset T ⊆ U c of nodes, we associate several events of interest: Let C T denote the event that the subgraph G on (T ) is itself connected. The event C T is completely determined by the random variables (rvs) {S i , v i ∈ T } and {C ij , v i , v j ∈ T }. We also introduce the event D U,T to capture the fact that T is isolated in G on (U c ), i.e., D U,T :=
vi∈T vj ∈U c \T E ij .
Finally, we let B U,T denote the event that each node in U has an edge with at least one node in T , i.e., B U,T :=
vi∈U vj ∈T E ij .
We also set
A U,T := B U,T ∩ C T ∩ D U,T .
The proof starts with the following observations: In graph G on , if the connectivity is ℓ (i.e., κ = ℓ) and yet each node has degree at least ℓ + 1 (i.e., δ > ℓ), then there must exist subsets U , T of nodes with
U ∈ N , |U | = ℓ and T ∈ N U c , |T | ≥ 2, such that G on (T ) is connected while T is isolated in G on (U c
). This ensures that G on can be disconnected by deleting an appropriately selected set of ℓ nodes; i..e, nodes in U . Notice that, this would not be possible for sets T in N U c with |T | = 1, since the degree of a node in T is at least ℓ+1 by virtue of the event δ > ℓ; this ensures that a single node in T is connected to at least one node in U c \ T . Moreover, the event κ = ℓ also enforces G on to remain connected after the deletion of any ℓ -1 nodes. Therefore, if there exists a subset U (with |U | = ℓ) such that some T in N U c is isolated in G on (U c ), then each of the ℓ nodes in U should be connected to at least one node in T and to at least one node in U c \ T . This can easily be seen by contradiction: Consider subsets U ∈ N with |U | = ℓ, and T ∈ N U c with |T | ≥ 2, such that there exists no edge between the nodes in T and the nodes in U c \ T . Suppose there exists a node v i in U such that v i is connected to at least one node in U c \ T but is not connected to any node in T . Then, G on can be disconnected by deleting the nodes in U \ {v i } since there will be no edge between the nodes in T and the nodes in {v i } ∪ U c \ T . But, |U \ {v i }| = ℓ -1, and this contradicts the fact that κ = ℓ.
The inclusion
[(κ = ℓ) ∩ (δ > ℓ)] ⊆ U∈N n,ℓ , T ∈N U c : |T |≥2 A U,T
is now immediate with N n,r denoting the collection of all subsets of {v 1 , . . . , v n } with exactly r elements. It is also easy to check that this union need only be taken over all subsets T of {v 1 , . . . , v n } with 2 ≤ |T | ≤ ⌊ n-ℓ 2 ⌋. We now use a standard union bound argument to obtain
P (κ = ℓ) ∩ (δ > ℓ) ∩ E(J ) ≤ U∈N n,ℓ ,T ∈N U c : 2≤|T |≤⌊ n-ℓ 2 ⌋ P A U,T ∩ E(J ) = ⌊ n-ℓ 2 ⌋ r=2 U∈N n,ℓ ,T ∈N U c ,r P A U,T ∩ E(J )(147)
with N U c ,r denoting the collection of all subsets of U c with exactly r elements.
For each r = 1, . . . , nℓ -1, we simplify the notation by writing A ℓ,r := A {v1,...,v ℓ },{v ℓ+1 ,...,v ℓ+r } , D ℓ,r := D {v1,...,v ℓ },{v ℓ+1 ,...,v ℓ+r } , B ℓ,r := B {v1,...,v ℓ },{v ℓ+1 ,...,v ℓ+r } and C r := C {v ℓ+1 ,...,v ℓ+r } . Under the enforced assumptions on the system model (viz. Section III), exchangeability yields
P [A U,T ] = P [A ℓ,r ] , U ∈ N n,ℓ , T ∈ N U c ,r
and the expression
U∈N n,ℓ ,T ∈N U c ,r P A U,T ∩ E(J ) = n ℓ n -ℓ r P A ℓ,r ∩ E(J ) follows since |N n,ℓ | = n ℓ and |N U c ,r | = n-ℓ r .
Substituting into (147) we obtain the key bound
P (κ = ℓ) ∩ (δ > ℓ) ∩ E(J ) ≤ ⌊ n-ℓ 2 ⌋ r=2 n ℓ n -ℓ r P A ℓ,r ∩ E(J ) . (148
)
The proof of Proposition 4 will be completed once we show
lim n→∞ ⌊ n-ℓ 2 ⌋ r=2 n ℓ n -ℓ r P A ℓ,r ∩ E(J ) = 0.(149)
The means to do so are provided in the next section.
XII. BOUNDING PROBABILITIES P A ℓ,r ∩ E(J )
First, for r = 2, 3, . . . , nℓ -1, observe the equivalence
D ℓ,r = n j=r+ℓ+1 ∪ i∈νr,j S i ∩ S j = ∅(150)
where ν r,j is defined via
ν r,j := {i = ℓ + 1, ℓ + 2, . . . , ℓ + r : C ij }(151)
for each j = 1, 2, . . . , ℓ and j = r + ℓ + 1, r + ℓ + 2, . . . , n. In words, ν r,j is the set of indices in i = ℓ + 1, ℓ + 2, . . . , ℓ + r for which v i is connected to the node v j in the communication graph G(n; p n ). Thus, the event ∪ i∈νr,j S i ∩ S j = ∅ ensures that node v j is not connected (in G on ) to any of the nodes {v ℓ+1 , . . . , v ℓ+r }. Under the enforced assumptions on the rvs S 1 , S 2 , . . . , S n , we readily obtain the expression
P   D ℓ,r S i , i = ℓ + 1, . . . , ℓ + r C ij , i = ℓ + 1, . . . , ℓ + r, j = ℓ + r + 1, . . . , n   = n j=r+ℓ+1   Pn-|∪i∈ν r,j Si| Kn Pn Kn   .
In a similar manner, we find
P   B ℓ,r S i , i = ℓ + 1, . . . , ℓ + r C ij , i = 1, . . . , ℓ, j = ℓ + 1, . . . , ℓ + r   = ℓ j=1   1 - Pn-|∪i∈ν r,j Si| Kn Pn Kn   .
It is clear that the distributional properties of the term | ∪ i∈νr,j S i | will play an important role in efficiently bounding P [D ℓ,r ] and P [B ℓ,r ]. Note that it is always the case that
| ∪ i∈νr,j S i | ≥ K n 1 [|ν r,j | > 0] .(152)
Also, on the event E(J ), we have
| ∪ i∈νr,j S i | ≥ J |νr,j| + 1 • 1 [|ν r,j | > 1](153)
for each j = r +ℓ+1, . . . , n. Finally, we note the crude bound
| ∪ i∈νr,j S i | ≤ |ν r,j |K n (154
)
for each j = 1, . . . , ℓ.
Conditioning on the rvs S ℓ+1 , . . . , S r+ℓ and {C ij , i, j = ℓ + 1, . . . , ℓ + r} (which determine the event C r ), we conclude via (152)-(154) that
P A ℓ,r ∩ E(J ) = P C r ∩ B ℓ,r ∩ D ℓ,r ∩ E(J ) ≤ E     1 [C r ] × ℓ j=1 1 - ( Pn -Kn |ν r,j | Kn ) ( Pn Kn ) × × n j=r+ℓ+1 ( Pn -L(ν r,j ) Kn ) ( Pn Kn )     ,
where for notational convenience we have set
L(ν r,j ) = max {K n • 1 [|ν r,j | > 0] , (155
) (J |νr,j | + 1) • 1 [|ν r,j | > 1] .
It is immediate that the rvs {|ν r,j |} n j=r+1+ℓ (as well as {|ν r,j |} ℓ j=1 ) are independent and identically distributed. Let ν r denote a generic random variable identically distributed with ν r,j , j = 1, . . . , ℓ, r + ℓ + 1, . . . , n. Then, we have
|ν r | = st Bin(r, p n ). (156
)
where we use the notation = st to indicate distributional equality. Then, we define L(|ν r |) as follows:
L(ν r ) = max K n • 1 [|ν r | > 0] , (J |νr | + 1) • 1 [|ν r | > 1] .(157)
Observe that the event C r is independent from the set-valued random variables ν r,j for each j = 1, . . . , ℓ and for each j = r + ℓ + 1, . . . , n. Also, as noted before {|ν r,j |} n j=r+1+ℓ (as well as {|ν r,j |} ℓ j=1 ) are independent and identically distributed. Using these we obtain
P A ℓ,r ∩ E(J ) ≤ P [C r ] × E 1 - Pn-Kn|νr | Kn Pn Kn ℓ × E Pn-L(νr) Kn Pn Kn n-r-ℓ . (158
)
We will give sufficiently tight bounds for each term appearing in the R.H.S. of (158). First, note from Lemma 11 (Appendix A-B) that ≤ min e -pe(1+ε/2) , e -peλr + e -Knµ 1 [r > r n ]
P [C r ] ≤ r r-2 p r-1 e , r=
for all n sufficiently large and for each r = 2, 3, . . . , n.
Substituting the bounds (159), ( 161) and ( 162) into (158), and noting that each of the terms in the RHS of (158) are trivially upper bounded by 1, we obtain the key bounds on the probabilities P A ℓ,r ∩ E(J ) that are summarized in the following Lemma. Lemma 6. With J defined in (131) for some ε, λ and µ in (0, 1  2 ), if Kn Pn = o(1) and p e = o(1), then the following two properties hold.
(a) For all n sufficiently large and for each r = 2, 3, . . . , Pn-Kn 2Kn , we have
P A ℓ,r ∩ E(J ) ≤ r r-2 (p e ) r-1 • (2rp e ) ℓ
× min e -pe(1+ε/2) , e -peλr + e -Knµ 1 [r > r n ] n-r-ℓ .
(b) For all n sufficiently large and for each r = 2, 3, . . . , n, we have
P A ℓ,r ∩ E(J ) ≤ min r r-2 (p e ) r-1 , 1 × min e -pe(1+ε/2) , e -peλr + e -Knµ 1 [r > r n ] n-r-ℓ .<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>XIII. ESTABLISHING (149)<h2>text</h2>We now proceed as follows: Given Kn Pn = o(1) and the definition of r n in (130), we necessarily have lim n→∞ r n = +∞, and for an given integer R ≥ 2, we have
r n > R for any n ≥ n ⋆ (R)(163)
for some finite integer n ⋆ (R). We define f n,ℓ,r as follows.
f n,ℓ,r = n ℓ n -ℓ r P A ℓ,r ∩ E(J ) .
Then, we have
L.H.S. of (149) = ⌊ n-ℓ 2 ⌋ r=2 f n,ℓ,r .(164)
For the time being, pick an arbitrarily large integer R ≥ 2 (to be specified in Section XIII-B), and on the range n ≥ n ⋆ (R) consider the decomposition
⌊ n-ℓ 2 ⌋ r=2 f n,ℓ,r = R r=2 f n,ℓ,r + rn r=R+1 f n,ℓ,r + ⌊ n-ℓ 2 ⌋ r=rn+1 f n,ℓ,r .
Let n go to infinity: The desired convergence (149) (for Proposition 4) will be established if we show 
R r=2 f n,ℓ,r = o(1),(165)
and
⌊ n-ℓ 2 ⌋ r=rn+1 f n,ℓ,r = o(1).(167)
The next subsections are devoted to proving the validity of (165), (166) and (167) by repeated applications of Lemma 6. Throughout, we also make repeated use of the standard bounds
n r ≤ en r r(168)
valid for all r, n = 1, 2, . . . with r ≤ n.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. Establishing (165)<h2>text</h2>Positive scalar ε in (0, 1) is picked arbitrarily as stated in Proposition 4. Consider K n , P n and p e as in the statement of Proposition 4. For any arbitrary integer R ≥ 2, it is clear that (165) will follow upon showing
lim n→∞ f n,ℓ,r = 0 if lim n→∞ β ℓ,n = +∞(169)
for each r = 2, 3, . . . , R. On that range, property (a) of Lemma 6 is valid since r ≤ ⌊ Pn-Kn 2Kn ⌋ for all n sufficiently large by virtue of the fact that Kn Pn = o(1).
From the easily obtained bounds n ℓ ≤ n ℓ and n-ℓ r ≤ n r , we now get
f n,ℓ,r ≤ n ℓ • n r • r r-2 p r-1 e (2rp e ) ℓ • e -pe(1+ε/2)(n-r-ℓ) = (2r) ℓ r r-2 • n ℓ+r p ℓ+r-1 e
• e -pen(1+ε/2) • e pe(1+ε/2)(r+ℓ) .
( • e -(ln n+ℓ ln ln
n+β ℓ,n )(1+ε/2) • e o(1) = n • (ln n) ℓ+r-1 • n -1 (ln n) -ℓ e -β ℓ,n 1+ε/2 = n -ε/2 (ln n) r-ℓε/2-1 e -β ℓ,n (1+ε/2) = o(1)
by virtue of the facts that r is bounded and lim n→∞ β ℓ,n = +∞. We get (169) and the desired result (165) is obtained.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>B. Establishing (166)<h2>text</h2>Positive scalars λ, µ are given in the statement of Proposition 4. Note that R can be taken to be arbitrarily large by virtue of the previous section. From n ℓ ≤ n ℓ , n-ℓ r ≤ e(n-ℓ) r r and property (b) of Lemma 6, for n ≥ n ⋆ (R) (with n ⋆ (R) as specified in (163)) and for each r = R + 1, . . . , r n , we obtain
f n,ℓ,r ≤ n ℓ • e(n -ℓ) r r • r r-2 (p e ) r-1 e -perλ(n-r-ℓ)
≤ n ℓ+r e r (p e ) r-1 e -perλ(n-r-ℓ) .
Now, observe that on the range r
= R + 1, R + 2, . . . , ⌊ n-ℓ 2 ⌋, from r ≤ n-ℓ 2 , we have for all n sufficiently large, n-r -ℓ ≥ 1 2 (n -ℓ) ≥ n 3 .
This yields e -perλ(n-r-ℓ) ≤ e -perλn/3 .
Substituting p e = ln n+ℓ ln ln n+β ℓ,n n into (172), we also get e -perλn/3 = e -rλ(ln n+ℓ ln ln n+β ℓ,n )/3
= n -rλ/3 (ln n) -rλℓ/3 e -rλβ ℓ,n /3 .
Applying (172), (173) and p e ≤ 2 ln n n to (171), we get
f n,ℓ,r ≤ n ℓ+r e r • 2 ln n n r-1 • n -rλ/3 (ln n) -rλℓ/3 e -rλβ ℓ,n /3 ≤ n ℓ+1-rλ/3 • (2e ln n) r = n ℓ+1 • (2en -λ/3 ln n) r . (174
)
Given 2en -λ/3 ln n = o(1) and (174), we obtain
rn r=R+1 f n,ℓ,r ≤ +∞ r=R+1 n ℓ+1 • (2en -λ/3 ln n) r = n ℓ+1 • (2en -λ/3 ln n) R+1 1 -2en -λ/3 ln n ∼ n ℓ+1-λ(R+1)/3 (2e ln n) R+1 .(175)
We pick R ≥ 3(ℓ+1) λ so that ℓ + 1λ(R + 1)/3 ≤ -λ 3 . As a result, we obtain R.H.S. of (175) = o(1) and thus rn r=R+1 f n,ℓ,r = o(1).We now obtain (166).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C. Establishing (167)<h2>text</h2>Positive scalars λ, µ are given in the statement of Proposition 4. We need consider only the case where r n ≤ ⌊ n-ℓ 2 ⌋ for infinitely many n, as otherwise (167) would hold trivially. From n ℓ ≤ n ℓ , n-ℓ r ≤ n r and property (b) of Lemma 6, we get for r = r n + 1, . . . , ⌊ n-ℓ 2 ⌋,
f n,ℓ,r ≤ n ℓ n r e -perλ + e -Knµ n-ℓ2
.
We will establish (167) in two steps. First set rn = 3 λpe . Obviously, the range r = r n + 1, . . . , ⌊ n-ℓ 2 ⌋ is intersecting the range r = rn , . . . , ⌊ n-ℓ 2 ⌋. We first consider the latter range below. For r = rn , . . . , ⌊ n-ℓ 2 ⌋, it follows that e -perλ ≤ e If rn ≤ r n + 1 for all n sufficiently large, then the desired condition (167) is automatically satisfied via (176). On the other hand, if r n + 1 < rn , we should still consider the range r = r n + 1, . . . , rn -1. On that range, we use arguments similar to those leading to (171) and obtain f n,ℓ,r ≤ n ℓ+r e r (p e ) r-1 e -perλ + e -Knµ n-r-ℓ (177) upon using also property (b) of Lemma 6. On the range r = r n + 1, . . Then for any given 0 < η < 1, there exists a finite integer n ⋆ (η) such that for all n ≥ n ⋆ (η), we have e -µKn ≤ e -3 η • p e rλ ≤ e -3 • (e ηperλ -1).
From r ≤ rn -1 ≤ 3 λpe , it follows that p e rλ ≤ 3 and e -perλ ≥ e -3 .
(179)
Given ( 178) and (179), we obtain for all n ≥ n ⋆ (η), e -µKn ≤ e -perλ • (e ηperλ -1) = e -perλ(1-η)e -perλ
and thus e -perλ + e -µKn ≤ e -perλ (1-η) .
(180)
Recalling (120) and the fact that nℓr ≥ n/3, we get e -perλ(1-η)(n-r-ℓ) (181) ≤ n -rλ(1-η)/3 (ln n) -rλℓ(1-η)/3 e -rλβ ℓ,n (1-η)/3 . Using (180) and ( 181) in (177), and noting p e ≤ 2 ln n n , we get f n,ℓ,r ≤ n ℓ+r e r 2 ln n n r-1
× n -rλ(1-η)/3 (ln n) -rλℓ(1-η)/3 e -rλβ ℓ,n (1-η)/3
≤ n ℓ+1-rλ(1-η)/3 • (2e ln n) r
= n ℓ+1 • (2en -λ(1-η)/3 ln n) r .
(182)
Given lim n→∞ r n = +∞, then for any arbitrarily large integer R, we have r n ≥ R for all n sufficiently large. From 2en -λ(1-η)/3 ln n = o(1) and (182), we have
rn-1 rn+1 f n,ℓ,r ≤ ∞ R+1 n ℓ+1 • (2en -λ(1-η)/3 ln n) r
∼ n ℓ+1 • (2en -λ(1-η)/3 ln n) R+1 1 -2en -λ(1-η)/3 ln n ∼ n ℓ+1-λ(1-η)( R+1)/3 (2e ln n) R+1 .
(183)
Since R was arbitrary, we pick R ≥ 3(ℓ+1) λ(1-η) . Then <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>XIV. CONCLUSION<h2>text</h2>We investigate random key graph with unreliable links which amounts to the intersection of random key graphs with Erdős-Rényi graphs. We derive zero-one laws for kconnectivity and minimum node degree being at lest k. These zero-one laws are shown to improve the existing results on 1-connectivity of random key graphs with unreliable links as well as k-connectivity of random key graphs.
An extension of our work would be to consider a different unreliability model than the independent on/off model used here. One possible candidate is the so-called disk model [29] where two nodes have to be within a certain distance to each other to have a link in between; this induces a random geometric graph. Intersection of random key graphs with random geometric graphs has already received some interest [22], [25], but the model is proven to be difficult to analyze with results obtained thus far for its connectivity [23], [24], [43], not for k-connectivity.
it is clear that the number of possible instances for realizing the event F is given by
n -2 m 1 • n -m 1 -2 m 2 • n -m 1 -m 2 -2 m 3 .(206)
The event J defined below is an instance of F . (212) upon using exchangeability.
For any constants m 1 , m 2 and m 3 , we have
n -2 m 1 n -m 1 -2 m 2 n -m 1 -m 2 -2 m 3 ∼ n m1 m 1 ! • n m2 m 2 ! • n m3 m 3 ! = n m1+m2+m3 m 1 !m 2 !m 3 ! . (213
)
Now, we evaluate the probability
{P[E xj∩yj | (|S xy | = u)]} n-m1-m2-m3-2 . (214
)
It is clear that 
Applying ( 213) and ( 220) into (212), we obtain (77) and this establishes Lemma 4.<h2>publication_ref</h2>['b28', 'b21', 'b24', 'b22', 'b23', 'b42']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>APPENDIX A ADDITIONAL FACTS AND LEMMAS<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. Facts<h2>text</h2>We introduce additional facts below. Proofs of Facts 2 and 3 are fairly standard and omitted here; interested reader is referred to the full version [41] for details. All other facts are established in Appendix B. Fact 2 is used in proving the one-law (12) of Theorem 1 as well as in proving Fact 4, Fact 5, Lemma 9, and Lemma 12.   <h2>publication_ref</h2>['b40', 'b11']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>B. Lemmas<h2>text</h2>We introduce additional lemmas below. The proofs of all the following lemmas are deferred to Appendix C. Lemma 7. Let ℓ be a non-negative constant integer. If P n = Ω(n) and ( 120) holds with β ℓ,n > 0, then K n = Ω √ ln n .
Lemma 7 is used in proving the one-law (12) of Theorem 1.
Lemma 8. In G on , given P n ≥ 2K n , then the following properties hold. (a)
s . Lemma 8 is used in the proof of the zero-law (11) of Theorem 1, as well as in the proofs of Lemma 7 and Lemma 9. Lemma 9. Consider K n , P n with K n ≤ P n . The following properties hold for any three distinct nodes v x , v y and v j .
(a) We have
, then for any u = 0, 1, 2, . . . , K n , we have
Lemma 9 is used in the proof of the zero-law (11) of Theorem 1 as well as in the proof of Lemma 4.
Lemma 10. If P n ≥ 2K n , then we have
Lemma 10 helps in proving the zero-law (11) of Theorem 1.
Lemma 11 ( [37, Lemma 10.2] via the argument of [36,Lemma 7.4.5,pp. 124]). For each r = 2, . . . , n, we have
Lemma 11 is used in proving the one-law (12) of Theorem 1.
Lemma 12. With J defined in (131) for some ǫ, λ and µ in (0, ≤ min e -pe(1+ǫ/2) , e -peλr + e
for all n sufficiently large and for each r = 2, 3, . . . , n.
Lemma 12 helps in proving the one-law (12) of Theorem 1.<h2>publication_ref</h2>['b11', 'b10', 'b10', 'b35']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>APPENDIX B PROOFS OF FACTS<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. Proof of Fact 1 (Section<h2>text</h2>2) Proof of property (b): For constant k, given P[δ = ℓ] = o(1) for ℓ = 0, 1, . . . , k -1, we obtain
3) Proof of property (c): Fix ℓ = 0, 1, . . . , k -1. From the method of second moment [20, Remark 3.1, p. 54], we have
Then, from E[X ℓ ] = 0, and
Therefore, we get lim n→∞ P[δ > ℓ] = 0. The desired result<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>B. Proof of Fact 4<h2>text</h2>From y-z x = (y-z)! x!(y-z-x)! and y x = (y)! x!(y-x)! , we get
We define g(t) = y-x-t y-t = 1 -x y-t , where t = 0, 1, 2, . . . , z. Clearly, g(t) decreases as t increases for t = 0, 1, 2, . . . , z, so g(z) ≤ g(t) ≤ g(0). As a result, we have
Given the above expressions, we use Fact 2 and obtain
From ( 195) and (196), we get (186). Using 0 ≤ z ≤ x in the R.H.S. of (197), we also have
To evaluate R.H.S. of (196), we have
Given y > 2x and 0 ≤ z ≤ x, it follows that z ≤ y 2 and thus yz ≥ y/2. Note that x ≥ 1. Then, we have
Applying (199) and ( 200) into (196), we get
Using ( 198) and ( 201) in (195), we obtain (187).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C. Proof of Fact 5<h2>text</h2>The proof is similar to that of Lemma 5.1 in Yagan [37]. First, given positive integer a, it holds that
Letting a = 1 in (202), we obtain
From property (b) of Fact 2, it follows that
where, in the last step we used the fact that a ≤ y-x 2x since y ≥ (2a + 1)x by assumption.
From ( 202), ( 203) and ( 204), we get (188).<h2>publication_ref</h2>['b36']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>APPENDIX C PROOFS OF LEMMAS<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. Proof of Lemma 2 (Section VI)<h2>text</h2>The events E 1i , E 2i , . . . , E i-1,i , E i+1,i . . . , E ni are mutually independent for any node v i . Thus, for each i = 1, 2, . . . , n, the degree of node v i follows a Binomial distribution Bin(n -1, p e ); i.e.,
Given
and constant ℓ, it follows that p e = o(1) and p e 2 (nℓ -1) = o(1). Then from property (b) of Fact 3, (1p e ) n-ℓ-1 ∼ e -pe(n-ℓ-1) holds. Then given p e = o(1) and constant ℓ, we further get (1p e ) n-ℓ-1 ∼ e -pen . Using this and n-1 ℓ ∼ (ℓ!) -1 n ℓ in (205), we obtain<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>B. Proof of Lemma 4 (Section VII-A)<h2>text</h2>In graph G on , besides v x and v y , there are (n -2) nodes, denoted by v j1 , v j2 , . . . , v jn-2 below. The (n -2) nodes are split into the four sets N xy , N xy , N xy and N x y as defined in Section V-D. According to the definition (76), under event 
2 for all n sufficiently large. From (221) and P n = Ω (n), we now get
The desired result
1) Proof of property (a): Recall from ( 5) that given P n ≥ 2K n , we have
We use Fact 4 (in particular (187)) to evaluate R.H.S. of ( 222) and obtain 
Then, we get<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>E. Proof of Lemma 9 1) Proof of property (a):<h2>text</h2>We start by computing the probability P
From the inclusion-exclusion principle, this yields
Note that for each u = 0, 1, 2, . . . , K n , events K xj and K yj are both independent of (|S xy | = u); however, K xj ∩ K yj is not independent of (|S xy | = u). Thus, we get
Substituting ( 226) and ( 227) into (225), it follows that
Given that the events K xy and (|S xy | = 0) are equivalent, letting u = 0 in (228), we obtain
Since events K xj and K yj are equivalent to [(S x ∩ S j ) = ∅] and [(S y ∩ S j ) = ∅], respectively, we have
Therefore, from (230), (K xj ∩ K yj ) equals the event that the K n keys forming S j are all from
Under K xy we have
Below we consider the case of P n ≥ 3K n . We have
Applying [37, Lemma 5.1] to R.H.S. of (232), we get
Using (233) in (229), we obtain<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>2) Proof of property (b):<h2>text</h2>We first establish (190). Given p s = o(1), from property (c) of Lemma 8,
Then P n > 3K n holds for all n sufficiently large. We first compute P
Then for all n sufficiently large, we have
Now, it is a simple matter to check that
and
We first evaluate R.H.S. of (235). It is clear that 0 < 2Kn-u Pn < 1 for all sufficiently large since P n > 3K n and u ≤ K n . We utilize Fact 2 to get R.H.S. of (235)
Applying ( 237) to (235), we obtain
Then we evaluate R.H.S. of (236). With 0 ≤ u ≤ K n and P n > 3K n , it follows that 0 < 2Kn-u Pn-Kn < 1 for all n sufficiently large. We utilize Fact 2 and (236) to get
It is easy to see that
Applying (240) to (239) and using (238) it follows that
Given p s = o(1), from property (d) of Lemma 8, we have that
Applying ( 241) to (228), we obtain
and this establishes (190). We now turn to the proof of (191). From (190), we obtain
The desired result ( 191) is now established.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>F. Proof of Lemma 10<h2>text</h2>It is not difficult to see that<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>G. Proof of Lemma 12<h2>text</h2>Recall J i defined in ( 131). Here we still use Y i defined in (136) for j ≥ 2. Then (137) follows. We define M (|ν r |) and Q(|ν r |) as follows:
Lemma 12 is an extension of a similar result established in [37,Lemma 10.1,pp. 11]. There, it was shown that for r = 1, 2, . . . ,
Recalling the definition of L(ν r ) in (157) and using the definitions of M (ν r ) and Q(ν r ) in ( 243) and (244), we have the following cases.
Then for case (c), we further have the following two subcases. (c1) If |ν r | = 2, 3, . . . , r n , given (246), (247) and 
Given Kn Pn = o(1), then ⌊µP n ⌋ ≥ ⌊(1 + ε)K n ⌋ for all n sufficiently large. Consequently, from (248) and (250), it follows that L(ν r ) = max {M (ν r ), Q(ν r )}.
Summarizing cases (a), (b), and (c1)-(c2) above, given any |ν r |, we have L(ν r ) = max {M (ν r ), Q(ν r )} for all n sufficiently large. This yields
We will show the following result: for all n sufficiently large and for any r = 2, 3, . . . , n,
Clearly, if (252) holds, we can substitute (245) and ( 252) into (251) and obtain (193), which establishes Lemma 12.
For any given n and any given r, from (244), we get
From Lemma 5.1 in Yagan [37], it follows that R.H.S. of (253 
We introduce a continuous variable γ and define f (γ, p n , p s ) as follows, where γ ≥ 1.
From ( 255) and (256), we obtain R.H.S. of (254) = f (r, p n , p s ).
Note that since r is an integer, we cannot take the partial derivative of f (r, p n , p s ) with respect to r. We have introduced continuous variable γ and hence can take the partial derivative of f (γ, p n , p s ) with respect to γ. We get
where, in the last step, we used the fact that ln(1p n ) ≤ 0. Therefore, it's clear that
Given p n and p s , then f (γ, p n , p s ) is decreasing with respect to γ for γ ≥ 1. Then given r ≥ 2, (254) and (257), we have
where in (259) we use 0 < p s < 1, 0 < ε < 1 and Fact 2 to obtain (1p s ) ε ≤ 1εp s ; and in (260) we use p e = p n p s ; and in (261) we use the 1x ≤ e -x that holds for any x ≥ 0.
Given p e = o(1), then p e ≤ 1 2 for all n sufficiently large. Using this and 0 < p n ≤ 1, we obtain <h2>publication_ref</h2>['b36', 'b36']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>Epidemics on random intersection graphs<h2>journal</h2>The Annals of Applied Probability<h2>year</h2>2014-06<h2>authors</h2>F G Ball; D J Sirl; P Trapman<h2>ref_id</h2>b1<h2>title</h2>Connectivity of the uniform random intersection graph<h2>journal</h2>Discrete Mathematics<h2>year</h2>2009<h2>authors</h2>S Blackburn; S Gerke<h2>ref_id</h2>b2<h2>title</h2>On the complexity of the herding attack and some related attacks on hash functions<h2>journal</h2>Designs, Codes and Cryptography<h2>year</h2>2012<h2>authors</h2>S Blackburn; D Stinson; J Upadhyay<h2>ref_id</h2>b3<h2>title</h2>Assortativity and clustering of sparse random intersection graphs<h2>journal</h2>The Electronic Journal of Probability<h2>year</h2>2013<h2>authors</h2>M Bloznelis; J Jaworski; V Kurauskas<h2>ref_id</h2>b4<h2>title</h2>Component evolution in a secure wireless sensor network<h2>journal</h2>Networks<h2>year</h2>2009-01<h2>authors</h2>M Bloznelis; J Jaworski; K Rybarczyk<h2>ref_id</h2>b5<h2>title</h2>Perfect matchings in random intersection graphs<h2>journal</h2>Acta Mathematica Hungarica<h2>year</h2>2013<h2>authors</h2>M Bloznelis; T Łuczak<h2>ref_id</h2>b6<h2>title</h2>The phase transition in inhomogeneous random intersection graphs<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>M Bradonjić; A Hagberg; N W Hengartner; N Lemons; A G Percus<h2>ref_id</h2>b7<h2>title</h2>Random key predistribution schemes for sensor networks<h2>journal</h2><h2>year</h2>2003<h2>authors</h2>H Chan; A Perrig; D Song<h2>ref_id</h2>b8<h2>title</h2>Redoubtable sensor networks<h2>journal</h2>ACM Transactions on Information and System Security<h2>year</h2>2008<h2>authors</h2>R Di Pietro; L V Mancini; A Mei; A Panconesi; J Radhakrishnan<h2>ref_id</h2>b9<h2>title</h2>Sharp thresholds for hamiltonicity in random intersection graphs<h2>journal</h2>Theoretical Computer Science<h2>year</h2>2010<h2>authors</h2>C Efthymiou; P Spirakis<h2>ref_id</h2>b10<h2>title</h2>On random graphs<h2>journal</h2>I. Publicationes Mathematicae (Debrecen)<h2>year</h2>1959<h2>authors</h2>P Erdős; A Rényi<h2>ref_id</h2>b11<h2>title</h2>On the strength of connectedness of random graphs<h2>journal</h2>Acta Mathematica Academiae Scientiarum Hungaricae<h2>year</h2>1961<h2>authors</h2>P Erdős; A Rényi<h2>ref_id</h2>b12<h2>title</h2>A key-management scheme for distributed sensor networks<h2>journal</h2><h2>year</h2>2002<h2>authors</h2>L Eschenauer; V Gligor<h2>ref_id</h2>b13<h2>title</h2>Hyperbolicity, degeneracy, and expansion of random intersection graphs<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>M Farrell; T Goodrich; N Lemons; F Reidl; F S Villaamil; B Sullivan<h2>ref_id</h2>b14<h2>title</h2>Highly-resilient, energy-efficient multipath routing in wireless sensor networks<h2>journal</h2>ACM SIGMOBILE Mobile Computing and Communications Review<h2>year</h2>2001-10<h2>authors</h2>D Ganesan; R Govindan; S Shenker; D Estrin<h2>ref_id</h2>b15<h2>title</h2>Random graphs<h2>journal</h2>The Annals of Mathematical Statistics<h2>year</h2>1959<h2>authors</h2>E N Gilbert<h2>ref_id</h2>b16<h2>title</h2>Fast encryption and authentication: XCBC encryption and XECB authentication modes<h2>journal</h2><h2>year</h2>2001<h2>authors</h2>V Gligor; P Donescu<h2>ref_id</h2>b17<h2>title</h2>Brief encounters with a random key graph<h2>journal</h2>Springer Verlag<h2>year</h2>2009-04<h2>authors</h2>V Gligor; A Perrig; J Zhao<h2>ref_id</h2>b18<h2>title</h2>Two models of random intersection graphs for classification<h2>journal</h2>Springer<h2>year</h2>2003<h2>authors</h2>E Godehardt; J Jaworski<h2>ref_id</h2>b19<h2>title</h2>Random graphs<h2>journal</h2>John Wiley & Sons<h2>year</h2>2011<h2>authors</h2>S Janson; T Luczak; A Rucinski<h2>ref_id</h2>b20<h2>title</h2>Encryption modes with almost free message integrity<h2>journal</h2><h2>year</h2>2001<h2>authors</h2>C S Jutla<h2>ref_id</h2>b21<h2>title</h2>On connectivity thresholds in superposition of random key graphs on random geometric graphs<h2>journal</h2><h2>year</h2>2013-07<h2>authors</h2>B Krishnan; A Ganesh; D Manjunath<h2>ref_id</h2>b22<h2>title</h2>On connectivity thresholds in superposition of random key graphs on random geometric graphs<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>B Krishnan; A Ganesh; D Manjunath<h2>ref_id</h2>b23<h2>title</h2>Geometric graphs with randomly deleted edges -connectivity and routing protocols<h2>journal</h2>Mathematical Foundations of Computer Science<h2>year</h2>2011<h2>authors</h2>K Krzywdziński; K Rybarczyk<h2>ref_id</h2>b24<h2>title</h2>Geometric graphs with randomly deleted edges -connectivity and routing protocols<h2>journal</h2><h2>year</h2>2011<h2>authors</h2>K Krzywdziski; K Rybarczyk<h2>ref_id</h2>b25<h2>title</h2>Fault tolerant deployment and topology control in wireless networks<h2>journal</h2><h2>year</h2>2003<h2>authors</h2>X Li; P Wan; Y Wang; C Yi<h2>ref_id</h2>b26<h2>title</h2>A lower-bound on the number of rankings required in recommender systems using collaborativ filtering<h2>journal</h2><h2>year</h2>2008<h2>authors</h2>P Marbach<h2>ref_id</h2>b27<h2>title</h2>On the independence number and hamiltonicity of uniform random intersection graphs<h2>journal</h2>Theoretical Computer Science<h2>year</h2>2011<h2>authors</h2>S Nikoletseas; C Raptopoulos; P Spirakis<h2>ref_id</h2>b28<h2>title</h2>On k-connectivity for a geometric random graph<h2>journal</h2>Random Structures & Algorithms<h2>year</h2>1999<h2>authors</h2>M Penrose<h2>ref_id</h2>b29<h2>title</h2>Random Geometric Graphs<h2>journal</h2>Oxford University Press<h2>year</h2>2003-07<h2>authors</h2>M Penrose<h2>ref_id</h2>b30<h2>title</h2>OCB: A blockcipher mode of operation for efficient authenticated encryption<h2>journal</h2><h2>year</h2>2001<h2>authors</h2>P Rogaway; M Bellare; J Black; T Krovetz<h2>ref_id</h2>b31<h2>title</h2>Diameter, connectivity and phase transition of the uniform random intersection graph<h2>journal</h2>Discrete Mathematics<h2>year</h2>2011<h2>authors</h2>K Rybarczyk<h2>ref_id</h2>b32<h2>title</h2>Sharp threshold functions for the random intersection graph via a coupling method<h2>journal</h2>The Electronic Journal of Combinatorics<h2>year</h2>2011<h2>authors</h2>K Rybarczyk<h2>ref_id</h2>b33<h2>title</h2>On the isolated vertices and connectivity in random intersection graphs<h2>journal</h2>International Journal of Combinatorics<h2>year</h2>2011<h2>authors</h2>Y Shang<h2>ref_id</h2>b34<h2>title</h2>Random Intersection Graphs<h2>journal</h2><h2>year</h2>1995<h2>authors</h2>K Singer<h2>ref_id</h2>b35<h2>title</h2>Random Graph Modeling of Key Distribution Schemes in Wireless Sensor Networks<h2>journal</h2><h2>year</h2>2011<h2>authors</h2>O Yagan<h2>ref_id</h2>b36<h2>title</h2>Performance of the Eschenauer-Gligor key distribution scheme under an on/off channel<h2>journal</h2>IEEE Transactions on Information Theory<h2>year</h2>2012-06<h2>authors</h2>O Yagan<h2>ref_id</h2>b37<h2>title</h2>Random key graphs -can they be small worlds<h2>journal</h2><h2>year</h2>2009-12<h2>authors</h2>O Yagan; A Makowski<h2>ref_id</h2>b38<h2>title</h2>Zero-one laws for connectivity in random key graphs<h2>journal</h2>IEEE Transactions on Information Theory<h2>year</h2>2012-05<h2>authors</h2>O Yagan; A Makowski<h2>ref_id</h2>b39<h2>title</h2>On the existence of triangles in random key graphs<h2>journal</h2><h2>year</h2>2009-10<h2>authors</h2>O Yagan; A M Makowski<h2>ref_id</h2>b40<h2>title</h2>k-Connectivity in secure wireless sensor networks with physical link constraints -the on/off channel model<h2>journal</h2><h2>year</h2>2012<h2>authors</h2>J Zhao; O Yagan; V Gligor<h2>ref_id</h2>b41<h2>title</h2>Secure k-connectivity in wireless sensor networks under an on/off channel model<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>J Zhao; O Yagan; V Gligor<h2>ref_id</h2>b42<h2>title</h2>Connectivity in secure wireless sensor networks under transmission constraints<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>J Zhao; O Yagan; V Gligor<h2>ref_id</h2>b43<h2>title</h2>Exact analysis of k-connectivity in secure sensor networks with unreliable links<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>J Zhao; O Yagan; V Gligor<h2>ref_id</h2>b44<h2>title</h2>On the strengths of connectivity and robustness in general random intersection graphs<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>J Zhao; O Yagan; V Gligor<h2>ref_id</h2>b45<h2>title</h2>Random intersection graphs and their applications in security, wireless communication, and social networks<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>J Zhao; O Yagan; V Gligor<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Fig. 1 .1Fig.1. Empirical probability that Gon(n, K, P, p) is 2-connected for p = 0.2, p = 0.4, p = 0.6, p = 0.8 with n = 2000 and P = 10, 000. Vertical dashed lines stand for the critical threshold of 2-connectivity asserted by Theorem 1.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>ln n n ) = o(1) since αn = O(ln n). It also follows that Kn Pn = o(1). In view of (27), we get ps = ln n + (k -1) ln ln n + αn ± o(1) n , n = 1, 2, . . . , and with p n = 1 for all n sufficiently large, we obtain pe = ln n + (k -1) ln ln n + αn ± o(1) n , n = 1, 2, . . . , It is clear that lim n→∞ αn ± o(1) = ∞. Thus, we get the desired one-law by applying (12) of Theorem 1.<h2>figure_data</h2><h2>figure_label</h2>9<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>( 9 )9, K n ≥ 2 for all n sufficiently large , K 2 n P n = o(1) (45) lim n→+∞ α n = -∞ and p e n > ǫ > 0 or lim n→∞ p e n = 0. (46)<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_4<h2>figure_caption</h2>Lemma 3 .3Let p s = o(1), K n ≥ 2 for all n sufficiently large, p e = ln n+(k-1) ln ln n+αn n with lim n→∞ α n = -∞. Then, properties (a) and (b) below hold.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_5<h2>figure_caption</h2>xy | = u] • 2e -2pen+ pnu Kn •pen • ℓ h=0 (p e n)2ℓ-h . (94) Given (94), it is clear that (73) follows once we prove R.H.S. of (94) = o ℓ h=0 P A h ∩ K xy . (95) Using p e n = Ω(1), it follows that ℓ h=0 (p e n) 2ℓ-h = O (p e n) 2ℓ . (96) Notice that (96) follows trivially for ℓ = 0 without requiring p e n = Ω(1). Applying (88) and (96) to R.H.S. of (94), we get R.H.S. of (94) = O(1) • (p e n) 2ℓ e -2pen •<h2>figure_data</h2><h2>figure_label</h2>322<h2>figure_type</h2>figure<h2>figure_id</h2>fig_6<h2>figure_caption</h2>3 4 2 nso that K 2 n322pn ln n . (101) on the same range. From Lemma 8, property (c) (Appendix A-B), it holds under p s = o(1) that p s ∼ K Pn Pn = o(1) and Kn Pn = o(1)<h2>figure_data</h2><h2>figure_label</h2>21<h2>figure_type</h2>figure<h2>figure_id</h2>fig_7<h2>figure_caption</h2>Proposition 2 . 1 .21Let ℓ be a positive integer constant. If p s = o(1), p e = ln n+ln ln n+αn n with lim n→∞ α n = -∞ and p e n = Ω(1), then ℓ-1 h=0<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_8<h2>figure_caption</h2>Proof.As given in (67), K xy = Kn u=1 [|S xy | = u]. Using this and the fact that E xy = K xy ∩ C xy , we get E xy = Kn u=1 (|S xy | = u) C xy . We use Y u to denote the event (|S xy | = u) ∩ C xy , where u = 1, 2, . . . , K n . Thus, E xy = Kn u=1 Y u . Then considering that the events Y 1 , Y 2 , . . . , Y Kn are disjoint, we get<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_9<h2>figure_caption</h2>ℓ,r = o(1),<h2>figure_data</h2><h2>figure_label</h2>23<h2>figure_type</h2>figure<h2>figure_id</h2>fig_10<h2>figure_caption</h2>) for each r = 2 , 3 ,23. . . , R. Given p e = ln n+ℓ ln ln n+β ℓ,n n ∼ ln n n = o(1) (since β ℓ,n = o(ln n)), we find R. H. S. of (170) (2r) ℓ r r-2 = n ℓ+r p ℓ+r-1 e • e -pen(1+ε/2) • e pe(1+ε/2)(r+ℓ) ∼ n ℓ+r ln n n ℓ+r-1<h2>figure_data</h2><h2>figure_label</h2>32<h2>figure_type</h2>figure<h2>figure_id</h2>fig_11<h2>figure_caption</h2>- 3 . 2 n32From Lemma 7 (Appendix A-B), K n = Ω √ ln n holds.Then e -Knµ = o(1) < 1 9e -3 . Therefore,e -perλ + e -Knµ ℓ,r ≤ 3 ℓ-n n ℓ ⌊ n-ℓ 2 ⌋ r=rn n r ≤ 3 ℓ-n n ℓ • 2 n = o(1)and the fact that ℓ is constant.<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_12<h2>figure_caption</h2>ℓ + 11λ(1η)( R + 1)/3 ≤ -λ(1η)/3.As a result, we have R.H.S. of (183) = o(1), whence rn-1 rn+1 f n,ℓ,r = o(1).The desired conclusion (167) is now established.Having established (165), (166) and (167), we now get (149) and this completes the proof of Proposition 4.<h2>figure_data</h2><h2>figure_label</h2>1231213<h2>figure_type</h2>figure<h2>figure_id</h2>fig_13<h2>figure_caption</h2>J 1 N 2 N 3 N= n -2 m 1 n -m 1 -2 m 2 n -m 1 -m 2 -2 m 3 ×1231213:= N xy = v j1 , v j2 , . . . , v jm xy = v jm 1 +1 , v jm 1 +2 , . . . , v jm 1 +m xy = v jm 1 +m 2 +1 , v jm 1 +m 2 +2 , . . . , v jm 1 +m 2 +m x y = v jm 1 +m 2 +m 3 +1 , v jm 1 +m 2 +m 3 +2 , . . . , v jn-2 .(207)It is clear that all instances of F happen with the same probability. Let node v j be any given node other than v x and v y in graph G on . ThenE xj∩yj ⇔ (v j ∈ N xy ) ; E xj∩yj ⇔ (v j ∈ N xy ) ; (208) E xj∩yj ⇔ (v j ∈ N xy ) ; and E xj∩yj ⇔ (v j ∈ N x y ) . (209)Applying the above equivalences (208) and (209) to the definition of J in (207), we obtainE xj = C xj ∩ K xj and E yj = C yj ∩ K yj , we have E xj∩yj = (C xj ∩ C yj ) ∩ (K xj ∩ K yj ) .(211)For any node v j distinct from v x and v y , we have the following observations: (a) events C xj , C yj , C xj ∩C yj , K xj , K yj and thus E xj , E yj given by (C-B) do not depend on any nodes other than v x , v y and v j ; (b) given (|S xy | = u), event K xj ∩K yj does not depend on any nodes other than v x , v y and v j ; (c) from (211), and observations (a) and (b) above, event E xj∩yj does not depend on any nodes other than v x , v y and v j given that (|S xy | = u); (d) since the relative complement of event E xj∩yj with respect to event E xj is event E xj∩yj , given observations (a) and (c) above, event E xj∩yj and then similarly, events E xj∩yj and E xj∩yj do not depend on any nodes other than v x , v y and v j . From observations (c) and (d) above, we conclude that E xj1∩yj1 , . . . , E xjm 1 ∩yjm 1 , E xjm 1 +1 ∩yjm 1 +1 , . . . , E xjm 1 +m 2 ∩yjm 1 +m 2 , E xjm 1 +m 2 +1∩yjm 1 +m 2 +1 , . . . , E xjm 1 +m 2 +m 3 ∩yjm 1 +m 2 +m 3 , E xjm 1 +m 2 +m 3 +1∩yjm 1 +m 2 +m 3 +1 , . . . , E xjn-2∩yjn-2 are mutually independent given that (|S xy | = u).Then from (206) and (210), we finally get P [F | |S xy | = u] {P[E xj∩yj | (|S xy | = u)]} m1 × {P[E xj∩yj | (|S xy | = u)]} m2 × {P[E xj∩yj | (|S xy | = u)]} m3 × {P[E xj∩yj | (|S xy | = u)]} n-m1-m2-m3-2 .<h2>figure_data</h2><h2>figure_label</h2>2212322<h2>figure_type</h2>figure<h2>figure_id</h2>fig_14<h2>figure_caption</h2>( 2 ) 2 = (n -m 1 -m 2 -m 3 - 2 ) • O ln n n 2 =2212322214) = (1 -P[E xj∪yj | (|S xy | = u)]) n-m1-m2-m3-2 . (215)From Lemma 9 and the fact that p e ≤ ln n+(k-1) ln ln n n for all n sufficiently large, we findP[E xj∪yj | (|S xy | = u)] = 2p e -p n u K n• p e ± O(p e above relation, given constants m 1 , m 2 and m 3 , we obtain(nm 1m 2m 3 -2){P[E xj∪yj | (|S xy | = u)]} o(1). (218)Given (217) and (218), we use property (b) of Fact 3 to evaluate R.H.S. of (215) (i.e., (214)). We get(214) ∼ e -(n-m1-m2-m3-2)P[Exj∪yj |(|Sxy|=u)] .(219)Substituting (216) and (217) into (219), given constants m 1 , m 2 and m 3 , we find (214) ∼ e -n[2pe-pn u Kn •pe±o( 1 n )] • e (m1+m2+m3+2)•o(1) ∼ e -2pen+ pnu Kn •pen .<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>33, pp. 7], we have P[G on (n, Kn , Pn , pn ) has property P] ≤ P[G on (n, Kn , Pn , pn ) has property P].<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>2, 3, . . . , n.<h2>figure_data</h2>(159)Next, we give an easy bound on the second term appearing inthe R.H.S. of (158). Withr ≤P n -K n 2K n(160)2Kn . Then we use Fact 5 and it follows that |ν r | ≤ r ≤ Pn-Kn Fact 2 successively to obtainPn-Kn|νr|1 -Kn Pn Kn≤ 1 -(1 -p s ) 2|νr | ≤ 2|ν r |p s .Taking the expectation in the above relation and noting thatE [|ν Pn-Kn|νr|Kn Pn Kn≤ 2rp s p n = 2rp e(161)Pn-L(νr)KnPnKn<h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_3<h2>figure_caption</h2>. , rn -1, we have P n ≥ σn and p e n ≥ 1 for all n sufficiently large. Given K n = Ω √ ln n , it follows that lim n→∞ K n e -µKn = 0 and lim<h2>figure_data</h2>n→∞e -µKn = 0,whence we getlim n→∞e -µKn p e rλ= 0.r ≥ r n + 1 = minP n K n,n 2+ 1 ≥ minP n K n,n 2,and thuse -µKn p e rλe -µKn p e λ • min{ Pn Kn , n 2 } ≤ max ≤ K n e -µKn σλ ,2e -µKn λ.<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>K 2 n n Pn ln n > 1 (resp., lim sup n→+∞ K 2 n n<h2>formula_coordinates</h2>[2.0, 312.0, 150.7, 250.98, 31.64]<h2>formula_id</h2>formula_1<h2>formula_text</h2>K 2 n Pn = ln n+αn n , then G(n, K n , P n ) is a.a.s. connected (resp., not connected) if lim n→∞ α n = +∞ (resp., lim n→∞ α n = -∞).<h2>formula_coordinates</h2>[2.0, 312.0, 182.14, 251.36, 45.22]<h2>formula_id</h2>formula_2<h2>formula_text</h2>K 2 n Pn = ln n+αn n has a limit α * ∈ [-∞, ∞], then the probability of G(n, K n , P n ) being connected approaches to e -e -α *<h2>formula_coordinates</h2>[2.0, 312.0, 255.7, 251.07, 40.25]<h2>formula_id</h2>formula_3<h2>formula_text</h2>K 2 n Pn = ln n+(k-1) ln ln n+αn n , then the G(n, K n , P n ) is a.a.s. k-connected (resp., not k-connected) if lim n→∞ α n = +∞ (resp., lim n→∞ α n = -∞).<h2>formula_coordinates</h2>[2.0, 312.0, 378.58, 251.13, 45.22]<h2>formula_id</h2>formula_4<h2>formula_text</h2>K n ≥ 2, P n = Ω(n) and p n • 1 - ( Pn -Kn Kn ) ( Pn Kn )<h2>formula_coordinates</h2>[2.0, 312.0, 477.7, 171.87, 23.02]<h2>formula_id</h2>formula_5<h2>formula_text</h2>K 2 n Pn = ln n+(k-1) ln ln n+αn n has a limit α * ∈ [-∞, ∞], then the probability of G(n, K n , P n ) being k-connected converges to e -e -α *<h2>formula_coordinates</h2>[2.0, 312.0, 594.34, 251.08, 33.34]<h2>formula_id</h2>formula_6<h2>formula_text</h2>α n defined through p n • 1 - ( Pn -Kn Kn ) ( Pn Kn ) = ln n+(k-1) ln ln n+αn n has a limit α * ∈ [-∞, ∞],<h2>formula_coordinates</h2>[3.0, 48.96, 83.62, 249.3, 40.06]<h2>formula_id</h2>formula_7<h2>formula_text</h2>K ij = [S i ∩ S j = ∅].<h2>formula_coordinates</h2>[3.0, 131.4, 252.56, 86.16, 17.29]<h2>formula_id</h2>formula_8<h2>formula_text</h2>P [C ij ] = p n , 1 ≤ i < j ≤ n.(1)<h2>formula_coordinates</h2>[3.0, 110.88, 373.16, 189.2, 17.29]<h2>formula_id</h2>formula_9<h2>formula_text</h2>E ij = K ij ∩ C ij , 1 ≤ i < j ≤ n(2)<h2>formula_coordinates</h2>[3.0, 103.68, 493.76, 196.4, 17.29]<h2>formula_id</h2>formula_10<h2>formula_text</h2>G on (n, K n , P n , p n ) = G(n, K n , P n ) ∩ G(n, p n ).(3)<h2>formula_coordinates</h2>[3.0, 74.52, 530.72, 225.56, 17.29]<h2>formula_id</h2>formula_11<h2>formula_text</h2>p s := P[K ij ].(4)<h2>formula_coordinates</h2>[3.0, 146.88, 663.45, 153.2, 10.66]<h2>formula_id</h2>formula_12<h2>formula_text</h2>p s = P[S i ∩ S j = ∅] =    1 - ( Pn -Kn Kn ) ( Pn Kn ) , if P n ≥ 2K n , 1 if P n < 2K n .(5)<h2>formula_coordinates</h2>[3.0, 54.6, 709.54, 245.48, 35.12]<h2>formula_id</h2>formula_13<h2>formula_text</h2>p e := P[E ij ] = P[C ij ] • P[K ij ] = p n • p s(6)<h2>formula_coordinates</h2>[3.0, 355.08, 75.92, 208.04, 17.29]<h2>formula_id</h2>formula_14<h2>formula_text</h2>p e = p n • 1 - Pn-Kn Kn Pn Kn if P n ≥ 2K n .(7)<h2>formula_coordinates</h2>[3.0, 349.44, 111.65, 213.68, 31.21]<h2>formula_id</h2>formula_15<h2>formula_text</h2>K n ≤ P n , n = 1, 2, . . . .(8)<h2>formula_coordinates</h2>[3.0, 383.76, 289.64, 179.36, 17.29]<h2>formula_id</h2>formula_16<h2>formula_text</h2>p e = ln n + (k -1) ln ln n + α n n .(9)<h2>formula_coordinates</h2>[3.0, 368.64, 367.52, 194.48, 23.77]<h2>formula_id</h2>formula_17<h2>formula_text</h2>K 2 n Pn = o(1)<h2>formula_coordinates</h2>[3.0, 349.2, 404.5, 46.23, 17.0]<h2>formula_id</h2>formula_18<h2>formula_text</h2>lim n→∞ P [G on is k-connected ] = 0 if lim n→∞ α n = -∞,(10)<h2>formula_coordinates</h2>[3.0, 325.2, 451.64, 238.04, 25.25]<h2>formula_id</h2>formula_19<h2>formula_text</h2>lim n→∞ P Minimum node degree of G on is no less than k = 0 if lim n→∞ α n = -∞. (11<h2>formula_coordinates</h2>[3.0, 312.0, 501.42, 253.2, 32.71]<h2>formula_id</h2>formula_20<h2>formula_text</h2>) (b) If P n = Ω(n) and Kn Pn = o(1), then lim n→∞ P [G on is k-connected ] = 1 if lim n→∞ α n = ∞,<h2>formula_coordinates</h2>[3.0, 321.96, 525.22, 241.28, 56.02]<h2>formula_id</h2>formula_21<h2>formula_text</h2>lim n→∞ P Minimum node degree of G on is no less than k = 1 if lim n→∞ α n = ∞.(13)<h2>formula_coordinates</h2>[3.0, 313.2, 601.74, 250.04, 32.71]<h2>formula_id</h2>formula_22<h2>formula_text</h2>K 2 n Pn = o(<h2>formula_coordinates</h2>[4.0, 50.16, 137.74, 36.37, 17.0]<h2>formula_id</h2>formula_23<h2>formula_text</h2>K 2 n<h2>formula_coordinates</h2>[4.0, 256.32, 341.26, 11.1, 9.8]<h2>formula_id</h2>formula_24<h2>formula_text</h2>p s ∼ K 2 n P n if K 2 n P n = o(1).(14)<h2>formula_coordinates</h2>[4.0, 117.12, 373.97, 183.08, 25.57]<h2>formula_id</h2>formula_25<h2>formula_text</h2>p n • K 2 n P n = ln n + (k -1) ln ln n + α n n . (15<h2>formula_coordinates</h2>[4.0, 93.24, 536.57, 202.77, 25.57]<h2>formula_id</h2>formula_26<h2>formula_text</h2>)<h2>formula_coordinates</h2>[4.0, 296.01, 545.39, 4.19, 8.91]<h2>formula_id</h2>formula_27<h2>formula_text</h2>K 2 n Pn = O( 1 ln n ) and lim n→∞ (ln n + (k -1) ln ln n + α n ) = ∞, then lim n→∞ P [G on is k-connected ] = 0 if lim n→∞ α n = -∞,(16)<h2>formula_coordinates</h2>[4.0, 48.96, 577.18, 251.24, 61.23]<h2>formula_id</h2>formula_28<h2>formula_text</h2>lim n→∞ P Minimum node degree of G on is no less than k = 0 if lim n→∞ α n = -∞. (17<h2>formula_coordinates</h2>[4.0, 48.96, 664.63, 253.2, 32.71]<h2>formula_id</h2>formula_29<h2>formula_text</h2>) (b) If P n = Ω(n) and K 2 n Pn = O( 1 ln n ), then lim n→∞ P [G on is k-connected ] = 1 if lim n→∞ α n = ∞, (18<h2>formula_coordinates</h2>[4.0, 58.92, 688.43, 241.28, 60.1]<h2>formula_id</h2>formula_30<h2>formula_text</h2>)<h2>formula_coordinates</h2>[4.0, 296.01, 732.23, 4.19, 8.91]<h2>formula_id</h2>formula_31<h2>formula_text</h2>lim n→∞ P Minimum node degree of G on is no less than k = 1 if lim n→∞ α n = ∞.(19)<h2>formula_coordinates</h2>[4.0, 313.08, 73.38, 250.16, 32.71]<h2>formula_id</h2>formula_32<h2>formula_text</h2>K 2 n Pn = O( 1 ln n ) enforced in Corollary 1 implies both Kn Pn = o(1)<h2>formula_coordinates</h2>[4.0, 312.0, 123.1, 250.98, 32.72]<h2>formula_id</h2>formula_33<h2>formula_text</h2>K 2 n Pn = o(1)<h2>formula_coordinates</h2>[4.0, 442.08, 138.82, 46.25, 17.0]<h2>formula_id</h2>formula_34<h2>formula_text</h2>K 2 n P n = ln n + (k -1) ln ln n + α n n , n = 1, 2, . . . ,(20)<h2>formula_coordinates</h2>[4.0, 325.08, 292.49, 238.16, 25.57]<h2>formula_id</h2>formula_35<h2>formula_text</h2>K 2 n<h2>formula_coordinates</h2>[4.0, 515.52, 331.9, 11.1, 9.8]<h2>formula_id</h2>formula_36<h2>formula_text</h2>lim n→∞ P [G(n, K n , P n ) is k-connected ] = 0 if lim n→∞ α n = -∞. (b) If P n = Ω(n), then we have lim n→∞ P [G(n, K n , P n ) is k-connected ] = 1 if lim n→∞ α n = ∞.<h2>formula_coordinates</h2>[4.0, 312.0, 371.6, 257.16, 57.97]<h2>formula_id</h2>formula_37<h2>formula_text</h2>P n = Θ(n ξ ) with ξ > 1) that if p s = ln n+(k-1) ln ln n+αn n , then G(n, K n , P n ) is almost surely k- connected (resp., not k-connected) if lim n→∞ α n = +∞ (resp., lim n→∞ α n = -∞).<h2>formula_coordinates</h2>[4.0, 312.0, 583.61, 251.04, 54.87]<h2>formula_id</h2>formula_38<h2>formula_text</h2>G on = G(n, K n , P n ) ∩ G(n, p n ).(21)<h2>formula_coordinates</h2>[4.0, 369.12, 676.16, 194.12, 17.29]<h2>formula_id</h2>formula_39<h2>formula_text</h2>G on ≃ G(n, p s ) ∩ G(n, p n ) = G(n, p n p s ) = G(n, p e ),<h2>formula_coordinates</h2>[4.0, 325.8, 738.2, 223.44, 17.29]<h2>formula_id</h2>formula_40<h2>formula_text</h2>p e ∼ c ln n n = ln n + (c -1) ln n n(22)<h2>formula_coordinates</h2>[5.0, 106.56, 699.8, 193.64, 24.01]<h2>formula_id</h2>formula_41<h2>formula_text</h2>G o n is k-connected p = 0.2 k = 4 k = 6 k = 8 k = 10<h2>formula_coordinates</h2>[5.0, 316.93, 67.46, 226.01, 147.95]<h2>formula_id</h2>formula_42<h2>formula_text</h2>p e = ln n + α n n(23)<h2>formula_coordinates</h2>[5.0, 405.12, 331.53, 158.12, 23.52]<h2>formula_id</h2>formula_43<h2>formula_text</h2>K 2 n Pn = o(1)<h2>formula_coordinates</h2>[5.0, 379.92, 393.7, 43.58, 17.0]<h2>formula_id</h2>formula_44<h2>formula_text</h2>p e = p • 1 - P -K K P K > ln n + ln ln n n .(24)<h2>formula_coordinates</h2>[6.0, 57.6, 230.36, 242.6, 24.01]<h2>formula_id</h2>formula_45<h2>formula_text</h2>K 2 n Pn = o(1) both hold.<h2>formula_coordinates</h2>[6.0, 86.88, 391.18, 86.49, 17.12]<h2>formula_id</h2>formula_46<h2>formula_text</h2>p e = ln n + (k -1) ln ln n + α ′ n n(25)<h2>formula_coordinates</h2>[6.0, 106.92, 433.63, 193.28, 25.1]<h2>formula_id</h2>formula_47<h2>formula_text</h2>α ′ n = α n ± O(1)(26)<h2>formula_coordinates</h2>[6.0, 139.68, 481.63, 160.52, 19.22]<h2>formula_id</h2>formula_48<h2>formula_text</h2>lim n→∞ α ′ n = ±∞ if lim n→∞ α n = ±∞.<h2>formula_coordinates</h2>[6.0, 48.96, 562.03, 251.0, 30.5]<h2>formula_id</h2>formula_49<h2>formula_text</h2>p n K 2 n Pn = O( ln n n ) under<h2>formula_coordinates</h2>[6.0, 48.96, 632.74, 94.88, 17.0]<h2>formula_id</h2>formula_50<h2>formula_text</h2>p n K 2 n Pn = O( ln n n )<h2>formula_coordinates</h2>[6.0, 48.96, 658.18, 67.96, 17.0]<h2>formula_id</h2>formula_51<h2>formula_text</h2>p s = K 2 n P n ± O K 4 n P 2 n .(27)<h2>formula_coordinates</h2>[6.0, 126.48, 692.33, 173.72, 26.65]<h2>formula_id</h2>formula_52<h2>formula_text</h2>p e = p n • K 2 n P n ± p n • K 2 n P n • O K 2 n P n .(28)<h2>formula_coordinates</h2>[6.0, 360.12, 74.21, 203.12, 25.57]<h2>formula_id</h2>formula_53<h2>formula_text</h2>p n K 2 n Pn = O( ln n n ) and K 2 n Pn = O 1 ln n into (28), we find p e = ln n + (k -1) ln ln n + α n ± O(1) n . (29<h2>formula_coordinates</h2>[6.0, 312.0, 107.02, 251.1, 57.35]<h2>formula_id</h2>formula_54<h2>formula_text</h2>)<h2>formula_coordinates</h2>[6.0, 559.05, 148.31, 4.19, 8.91]<h2>formula_id</h2>formula_55<h2>formula_text</h2>K 2 n Pn = O( ln n n ) = o(1)<h2>formula_coordinates</h2>[6.0, 456.72, 248.74, 90.05, 17.0]<h2>formula_id</h2>formula_56<h2>formula_text</h2>p s = ln n + (k -1) ln ln n + α n ± o(1) n , n = 1, 2, . . . Let p n = 1 for all n. In this case, graph G on becomes equivalent to G(n, K n , P n ) with p e = ln n + (k -1) ln ln n + α n ± o(1) n , n = 1, 2, . . .(30)<h2>formula_coordinates</h2>[6.0, 312.0, 282.8, 251.24, 89.21]<h2>formula_id</h2>formula_57<h2>formula_text</h2>K 2 n Pn ± o(1) so that i) if there exists an ǫ > 0 such that n K 2 n<h2>formula_coordinates</h2>[6.0, 312.0, 371.02, 251.05, 30.11]<h2>formula_id</h2>formula_58<h2>formula_text</h2>lim n→∞ P [G(n, K n , P n ) is k-connected ] = 0 follows from (10) of Theorem 1.<h2>formula_coordinates</h2>[6.0, 312.0, 464.37, 251.23, 21.64]<h2>formula_id</h2>formula_59<h2>formula_text</h2>Kn ≤ K n and Pn = P n , n = 1, 2, . . . ,and<h2>formula_coordinates</h2>[6.0, 312.0, 542.25, 215.88, 32.32]<h2>formula_id</h2>formula_60<h2>formula_text</h2>K2 n Pn = ln n + (k -1) ln ln n + αn n , n = 1, 2, . . . ,(31)<h2>formula_coordinates</h2>[6.0, 327.84, 580.05, 235.4, 26.04]<h2>formula_id</h2>formula_61<h2>formula_text</h2>P G(n, Kn , Pn ) is k-connected ≤ P [G(n, K n , P n ) is k-connected ] .<h2>formula_coordinates</h2>[6.0, 361.56, 668.97, 151.92, 37.32]<h2>formula_id</h2>formula_62<h2>formula_text</h2>lim n→∞ P G(n, Kn , Pn ) is k-connected = 1.<h2>formula_coordinates</h2>[6.0, 346.68, 728.97, 181.68, 17.61]<h2>formula_id</h2>formula_63<h2>formula_text</h2>[G is k-connected ] ⊆<h2>formula_coordinates</h2>[7.0, 72.24, 283.76, 88.62, 17.29]<h2>formula_id</h2>formula_64<h2>formula_text</h2>P [G is k-connected ] ≤ P Minimum node degree of G is no less than k follows immediately.<h2>formula_coordinates</h2>[7.0, 48.96, 324.7, 225.58, 37.83]<h2>formula_id</h2>formula_65<h2>formula_text</h2>α n = o(ln n) ⇒ part (b) of Theorem 1(32)<h2>formula_coordinates</h2>[7.0, 96.0, 497.25, 204.2, 32.04]<h2>formula_id</h2>formula_66<h2>formula_text</h2>lim n→∞ P G(n, Kn , Pn , pn ) is k-connected = 1(33<h2>formula_coordinates</h2>[7.0, 70.2, 606.45, 225.81, 17.74]<h2>formula_id</h2>formula_68<h2>formula_text</h2>P[G on (n, Kn , Pn , pn ) is k-connected ] ≥ P[G on (n, Kn , Pn , pn ) is k-connected ].(38)<h2>formula_coordinates</h2>[7.0, 350.4, 155.85, 212.84, 35.64]<h2>formula_id</h2>formula_69<h2>formula_text</h2>lim n→∞ P[G on (n, Kn , Pn , pn ) is k-connected ] = 1(39)<h2>formula_coordinates</h2>[7.0, 329.76, 235.05, 233.48, 17.73]<h2>formula_id</h2>formula_70<h2>formula_text</h2>pn •   1 - Pn-Kn Kn Pn Kn   = ln n + (k -1) ln ln n + αn n (40<h2>formula_coordinates</h2>[7.0, 323.52, 382.73, 235.53, 34.93]<h2>formula_id</h2>formula_71<h2>formula_text</h2>)<h2>formula_coordinates</h2>[7.0, 559.05, 396.71, 4.19, 8.91]<h2>formula_id</h2>formula_72<h2>formula_text</h2>( Pn -Kn Kn ) ( Pn Kn )<h2>formula_coordinates</h2>[7.0, 400.92, 425.86, 33.03, 25.55]<h2>formula_id</h2>formula_74<h2>formula_text</h2>G on (n, Kn , Pn , pn ) = G(n, Kn , Pn ) ∩ G(n, pn )(42)<h2>formula_coordinates</h2>[8.0, 66.24, 86.37, 233.96, 19.56]<h2>formula_id</h2>formula_75<h2>formula_text</h2>G on (n, Kn , Pn , pn ) = G(n, Kn , Pn ) ∩ G(n, pn ).(43)<h2>formula_coordinates</h2>[8.0, 66.24, 102.09, 233.96, 19.56]<h2>formula_id</h2>formula_76<h2>formula_text</h2>lim n→∞ P [δ = ℓ] = 0. (44<h2>formula_coordinates</h2>[8.0, 135.0, 400.65, 161.01, 15.09]<h2>formula_id</h2>formula_77<h2>formula_text</h2>)<h2>formula_coordinates</h2>[8.0, 296.01, 401.39, 4.19, 8.91]<h2>formula_id</h2>formula_78<h2>formula_text</h2>lim n→∞ P[δ ≥ k] = 1. (c) If E X ℓ 2 ∼ E X ℓ 2 and E X ℓ → +∞ as n →<h2>formula_coordinates</h2>[8.0, 58.92, 442.4, 241.08, 43.93]<h2>formula_id</h2>formula_79<h2>formula_text</h2>lim n→∞ P[δ ≥ k] = 0.<h2>formula_coordinates</h2>[8.0, 135.12, 502.4, 78.84, 17.29]<h2>formula_id</h2>formula_80<h2>formula_text</h2>E xj∩yj := E xj ∩ E yj , E xj∩yj := E xj ∩ E yj , E xj∩yj := E xj ∩ E yj , and E xj∩yj := E xj ∩ E yj .<h2>formula_coordinates</h2>[8.0, 334.8, 99.08, 205.44, 33.37]<h2>formula_id</h2>formula_81<h2>formula_text</h2>lim n→∞ E X ℓ = +∞,(47)<h2>formula_coordinates</h2>[8.0, 395.88, 399.93, 167.36, 15.21]<h2>formula_id</h2>formula_82<h2>formula_text</h2>E X ℓ 2 ∼ E X ℓ 2 . (48<h2>formula_coordinates</h2>[8.0, 386.16, 435.05, 172.89, 20.8]<h2>formula_id</h2>formula_83<h2>formula_text</h2>)<h2>formula_coordinates</h2>[8.0, 559.05, 439.55, 4.19, 8.91]<h2>formula_id</h2>formula_84<h2>formula_text</h2>E X ℓ = nP [D x,ℓ ] ,(49)<h2>formula_coordinates</h2>[8.0, 341.76, 569.85, 221.48, 10.66]<h2>formula_id</h2>formula_85<h2>formula_text</h2>E X ℓ 2 = nP [D x,ℓ ] + n(n -1)P [D x,ℓ D y,ℓ ] . (50<h2>formula_coordinates</h2>[8.0, 325.32, 584.09, 233.73, 20.79]<h2>formula_id</h2>formula_86<h2>formula_text</h2>)<h2>formula_coordinates</h2>[8.0, 559.05, 588.59, 4.19, 8.91]<h2>formula_id</h2>formula_87<h2>formula_text</h2>X ℓ = n i=1 1[D i,ℓ ].<h2>formula_coordinates</h2>[8.0, 322.44, 619.53, 240.54, 24.21]<h2>formula_id</h2>formula_88<h2>formula_text</h2>lim n→+∞ (nP [D x,ℓ ]) = +∞.(51)<h2>formula_coordinates</h2>[8.0, 383.4, 673.89, 179.84, 15.22]<h2>formula_id</h2>formula_89<h2>formula_text</h2>E X ℓ 2 E X ℓ 2 = 1 nP [D x,ℓ ] + n -1 n • P [D x,ℓ D y,ℓ ] P [D x,ℓ ] 2 . (52<h2>formula_coordinates</h2>[8.0, 325.92, 711.41, 233.13, 34.21]<h2>formula_id</h2>formula_90<h2>formula_text</h2>)<h2>formula_coordinates</h2>[8.0, 559.05, 726.23, 4.19, 8.91]<h2>formula_id</h2>formula_91<h2>formula_text</h2>P [D x,ℓ D y,ℓ ] ∼ P [D x,ℓ ] 2(53)<h2>formula_coordinates</h2>[9.0, 113.16, 73.25, 187.04, 20.79]<h2>formula_id</h2>formula_92<h2>formula_text</h2>P [D x,ℓ ] ∼ (ℓ!) -1 (p e n) ℓ e -pen .(54)<h2>formula_coordinates</h2>[9.0, 110.4, 157.99, 189.8, 20.06]<h2>formula_id</h2>formula_93<h2>formula_text</h2>P [D x,ℓ ∩ D y,ℓ ] ∼ (ℓ!) -2 (p e n) 2ℓ e -2pen . (55<h2>formula_coordinates</h2>[9.0, 91.56, 276.53, 204.45, 19.84]<h2>formula_id</h2>formula_94<h2>formula_text</h2>)<h2>formula_coordinates</h2>[9.0, 296.01, 280.07, 4.19, 8.91]<h2>formula_id</h2>formula_95<h2>formula_text</h2>P [D x,0 ∩ D y,0 ] ∼ e -2pen .(56)<h2>formula_coordinates</h2>[9.0, 120.72, 316.15, 179.48, 19.1]<h2>formula_id</h2>formula_96<h2>formula_text</h2>P [D x,ℓ ∩ D y,ℓ ] = P[D x,ℓ ∩ D y,ℓ ∩ E xy ] + P[D x,ℓ ∩ D y,ℓ ∩ E xy ].(57)<h2>formula_coordinates</h2>[9.0, 63.96, 380.72, 236.24, 32.65]<h2>formula_id</h2>formula_97<h2>formula_text</h2>P[D x,ℓ ∩ D y,ℓ ∩ E xy ] ∼ (ℓ!) -2 (p e n) 2ℓ e -2pen . (58<h2>formula_coordinates</h2>[9.0, 70.2, 511.01, 225.81, 19.72]<h2>formula_id</h2>formula_98<h2>formula_text</h2>) (b) We have P[D x,0 ∩ D y,0 ∩ E xy ] ∼ e -2pen .(59)<h2>formula_coordinates</h2>[9.0, 58.92, 514.43, 241.28, 55.3]<h2>formula_id</h2>formula_99<h2>formula_text</h2>P[D x,ℓ ∩ D y,ℓ ∩ E xy ] = o P[D x,ℓ ∩ D y,ℓ ∩ E xy ] . (60)<h2>formula_coordinates</h2>[9.0, 61.44, 627.8, 238.76, 17.29]<h2>formula_id</h2>formula_100<h2>formula_text</h2>nP [D x,ℓ ] ∼ n • (ℓ!) -1 (p e n) ℓ e -pen(61)<h2>formula_coordinates</h2>[9.0, 365.28, 116.95, 197.96, 20.06]<h2>formula_id</h2>formula_101<h2>formula_text</h2>nP [D x,k-1 ](62)<h2>formula_coordinates</h2>[9.0, 323.88, 240.33, 239.36, 10.65]<h2>formula_id</h2>formula_102<h2>formula_text</h2>∼ n • [(k -1)!] -1 (p e n) k-1 e -ln n-(k-1) ln ln n-αn = [(k -1)!] -1 × (ln n + (k -1) ln ln n + α n ) k-1 e -(k-1) ln ln n-αn . Let f n (k; α n ) := (ln n + (k -1) ln ln n + α n ) k-1 e -(k-1) ln ln n-αn ,<h2>formula_coordinates</h2>[9.0, 312.0, 253.87, 239.16, 115.34]<h2>formula_id</h2>formula_103<h2>formula_text</h2>f n (k; α n ) ≥ ǫ • e -(k-1) ln ln n+(1-γ) ln n ,<h2>formula_coordinates</h2>[9.0, 346.08, 433.51, 162.96, 19.22]<h2>formula_id</h2>formula_104<h2>formula_text</h2>f n (k; α n ) ≥ (γ ln n) k-1 e -(k-1) ln ln n-αn = γ k-1 e -αn .<h2>formula_coordinates</h2>[9.0, 312.6, 480.41, 229.92, 19.83]<h2>formula_id</h2>formula_105<h2>formula_text</h2>f n (k; α n ) ≥ min ǫ • e -(k-1) ln ln n+(1-γ) ln n , γ k-1 e -αn .<h2>formula_coordinates</h2>[9.0, 316.08, 530.35, 242.88, 19.1]<h2>formula_id</h2>formula_106<h2>formula_text</h2>nP [D x,0 ] ∼ ne -pen ∼ n.<h2>formula_coordinates</h2>[9.0, 385.32, 676.63, 104.4, 19.22]<h2>formula_id</h2>formula_107<h2>formula_text</h2>A h = [|N xy | = h] [|N xy | = ℓ -h] [|N xy | = ℓ -h] (63)<h2>formula_coordinates</h2>[10.0, 54.12, 149.36, 246.08, 17.29]<h2>formula_id</h2>formula_108<h2>formula_text</h2>D x,ℓ ∩ D y,ℓ ∩ E xy = ℓ h=0 A h ∩ E xy . (64<h2>formula_coordinates</h2>[10.0, 94.44, 207.65, 201.57, 31.33]<h2>formula_id</h2>formula_109<h2>formula_text</h2>)<h2>formula_coordinates</h2>[10.0, 296.01, 218.51, 4.19, 8.91]<h2>formula_id</h2>formula_110<h2>formula_text</h2>P D x,ℓ ∩ D y,ℓ ∩ E xy = ℓ h=0 P A h ∩ E xy . (65<h2>formula_coordinates</h2>[10.0, 73.68, 271.97, 222.33, 31.33]<h2>formula_id</h2>formula_111<h2>formula_text</h2>)<h2>formula_coordinates</h2>[10.0, 296.01, 282.95, 4.19, 8.91]<h2>formula_id</h2>formula_112<h2>formula_text</h2>E xy = K xy ∩ C xy . Hence E xy = K xy ∪ C xy = K xy ∪ (K xy ∩ C xy ).(66)<h2>formula_coordinates</h2>[10.0, 87.96, 319.64, 212.24, 35.29]<h2>formula_id</h2>formula_113<h2>formula_text</h2>K xy = Kn u=1 (|S xy | = u).(67)<h2>formula_coordinates</h2>[10.0, 126.36, 372.05, 173.84, 31.09]<h2>formula_id</h2>formula_114<h2>formula_text</h2>X u = (|S xy | = u) ∩ C xy(68)<h2>formula_coordinates</h2>[10.0, 124.08, 425.36, 176.12, 17.29]<h2>formula_id</h2>formula_115<h2>formula_text</h2>E xy = K xy ∪ Kn u=1 (|S xy | = u) ∩ C xy = K xy ∪ Kn u=1 X u .(69)<h2>formula_coordinates</h2>[10.0, 86.28, 459.77, 213.92, 65.05]<h2>formula_id</h2>formula_116<h2>formula_text</h2>P A h ∩ E xy = P A h ∩ K xy + Kn u=1 P [A h ∩ X u ] .(70)<h2>formula_coordinates</h2>[10.0, 61.44, 559.61, 238.76, 31.09]<h2>formula_id</h2>formula_117<h2>formula_text</h2>P D x,ℓ ∩ D y,ℓ ∩ E xy = ℓ h=0 P A h ∩ K xy + ℓ h=0 Kn u=1 P [A h ∩ X u ] .(71)<h2>formula_coordinates</h2>[10.0, 73.2, 613.04, 227.0, 46.3]<h2>formula_id</h2>formula_118<h2>formula_text</h2>ℓ h=0 P A h ∩ K xy ∼ (ℓ!) -2 (p e n) 2ℓ e -2pen . (72<h2>formula_coordinates</h2>[10.0, 85.68, 721.25, 210.33, 31.33]<h2>formula_id</h2>formula_119<h2>formula_text</h2>)<h2>formula_coordinates</h2>[10.0, 296.01, 732.11, 4.19, 8.91]<h2>formula_id</h2>formula_120<h2>formula_text</h2>ℓ h=0 Kn u=1 P [A h ∩ X u ] = o ℓ h=0 P A h ∩ K xy . (73<h2>formula_coordinates</h2>[10.0, 331.2, 139.49, 227.85, 31.57]<h2>formula_id</h2>formula_121<h2>formula_text</h2>) (b) We have Kn u=1 P [A 0 ∩ X u ] = o P A 0 ∩ K xy .(74)<h2>formula_coordinates</h2>[10.0, 321.96, 150.59, 241.28, 83.36]<h2>formula_id</h2>formula_122<h2>formula_text</h2>P[K xy ] = 1 -p s → 1 as n → ∞, it is clear that ℓ h=0 P A h ∩ K xy ∼ ℓ h=0 P A h | K xy(75)<h2>formula_coordinates</h2>[10.0, 349.32, 438.8, 213.92, 53.74]<h2>formula_id</h2>formula_123<h2>formula_text</h2>F := [|N xy | = m 1 ] [|N xy | = m 2 ] [|N xy | = m 3 ] . (76<h2>formula_coordinates</h2>[10.0, 321.0, 581.84, 238.05, 17.29]<h2>formula_id</h2>formula_124<h2>formula_text</h2>)<h2>formula_coordinates</h2>[10.0, 559.05, 582.83, 4.19, 8.91]<h2>formula_id</h2>formula_125<h2>formula_text</h2>P [F | (|S xy | = u)] ∼ n m1+m2+m3 m 1 !m 2 !m 3 ! • e -2pen+ pepn u Kn n × {P[E xj∩yj | (|S xy | = u)]} m1 × {P[E xj∩yj | (|S xy | = u)]} m2 × {P[E xj∩yj | (|S xy | = u)]} m3 (77)<h2>formula_coordinates</h2>[10.0, 318.0, 638.09, 245.24, 75.28]<h2>formula_id</h2>formula_126<h2>formula_text</h2>P A h | K xy ∼ n 2ℓ-h h![(ℓ -h)!] 2 • e -2pen • P[E xj∩yj | K xy ] h × {P[E xj∩yj | K xy ]} ℓ-h {P[E xj∩yj | K xy ]} ℓ-h . (78)<h2>formula_coordinates</h2>[11.0, 61.56, 99.09, 238.64, 60.12]<h2>formula_id</h2>formula_127<h2>formula_text</h2>P[E xj∩yj | K xy ], P[E xj∩yj | K xy ], and P[E xj∩yj | K xy ].<h2>formula_coordinates</h2>[11.0, 57.48, 189.93, 234.0, 12.1]<h2>formula_id</h2>formula_128<h2>formula_text</h2>P[E xj∩yj | K xy ], we use E xj = K xj ∩ C xj and E yj = K yj ∩ C yj to obtain P[E xj∩yj | K xy ] = P[(C xj ∩ C yj ) ∩ (K xj ∩ K yj ) | K xy ]. = p n 2 • P[K xj ∩ K yj | K xy ](79)<h2>formula_coordinates</h2>[11.0, 48.96, 208.76, 251.24, 77.89]<h2>formula_id</h2>formula_129<h2>formula_text</h2>P[E xj∩yj | K xy ] ≤ p e 2 . (80<h2>formula_coordinates</h2>[11.0, 125.76, 315.53, 170.25, 18.88]<h2>formula_id</h2>formula_130<h2>formula_text</h2>)<h2>formula_coordinates</h2>[11.0, 296.01, 318.11, 4.19, 8.91]<h2>formula_id</h2>formula_131<h2>formula_text</h2>P[E xj∩yj | K xy ]. It is clear that E xj is independent of K xy . Hence, P[E xj | K xy ] = p e .(81)<h2>formula_coordinates</h2>[11.0, 48.96, 335.61, 251.24, 41.61]<h2>formula_id</h2>formula_132<h2>formula_text</h2>P[E xj∩yj | K xy ] = P[E xj | K xy ] -P[E xj∩yj | K xy ] = p e -O p e 2 ∼ p e .(82)<h2>formula_coordinates</h2>[11.0, 67.08, 422.0, 233.12, 33.13]<h2>formula_id</h2>formula_133<h2>formula_text</h2>P[E xj∩yj | K xy ] ∼ p e .(83)<h2>formula_coordinates</h2>[11.0, 128.04, 475.16, 172.16, 17.29]<h2>formula_id</h2>formula_134<h2>formula_text</h2>P A h | K xy ∼ n 2ℓ-h h![(ℓ -h)!] 2 • e -2pen • P[E xj∩yj | K xy ] h • p 2(ℓ-h) e . (84<h2>formula_coordinates</h2>[11.0, 57.72, 523.05, 238.29, 49.12]<h2>formula_id</h2>formula_135<h2>formula_text</h2>)<h2>formula_coordinates</h2>[11.0, 296.01, 563.27, 4.19, 8.91]<h2>formula_id</h2>formula_136<h2>formula_text</h2>P A 0 | K xy ∼ (ℓ!) -2 (p e n) 2ℓ e -2pen .(85)<h2>formula_coordinates</h2>[11.0, 96.96, 594.79, 203.24, 19.22]<h2>formula_id</h2>formula_137<h2>formula_text</h2>P A h | K xy P A 0 | K xy ∼ n -h (ℓ!) 2 h![(ℓ -h)!] 2 P[E xj∩yj | K xy ] h p -2h e ≤ n -h (ℓ!) 2 h![(ℓ -h)!] 2 = o(1).<h2>formula_coordinates</h2>[11.0, 59.52, 630.79, 230.62, 61.58]<h2>formula_id</h2>formula_138<h2>formula_text</h2>P A h | K xy = o P A 0 | K xy , h = 1, 2, . . . , ℓ. (86)<h2>formula_coordinates</h2>[11.0, 57.6, 708.57, 242.6, 10.66]<h2>formula_id</h2>formula_139<h2>formula_text</h2>P [A h ∩ X u ] ≤ P [A h ∩ (|S xy | = u)] .<h2>formula_coordinates</h2>[11.0, 361.68, 149.84, 151.68, 17.29]<h2>formula_id</h2>formula_140<h2>formula_text</h2>ℓ h=0 Kn u=1 P [A h ∩ X u ] ≤ ℓ h=0 Kn u=1 P [A h ∩ (|S xy | = u)] = Kn u=1 P[|S xy | = u] • ℓ h=0 P [A h | (|S xy | = u)] . (87)<h2>formula_coordinates</h2>[11.0, 325.92, 184.25, 237.32, 100.09]<h2>formula_id</h2>formula_141<h2>formula_text</h2>P[|S xy | = u] ≤ 1 u! K 2 n P n -K n u .(88)<h2>formula_coordinates</h2>[11.0, 366.48, 303.17, 196.76, 34.0]<h2>formula_id</h2>formula_142<h2>formula_text</h2>P [A h | (|S xy | = u)]<h2>formula_coordinates</h2>[11.0, 394.2, 336.44, 81.1, 17.29]<h2>formula_id</h2>formula_143<h2>formula_text</h2>P [A h | (|S xy | = u)] ∼ n 2ℓ-h h![(ℓ -h)!] 2 • e -2pen+ pepnu Kn n × {P[E xj∩yj | (|S xy | = u)]} h × {P[E xj∩yj | (|S xy | = u)]} ℓ-h × {P[E xj∩yj | (|S xy | = u)]} ℓ-h . (89<h2>formula_coordinates</h2>[11.0, 322.8, 363.53, 236.25, 84.32]<h2>formula_id</h2>formula_144<h2>formula_text</h2>)<h2>formula_coordinates</h2>[11.0, 559.05, 438.95, 4.19, 8.91]<h2>formula_id</h2>formula_145<h2>formula_text</h2>P[E xj∩yj | (|S xy | = u)] ≤ P[E xj | (|S xy | = u)] = p e (90) P[E xj∩yj | (|S xy | = u)] ≤ P[E xj | (|S xy | = u)] = p e (91) P[E xj∩yj | (|S xy | = u)] ≤ P[E yj | (|S xy | = u)] = p e . (92)<h2>formula_coordinates</h2>[11.0, 320.52, 485.0, 242.72, 47.77]<h2>formula_id</h2>formula_146<h2>formula_text</h2>P [A h | (|S xy | = u)] ≤ 2n 2ℓ-h • e -2pen+ pepnnu Kn • (p e ) 2ℓ-h = 2e -2pen+ pepn nu Kn (p e n) 2ℓ-h (93)<h2>formula_coordinates</h2>[11.0, 319.08, 548.85, 244.16, 30.82]<h2>formula_id</h2>formula_147<h2>formula_text</h2>= ℓ h=0 P A h ∩ K xy • O((ℓ!) 2 ) • Kn u=1 K 2 n P n -K n • e pnpen Kn u . (98<h2>formula_coordinates</h2>[12.0, 51.72, 218.69, 248.28, 41.24]<h2>formula_id</h2>formula_148<h2>formula_text</h2>)<h2>formula_coordinates</h2>[12.0, 296.01, 251.03, 4.19, 8.91]<h2>formula_id</h2>formula_149<h2>formula_text</h2>K 2 n P n -K n • e pn Kn •pen = o(1),(99)<h2>formula_coordinates</h2>[12.0, 118.44, 281.45, 181.76, 31.96]<h2>formula_id</h2>formula_150<h2>formula_text</h2>Kn u=1 K 2 n P n -K n • e pn pen Kn u ≤ K 2 n Pn-Kn • e pn Kn •pen 1 - K 2 n Pn-Kn • e pn Kn •pen = o(1),(100)<h2>formula_coordinates</h2>[12.0, 50.04, 325.54, 250.16, 44.31]<h2>formula_id</h2>formula_151<h2>formula_text</h2>K 2 n P n -K n ∼ K 2 n P n ∼ p s .<h2>formula_coordinates</h2>[12.0, 128.16, 496.73, 93.96, 32.08]<h2>formula_id</h2>formula_152<h2>formula_text</h2>K 2 n<h2>formula_coordinates</h2>[12.0, 84.12, 527.02, 11.1, 9.8]<h2>formula_id</h2>formula_153<h2>formula_text</h2>K 2 n P n -K n • e pn Kn •pen ≤ 2p s • e 3 4 pn ln n . (102<h2>formula_coordinates</h2>[12.0, 99.48, 559.25, 196.36, 31.96]<h2>formula_id</h2>formula_154<h2>formula_text</h2>)<h2>formula_coordinates</h2>[12.0, 295.84, 568.07, 4.36, 8.91]<h2>formula_id</h2>formula_155<h2>formula_text</h2>F (n) = 2p s • e 3 4 pn ln n . (103<h2>formula_coordinates</h2>[12.0, 127.92, 605.14, 167.92, 20.14]<h2>formula_id</h2>formula_156<h2>formula_text</h2>)<h2>formula_coordinates</h2>[12.0, 295.84, 608.99, 4.36, 8.91]<h2>formula_id</h2>formula_157<h2>formula_text</h2>p s ≤ 3 2 ln n np n . (104<h2>formula_coordinates</h2>[12.0, 148.8, 646.29, 147.04, 24.22]<h2>formula_id</h2>formula_158<h2>formula_text</h2>)<h2>formula_coordinates</h2>[12.0, 295.84, 653.75, 4.36, 8.91]<h2>formula_id</h2>formula_159<h2>formula_text</h2>F (n) ≤ 3 ln n np n e 3 4 pn ln n < 3 (ln n) 2 n • n 3/4<h2>formula_coordinates</h2>[12.0, 90.84, 727.49, 166.81, 25.57]<h2>formula_id</h2>formula_160<h2>formula_text</h2>F (n) ≤ max 2p s e 3/4 , 3n -1/4 (ln n) 2 (105)<h2>formula_coordinates</h2>[12.0, 343.56, 75.55, 219.68, 19.1]<h2>formula_id</h2>formula_161<h2>formula_text</h2>P[D x,ℓ ∩ D y,ℓ ∩ E xy ] = o ℓ h=0 P A h ∩ K xy . (106)<h2>formula_coordinates</h2>[12.0, 325.68, 252.29, 237.56, 31.45]<h2>formula_id</h2>formula_162<h2>formula_text</h2>B h = (|N xy | = h) (|N xy | = ℓ -h -1) (|N xy | = ℓ -h -1) . (107<h2>formula_coordinates</h2>[12.0, 352.92, 332.72, 205.96, 32.17]<h2>formula_id</h2>formula_163<h2>formula_text</h2>)<h2>formula_coordinates</h2>[12.0, 558.88, 348.59, 4.36, 8.91]<h2>formula_id</h2>formula_164<h2>formula_text</h2>D x,ℓ ∩ D y,ℓ ∩ E xy = ℓ-1 h=0 (B h ∩ E xy ) . (108<h2>formula_coordinates</h2>[12.0, 358.08, 396.05, 200.8, 31.45]<h2>formula_id</h2>formula_165<h2>formula_text</h2>)<h2>formula_coordinates</h2>[12.0, 558.88, 407.03, 4.36, 8.91]<h2>formula_id</h2>formula_166<h2>formula_text</h2>P[D x,ℓ ∩ D y,ℓ ∩ E xy ] = ℓ-1 h=0 P [B h ∩ E xy ] .(109)<h2>formula_coordinates</h2>[12.0, 338.76, 498.17, 224.48, 31.45]<h2>formula_id</h2>formula_167<h2>formula_text</h2>P [B h ∩ E xy ] = o ℓ h=0 P A h ∩ K xy . (110<h2>formula_coordinates</h2>[12.0, 351.12, 606.65, 207.76, 31.33]<h2>formula_id</h2>formula_168<h2>formula_text</h2>)<h2>formula_coordinates</h2>[12.0, 558.88, 617.63, 4.36, 8.91]<h2>formula_id</h2>formula_169<h2>formula_text</h2>P [B h ∩ E xy ] = P B h ∩ Kn u=1 Y u = Kn u=1 P [B h ∩ Y u ] .(111)<h2>formula_coordinates</h2>[13.0, 55.8, 100.61, 244.4, 41.0]<h2>formula_id</h2>formula_170<h2>formula_text</h2>Y u = [(|S xy | = u) ∩ C xy ],<h2>formula_coordinates</h2>[13.0, 76.44, 151.4, 108.33, 17.29]<h2>formula_id</h2>formula_171<h2>formula_text</h2>P [B h ∩ Y u ] ≤ P [B h ∩ (|S xy | = u)] .(112)<h2>formula_coordinates</h2>[13.0, 98.76, 172.04, 201.44, 17.29]<h2>formula_id</h2>formula_172<h2>formula_text</h2>ℓ-1 h=0 P [B h ∩ E xy ] ≤ ℓ-1 h=0 Kn u=1 P [B h ∩ (|S xy | = u)] = Kn u=1 P[|S xy | = u] • ℓ-1 h=0 P [B h | (|S xy | = u)] .(113)<h2>formula_coordinates</h2>[13.0, 59.76, 211.73, 240.44, 99.97]<h2>formula_id</h2>formula_173<h2>formula_text</h2>P [B h | (|S xy | = u)]. Given the definition of B h in (107), we let m 1 = h and m 2 = m 3 = ℓ -h -1 in Lemma 4 to obtain P [B h | (|S xy | = u)] ∼ n 2ℓ-h-2 h![(ℓ -h -1)!] 2 • e -2pen+ pepnu Kn n × {P[E xj∩yj | (|S xy | = u)]} h × {P[E xj∩yj | (|S xy | = u)]} ℓ-h-1 × {P[E xj∩yj | (|S xy | = u)]} ℓ-h-1 . (114<h2>formula_coordinates</h2>[13.0, 48.96, 343.04, 251.22, 113.57]<h2>formula_id</h2>formula_174<h2>formula_text</h2>)<h2>formula_coordinates</h2>[13.0, 295.84, 447.71, 4.36, 8.91]<h2>formula_id</h2>formula_175<h2>formula_text</h2>P [B h | (|S xy | = u)] ≤ 2e -2pen+ pepnnu Kn (p e n) 2ℓ-h-2 .<h2>formula_coordinates</h2>[13.0, 64.44, 482.85, 220.08, 20.88]<h2>formula_id</h2>formula_176<h2>formula_text</h2>ℓ-1 h=0 P [B h ∩ E xy ] ≤ Kn u=1 P[|S xy | = u] • 2e -2pen+ pn u Kn •pen • ℓ h=0 (p e n) 2ℓ-h-2 = (p e n) -2 × R.H.S. of (94). (116<h2>formula_coordinates</h2>[13.0, 49.32, 553.73, 246.52, 87.04]<h2>formula_id</h2>formula_177<h2>formula_text</h2>)<h2>formula_coordinates</h2>[13.0, 295.84, 624.47, 4.36, 8.91]<h2>formula_id</h2>formula_178<h2>formula_text</h2>ℓ-1 h=0 P [B h ∩ E xy ] = O (R.H.S. of (94)) .(117)<h2>formula_coordinates</h2>[13.0, 92.52, 663.53, 207.68, 31.33]<h2>formula_id</h2>formula_179<h2>formula_text</h2>FOR k-CONNECTIVITY IN G on )<h2>formula_coordinates</h2>[13.0, 381.96, 59.05, 152.19, 20.57]<h2>formula_id</h2>formula_180<h2>formula_text</h2>K n P n = o(1), lim n→∞ α n = +∞ and α n = o(ln n).(119)<h2>formula_coordinates</h2>[13.0, 322.8, 150.81, 240.44, 24.21]<h2>formula_id</h2>formula_181<h2>formula_text</h2>p e = ln n + ℓ ln ln n + β ℓ,n n(120)<h2>formula_coordinates</h2>[13.0, 380.64, 219.69, 182.6, 23.52]<h2>formula_id</h2>formula_182<h2>formula_text</h2>β ℓ,n := np e -ln n -ℓ ln ln n. (121<h2>formula_coordinates</h2>[13.0, 376.08, 265.28, 182.8, 17.29]<h2>formula_id</h2>formula_183<h2>formula_text</h2>)<h2>formula_coordinates</h2>[13.0, 558.88, 266.27, 4.36, 8.91]<h2>formula_id</h2>formula_184<h2>formula_text</h2>lim n→∞ P [κ = ℓ] = 0. (122<h2>formula_coordinates</h2>[13.0, 397.56, 380.49, 161.32, 15.21]<h2>formula_id</h2>formula_185<h2>formula_text</h2>)<h2>formula_coordinates</h2>[13.0, 558.88, 381.23, 4.36, 8.91]<h2>formula_id</h2>formula_186<h2>formula_text</h2>β ℓ,n = (k -1 -ℓ) ln ln n + α n . (123<h2>formula_coordinates</h2>[13.0, 372.0, 443.12, 186.88, 17.29]<h2>formula_id</h2>formula_187<h2>formula_text</h2>)<h2>formula_coordinates</h2>[13.0, 558.88, 444.11, 4.36, 8.91]<h2>formula_id</h2>formula_188<h2>formula_text</h2>lim n→∞ β ℓ,n = +∞ and β ℓ,n = o(ln n).(124)<h2>formula_coordinates</h2>[13.0, 343.8, 492.69, 219.44, 15.1]<h2>formula_id</h2>formula_189<h2>formula_text</h2>lim n→∞ P [κ = ℓ] = 0, ℓ = 0, 1, . . . , k -1.<h2>formula_coordinates</h2>[13.0, 352.08, 531.44, 171.0, 17.29]<h2>formula_id</h2>formula_190<h2>formula_text</h2>lim n→∞ P [G on is k-connected ] = 1.<h2>formula_coordinates</h2>[13.0, 368.16, 583.41, 138.72, 15.21]<h2>formula_id</h2>formula_191<h2>formula_text</h2>P [κ = ℓ] ≤ P [(κ = ℓ) ∩ (δ > ℓ)] + P [δ ≤ ℓ] ,<h2>formula_coordinates</h2>[13.0, 333.24, 670.04, 208.56, 17.29]<h2>formula_id</h2>formula_192<h2>formula_text</h2>lim n→∞ P [δ ≤ ℓ] = 0 if lim n→∞ β ℓ,n = +∞,(125)<h2>formula_coordinates</h2>[13.0, 341.52, 731.24, 221.72, 17.29]<h2>formula_id</h2>formula_193<h2>formula_text</h2>lim n→∞ P [κ = ℓ ∩ δ > ℓ] = 0 if lim n→∞ β ℓ,n = +∞.(126)<h2>formula_coordinates</h2>[14.0, 58.68, 74.6, 241.52, 17.29]<h2>formula_id</h2>formula_194<h2>formula_text</h2>E X ℓ = nP [D x,ℓ ] ∼ n • (ℓ!) -1 (p e n) ℓ e -pen .(127)<h2>formula_coordinates</h2>[14.0, 69.96, 146.71, 230.24, 20.06]<h2>formula_id</h2>formula_195<h2>formula_text</h2>E X ℓ ∼ n (ℓ!) -1 (ln n) ℓ e -ln n-ℓ ln ln n-β ℓ,n = (ℓ!) -1 e -β ℓ,n .<h2>formula_coordinates</h2>[14.0, 48.96, 184.27, 251.64, 20.06]<h2>formula_id</h2>formula_196<h2>formula_text</h2>lim n→∞ P[δ = ℓ] = 0.(128)<h2>formula_coordinates</h2>[14.0, 136.68, 241.05, 163.52, 15.21]<h2>formula_id</h2>formula_197<h2>formula_text</h2>lim n→∞ β ℓ ⋆ ,n = +∞ for each ℓ ⋆ = 0, 1, . . . , ℓ. It is also im- mediate from (121) that β ℓ ⋆ ,n = o(ln n) since β ℓ,n = o(ln n).<h2>formula_coordinates</h2>[14.0, 48.96, 271.49, 251.12, 24.01]<h2>formula_id</h2>formula_198<h2>formula_text</h2>lim n→∞ P[δ = ℓ ⋆ ] = 0, ℓ ⋆ = 0, 1, . . . , ℓ,<h2>formula_coordinates</h2>[14.0, 94.92, 318.89, 159.12, 17.05]<h2>formula_id</h2>formula_199<h2>formula_text</h2>N * = {T | T ∈ N , |T | ≥ 2} and K T = ∪ vi∈T S i .<h2>formula_coordinates</h2>[14.0, 76.92, 483.32, 205.77, 17.29]<h2>formula_id</h2>formula_200<h2>formula_text</h2>E(J ) = T ∈N * |K T | ≤ J |T | ,(129)<h2>formula_coordinates</h2>[14.0, 107.16, 526.16, 193.04, 26.23]<h2>formula_id</h2>formula_201<h2>formula_text</h2>r n := min P n K n , n 2 . (130<h2>formula_coordinates</h2>[14.0, 117.24, 581.01, 178.6, 24.33]<h2>formula_id</h2>formula_202<h2>formula_text</h2>)<h2>formula_coordinates</h2>[14.0, 295.84, 588.59, 4.36, 8.91]<h2>formula_id</h2>formula_203<h2>formula_text</h2>J i = max{⌊(1 + ε)K n ⌋ , ⌊λK n i⌋} i = 2, . . . , r n , ⌊µP n ⌋ i = r n + 1, . . . , n.(131)<h2>formula_coordinates</h2>[14.0, 57.12, 627.92, 243.08, 37.49]<h2>formula_id</h2>formula_204<h2>formula_text</h2>P [(κ = ℓ) ∩ (δ > ℓ)] ≤ P [E(J )] + P (κ = ℓ) ∩ (δ > ℓ) ∩ E(J ) . (132)<h2>formula_coordinates</h2>[14.0, 63.84, 713.6, 236.36, 35.4]<h2>formula_id</h2>formula_205<h2>formula_text</h2>lim n→∞ P [E(J )] = 0,(133)<h2>formula_coordinates</h2>[14.0, 398.52, 131.61, 164.72, 15.09]<h2>formula_id</h2>formula_206<h2>formula_text</h2>max 2λσ, λ e 2 σ λ 1-2λ < 1,(134)<h2>formula_coordinates</h2>[14.0, 372.24, 194.85, 191.0, 27.96]<h2>formula_id</h2>formula_207<h2>formula_text</h2>max 2 √ µ e µ µ σ , √ µ e µ µ < 1.(135)<h2>formula_coordinates</h2>[14.0, 338.04, 251.09, 225.2, 27.04]<h2>formula_id</h2>formula_208<h2>formula_text</h2>lim µ↓0 √ µ e µ µ<h2>formula_coordinates</h2>[14.0, 312.0, 355.73, 68.8, 19.45]<h2>formula_id</h2>formula_209<h2>formula_text</h2>lim n→∞ P (κ = ℓ) ∩ (δ > ℓ) ∩ E(J ) = 0,<h2>formula_coordinates</h2>[14.0, 348.12, 475.52, 178.92, 17.29]<h2>formula_id</h2>formula_210<h2>formula_text</h2>Y i = ⌊λK n i⌋ i = 2, . . . , r n , ⌊µP n ⌋ i = r n + 1, . . . , n.(136)<h2>formula_coordinates</h2>[14.0, 363.36, 664.52, 199.88, 31.69]<h2>formula_id</h2>formula_211<h2>formula_text</h2>J i = max{⌊(1 + ε)K n ⌋ , Y i } i = 2, . . . , r n , Y i i = r n + 1, . . . , n.(137)<h2>formula_coordinates</h2>[14.0, 320.76, 718.88, 242.48, 25.3]<h2>formula_id</h2>formula_212<h2>formula_text</h2>N -:= {T | T ∈ N , 2 ≤ |T | ≤ r n },<h2>formula_coordinates</h2>[15.0, 100.92, 76.16, 147.24, 17.29]<h2>formula_id</h2>formula_213<h2>formula_text</h2>N + := {T | T ∈ N , |T | > r n }.<h2>formula_coordinates</h2>[15.0, 110.04, 115.04, 129.0, 17.29]<h2>formula_id</h2>formula_214<h2>formula_text</h2>E(J ) =   T ∈N- |K T | ≤ J |T |   ∪   T ∈N+ |K T | ≤ Y |T |   .(138)<h2>formula_coordinates</h2>[15.0, 49.32, 163.66, 250.88, 46.59]<h2>formula_id</h2>formula_215<h2>formula_text</h2>J i = max{⌊(1 + ε)K n ⌋, Y i } for i = 2, 3, . . . , r n , we have   T ∈N- |K T | ≤ J |T |   (139) =   T ∈N- [|K T | ≤ (1 + ε)K n ]   ∪   T ∈N- |K T | ≤ Y |T |   .<h2>formula_coordinates</h2>[15.0, 48.96, 218.84, 253.56, 105.19]<h2>formula_id</h2>formula_216<h2>formula_text</h2>E(J ) (140) =   T ∈N- [|K T | ≤ (1 + ε)K n ]   ∪ T ∈N * |K T | ≤ Y |T | .<h2>formula_coordinates</h2>[15.0, 48.96, 359.12, 251.24, 53.23]<h2>formula_id</h2>formula_217<h2>formula_text</h2>T ∈N- [|K T | ≤ (1 + ε)K n ] = T ∈Nn,2 [|K T | ≤ (1 + ε)K n ]<h2>formula_coordinates</h2>[15.0, 59.52, 435.92, 231.6, 26.23]<h2>formula_id</h2>formula_218<h2>formula_text</h2>E(Y ) = T ∈N * |K T | ≤ Y |T |(142)<h2>formula_coordinates</h2>[15.0, 108.24, 513.92, 191.96, 26.23]<h2>formula_id</h2>formula_219<h2>formula_text</h2>E(J ) =   T ∈Nn,2 [|K T | ≤ (1 + ε)K n ]   ∪ E(Y ).<h2>formula_coordinates</h2>[15.0, 74.28, 558.82, 200.4, 39.05]<h2>formula_id</h2>formula_220<h2>formula_text</h2>P [E(J )] ≤ P [E(Y )] + T ∈Nn,2 P [|K T | ≤ (1 + ε)K n ] .<h2>formula_coordinates</h2>[15.0, 64.44, 633.56, 220.08, 26.23]<h2>formula_id</h2>formula_221<h2>formula_text</h2>P [E(Y )] = o(1).(143)<h2>formula_coordinates</h2>[15.0, 139.44, 695.01, 160.76, 9.96]<h2>formula_id</h2>formula_222<h2>formula_text</h2>[|K T | ≤ (1 + ε)K n ],<h2>formula_coordinates</h2>[15.0, 447.36, 56.72, 84.21, 17.29]<h2>formula_id</h2>formula_223<h2>formula_text</h2>|N n,2 | = n 2 . With K 1,2 = S 1 ∪ S 2 , we find P [E(J )] ≤ o(1) + n 2 P [K 1,2 ≤ ⌊(1 + ε)K n ⌋] .(144)<h2>formula_coordinates</h2>[15.0, 312.0, 68.21, 251.24, 53.91]<h2>formula_id</h2>formula_224<h2>formula_text</h2>n 2 P [K 1,2 ≤ ⌊(1 + ε)K n ⌋] = o(1).(145)<h2>formula_coordinates</h2>[15.0, 366.84, 157.13, 196.4, 18.87]<h2>formula_id</h2>formula_225<h2>formula_text</h2>P [K 1,2 ≤ ⌊(1 + ε)K n ⌋] ≤ Γ(ε) K n P n Kn(1-ε)<h2>formula_coordinates</h2>[15.0, 339.12, 220.49, 191.87, 27.25]<h2>formula_id</h2>formula_226<h2>formula_text</h2>n 2 P [K 1,2 ≤ ⌊(1 + ε)K n ⌋] ≤ Γ(ε)n 2 (1-ε)Kn K n P n Kn(1-ε) . (146<h2>formula_coordinates</h2>[15.0, 315.36, 274.01, 244.2, 37.28]<h2>formula_id</h2>formula_227<h2>formula_text</h2>)<h2>formula_coordinates</h2>[15.0, 558.88, 302.39, 4.36, 8.91]<h2>formula_id</h2>formula_228<h2>formula_text</h2>P n ≥ max{σn, K n w n }.<h2>formula_coordinates</h2>[15.0, 387.84, 363.44, 99.36, 17.29]<h2>formula_id</h2>formula_229<h2>formula_text</h2>n 2 Kn (1-ε) K n P n ≤ min n -1+ 2 Kn (1-ε) K n σ , e 2 ln n Kn (1-ε) w n ≤ max n -1 2 ln n σ , e 2 (1-ε)<h2>formula_coordinates</h2>[15.0, 331.08, 410.62, 201.9, 60.34]<h2>formula_id</h2>formula_230<h2>formula_text</h2>lim n→∞ Γ(ε)n 2 Kn (1-ε) K n P n = 0,<h2>formula_coordinates</h2>[15.0, 379.8, 530.49, 115.44, 24.22]<h2>formula_id</h2>formula_231<h2>formula_text</h2>E ij , v i ∈ T, v j ∈ U c \ T.<h2>formula_coordinates</h2>[15.0, 379.68, 736.49, 115.68, 18.99]<h2>formula_id</h2>formula_232<h2>formula_text</h2>vi∈T vj ∈U c \T E ij .<h2>formula_coordinates</h2>[16.0, 167.52, 135.21, 51.6, 33.3]<h2>formula_id</h2>formula_233<h2>formula_text</h2>vi∈U vj ∈T E ij .<h2>formula_coordinates</h2>[16.0, 153.84, 199.89, 57.96, 25.98]<h2>formula_id</h2>formula_234<h2>formula_text</h2>A U,T := B U,T ∩ C T ∩ D U,T .<h2>formula_coordinates</h2>[16.0, 116.28, 238.76, 116.52, 17.29]<h2>formula_id</h2>formula_235<h2>formula_text</h2>U ∈ N , |U | = ℓ and T ∈ N U c , |T | ≥ 2, such that G on (T ) is connected while T is isolated in G on (U c<h2>formula_coordinates</h2>[16.0, 48.96, 291.8, 251.1, 34.9]<h2>formula_id</h2>formula_236<h2>formula_text</h2>[(κ = ℓ) ∩ (δ > ℓ)] ⊆ U∈N n,ℓ , T ∈N U c : |T |≥2 A U,T<h2>formula_coordinates</h2>[16.0, 69.12, 572.12, 209.18, 26.71]<h2>formula_id</h2>formula_237<h2>formula_text</h2>P (κ = ℓ) ∩ (δ > ℓ) ∩ E(J ) ≤ U∈N n,ℓ ,T ∈N U c : 2≤|T |≤⌊ n-ℓ 2 ⌋ P A U,T ∩ E(J ) = ⌊ n-ℓ 2 ⌋ r=2 U∈N n,ℓ ,T ∈N U c ,r P A U,T ∩ E(J )(147)<h2>formula_coordinates</h2>[16.0, 71.88, 666.56, 228.32, 88.75]<h2>formula_id</h2>formula_238<h2>formula_text</h2>P [A U,T ] = P [A ℓ,r ] , U ∈ N n,ℓ , T ∈ N U c ,r<h2>formula_coordinates</h2>[16.0, 343.32, 151.16, 187.66, 17.29]<h2>formula_id</h2>formula_239<h2>formula_text</h2>U∈N n,ℓ ,T ∈N U c ,r P A U,T ∩ E(J ) = n ℓ n -ℓ r P A ℓ,r ∩ E(J ) follows since |N n,ℓ | = n ℓ and |N U c ,r | = n-ℓ r .<h2>formula_coordinates</h2>[16.0, 312.0, 196.76, 202.47, 78.49]<h2>formula_id</h2>formula_240<h2>formula_text</h2>P (κ = ℓ) ∩ (δ > ℓ) ∩ E(J ) ≤ ⌊ n-ℓ 2 ⌋ r=2 n ℓ n -ℓ r P A ℓ,r ∩ E(J ) . (148<h2>formula_coordinates</h2>[16.0, 337.32, 293.6, 221.56, 52.06]<h2>formula_id</h2>formula_241<h2>formula_text</h2>)<h2>formula_coordinates</h2>[16.0, 558.88, 325.55, 4.36, 8.91]<h2>formula_id</h2>formula_242<h2>formula_text</h2>lim n→∞ ⌊ n-ℓ 2 ⌋ r=2 n ℓ n -ℓ r P A ℓ,r ∩ E(J ) = 0.(149)<h2>formula_coordinates</h2>[16.0, 325.08, 374.73, 238.16, 33.94]<h2>formula_id</h2>formula_243<h2>formula_text</h2>D ℓ,r = n j=r+ℓ+1 ∪ i∈νr,j S i ∩ S j = ∅(150)<h2>formula_coordinates</h2>[16.0, 357.0, 490.01, 206.24, 31.33]<h2>formula_id</h2>formula_244<h2>formula_text</h2>ν r,j := {i = ℓ + 1, ℓ + 2, . . . , ℓ + r : C ij }(151)<h2>formula_coordinates</h2>[16.0, 342.36, 553.04, 220.88, 17.29]<h2>formula_id</h2>formula_245<h2>formula_text</h2>P   D ℓ,r S i , i = ℓ + 1, . . . , ℓ + r C ij , i = ℓ + 1, . . . , ℓ + r, j = ℓ + r + 1, . . . , n   = n j=r+ℓ+1   Pn-|∪i∈ν r,j Si| Kn Pn Kn   .<h2>formula_coordinates</h2>[16.0, 326.64, 677.02, 171.52, 73.16]<h2>formula_id</h2>formula_246<h2>formula_text</h2>P   B ℓ,r S i , i = ℓ + 1, . . . , ℓ + r C ij , i = 1, . . . , ℓ, j = ℓ + 1, . . . , ℓ + r   = ℓ j=1   1 - Pn-|∪i∈ν r,j Si| Kn Pn Kn   .<h2>formula_coordinates</h2>[17.0, 64.8, 71.98, 167.19, 72.92]<h2>formula_id</h2>formula_247<h2>formula_text</h2>| ∪ i∈νr,j S i | ≥ K n 1 [|ν r,j | > 0] .(152)<h2>formula_coordinates</h2>[17.0, 108.96, 194.12, 191.24, 17.29]<h2>formula_id</h2>formula_248<h2>formula_text</h2>| ∪ i∈νr,j S i | ≥ J |νr,j| + 1 • 1 [|ν r,j | > 1](153)<h2>formula_coordinates</h2>[17.0, 78.6, 231.56, 221.6, 17.29]<h2>formula_id</h2>formula_249<h2>formula_text</h2>| ∪ i∈νr,j S i | ≤ |ν r,j |K n (154<h2>formula_coordinates</h2>[17.0, 126.72, 267.92, 169.12, 17.29]<h2>formula_id</h2>formula_250<h2>formula_text</h2>)<h2>formula_coordinates</h2>[17.0, 295.84, 268.91, 4.36, 8.91]<h2>formula_id</h2>formula_251<h2>formula_text</h2>P A ℓ,r ∩ E(J ) = P C r ∩ B ℓ,r ∩ D ℓ,r ∩ E(J ) ≤ E     1 [C r ] × ℓ j=1 1 - ( Pn -Kn |ν r,j | Kn ) ( Pn Kn ) × × n j=r+ℓ+1 ( Pn -L(ν r,j ) Kn ) ( Pn Kn )     ,<h2>formula_coordinates</h2>[17.0, 70.8, 341.36, 207.36, 87.0]<h2>formula_id</h2>formula_252<h2>formula_text</h2>L(ν r,j ) = max {K n • 1 [|ν r,j | > 0] , (155<h2>formula_coordinates</h2>[17.0, 83.76, 450.8, 212.08, 17.29]<h2>formula_id</h2>formula_253<h2>formula_text</h2>) (J |νr,j | + 1) • 1 [|ν r,j | > 1] .<h2>formula_coordinates</h2>[17.0, 147.24, 451.79, 152.96, 31.3]<h2>formula_id</h2>formula_254<h2>formula_text</h2>|ν r | = st Bin(r, p n ). (156<h2>formula_coordinates</h2>[17.0, 134.88, 538.64, 160.96, 17.29]<h2>formula_id</h2>formula_255<h2>formula_text</h2>)<h2>formula_coordinates</h2>[17.0, 295.84, 539.63, 4.36, 8.91]<h2>formula_id</h2>formula_256<h2>formula_text</h2>L(ν r ) = max K n • 1 [|ν r | > 0] , (J |νr | + 1) • 1 [|ν r | > 1] .(157)<h2>formula_coordinates</h2>[17.0, 52.68, 587.0, 247.52, 21.89]<h2>formula_id</h2>formula_257<h2>formula_text</h2>P A ℓ,r ∩ E(J ) ≤ P [C r ] × E 1 - Pn-Kn|νr | Kn Pn Kn ℓ × E Pn-L(νr) Kn Pn Kn n-r-ℓ . (158<h2>formula_coordinates</h2>[17.0, 49.32, 683.84, 250.32, 60.65]<h2>formula_id</h2>formula_258<h2>formula_text</h2>)<h2>formula_coordinates</h2>[17.0, 295.84, 735.59, 4.36, 8.91]<h2>formula_id</h2>formula_259<h2>formula_text</h2>P [C r ] ≤ r r-2 p r-1 e , r=<h2>formula_coordinates</h2>[17.0, 356.04, 100.73, 114.9, 19.0]<h2>formula_id</h2>formula_261<h2>formula_text</h2>P A ℓ,r ∩ E(J ) ≤ r r-2 (p e ) r-1 • (2rp e ) ℓ<h2>formula_coordinates</h2>[17.0, 315.84, 596.6, 103.17, 36.25]<h2>formula_id</h2>formula_262<h2>formula_text</h2>P A ℓ,r ∩ E(J ) ≤ min r r-2 (p e ) r-1 , 1 × min e -pe(1+ε/2) , e -peλr + e -Knµ 1 [r > r n ] n-r-ℓ .<h2>formula_coordinates</h2>[17.0, 315.84, 693.44, 243.36, 61.57]<h2>formula_id</h2>formula_263<h2>formula_text</h2>r n > R for any n ≥ n ⋆ (R)(163)<h2>formula_coordinates</h2>[18.0, 118.32, 118.97, 181.88, 18.88]<h2>formula_id</h2>formula_264<h2>formula_text</h2>f n,ℓ,r = n ℓ n -ℓ r P A ℓ,r ∩ E(J ) .<h2>formula_coordinates</h2>[18.0, 91.92, 161.84, 165.12, 24.01]<h2>formula_id</h2>formula_265<h2>formula_text</h2>L.H.S. of (149) = ⌊ n-ℓ 2 ⌋ r=2 f n,ℓ,r .(164)<h2>formula_coordinates</h2>[18.0, 111.72, 213.69, 188.48, 33.93]<h2>formula_id</h2>formula_266<h2>formula_text</h2>⌊ n-ℓ 2 ⌋ r=2 f n,ℓ,r = R r=2 f n,ℓ,r + rn r=R+1 f n,ℓ,r + ⌊ n-ℓ 2 ⌋ r=rn+1 f n,ℓ,r .<h2>formula_coordinates</h2>[18.0, 60.6, 299.37, 227.88, 34.17]<h2>formula_id</h2>formula_267<h2>formula_text</h2>R r=2 f n,ℓ,r = o(1),(165)<h2>formula_coordinates</h2>[18.0, 144.6, 373.85, 155.6, 31.09]<h2>formula_id</h2>formula_269<h2>formula_text</h2>⌊ n-ℓ 2 ⌋ r=rn+1 f n,ℓ,r = o(1).(167)<h2>formula_coordinates</h2>[18.0, 131.4, 464.01, 168.8, 33.93]<h2>formula_id</h2>formula_270<h2>formula_text</h2>n r ≤ en r r(168)<h2>formula_coordinates</h2>[18.0, 150.24, 550.61, 149.96, 23.8]<h2>formula_id</h2>formula_271<h2>formula_text</h2>lim n→∞ f n,ℓ,r = 0 if lim n→∞ β ℓ,n = +∞(169)<h2>formula_coordinates</h2>[18.0, 79.8, 690.09, 220.4, 15.1]<h2>formula_id</h2>formula_272<h2>formula_text</h2>f n,ℓ,r ≤ n ℓ • n r • r r-2 p r-1 e (2rp e ) ℓ • e -pe(1+ε/2)(n-r-ℓ) = (2r) ℓ r r-2 • n ℓ+r p ℓ+r-1 e<h2>formula_coordinates</h2>[18.0, 317.28, 91.89, 202.55, 48.84]<h2>formula_id</h2>formula_274<h2>formula_text</h2>n+β ℓ,n )(1+ε/2) • e o(1) = n • (ln n) ℓ+r-1 • n -1 (ln n) -ℓ e -β ℓ,n 1+ε/2 = n -ε/2 (ln n) r-ℓε/2-1 e -β ℓ,n (1+ε/2) = o(1)<h2>formula_coordinates</h2>[18.0, 321.96, 243.41, 233.51, 67.84]<h2>formula_id</h2>formula_275<h2>formula_text</h2>f n,ℓ,r ≤ n ℓ • e(n -ℓ) r r • r r-2 (p e ) r-1 e -perλ(n-r-ℓ)<h2>formula_coordinates</h2>[18.0, 322.92, 460.73, 228.71, 26.8]<h2>formula_id</h2>formula_277<h2>formula_text</h2>= R + 1, R + 2, . . . , ⌊ n-ℓ 2 ⌋, from r ≤ n-ℓ 2 , we have for all n sufficiently large, n-r -ℓ ≥ 1 2 (n -ℓ) ≥ n 3 .<h2>formula_coordinates</h2>[18.0, 312.0, 514.01, 251.01, 42.87]<h2>formula_id</h2>formula_280<h2>formula_text</h2>f n,ℓ,r ≤ n ℓ+r e r • 2 ln n n r-1 • n -rλ/3 (ln n) -rλℓ/3 e -rλβ ℓ,n /3 ≤ n ℓ+1-rλ/3 • (2e ln n) r = n ℓ+1 • (2en -λ/3 ln n) r . (174<h2>formula_coordinates</h2>[18.0, 320.76, 674.01, 238.12, 77.88]<h2>formula_id</h2>formula_281<h2>formula_text</h2>)<h2>formula_coordinates</h2>[18.0, 558.88, 735.59, 4.36, 8.91]<h2>formula_id</h2>formula_282<h2>formula_text</h2>rn r=R+1 f n,ℓ,r ≤ +∞ r=R+1 n ℓ+1 • (2en -λ/3 ln n) r = n ℓ+1 • (2en -λ/3 ln n) R+1 1 -2en -λ/3 ln n ∼ n ℓ+1-λ(R+1)/3 (2e ln n) R+1 .(175)<h2>formula_coordinates</h2>[19.0, 74.16, 72.41, 226.04, 79.83]<h2>formula_id</h2>formula_283<h2>formula_text</h2>f n,ℓ,r ≤ n ℓ n r e -perλ + e -Knµ n-ℓ2<h2>formula_coordinates</h2>[19.0, 92.04, 304.41, 158.85, 23.76]<h2>formula_id</h2>formula_285<h2>formula_text</h2>rn-1 rn+1 f n,ℓ,r ≤ ∞ R+1 n ℓ+1 • (2en -λ(1-η)/3 ln n) r<h2>formula_coordinates</h2>[19.0, 330.6, 511.99, 185.74, 31.56]<h2>formula_id</h2>formula_286<h2>formula_text</h2>n -2 m 1 • n -m 1 -2 m 2 • n -m 1 -m 2 -2 m 3 .(206)<h2>formula_coordinates</h2>[23.0, 67.68, 89.36, 232.52, 24.46]<h2>formula_id</h2>formula_287<h2>formula_text</h2>n -2 m 1 n -m 1 -2 m 2 n -m 1 -m 2 -2 m 3 ∼ n m1 m 1 ! • n m2 m 2 ! • n m3 m 3 ! = n m1+m2+m3 m 1 !m 2 !m 3 ! . (213<h2>formula_coordinates</h2>[23.0, 361.68, 224.36, 197.2, 51.58]<h2>formula_id</h2>formula_288<h2>formula_text</h2>)<h2>formula_coordinates</h2>[23.0, 558.88, 259.31, 4.36, 8.91]<h2>formula_id</h2>formula_289<h2>formula_text</h2>{P[E xj∩yj | (|S xy | = u)]} n-m1-m2-m3-2 . (214<h2>formula_coordinates</h2>[23.0, 338.04, 305.93, 220.84, 19.0]<h2>formula_id</h2>formula_290<h2>formula_text</h2>)<h2>formula_coordinates</h2>[23.0, 558.88, 308.63, 4.36, 8.91]<h1>doi</h1><h1>title</h1>k-Connectivity of Random Key Graphs<h1>authors</h1>Jun Zhao; Osman Yagan; Virgil Gligor<h1>pub_date</h1>2015-02-02<h1>abstract</h1>Random key graphs represent topologies of secure wireless sensor networks that apply the seminal Eschenauer-Gligor random key predistribution scheme to secure communication between sensors. These graphs have received much attention and also been used in diverse application areas beyond secure sensor networks; e.g., cryptanalysis, social networks, and recommender systems. Formally, a random key graph with n nodes is constructed by assigning each node Xn keys selected uniformly at random from a pool of Yn keys and then putting an undirected edge between any two nodes sharing at least one key. Considerable progress has been made in the literature to analyze connectivity and k-connectivity of random key graphs; e.g., Yagan and Makowski [ISIT '09, Trans. IT'12] on connectivity under Xn = Ω( √ ln n ), Rybarczyk [Discrete Mathematics '11] on connectivity under Xn ≥ 2, and our recent work [CDC '14] on k-connectivity under Xn = Ω( √ ln n ), where k-connectivity of a graph ensures connectivity even after the removal of k nodes or k edges. Yet, it still remains an open question for k-connectivity in random key graphs under Xn ≥ 2 and Xn = o( √ ln n ) (the case of Xn = 1 is trivial). In this paper, we answer the above problem by providing an exact analysis of k-connectivity in random key graphs under Xn ≥ 2.<h1>sections</h1><h2>heading</h2>I. INTRODUCTION<h2>text</h2>Random key graphs, also known as homogeneous random intersection graphs, have been investigated widely in the literature [1], [2], [5]- [7], [9], [10], [12], [13]. The notion of random key graph results from the seminal Eschenauer-Gligor (EG) random key predistribution scheme [4], which is the most recognized solution to secure communication using cryptographic keys in wireless sensor networks [10]. The definition of a random key graph can also be generalized beyond cryptographic keys. Consider a random key graph G(n, X n , Y n ) that is constructed on a set of n nodes as follows. Each node is independently assigned a set of X n distinct objects, selected uniformly at random from a pool of Y n objects, where X n and Y n are both functions of n. An undirected edge exists between two nodes if and only if they possess at least one common object. An object is a cryptographic key in the application of random key graphs to the Eschenauer-Gligor random key predistribution scheme. In addition to the area of secure sensor networks, random key graphs have also been used in various applications including cryptanalysis [1], social networks [12], and recommender systems [5].
(k-)Connectivity of a random key graph has received much interest [2], [6], [7], [9], [10], [12], [13]. A graph is said to be k-connected if it remains connected despite the deletion of at most (k -1) nodes or edges 1 ; an equivalent definition of k-connectivity is that for each pair of nodes there exist at least k mutually disjoint paths connecting them [3]. In the case of k being 1, k-connectivity becomes connectivity, meaning that each node in the graph can find at least one path to any other node, either directly or with the help of other relaying nodes. A graph property related to and implied by k-connectivity is that the minimum degree of the the graph is at least k (i.e., each node is directly connected to no less than k other nodes), where the minimum degree refers to the minimum among the numbers of neighbors that nodes have.
We investigate k-connectivity of random key graphs. Our contribution is, for a random key graph, to derive the asymptotically exact probabilities for k-connectivity and the property that the minimum degree is at least k.
The rest of the paper is organized as follows. Section II presents the results. We elaborate the proof of Theorem 1 in Section III. Section IV provides numerical findings to support the theoretical results. Section V surveys related work; and Section VI concludes the paper.<h2>publication_ref</h2>['b0', 'b1', 'b4', 'b6', 'b8', 'b9', 'b11', 'b12', 'b3', 'b9', 'b0', 'b11', 'b4', 'b1', 'b5', 'b6', 'b8', 'b9', 'b11', 'b12', 'b2']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>II. THE RESULTS<h2>text</h2>For a random key graph G(n, X n , Y n ), Theorem 1 and Corollary 1 below present the asymptotically exact probabilities for k-connectivity and the property of the minimum degree being at least k, where k is a positive integer and does not scale with n. The term ln stands for the natural logarithm function, and e is its base. We use the standard asymptotic notation O(•), o(•), Ω(•), ω(•), Θ(•), ∼; in particular, for two positive sequences x n and y n , the relation x n ∼ y n means lim n→∞ (x n /y n ) = 1. All asymptotic statements are understood with n → ∞. Also, P[E] denotes the probability that event E occurs.
Theorem 1. For a random key graph G(n, X n , Y n ), let q n be the probability that there exists an edge between two nodes. With a sequence α n defined by
q n = ln n + (k -1) ln ln n + α n n ,(1)
then under X n ≥ 2, it follows that
lim n→∞ P G(n, X n , Y n ) is k-connected. = lim n→∞ P [ G(n, X n , Y n ) has a minimum degree at least k. ] =      0, if lim n→∞ α n = -∞, 1, if lim n→∞ α n = ∞, e -e -α * (k-1)! , if lim n→∞ α n = α * ∈ (-∞, ∞).(2)
We have the following corollary by replacing the condition (1) on the edge probability q n with a condition on the asymptotics Xn 2  Yn of q n (formally, q n ∼ Xn 
Y n = ln n + (k -1) ln ln n + β n n ,(3)
then under X n ≥ 2, it follows that
lim n→∞ P G(n, X n , Y n ) is k-connected. = lim n→∞ P [ G(n, X n , Y n ) has a minimum degree at least k. ] =      0, if lim n→∞ β n = -∞, 1, if lim n→∞ β n = ∞, e -e -β * (k-1)! , if lim n→∞ β n = β * ∈ (-∞, ∞).
Remark 1. From Lemma 4 (resp., Lemma 5) in the Appendix, we can introduce an extra condition α n = ±O(ln ln n) = ±o(ln n) (resp., β n = ±O(ln ln n) = ±o(ln n)) in proving Theorem 1 (resp., Corollary 1).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Remark 2.<h2>text</h2>In Theorem 1 and Corollary 1, since the results are in the asymptotic sense, the conditions only need to hold for all n sufficiently large.
Establishing Corollary 1 given Theorem 1 is straightforward and is given in the Appendix. Below we explain how to obtain Theorem 1. Since a necessary condition for a graph to be kconnected is that the minimum degree is at least k, the proof of Theorem 1 will be completed once we have the following two lemmas. Lemma 1 is from our prior work [12]. Lemma 2 simply reproduces the result on the minimum degree in Theorem 1.<h2>publication_ref</h2>['b11']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Lemma 1 (Our work [12, Lemma 5]). For a random key graph<h2>text</h2>G(n, X n , Y n ) under (1) and X n ≥ 2, it follows that lim n→∞ P G(n, X n , Y n ) has a minimum degree at least k, but is not k-connected. =0.(4)
Lemma 2. For a random key graph G(n, X n , Y n ) under ( 1) and X n ≥ 2, it follows that
lim n→∞ P [ G(n, X n , Y n ) has a minimum degree at least k. ]
= right hand side of (2).
By [11,Lemma 2], Lemma 2 will follow once we show Lemma 3 below, where we let V = {v 1 , v 2 , . . . , v n } be the set of nodes in a random key graph G(n, X n , Y n ).
Lemma 3. For a random key graph G(n, X n , Y n ) under ( 1) and X n ≥ 2, it follows for integers m ≥ 1 and h ≥ 0 that
P[Nodes v 1 , v 2 , . . . , v m have degree h] ∼ (h!) -m (nq n ) hm e -mnqn .(5)
We detail the proof of Lemma 3 in the next section.<h2>publication_ref</h2>['b1', 'b10']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>III. THE PROOF OF LEMMA 3<h2>text</h2>In a random key graph G(n, X n , Y n ), recalling that V = {v 1 , v 2 , . . . , v n } is the set of nodes, we let S i be the set of X n distinct objects assigned to node v i ∈ V. We further define V m as {v 1 , v 2 , . . . , v m } and V m as V \ V m . Among nodes in V m , we denote by N i the set of nodes neighboring to v i for i = 1, 2, . . . , m. We denote N i ∩ N j by N ij , and S i ∩ S j by S ij .
We 
(S ij = ∅) happens. ii) If |N i | ≤ h for any i = 1, 2, . . . , m, then 1≤i≤m N i ≤ 1≤i≤m N i ≤ hm,(6)
where the two equal signs in ( 6) both hold if and only if
1≤i<j≤m (N ij = ∅) ∩ 1≤i≤m (|N i | = h) .(7)
From i) and ii) above, if nodes v 1 , v 2 , . . . , v m have degree h, we have either of the following two cases: (a) Any two of v 1 , v 2 , . . . , v m have no edge in between (namely, 1≤i<j≤m (S ij = ∅)); and event (7) happens.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>(b)<h2>text</h2>1≤i≤m N i ≤ hm -1. In addition, if case (a) happens, then nodes v 1 , v 2 , . . . , v m have degree h. However, if case (b) occurs, there is no such conclusion. With P a (resp., P b ) denoting the probability of case (a) (resp., case (b)), we obtain
P a ≤ P[Nodes v 1 , v 2 , . . . , v m have degree h] ≤ P a + P b ,
where
P a = P 1≤i<j≤m (S ij = ∅) ∩ 1≤i<j≤m (N ij = ∅) ∩ 1≤i≤m (|N i | = h) ,
and
P b = P 1≤i≤m N i ≤ hm -1 .
Hence, (5) holds after we prove the following ( 8) and ( 9):
P b = o (nq n ) hm e -mnqn .(8)
and
P a ∼ (h!) -m (nq n ) hm e -mnqn • [1 + o(1)],(9)
We will prove ( 8) and ( 9) below. We let S m denote the tuple (S 
:= S * i ∩ S * j .
For two different nodes v and w in the graph G(n, X n , Y n ), we use v ↔ w to denote the event that there is an edge between v and w; i.e., the symbol "↔" means "is directly connected to".
A. The Proof of (8) Let w be an arbitrary node in V m . We have
P 1≤i≤m N i = t|S m = S * m (10) = (n -m)! t!(n -m -t)! × P[w ↔ at least one of nodes in V m |S m = S * m ] t × P[w ↔ none of nodes in V m |S m = S * m ] n-m-t . (11
)
By the union bound, it holds that
P[w ↔ at least one of nodes in V m |S m = S * m ] ≤ 1≤i≤m P[w ↔ v i |S m = S * m ] = mq n ,(12)
which yields
P[w ↔ none of nodes in V m |S m = S * m ] ≥ 1 -mq n . (13) In addition, P[w ↔ none of nodes in V m |S m = S * m ] = Yn-| 1≤i≤m S * i | Xn Yn Xn ≤ (1 -q n ) Xn -1 | 1≤i≤m S * i | (by [9, Lemma 5.1]) ≤ e -Xn -1 qn| 1≤i≤m S * i |
(by 1 + x ≤ e x for any real x).
(
)14
We will prove
S * m P[S m = S * m ] × P[w ↔ none of nodes in V m |S m = S * m ] n-m-hm (15) ≤ e -mnqn • [1 + o(1)].(16)
From (11) ( 12) and ( 16), we derive
P b = P 1≤i≤m N i ≤ hm -1 = hm-1 t=0 S * m P[S m = S * m ] • (10) ≤ hm-1 t=0 n t • (mq n ) t • (15) ≤ (nq n ) hm e -mnqn • [1 + o(1)] • m hm hm-1 t=0 (mnq n ) t-hm .(17)
As noted in Remark 1, we can introduce an extra condition α n = ±O(ln ln n) = ±o(ln n) in establishing Theorem 1. From α n = ±o(ln n) and (1), we obtain
q n ∼ ln n n .(18)
Applying ( 18) to (17), we obtain (8). Hence, we complete the proof of (8) once showing (16), whose proof is detailed below. From (13) ( 14) and (18), we have
(15)≤ (1 -mq n ) -m-hm × S * m P[S m = S * m ] • e -Xn -1 nqn| 1≤i≤m S * i | ≤ [1 + o(1)] • S * m P[S m = S * m ] • e -Xn -1 nqn| 1≤i≤m S * i | ,(19)
so ( 16) holds once we demonstrate
S * m P[S m = S * m ] • e -Xn -1 nqn| 1≤i≤m S * i | ≤ e -mnqn • [1 + o(1)].(20)
We denote the left hand side of (20) by Z m,n . Dividing S * m into two parts S * m-1 and S * m , we derive
Z m,n = S * m-1 S * m P[(S m-1 = S * m-1 ) ∩ (S m = S * m )] × e -Xn -1 nqn| 1≤i≤m S * i | = S * m-1 P[S m-1 = S * m-1 ] e -Xn -1 nqn| 1≤i≤m-1 S * i | × S * m P[S m = S * m ]e -Xn -1 nqn|S * m \ 1≤i≤m-1 S * i | ,(21) where S
* m P[S m = S * m ]e -Xn -1 nqn|S * m \ 1≤i≤m-1 S * i | ≤ e -nqn S * m P[S m = S * m ]e Xn -1 nqn S * m ∩ m-1 i=1 S * i = e -nqn Xn r=0 P S m ∩ m-1 i=1 S * i = r e Xn -1 nqnr . (22) For r satisfying 0 ≤ r ≤ |S m | = X n and r = |S m | + m-1 i=1 S * i -S m ∪ m-1 i=1 S * i ≥ X n + m-1 i=1 S * i -Y n ,
as given in [11, Eq. ( 36)], we have
P S m ∩ m-1 i=1 S * i = r ≤ 1 r! mX n 2 Y n -X n r .(23)
Applying ( 23) to ( 22), we establish
S * m P[S m = S * m ]e -Xn -1 nqn|S * m \ 1≤i≤m-1 S * i | ≤ e -nqn Xn r=0 1 r! mX n 2 Y n -X n r • e Xn -1 nqnr ≤ e -nqn • e mXn2
Yn -Xn •e Xn -1 nqn .
(24)
From ( 18) and ( 46), it holds that Xn 2 Yn ∼ ln n n , resulting in
mX n 2 Y n -X n ∼ mX n 2 Y n ∼ m ln n n . (25
)
For an arbitrary ǫ > 0, from (18), we obtain q n ≤ (1 + ǫ) ln n n for all n sufficiently large, which with condition X n ≥ 2 yields that for all n sufficiently large,
e Xn -1 nqn ≤ e 1 2 (1+ǫ) ln n = n 1 2 (1+ǫ) .(26)
From ( 25) and ( 26), we get 27) that for arbitrary 0 < c < 1  2 , then for all n sufficiently large, it is clear that
mX n 2 Y n -X n • e Xn -1 nqn ≤ m ln n n • [1 + o(1)] • n 1 2 (1+ǫ) ≤ m ln n • n 1 2 (ǫ-1) • [1 + o(1)]. (27) Since ǫ > 0 is arbitrary, it follows from (
mX n 2 Y n -X n • e Xn -1 nqn ≤ n -c .(28)
Using ( 28) in ( 24), for all n sufficiently large, it follows that
S * m P[S m = S * m ]e -Xn -1 nqn|S * m \ 1≤i≤m-1 S * i | ≤ e -nqn • e n -c .
(29) Substituting (29) into (21), for all n sufficiently large, we obtain
Z m,n ≤ e -nqn •e n -c • S * m-1 P[S m-1 = S * m-1 ]e -Xn -1 nqn| 1≤i≤m-1 S * i | ≤ e -nqn • e n -c • Z m-1,n .(30)
We then evaluate Z 2,n . By (20), it holds that
Z 2,n = S * 1 S * 2 P[(S 1 = S * 1 )∩ (S 2 = S * 2 )]•e -Xn -1 nqn|S * 1 ∪ S * 2 | = S * 1 P[S 1 = S * 1 ] S * 2 P[S 2 = S * 2 ]e -Xn -1 nqn|S * 1 ∪ S * 2 | . (31
)
Setting m = 2 in (29), for all n sufficiently large, we derive
S * 2 P[S 2 = S * 2 ]e -Xn -1 nqn|S * 2 \S * 1 | ≤ e -nqn • e n -c .
Then for all n sufficiently large, it follows that
S * 2 P[S 2 = S * 2 ]e -Xn -1 nqn|S * 1 ∪ S * 2 | = e -nqn S * 2 P[S 2 = S * 2 ]e -Xn -1 nqn|S * 2 \S * 1 | ≤ e -2nqn • e n -c .(32)
From ( 31) and ( 32), for all n sufficiently large, we obtain
Z m,n ≤ e -nqn • e n -c m-2 • Z 2,n ≤ e -nqn • e n -c m-2 • e -2nqn • e n -c ≤ e -mnqn • e (m-1)n -c . (33) Letting n → ∞, we finally establish Z m,n ≤ e -mnqn • [1 + o(1)];
i.e., (20) is proved. Then as explained above, (16) holds; and then (8) follows.
B. The Proof of ( 9) Again let w be an arbitrary node in V m . We have
P 1≤i<j≤m (N ij = ∅) ∩ 1≤i≤m (|N i | = h) |S m = S * m (34) = (n -m)! (h!) m (n -m -hm)! × 1≤i≤m       P   w ↔ v i , but w ↔ none of nodes in V m \ {v i } S m = S * m      h    × P[w ↔ none of nodes in V m |S m = S * m ] n-m-hm(35) and P a
= S * m : 1≤i<j≤m (S * ij =∅) P[S m = S * m ] • (34) ,(36)
where [11,Lemma 3]).
S * ij := S * i ∩ S * j . For i = 1, 2, . . . , m, under S * m : 1≤i<j≤m (S * ij = ∅), we have P[w ↔ v i , but none of nodes in V m \ {v i }|S m = S * m ] ≥ P[w ↔ v i |S m = S * m ] - 1≤j≤m j =i P[w ↔ both v i and v j |S m = S * m ] ≥ q n -(m -1) • 2q n 2 (by
(37) Substituting ( 13) and (37) to (35), and then from (36), we obtain
P a ≥ (n -m -hm) hm (h!) m • [q n -2(m -1)q 2 n ] hm × (1 -mq n ) n-m-hm S * m : 1≤i<j≤m (S * ij =∅) P[S m = S * m ].
Then from (18), it further hold that
P a ≥ n hm (h!) m • (q n ) hm • e -mnqn × [1 -o(1)] • P 1≤i<j≤m (S ij = ∅) .(38)
From ( 14), under S * m : 1≤i<j≤m (S * ij = ∅), it holds that
P[w ↔ none of nodes in V m |S m = S * m ] ≤ e -mqn .
(39) For each i = 1, 2, . . . , m, we have
P[w ↔ v i , but w ↔ none of nodes in V m \ {v i }|S m = S * m ] ≤ P[w ↔ v i |S m = S * m ] = q n .(40)
Substituting ( 40) and ( 39) to (35), and then from (36), we obtain
P a ≤ n hm (h!) m •(q n ) hm •e -mnqn • S * m : 1≤i<j≤m (S * ij =∅) P[S m = S * m ] = n hm (h!) m • (q n ) hm • e -mnqn • P 1≤i<j≤m (S ij = ∅) .
(41) From ( 38) and (41), we obtain
P a ∼ n hm (h!) m • (q n ) hm • e -mnqn • P 1≤i<j≤m (S ij = ∅) .
(42) By the union bound, it is clear that
P 1≤i<j≤m (S ij = ∅) = 1 -P 1≤i<j≤m (S ij = ∅) ≥ 1 - 1≤i<j≤m P[S ij = ∅] = 1 - m 2 q n .(43)
From ( 18) and ( 43), since a probability is at most 1, we get
lim n→∞ P 1≤i<j≤m (S ij = ∅) = 1.(44)
Using ( 44) in (42), we establish (9).
IV. NUMERICAL EXPERIMENTS We present numerical experiments to back up our theoretical results. Figure 1 depicts the probability that graph G(n, X, Y ) is k-connected. We let X vary, with other parameters fixed at n = 3, 000, Y = 30, 000 and k = 3, 7. The empirical probabilities corresponding to the experimental curves are obtained as follows: we count the times of k-connectivity out of 500 independent samples of G(n, X, Y ), and derive the empirical probability through dividing its corresponding count by 500. For the theoretical curves, we first compute α by setting the edge probability 1 -
( Y -X X ) ( Y X )
(viz., (45) in the Appendix) as ln n+(k-1) ln ln n+α n and then use e -e -α (k-1)! as the theoretical value for the probability of k-connectivity. Figure 1 confirms our analytical results as the experimental and theoretical curves are close.
V. RELATED WORK For a random key graph G(n, X n , Y n ), Rybarczyk [6] derives the asymptotically exact probabilities of connectivity and of the property that the minimum node degree is no less than 1, covering a weaker form of the results -the zero-one laws which are also obtained in [2], [10]. As demonstrated in [6], in G(n, X n , Y n ) with X n ≥ 2, Xn 2 Yn = ln n+αn n and lim n→∞ α n = α * , the probability of connectivity and that of the minimum degree being at least 1 both approach to e -e -α * as n → ∞. Rybarczyk [7] implicitly obtains zeroone laws (but not the asymptotically exact probabilities) for k-connectivity and for the property that the minimum degree is at least k. The implicit result is that if X n Θ(n β ) for some  , graph G(n, X n , Y n ) has (resp., does not have) the two properties with probability approaching to 1, given that α n tends to ∞ (resp., -∞) as n → ∞. Our Corollary 1 significantly improves her result [7] in the following two aspects: (i) we cover the wide range of X n ≥ 2 all n sufficiently large, instead of the much stronger condition X n = Ω (ln n) 3 in [7] (note that the analysis under X n = 1 is trivial), and (ii) we establish not only zero-one laws for k-connectivity and the minimum degree, but also the asymptotically exact probabilities. The latter results are not given by Rybarczyk [7]. Recently, we [14] give the asymptotically exact probability of k-connectivity in graph G(n, X n , Y n ) under X n = Ω( √ ln n ) through a rather involved proof. We improve this result to cover X n ≥ 2 through a simpler proof and fill the gap where X n is at least 2, but is not Ω( √ ln n ). This improvement is of technical interest as well as of practical importance since random key graphs have been used in diverse applications including modeling the Eschenauer-Gligor random key predistribution scheme (the most recognized solution to secure communication in wireless sensor networks).
[G(n, X, Y ) is k-connected] k = 3 (Theoretical) k = 3 (Experimental) k = 7 (Theoretical) k = 7 (Experimental)<h2>publication_ref</h2>['b7', 'b7', 'b10', 'b8', 'b5', 'b1', 'b9', 'b5', 'b6', 'b6', 'b6', 'b6', 'b13']<h2>figure_ref</h2>['fig_1', 'fig_1']<h2>table_ref</h2>[]<h2>heading</h2>VI. CONCLUSION<h2>text</h2>In this paper, for a random key graph, we derive the asymptotically exact probabilities for two properties with an arbitrary k: (i) the graph is k-connected; and (ii) each node has at least k neighboring nodes. Numerical experiments are in accordance with our analytical results.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>APPENDIX<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. Establishing Corollary 1 given Theorem 1:<h2>text</h2>As noted in Remark 1, we can use an extra condition β n = ±O(ln ln n) = ±o(ln n) in establishing Corollary 1.
With q n denoting the probability that there exists an edge between two nodes in graph G(n, X n , Y n ), as shown in previous work [2], [6], [10], we have
Further, it holds from [12,Lemma 8] that
From (3) and β n = ±o(ln n), it follows that
Substituting ( 3) and (47) to (46), we further obtain
With α n defined by (1), from (1) and (48), it holds that
Therefore, lim n→∞ α n exists if and only if lim n→∞ β n exists, and lim n→∞ α n = lim n→∞ β n holds. Then Theorem 1 clearly implies Corollary 1.<h2>publication_ref</h2>['b1', 'b5', 'b9', 'b11']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>B. Lemma 4 to confine |α n | as O(ln ln n) in Theorem 1<h2>text</h2>Lemma 4. For a random key graph G(n, X n , Y n ) under X n ≥ 2 and q n = ln n+(k-1) ln ln n+ αn n , the following results hold: (i) If lim n→∞ α n = -∞, there exists graph G(n, X n , Y n ) under X n ≥ 2 and q n = ln n+(k-1) ln ln n+ αn n with lim n→∞ α n = -∞ and α n = -O(ln ln n) ( q n is the edge probability of G(n, X n , Y n )), such that there exists a graph
(ii) If lim n→∞ α n = ∞, there exists graph G(n, X n , Y n ) under X n ≥ 2 and q n = ln n+(k-1) ln ln n+ αn n with lim n→∞ α n = ∞ and α n = O(ln ln n) ( q n is the edge probability of G(n, X n , Y n )), such that there exists a graph coupling under which  
2 As used by Rybarczyk [7], [8], a coupling of two random graphs G 1 and G 2 means a probability space on which random graphs G ′ 1 and G ′ 2 are defined such that G ′ 1 and G ′ 2 have the same distributions as G 1 and G 2 , respectively. If G ′ 1 is a spanning subgraph (resp., spanning supergraph) of G ′ 2 , we say that under the coupling, G 1 is a spanning subgraph (resp., spanning supergraph) of G 2 , which yields that for any monotone increasing property I, the probability of G 1 having I is at most (reap., at least) the probability of G 2 having I.<h2>publication_ref</h2>['b6', 'b7']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>On the complexity of the herding attack and some related attacks on hash functions<h2>journal</h2>Designs, Codes and Cryptography<h2>year</h2>2012<h2>authors</h2>S Blackburn; D Stinson; J Upadhyay<h2>ref_id</h2>b1<h2>title</h2>Connectivity of the uniform random intersection graph<h2>journal</h2>Discrete Mathematics<h2>year</h2>2009-08<h2>authors</h2>S R Blackburn; S Gerke<h2>ref_id</h2>b2<h2>title</h2>On the strength of connectedness of random graphs<h2>journal</h2>Acta Math. Acad. Sci. Hungar<h2>year</h2>1961<h2>authors</h2>P Erdős; A Rényi<h2>ref_id</h2>b3<h2>title</h2>A key-management scheme for distributed sensor networks<h2>journal</h2><h2>year</h2>2002<h2>authors</h2>L Eschenauer; V Gligor<h2>ref_id</h2>b4<h2>title</h2>A lower-bound on the number of rankings required in recommender systems using collaborativ filtering<h2>journal</h2><h2>year</h2>2008<h2>authors</h2>P Marbach<h2>ref_id</h2>b5<h2>title</h2>Diameter, connectivity and phase transition of the uniform random intersection graph<h2>journal</h2>Discrete Mathematics<h2>year</h2>2011<h2>authors</h2>K Rybarczyk<h2>ref_id</h2>b6<h2>title</h2>Sharp threshold functions for the random intersection graph via a coupling method<h2>journal</h2>Electr. Journal of Combinatorics<h2>year</h2>2011<h2>authors</h2>K Rybarczyk<h2>ref_id</h2>b7<h2>title</h2>The coupling method for inhomogeneous random intersection graphs<h2>journal</h2><h2>year</h2>2013-01<h2>authors</h2>K Rybarczyk<h2>ref_id</h2>b8<h2>title</h2>Performance of the Eschenauer-Gligor key distribution scheme under an on/off channel<h2>journal</h2>IEEE Transactions on Information Theory<h2>year</h2>2012-06<h2>authors</h2>O Yagan<h2>ref_id</h2>b9<h2>title</h2>Zero-one laws for connectivity in random key graphs<h2>journal</h2>IEEE Transactions on Information Theory<h2>year</h2>2012-05<h2>authors</h2>O Yagan; A M Makowski<h2>ref_id</h2>b10<h2>title</h2>On asymptotically exact probability of k-connectivity in random key graphs intersecting Erdős-Rényi graphs<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>J Zhao; O Yagan; V Gligor<h2>ref_id</h2>b11<h2>title</h2>k-Connectivity in secure wireless sensor networks with physical link constraints -the on/off channel model<h2>journal</h2><h2>year</h2>2012<h2>authors</h2>J Zhao; O Yagan; V Gligor<h2>ref_id</h2>b12<h2>title</h2>Secure k-connectivity in wireless sensor networks under an on/off channel model<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>J Zhao; O Yagan; V Gligor<h2>ref_id</h2>b13<h2>title</h2>On the strengths of connectivity and robustness in general random intersection graphs<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>J Zhao; O Yagan; V Gligor<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Fig. 1 .1Fig. 1. A plot for the probability of k-connectivity in graph G(n, X, Y ) with k = 3, 7 under n = 3, 000 and Y = 20, 000.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>have the following two observations: i) If node v i has degree h, then |N i | ≤ h, where the equal sign holds if and only if v i is directly connected to none of nodes in V m \ {v i }; i.e., if and only if event<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>1 , S 2 , . . . , S m ). The expression "|S m = S * m " means "given S 1 = S * 1 , S 2 = S * 2 , . . . , S m = S * m ", where S * m = (S * 1 , S * 2 , . . . , S * m ) with S * 1 , S * 2 , . . . , S * m being arbitrary X n -size subsets of the object pool. Note that S * ij<h2>figure_data</h2><h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>q n = ln n + (k -1) ln ln n + α n n ,(1)<h2>formula_coordinates</h2>[1.0, 370.32, 637.07, 196.76, 23.74]<h2>formula_id</h2>formula_1<h2>formula_text</h2>lim n→∞ P G(n, X n , Y n ) is k-connected. = lim n→∞ P [ G(n, X n , Y n ) has a minimum degree at least k. ] =      0, if lim n→∞ α n = -∞, 1, if lim n→∞ α n = ∞, e -e -α * (k-1)! , if lim n→∞ α n = α * ∈ (-∞, ∞).(2)<h2>formula_coordinates</h2>[2.0, 49.32, 74.37, 250.76, 87.12]<h2>formula_id</h2>formula_2<h2>formula_text</h2>Y n = ln n + (k -1) ln ln n + β n n ,(3)<h2>formula_coordinates</h2>[2.0, 103.08, 244.79, 197.0, 24.11]<h2>formula_id</h2>formula_3<h2>formula_text</h2>lim n→∞ P G(n, X n , Y n ) is k-connected. = lim n→∞ P [ G(n, X n , Y n ) has a minimum degree at least k. ] =      0, if lim n→∞ β n = -∞, 1, if lim n→∞ β n = ∞, e -e -β * (k-1)! , if lim n→∞ β n = β * ∈ (-∞, ∞).<h2>formula_coordinates</h2>[2.0, 49.32, 287.13, 248.04, 87.12]<h2>formula_id</h2>formula_4<h2>formula_text</h2>G(n, X n , Y n ) under (1) and X n ≥ 2, it follows that lim n→∞ P G(n, X n , Y n ) has a minimum degree at least k, but is not k-connected. =0.(4)<h2>formula_coordinates</h2>[2.0, 45.0, 568.55, 255.08, 48.73]<h2>formula_id</h2>formula_5<h2>formula_text</h2>lim n→∞ P [ G(n, X n , Y n ) has a minimum degree at least k. ]<h2>formula_coordinates</h2>[2.0, 55.08, 652.53, 236.52, 14.77]<h2>formula_id</h2>formula_6<h2>formula_text</h2>P[Nodes v 1 , v 2 , . . . , v m have degree h] ∼ (h!) -m (nq n ) hm e -mnqn .(5)<h2>formula_coordinates</h2>[2.0, 360.0, 85.77, 207.08, 32.52]<h2>formula_id</h2>formula_7<h2>formula_text</h2>(S ij = ∅) happens. ii) If |N i | ≤ h for any i = 1, 2, . . . , m, then 1≤i≤m N i ≤ 1≤i≤m N i ≤ hm,(6)<h2>formula_coordinates</h2>[2.0, 317.16, 284.27, 249.92, 56.35]<h2>formula_id</h2>formula_8<h2>formula_text</h2>1≤i<j≤m (N ij = ∅) ∩ 1≤i≤m (|N i | = h) .(7)<h2>formula_coordinates</h2>[2.0, 355.44, 362.17, 211.64, 22.85]<h2>formula_id</h2>formula_9<h2>formula_text</h2>P a ≤ P[Nodes v 1 , v 2 , . . . , v m have degree h] ≤ P a + P b ,<h2>formula_coordinates</h2>[2.0, 322.68, 513.95, 233.52, 17.26]<h2>formula_id</h2>formula_10<h2>formula_text</h2>P a = P 1≤i<j≤m (S ij = ∅) ∩ 1≤i<j≤m (N ij = ∅) ∩ 1≤i≤m (|N i | = h) ,<h2>formula_coordinates</h2>[2.0, 322.56, 545.53, 233.88, 54.53]<h2>formula_id</h2>formula_11<h2>formula_text</h2>P b = P 1≤i≤m N i ≤ hm -1 .<h2>formula_coordinates</h2>[2.0, 371.64, 621.11, 135.6, 21.31]<h2>formula_id</h2>formula_12<h2>formula_text</h2>P b = o (nq n ) hm e -mnqn .(8)<h2>formula_coordinates</h2>[2.0, 383.76, 659.79, 183.32, 12.43]<h2>formula_id</h2>formula_13<h2>formula_text</h2>P a ∼ (h!) -m (nq n ) hm e -mnqn • [1 + o(1)],(9)<h2>formula_coordinates</h2>[2.0, 353.28, 690.99, 213.8, 19.13]<h2>formula_id</h2>formula_14<h2>formula_text</h2>:= S * i ∩ S * j .<h2>formula_coordinates</h2>[3.0, 214.44, 80.43, 49.89, 18.53]<h2>formula_id</h2>formula_15<h2>formula_text</h2>P 1≤i≤m N i = t|S m = S * m (10) = (n -m)! t!(n -m -t)! × P[w ↔ at least one of nodes in V m |S m = S * m ] t × P[w ↔ none of nodes in V m |S m = S * m ] n-m-t . (11<h2>formula_coordinates</h2>[3.0, 54.6, 177.63, 245.6, 90.53]<h2>formula_id</h2>formula_16<h2>formula_text</h2>)<h2>formula_coordinates</h2>[3.0, 296.01, 251.8, 4.19, 8.97]<h2>formula_id</h2>formula_17<h2>formula_text</h2>P[w ↔ at least one of nodes in V m |S m = S * m ] ≤ 1≤i≤m P[w ↔ v i |S m = S * m ] = mq n ,(12)<h2>formula_coordinates</h2>[3.0, 76.44, 282.99, 223.76, 40.11]<h2>formula_id</h2>formula_18<h2>formula_text</h2>P[w ↔ none of nodes in V m |S m = S * m ] ≥ 1 -mq n . (13) In addition, P[w ↔ none of nodes in V m |S m = S * m ] = Yn-| 1≤i≤m S * i | Xn Yn Xn ≤ (1 -q n ) Xn -1 | 1≤i≤m S * i | (by [9, Lemma 5.1]) ≤ e -Xn -1 qn| 1≤i≤m S * i |<h2>formula_coordinates</h2>[3.0, 45.0, 342.63, 255.2, 121.85]<h2>formula_id</h2>formula_19<h2>formula_text</h2>)14<h2>formula_coordinates</h2>[3.0, 287.63, 459.88, 12.57, 8.97]<h2>formula_id</h2>formula_20<h2>formula_text</h2>S * m P[S m = S * m ] × P[w ↔ none of nodes in V m |S m = S * m ] n-m-hm (15) ≤ e -mnqn • [1 + o(1)].(16)<h2>formula_coordinates</h2>[3.0, 55.2, 491.67, 245.0, 77.93]<h2>formula_id</h2>formula_21<h2>formula_text</h2>P b = P 1≤i≤m N i ≤ hm -1 = hm-1 t=0 S * m P[S m = S * m ] • (10) ≤ hm-1 t=0 n t • (mq n ) t • (15) ≤ (nq n ) hm e -mnqn • [1 + o(1)] • m hm hm-1 t=0 (mnq n ) t-hm .(17)<h2>formula_coordinates</h2>[3.0, 48.96, 590.27, 251.24, 136.33]<h2>formula_id</h2>formula_22<h2>formula_text</h2>q n ∼ ln n n .(18)<h2>formula_coordinates</h2>[3.0, 417.36, 95.13, 149.84, 23.76]<h2>formula_id</h2>formula_23<h2>formula_text</h2>(15)≤ (1 -mq n ) -m-hm × S * m P[S m = S * m ] • e -Xn -1 nqn| 1≤i≤m S * i | ≤ [1 + o(1)] • S * m P[S m = S * m ] • e -Xn -1 nqn| 1≤i≤m S * i | ,(19)<h2>formula_coordinates</h2>[3.0, 313.32, 156.27, 253.88, 82.17]<h2>formula_id</h2>formula_24<h2>formula_text</h2>S * m P[S m = S * m ] • e -Xn -1 nqn| 1≤i≤m S * i | ≤ e -mnqn • [1 + o(1)].(20)<h2>formula_coordinates</h2>[3.0, 352.2, 256.46, 215.0, 46.87]<h2>formula_id</h2>formula_25<h2>formula_text</h2>Z m,n = S * m-1 S * m P[(S m-1 = S * m-1 ) ∩ (S m = S * m )] × e -Xn -1 nqn| 1≤i≤m S * i | = S * m-1 P[S m-1 = S * m-1 ] e -Xn -1 nqn| 1≤i≤m-1 S * i | × S * m P[S m = S * m ]e -Xn -1 nqn|S * m \ 1≤i≤m-1 S * i | ,(21) where S<h2>formula_coordinates</h2>[3.0, 312.0, 330.15, 255.2, 173.95]<h2>formula_id</h2>formula_26<h2>formula_text</h2>* m P[S m = S * m ]e -Xn -1 nqn|S * m \ 1≤i≤m-1 S * i | ≤ e -nqn S * m P[S m = S * m ]e Xn -1 nqn S * m ∩ m-1 i=1 S * i = e -nqn Xn r=0 P S m ∩ m-1 i=1 S * i = r e Xn -1 nqnr . (22) For r satisfying 0 ≤ r ≤ |S m | = X n and r = |S m | + m-1 i=1 S * i -S m ∪ m-1 i=1 S * i ≥ X n + m-1 i=1 S * i -Y n ,<h2>formula_coordinates</h2>[3.0, 312.0, 479.78, 255.2, 201.32]<h2>formula_id</h2>formula_27<h2>formula_text</h2>P S m ∩ m-1 i=1 S * i = r ≤ 1 r! mX n 2 Y n -X n r .(23)<h2>formula_coordinates</h2>[3.0, 332.16, 697.8, 235.04, 33.8]<h2>formula_id</h2>formula_28<h2>formula_text</h2>S * m P[S m = S * m ]e -Xn -1 nqn|S * m \ 1≤i≤m-1 S * i | ≤ e -nqn Xn r=0 1 r! mX n 2 Y n -X n r • e Xn -1 nqnr ≤ e -nqn • e mXn2<h2>formula_coordinates</h2>[4.0, 82.56, 72.26, 182.24, 85.27]<h2>formula_id</h2>formula_29<h2>formula_text</h2>mX n 2 Y n -X n ∼ mX n 2 Y n ∼ m ln n n . (25<h2>formula_coordinates</h2>[4.0, 110.28, 174.29, 185.73, 32.67]<h2>formula_id</h2>formula_30<h2>formula_text</h2>)<h2>formula_coordinates</h2>[4.0, 296.01, 183.76, 4.19, 8.97]<h2>formula_id</h2>formula_31<h2>formula_text</h2>e Xn -1 nqn ≤ e 1 2 (1+ǫ) ln n = n 1 2 (1+ǫ) .(26)<h2>formula_coordinates</h2>[4.0, 99.84, 241.7, 200.36, 20.23]<h2>formula_id</h2>formula_32<h2>formula_text</h2>mX n 2 Y n -X n • e Xn -1 nqn ≤ m ln n n • [1 + o(1)] • n 1 2 (1+ǫ) ≤ m ln n • n 1 2 (ǫ-1) • [1 + o(1)]. (27) Since ǫ > 0 is arbitrary, it follows from (<h2>formula_coordinates</h2>[4.0, 45.0, 274.25, 255.2, 56.2]<h2>formula_id</h2>formula_33<h2>formula_text</h2>mX n 2 Y n -X n • e Xn -1 nqn ≤ n -c .(28)<h2>formula_coordinates</h2>[4.0, 114.6, 346.85, 185.6, 32.67]<h2>formula_id</h2>formula_34<h2>formula_text</h2>S * m P[S m = S * m ]e -Xn -1 nqn|S * m \ 1≤i≤m-1 S * i | ≤ e -nqn • e n -c .<h2>formula_coordinates</h2>[4.0, 48.36, 388.82, 249.72, 25.96]<h2>formula_id</h2>formula_35<h2>formula_text</h2>Z m,n ≤ e -nqn •e n -c • S * m-1 P[S m-1 = S * m-1 ]e -Xn -1 nqn| 1≤i≤m-1 S * i | ≤ e -nqn • e n -c • Z m-1,n .(30)<h2>formula_coordinates</h2>[4.0, 45.0, 446.25, 255.2, 63.72]<h2>formula_id</h2>formula_36<h2>formula_text</h2>Z 2,n = S * 1 S * 2 P[(S 1 = S * 1 )∩ (S 2 = S * 2 )]•e -Xn -1 nqn|S * 1 ∪ S * 2 | = S * 1 P[S 1 = S * 1 ] S * 2 P[S 2 = S * 2 ]e -Xn -1 nqn|S * 1 ∪ S * 2 | . (31<h2>formula_coordinates</h2>[4.0, 50.64, 525.81, 245.37, 69.92]<h2>formula_id</h2>formula_37<h2>formula_text</h2>)<h2>formula_coordinates</h2>[4.0, 296.01, 573.52, 4.19, 8.97]<h2>formula_id</h2>formula_38<h2>formula_text</h2>S * 2 P[S 2 = S * 2 ]e -Xn -1 nqn|S * 2 \S * 1 | ≤ e -nqn • e n -c .<h2>formula_coordinates</h2>[4.0, 73.32, 612.86, 200.88, 26.07]<h2>formula_id</h2>formula_39<h2>formula_text</h2>S * 2 P[S 2 = S * 2 ]e -Xn -1 nqn|S * 1 ∪ S * 2 | = e -nqn S * 2 P[S 2 = S * 2 ]e -Xn -1 nqn|S * 2 \S * 1 | ≤ e -2nqn • e n -c .(32)<h2>formula_coordinates</h2>[4.0, 87.36, 654.95, 212.84, 78.46]<h2>formula_id</h2>formula_40<h2>formula_text</h2>Z m,n ≤ e -nqn • e n -c m-2 • Z 2,n ≤ e -nqn • e n -c m-2 • e -2nqn • e n -c ≤ e -mnqn • e (m-1)n -c . (33) Letting n → ∞, we finally establish Z m,n ≤ e -mnqn • [1 + o(1)];<h2>formula_coordinates</h2>[4.0, 312.0, 70.44, 255.2, 83.6]<h2>formula_id</h2>formula_41<h2>formula_text</h2>P 1≤i<j≤m (N ij = ∅) ∩ 1≤i≤m (|N i | = h) |S m = S * m (34) = (n -m)! (h!) m (n -m -hm)! × 1≤i≤m       P   w ↔ v i , but w ↔ none of nodes in V m \ {v i } S m = S * m      h    × P[w ↔ none of nodes in V m |S m = S * m ] n-m-hm(35) and P a<h2>formula_coordinates</h2>[4.0, 312.0, 207.01, 255.2, 175.53]<h2>formula_id</h2>formula_42<h2>formula_text</h2>= S * m : 1≤i<j≤m (S * ij =∅) P[S m = S * m ] • (34) ,(36)<h2>formula_coordinates</h2>[4.0, 345.48, 370.11, 221.72, 25.34]<h2>formula_id</h2>formula_43<h2>formula_text</h2>S * ij := S * i ∩ S * j . For i = 1, 2, . . . , m, under S * m : 1≤i<j≤m (S * ij = ∅), we have P[w ↔ v i , but none of nodes in V m \ {v i }|S m = S * m ] ≥ P[w ↔ v i |S m = S * m ] - 1≤j≤m j =i P[w ↔ both v i and v j |S m = S * m ] ≥ q n -(m -1) • 2q n 2 (by<h2>formula_coordinates</h2>[4.0, 312.0, 396.99, 254.98, 121.85]<h2>formula_id</h2>formula_44<h2>formula_text</h2>P a ≥ (n -m -hm) hm (h!) m • [q n -2(m -1)q 2 n ] hm × (1 -mq n ) n-m-hm S * m : 1≤i<j≤m (S * ij =∅) P[S m = S * m ].<h2>formula_coordinates</h2>[4.0, 313.56, 528.72, 251.88, 53.92]<h2>formula_id</h2>formula_45<h2>formula_text</h2>P a ≥ n hm (h!) m • (q n ) hm • e -mnqn × [1 -o(1)] • P 1≤i<j≤m (S ij = ∅) .(38)<h2>formula_coordinates</h2>[4.0, 350.88, 597.6, 216.32, 55.61]<h2>formula_id</h2>formula_46<h2>formula_text</h2>P[w ↔ none of nodes in V m |S m = S * m ] ≤ e -mqn .<h2>formula_coordinates</h2>[4.0, 327.24, 670.59, 207.84, 19.01]<h2>formula_id</h2>formula_47<h2>formula_text</h2>P[w ↔ v i , but w ↔ none of nodes in V m \ {v i }|S m = S * m ] ≤ P[w ↔ v i |S m = S * m ] = q n .(40)<h2>formula_coordinates</h2>[4.0, 315.84, 699.51, 251.36, 34.49]<h2>formula_id</h2>formula_48<h2>formula_text</h2>P a ≤ n hm (h!) m •(q n ) hm •e -mnqn • S * m : 1≤i<j≤m (S * ij =∅) P[S m = S * m ] = n hm (h!) m • (q n ) hm • e -mnqn • P 1≤i<j≤m (S ij = ∅) .<h2>formula_coordinates</h2>[5.0, 47.52, 71.76, 249.96, 63.77]<h2>formula_id</h2>formula_49<h2>formula_text</h2>P a ∼ n hm (h!) m • (q n ) hm • e -mnqn • P 1≤i<j≤m (S ij = ∅) .<h2>formula_coordinates</h2>[5.0, 57.24, 162.72, 230.52, 28.97]<h2>formula_id</h2>formula_50<h2>formula_text</h2>P 1≤i<j≤m (S ij = ∅) = 1 -P 1≤i<j≤m (S ij = ∅) ≥ 1 - 1≤i<j≤m P[S ij = ∅] = 1 - m 2 q n .(43)<h2>formula_coordinates</h2>[5.0, 78.6, 226.07, 221.6, 84.79]<h2>formula_id</h2>formula_51<h2>formula_text</h2>lim n→∞ P 1≤i<j≤m (S ij = ∅) = 1.(44)<h2>formula_coordinates</h2>[5.0, 103.92, 333.59, 196.28, 21.43]<h2>formula_id</h2>formula_52<h2>formula_text</h2>( Y -X X ) ( Y X )<h2>formula_coordinates</h2>[5.0, 137.52, 495.98, 24.51, 21.52]<h2>formula_id</h2>formula_53<h2>formula_text</h2>[G(n, X, Y ) is k-connected] k = 3 (Theoretical) k = 3 (Experimental) k = 7 (Theoretical) k = 7 (Experimental)<h2>formula_coordinates</h2>[5.0, 362.78, 70.59, 168.8, 99.04]<h1>doi</h1><h1>title</h1>k-Pareto Optimality-Based Sorting with Maximization of Choice<h1>authors</h1>Jean Ruppert; Marharyta Aleksandrova; Thomas Engel<h1>pub_date</h1><h1>abstract</h1>Topological sorting 1 is an important technique in numerous practical applications, such as information retrieval, recommender systems, optimization, etc. In this paper, we introduce a problem of generalized topological sorting with maximization of choice, that is, of choosing a subset of items of a predefined size that contains the maximum number of equally preferable options (items) with respect to a dominance relation. We formulate this problem in a very abstract form and prove that sorting by k-Pareto optimality yields a valid solution. Next, we show that the proposed theory can be useful in practice. We apply it during the selection step of genetic optimization and demonstrate that the resulting algorithm outperforms existing state-of-the-art approaches such as NSGA-II and NSGA-III. We also demonstrate that the provided general formulation allows discovering interesting relationships and applying the developed theory to different applications. 1 Topological sorting [E. Knuth, 1997]  here means the process of sorting a set of items with respect to a preference or dominance relation. We use the terms preference and dominance interchangeably. 2 We use the terms item and element interchangeably.<h1>sections</h1><h2>heading</h2>Introduction<h2>text</h2>In the modern era of information overload, the task of choosing a subset of the most useful items is extremely important. Various tools were developed with the aim to assist a user with this task, for example, text search engines [Croft et al., 2010] and recommender systems [Resnick and Varian, 1997]. In most of the cases, such systems suggest to the user a small set of elements 2 . Thereby, if the number of equally preferable options is large, a heuristic is used to discard a fraction of them. However, in some applications the user might be willing to analyze all equally preferable options with the aim to choose the best one. This can happen, for example, in the case of choosing a habitation.
A similar problem of choosing a subset of most preferable elements also arises as an important step when solving various practical tasks. A straightforward example would be the selection step in genetic optimization algorithms [Mitchell, 1998]. At this step, a subset of the current population is chosen to advance to the next generation. Having the chosen subset made up of elements with large fitness values guides the evolution process in the desired direction. At the same time, selecting a subset with the largest variety of genes ensures variability of characteristics and allows faster exploration of the search space.
These examples bring us to the problem of generalizedfoot_0 topological sorting with choice maximisation which we also refer to as maximum choice problem. This problem aims to choose a subset of a predefined maximum size, consisting of most preferable items and containing the largest number of equally preferred elementsfoot_1 . To the best of our knowledge, this problems has not yet been studied in the literature. In this text, we propose a theoretical solution to the maximum choice problem and demonstrate how both the problem and its solution can be applied in practice.
The contributions of this work are the following:
1. We formulate the maximum choice problem in a broad sense for arbitrary elements, preference relations R, and measures µ indicating the set sizes, see Section 2.
2. We propose a solution based on the concept of k-Pareto optimality, whose definition relies on the relation R, see Section 3.
3. We further investigate the proposed solution from a theoretical point of view and discover interesting characteristics, such as the relationship between k-Pareto optimal elements and the arc of hyperbola, see Section 4.
4. Finally, we demonstrate the applicability of our approach to real-world problems by considering genetic optimization, see Section 5.<h2>publication_ref</h2>['b1', 'b39']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Sorting with Choice Maximization<h2>text</h2>To formally define the problem of maximization of choice, we introduce several definitions in Sections 2.1 and 2.2. The resulting formalization of this problem is abstract and quite general. However, this generality allows discovering novel connections and applying the developed theory to numerous practical problems. We also illustrate the defined concepts with an example in Section 2.3.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Definitions<h2>text</h2>We consider a set X with a binary relation R. Intuitively xRy means that x is preferable to y. The case xRy and not yRx means that x is strictly preferable to y. This situation is denoted by xR * y. We also consider a positive and σ-finite measure µ [Halmos, 2013] defined on X. Thus, we have a measure space (X, Σ, µ), where Σ is a set of subsets of X, and µ intuitively indicates the size of these subsets. To ensure measurability, throughout this text the characteristic function 1 R of the relation R is assumed to be sufficiently regular 5 . The µ can be defined in different ways. Important examples are the counting measure and probability measures P . Depending on the definition of µ, it can indicate the following characteristics of the elements in X: how many?, how likely?, how important?, or what volume?
To illustrate these definitions, we consider the following example. Let X be a set of possible habitations of which the user has to choose the best according to his preferences encoded by the relation R. In such a situation, the relation R can be multidimensional. Let us assume, for simplicity, that an optimal habitation for the user is close to a given location, for example, his workplace (relation R l ), is situated in a district with a smaller population size (relation R p ), and is close to a river (relation R r ). Thus, the user's preferences can be represented by the preorder relation R = R l &R p &R r . In our example, all available habitations from X can be mapped onto points in a 3dimensional space of Proximity to the location × Population × River. The fact that R is a preorder rela-5 1R is equal to 1 if xRy and is equal to 0 otherwise.
tion means that some elements of X can be comparable, while others not. For example, the habitation x with coordinates (50, 100, T rue) is strictly preferable to y with coordinates (60, 100, T rue), that is xR * y.
At the same time, the habitation z with coordinates (40, 100, F alse) is incomparable with x. Indeed, z is better with respect to R l , it is situated closer to the required location, but x is better with respect to R r , as the latter is situated near a river.
Having the task to find a subset of X that is 'best' according to R, a rational solution can be formulated with the following recursive expression: if an element x is selected, then all elements that are strictly preferable to x should be also selected. In our example, this translates into the task of finding a subset of habitations S R that might be suitable for the user. Naturally, if y ∈ S R , then x ∈ S R as the latter corresponds better to the preferences of the user defined by the relation R. We formalize this rationality condition by defining selections as follows.
Definition 1. A selection S is a subset of X such that x ∈ S and yR * x implies y ∈ S. The set of all selections in Σ is denoted by Sfoot_2 .<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>The Maximum Choice Problem<h2>text</h2>As discussed in Section 1, in practical applications when selecting a subset of X one might want not only to respect the above rationality constraint, but also to maximize the number of incomparable pairs. The latter condition is equivalent to the maximization of the diversity of the selected subset, or the maximization of the provided choice. In our example with habitations, if both x and z are presented to the user, then he can choose an appropriate habitation by himselffoot_3 .
In terms of our notations, this will be translated into the condition of selecting as many pairs x, y such that neither x is strictly preferable to y (¬xR * y) nor y is strictly preferable to x (¬yR * x). This means that there is freedom of choice between x and y (xRy = yRx). This motivates the following quantitative definition of choice for measurable subsets of X.
Definition 2. Choice offered by a set A is the number
cho(A) = (µ × µ)({(x, y) ∈ A 2 |xRy = yRx}).
The choice offered by a measurable set A essentially measures how many pairs of items offering choice can be extracted from A. Additionally, if one wants to restrict the size of the selected subset, in our example, to present to the user a small set of suitable habitations with µ(S R ) ≤ m, then this leads us to the definition of the maximum choice problem: Maximum Choice Problem. For a given m find all selections T such that cho(T ) = max S∈S,µ(S)≤m cho(S).
Any such selection T will be said to offer maximum choice for m.
In practical applications, it might be more insightful to consider the concept of diversity that is functionally related to the concept of choice. Definition 3. For any measurable set A, the diversity of A is the ratio div(A) = cho(A)/µ(A) 2 .
Thus div(A) is the likelihood that there is choice between the two randomly chosen elements. The main ingredient of our solution to the maximum choice problem is the following concept. Definition 4. The k-Pareto optimality of an element x ∈ X is the measure of the subset of X containing all elements strictly preferable to x:
po(x) = µ({y|yR * x}).
If µ is the probability measure, the k-Pareto optimality of an element x, po(x), is the likelihood an element drawn at random from X is strictly preferable to x.
Finally, we introduce the sets of at least k-Pareto optimal elements. Such sets consist of all elements in X with po ≤ k. In Section 3 we show that this concept yields a solution for the maximum choice problem. Definition 5. The at least k-Pareto optimal elements T k form the measurable set defined as follows:
T k = {x ∈ X| po(x) ≤ k} 8 .<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Example<h2>text</h2>In this subsection, we discuss an illustrative example to demonstrate the concepts defined in Sections 2.1 and 2.2. Let us consider a finite subset X of R 2 , counting measure µ, and the relation R defined as follows:
(x 1 , x 2 )R (y 1 , y 2 ) iff x 1 ≤ y 1 and x 2 ≤ y 2 , see Fig. 1.
In economics, x is Pareto optimal if there is no y in X such that xR * y. In our language, this means that po(x) = 0. Thus, k-Pareto optimality indicates how much an element is away from being Pareto optimal.
Let X be comprised of six points presented in Fig. 2. As we are considering the counting measure, µ(X) = 6.  Sorting the set X by k-Pareto optimality of its elements will produce the following result: ({A, B, C}, {E}, {D, F }). This sorting is different from sorting by Pareto fronts. The latter approach is widely used in practice and is the basis of all Pareto dominance-based genetic optimization algorithms [Li et al., 2015]. Sorting by Pareto fronts is done in the following way. First, the first Pareto front, which is the set of non-dominated points, is identified. Next, the points from this front are removed from the consideration and the process is repeated until all points are assigned to a front. Sorting the points from Fig. 2 
k that µ(T k ) = m.
We prove the above stated theorem in several steps. First, we show that for selections the computation of choice can be simplified. It only requires to compute a simple integral instead of a double integral.
Integral Formula. If S is a measurable selection and µ(S) < +∞, then
cho(S) = S (µ(S) -2 po(x))dµ(x).(1)
Proof. The fact that set S is a selection means that ∀y ∈ S : {x ∈ S|xR * y} = {x ∈ X|xR * y}. That is, any element x from X strictly preferable to any element y in S, also belong to S (x ∈ S). Using the definition of choice from Def. 2 and µ(S) < +∞ we obtain
cho(S) = µ(S) 2 -2(µ × µ)({(x, y) ∈ S 2 |xR * y}) = µ(S) 2 -2(µ × µ)({(x, y) ∈ S × X|yR * x}).
Fubini's theorem [Halmos, 2013] indicates that
(µ×µ)({(x, y) ∈ S × X|yR * x} = = S X 1 R * d(µ(y)) dµ(x) = S po(x)dµ(x),
where 1 R * is the characteristic function of R *foot_5 .
Finally, the integral formula results from the fact that µ(S) 2 -2 S po(x)dµ(x) = S (µ(S) -2 po(x))dµ(x).
Let's now consider the function c defined on Σ for any A of finite measure by
c(A) = A (µ(A) -2 po(x))dµ(x).
The integral formula defined in Eq. ( 1) means that for any selection S, we have c(S) = cho(S). The second step of our proof of Theorem 1 is to show that T k is the largest measurable set that maximizes c for its respective measure. More precisely, we will prove the following lemma. Lemma 1. For any k such as µ(T k ) < +∞ we have
c(T k ) = max A∈Σ,µ(A)≤µ(T k ) c(A). Moreover, if µ(A) ≤ µ(T k ) and c(A) = c(T k ), then A ⊆ T k almost-everywhere.
The context of this lemma is very similar to the knapsack problem [Martello, 1990]. In this problem, one needs to find a subset A of a finite set of items {x 1 , ..., x n } maximizing the total value Σ xi∈A v(x i ) under the constraint that the total weight Σ xi∈A w(x i ) of A does not exceed a predefined maximum weight w * .
In Lemma 1, the total value is c(A), the ratio of an element's value to its weight becomes µ(A)-2 po(x), and the weight constraint is expressed as µ(A) ≤ µ(T k ).
The solutions given by the lemma correspond to those yielded for the knapsack problem by George Dantzig's greedy approximation algorithm [Dantzig, 1957]. This algorithm consists of ordering elements by decreasing value-to-weight ratio and then taking the N first elements. N is chosen in such a way, that taking one more element would cause excessive weight. The process of proving Lemma 1 is similar to proving that George Dantzig's solutions are optimal for their respective weights.
Proof of Lemma 1. Let's consider T k such that µ(T k ) < +∞. To prove c(T k ) is the maximum, we need to show for any
A ∈ Σ, that c(A) ≤ c(T k ) if µ(A) ≤ µ(T k ). As A = (A ∩ T k ) ∪ (A \ T k ) and T k = (A ∩ T k ) ∪ (T k \ A), requiring c(A) ≤ c(T k ) is equivalent to requiring A\T k (µ(A)-2 po(x))dµ(x) ≤ T k \A (µ(T k )-2 po(x))dµ(x).
(2) By definition of T k , we have that po
(x) > k for x ∈ A \ T k , while po(x) ≤ k for x ∈ T k \ A. Therefore, A\T k (µ(A) -2 po(x))dµ(x) ≤ µ(A \ T k )(µ(A) -2k),
(3) and
T k \A (µ(T k ) -2 po(x))dµ(x) ≥ µ(T k \ A)(µ(T k ) -2k). (4) The constraint µ(A) ≤ µ(T k ) means that µ(A \ T k ) ≤ µ(T k \ A). Thus, we have µ(A \ T k )(µ(A) -2k) ≤ µ(T k \ A)(µ(T k ) -2k). (5)
Finally, combining Eq. ( 5) with Eq. (3) and Eq. ( 4) guarantees the inequality in Eq. ( 2) under the constraint µ(A) ≤ µ(T k ).
Let us now consider A ∈ Σ such that µ(A) ≤ µ(T k ) and c(A) = c(T k ). We now proceed to show that A ⊆ T k almost everywhere. If c(A) = c(T k ), the inequality in Eq. ( 2) must be an equality. Under the assumption µ(A) ≤ µ(T k ), we again have the inequalities in Eq. (3), Eq. ( 4) and Eq. ( 5), which must be equalities if Eq. ( 2) is an equality. However, because po(x) > k for x ∈ A \ T k , the inequality in Eq. ( 3) can only become an equality if µ(A \ T k ) = 0. Now everything is in place to prove the theorem.
Proof of Theorem 1. Let us first prove that the at least k-Pareto optimal elements T k offer maximum choice. Lemma 1 says that on Σ the set T k maximises c for its respective measure. We assumed T k is a selection. At the same time, for any selection c = cho. It means that T k offers maximum choice for its respective measure. Moreover, from Lemma 1 and from S ⊆ Σ directly results that if a selection A offers maximum choice for µ(T k ), then A ⊆ T k almost everywhere.
The theorem does not guarantee uniqueness. For example, in the case of the relation ≤ on R and the Lebesgue measure, selections are all left-unbounded intervals and the choice of any selection is 0. However, if for any selection
A ⊂ T k & µ(A) < µ(T k ) =⇒ cho(A) < cho(T k ), (6)
then T k is a unique maximum. This is a direct consequence from the fact that T k contains any other selection offering maximum choice for µ(T k ) < +∞. Also, the proof of the theorem indicates that transitivity is not necessary. The set T k is only required to be a selection. This is the case for Lebesgue area measure and the non-transitive relation R defined on the unit square by (
x 1 , x 2 )R (y 1 , y 2 ) iff y1 2 ≤ x 1 ≤ y 1 and y2 2 ≤ x 2 ≤ y 2 . The sets T k fulfill the requirement of be- ing selections if R * satisfies the weakened transitivity condition xR * y =⇒ po(x) ≤ po(y).
It is possible to prove that for the counting measure, finite sets, and partial order relations, all selections offering maximum choice can be obtained when sorting by k-Pareto optimality and taking the first l elements. However, for discrete measures with nonconstant weights the above construction might not yield all selections offering maximum choice. A relative example is given in Fig. 3. The partial order relation represented by its Hasse diagram. [Davey and Priestley, 2002]. The set S is not of the form with A ⊆ T k \ T * k . Nevertheless, it offers maximum choice for m = 3.
T * k ∪ A • (1, 1) • (1, 1) • (1, 0) • (4, 0) S<h2>publication_ref</h2>['b29', 'b5']<h2>figure_ref</h2>['fig_1', 'fig_1', 'fig_2']<h2>table_ref</h2>[]<h2>heading</h2>Further Theoretical Explorations<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Efficient Computation of Choice<h2>text</h2>Computation of choice may be simplified by performing the change of variable y = po(x) in the integral formula (1).
Proposition 1. For any selection of at least k-Pareto optimal elements T k such that µ(T k ) < +∞ cho(T k ) = µ(T k ) 2 -2 [0,k] xd(po * µ)(x),(7)
where po * µ is the image measure defined by
(po * µ)([a, b]) = µ(po -1 ([a, b])).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Characteristics of Random Vectors<h2>text</h2>Let us study a probability space (Ω, Σ, P ). We consider two random variables X 1 and X 2 , as well as the partial order relation R Ω defined on Ω as follows:
ω x R Ω ω y iff X 1 (ω x ) ≤ X 1 (ω y ) and X 2 (ω x ) ≤ X 2 (ω y ).
For convenience, we use the following notations interchangeably: (X 1 (ω x ), X 2 (ω x )), (x 1 , x 2 ) or simply x.
In this case, the selections are the sets situated below any decreasing curve, like for example the hyperbola x 1 x 2 = 1/5 and the curve defined by max(x 1 , x 2 ) = 2/5 in Fig. 4. The k-Pareto optimality po((x 1 , x 2 )) is the joint cumulative probability distribution function F (x 1 , x 2 ). Finally, the at least k-Pareto optimal elements are situated below the curve F (x 1 , x 2 ) = k.
Having choice between x with coordinates (x 1 , x 2 ) and y with coordinates (y 1 , y 2 ) means the rectangles ((0, 0), (x 1 , 0), x, (0, x 2 )) and ((0, 0), (y 1 , 0), y, (0, y 2 )) are not nested, as depicted in Fig. 4a.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_3', 'fig_3']<h2>table_ref</h2>[]<h2>heading</h2>Continuous Independent Variables<h2>text</h2>If X 1 and X 2 are continuous, then the function po can be considered to be a linear extension of R Ω . Indeed, if x and y are chosen independently and at random
x y x 1 x 2 = 1 5 (0, 0) (1, 1) (a)
x and y offer choice.
x y from Ω, then P (po(x) = po(y)) = 0. On the other hand, it can be shown that div(Ω) = 1/2. This means that in half of the cases the relation R Ω cannot tell which element is preferable out of two elements extracted independently and at random from Ω.
T max 2 5 (0, 0) (1, 1) (b) y is preferable to x.
Moreover if X 1 and X 2 are independent, then the condition for uniqueness from Eq. ( 6) holds. In the special case where X 1 and X 2 are uniformly distributed on the interval [0, 1], the joint cumulative probability distribution function takes form of F (x 1 , x 2 ) = x 1 x 2 and the at least k-Pareto optimal elements are situated below a hyperbola x 1 x 2 = k. Combining this fact with Corollary 1 yields a surprising characterization of hyperbola, see Fig. 4 and the statement below.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_3']<h2>table_ref</h2>[]<h2>heading</h2>Characterization of Hyperbolas.<h2>text</h2>Out of all descending functions f from [0, 1] to [0, 1] delimiting an area 1 0 f (x)dx = c, the arc of hyperbola is the one offering the highest likelihood the rectangles ((0, 0), (x 1 , 0), x, (0, x 2 )) and ((0, 0), (y 1 , 0), y, (0, y 2 )) are not nested for two points x and y being drawn independently and at random from the delimited area.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Independence on Marginal Distribution<h2>text</h2>Sklar's theorem [Sklar, 1959, Durante et al., 2013] states that the cumulative distribution function F (x 1 , x 2 ) can be represented as C(F 1 (x 1 ), F 2 (x 2 )) for a copula C. Marginas of X 1 and X 2 are fully described by the marginal cumulative probability distributions F 1 and F 2 , whereas the copula describes the dependence structure between X 1 and X 2 . The copula can be considered as a joint cumulative distribution function having two uniform marginal distributions on [0, 1]. Below we show that the introduced concepts do not depend on the marginal distribution of X 1 and X 2 .
Proposition 2. For a continuous random vector (X 1 , X 2 ) and the relation R Ω , k-Pareto optimality, choice, diversity and selections offering maximum choice only depend on the copula C of X 1 , X 2 .
Proof. Let us consider the mapping
G : Ω → [0, 1] 2 , ω x → (F 1 (x 1 ), F 2 (x 2 )).
We consider R Ω and P defined on Ω. At the same time, on [0, 1] 2 we consider R defined by (x 1 , x 2 )R (y 1 , y 2 ) iff x 1 ≤ x 2 and y 1 ≤ y 2 , as well as the image measure G * P defined on [0, 1] 2 by (G * P )(A) = P (G -1 )(A). The map G preserves probabilities in the sense that for any measurable A in Ω we have (G * P )(G(A)) = P (A). Moreover, G preserves the relations in the sense that xR Ω y iff G(x)R G(y). Selections are preserved in the sense that if S is a selection for R Ω , then G(S) is a selection for R . Ignoring negligible subsets, this mapping between selections is one-to-one. Therefore, G also preserves selections, k-Pareto optimality, choice, and diversity.
The proposition finally results from the fact that G * P only depends on the copula. This is a consequence of the fact that for any (a 1 , a 2 ) ∈ [0, 1] 2 , we have
(G * P )([0, a 1 ] × [0, a 2 ]) = C(a 1 , a 2 ).
This equality is a result of the following statements: 1) continuity which guarantees that a 1 and a 2 can be written as F 1 (x 1 ) and F 2 (x 2 ) for some appropriate x 1 and x 2 ; 2) the definition of image measure; 3) the fact that G = (F 1 , F 2 ) • (X 1 , X 2 ); and 4) the equality
F (x 1 , x 2 ) = C(F 1 (x 1 ), F 2 (x 2 )).
Proposition 3. If X 1 and X 2 are two continuous independent random variables, then for the relation
R Ω P (T k ) = k -k ln(k), cho(T k ) = (k -k ln(k)) 2 -k 2 1 2 -ln k .
Proof. Let us again consider the map G. The image measure G * P induced by G on [0, 1] 2 is the Lesbegue area measure. Independently of P , we have
G(T k ) = {(x 1 , x 2 ) ∈ [0, 1] 2 |x 1 x 2 ≤ k}.
Simple integration for Eq. ( 7) yields
P (po -1 (] -∞, x])) = P (T x ), = (x1,x2)∈[0,1] 2 (G(T x )) = x -x ln(x).
Therefore, po * P = -ln(x)dx and Eq. ( 7) results in
cho(T k ) = (k -k ln(k)) 2 -2 k 0 x(-ln(x))dx, = (k -k ln(k)) 2 -k 2 (1/2 -ln k) .
Now, it is possible to show that lim k→0 div(T k ) = 1.
The fact that diversity slowly tends to the maximum 0 0.2 0.4 0.6 0.8 1 0.0 0.2 0.4 0.6 0.8 1.0 
x 1 x 2 min(x 1 , x 2 ) max(x 1 , x 2 ) (x 1 + x 2 )/2 x 1 x 2 (po)
min(x 1 , x 2 ) max(x 1 , x 2 ) (x 1 + x 2 )/2 x 1 x 2 (po)
Figure 6: Fraction of the best elements and diversity.
possible value of 1 as k tends to zero becomes even more surprising if one looks at the selections
T min a = {(x 1 , x 2 ) ∈ [0, 1] 2 | min(x 1 , x 2 ) ≤ a}, T max a = {(x 1 , x 2 ) ∈ [0, 1] 2 | max(x 1 , x 2 ) ≤ a}.
Here, lim a→0 div(T min a ) = 3 4 , and div(T max a ) = 1 2 . To further illustrate this observation, let us consider selections of a fixed measure m, which represent the best m * 100% of elements, defined with different sorting criteria: minimum min(x 1 , x 2 ), maximum max(x 1 , x 2 ), average x1+x2 2 , and Pareto optimality x 1 x 2foot_6 . As we can see in Fig. 6, min(x 1 , x 2 ) delimits selections containing too many large values, that is, extremes are overvalued. On the other hand, all other sorting criteria except po undervalue extremes and include too many elements situated around the diagonal
x 1 = x 2 .
Finally, in Fig. 6 we demonstrate how diversity of the selections defined above depends on the fraction of selected elements mfoot_7 . We can see that diversity is the largest when sorting by k-Pareto optimality. This is a direct consequence of Theorem 1.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Further Practical Explorations<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Computation Complexity<h2>text</h2>A common solution for ranking n elements of a set X according to a partial order relation R is to rank the elements according to their average ranking with respect to all linear extensions of R. However, the total number of linear extensions exponentially increases with n, and the resulting algorithms are complex and slow [de Loof, 2010, p. 48]. For example, random sampling of linear extensions has an expected running time of O(n 3 log n) [Huber, 2006]. Below we show that sorting by k-Pareto optimality offers an efficient alternative.
The basic algorithm for the k-Pareto optimality based sorting is straightforward. In the case of an arbitrary relation R, po(x) is computed by summing up the measures of the items that are strictly preferable to x. This requires one pass through the whole set X for every element x ∈ X with computation complexity O(n 2 ). The complexity of sorting X by increasing values of po is O(n log n). Therefore, the total computational complexity is O(n 2 ).
The case of composite relations defined on the probability space allows constructing even faster sorting procedures. We illustrate this idea for R = R l &R p &R r from our housings example. We define the component relations as follows: for i ∈ {l, r, p}, aR i b iff X i (a) ≤ X i (b), where the real valued random variable X l represents proximity to the location, X p represents population size, and X r (x) is 0 when x is close to a river and 1 otherwise. For independent X i , we have: po(x) = P ({y|yR * x}), = P ({y|yRx}) -P ({y|yRx and xRy}), = i∈{1,...,m}
P (X i ≤ x i ) - i∈{1,...,m} P (X i = x i ).
The cumulative probability distributions F i (x) = P (X i ≤ x) can be approximated by the respective empirical cumulative probability distributions Fi (x). The computation complexity of estimating Fi is O(n log n). This needs to be done for every component relation R i , resulting in the total complexity of O(n log n).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Application to Genetic Optimization<h2>text</h2>In Section 1, we hypothesized that sorting with choice maximization can be beneficial for genetic optimization. Indeed, this strategy results in the maximization of the population diversity and allows exploring the search space more efficiently. Additionally, Pareto dominance-based many-objective 12 genetic optimiza-tion algorithms are known to suffer from the lack of selection pressure [Palakonda et al., 2018]. When the number of objectives increases, the number of incomparable solutions grows exponentially. However, as shown in Section 4.2, sorting random independent vectors by their Pareto optimality can be considered as a liner extension of the defined preference relation. The fact that P (po(x) = po(y)) = 0 means that such sorting rarely produces ties and for any two solutions either x is preferable to y or vice versa. In the rest of this subsection, we demonstrate that the proposed approach indeed improves the performance of genetic algorithms in the case of independent objectives.
To evaluate the proposed sorting procedure, we use it in NSGA-II instead of Pareto dominance-based sorting. We experiment with two measures µ: counting and probability measures. This gives us two versions of genetic algorithms referred to as PO-count and PO-prob respectively. These algorithms are compared with implementations of the state-of-the-art algorithms NSGA-II and NSGA-III [Deb and Jain, 2013] from the deap python library 13 . For the experimental evaluation, we use the 0/1 knapsack problem with independent objectives as defined in [Zitzler and Thiele, 1999]. The number of knapsacks (objectives) is varied within the following set n k ∈ {2 -8, 10, 15, 25} and the number of items is set to 250. We adopt random selection with replacement and uniform crossover with mutation probability 0.01. We set the population size to 250 and the number of generations to 500. All results are the average among 30 independent runs.
Below we analyze the performance of different algorithms in terms of the classical hypervolume metric [Shang et al., 2020] with the origin of coordinates as a reference point. In our setup, this metric is to be maximized. We choose NSGA-II as the baseline, and present the relative changes in the hypervolume indicator for the rest of the algorithms in Fig. 7 (increase: positive number, decrease: negative number). We notice that despite having been developed for the manyobjective optimization, NSGA-III almost always results in lower values of hypervolume, even for a large number of knapsacks. This confirms a similar observation from [Ishibuchi et al., 2016], and supports our choice of NSGA-II as a baseline for implementation and comparison instead of NSGA-III. Further, we see that the value of relative increase for PO-count is always very close to 0. It means that PO-count yields a population covering the same hypervolume as NSGA-II. Contrarily, PO-prob improves the hypervolume, as compared to NSGA-II. This difference is visible for small n k (+4% for n k = 2) and is especially prominent for large n k (+60% for n k = 25). For n k between 13 https://deap.readthedocs.io/en/master/ 1 2 3 4 5 6 7 8  5 and 7, PO-prob results in lower values of hypervolume than NSGA-II. However, the relative decrease in thes cases does not exceed -1.63%. Also, within this range, PO-count performs slightly better than other algorithms. These results demonstrate that the proposed approach improves the performance of genetic algorithms, especially in the case of many-objective optimization. It also suggests that the choice of the measure µ has a large impact on the performance. The latter relationship will be studied in future work.<h2>publication_ref</h2>['b37', 'b9', 'b45', 'b41', 'b21']<h2>figure_ref</h2>['fig_5']<h2>table_ref</h2>[]<h2>heading</h2>Conclusion<h2>text</h2>In this paper, we formulate the problem of generalized topological sorting with choice maximization, which, to the best of our knowledge, was not considered in the literature before. We also prove that the at least k-Pareto optimal sets provide unique solutions. Further theoretical analysis of this problem leads us to an interesting relationship between the diversity of random points and the arc of hyperbola. Additionally, we propose a computationally efficient algorithm for the calculation of k-Pareto optimality for probability measures. Finally, we demonstrate a successful application of the developed theory. We show that sorting by k-Pareto optimality can drastically improve the performance of many-objective genetic optimization algorithms. In our experiments, the proposed solution based on the probability measure allows increasing the value of hypervolume by up to 60% for 25 objectives. This result can be considered as a potential solution to the problem of searchability deterioration in Paretodominance optimization.
We also believe that the proposed general framework can be used in different applications. In future work, we plan to study the applicability of k-Pareto optimality for constrained optimization, scheduling problems, recommender systems, and the development of statistical indicators. Maximization of choice might be also useful when studying causality and fairness.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A.2 Cone-based Relations<h2>text</h2>Let us again consider the unit square, the Lebesgue measure, and a positive constant a. However, this time the preference relation is defined as follows: yR a x iff y 2 ≤ x 2 and x 2 -y 2 ≥ a(y 1 -x 1 ). The above relation R a is an example of a cone-based relation illustrated in Fig. 9. This relation has the intuitive meaning of giving up (x 1 , x 2 ) for getting (y 1 , y 2 ) if the improvement (diminution) in the second characteristic is at least a times the trade-off (increase) in the first characteristic. In this case, selections are sets delimited by descending curves Let us now consider the sets of at least k-Pareto optimal elements of measure 0.25 for the three values of a: a = 1 10 , a = 1 2 , and a = 2, see Fig. 8b. Larger values of a represent higher maximum accepted trade-offs. This is represented by the gradual degeneration of the hyperbola into a straight horizontal line when a increases. As shown in the figure, the three sets demonstrate plausible behavior. In the situation discussed in Appendix A.1, the relation R corresponds to the extreme case of the relation R a with a = 0.
x 2 = f (x 1 ) such that -1 a ≤ df dx1 ≤ 0.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_6']<h2>table_ref</h2>[]<h2>heading</h2>A.3 Transitive relations<h2>text</h2>In general, it is possible to show that if R * is transitive, then for any k, the set T k is a selection. In particular, if R is a partial order relation, µ is strictly positive, and X is countable, we obtain a linear extension [Dantzig, 1957] of R when sorting X by increasing values of po and sorting ties in any order. Selections are represented by downsets. The latter are obtained when topologically sorting X and taking the first n elements, for any n. If µ is the counting measure, then po(x) is simply the number of elements that can be reached by following downwards the edges of the corresponding Hasse diagram. An example of such a relation represented by its Hasse diagram is depicted in Fig. 10.    Figure 13: Sorting 500 uniformly distributed points. Points in black belong to the first 10 equivalence classes (fronts). Note, that the total number of equivalence classes is larger for po-based sorting. The latter approach results in fewer ties.
results of sorting for uniformly distributed points, we observe that both sorting methods result in hyperbola-like selections, see Fig. 13. This means that sorting by Pareto fronts is more sensitive to the topological structure of the analyzed space, while sorting by po preserves its characteristics.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_7', 'fig_2', 'fig_2']<h2>table_ref</h2>[]<h2>heading</h2>B.2 Further Solutions of the Maximum Choice Problem<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Further similar solutions<h2>text</h2>It is possible to prove that Theorem 1 also holds for T * k defined with a strict inequality (<) as follows, see Def. 5 for comparison.
T * k = {x ∈ X| po(x) < k}. In this case, the proof of the fact that T * k is the largest in Lemma 1 requires analysis of inequality (4) instead of inequality (3). However, the proof of the fact that any selection T such that T * k ⊆ T ⊆ T k offers maximum choice for µ(T ) becomes a bit more technical. Moreover, it is possible to prove that T is the largest selection of this kind. Precisely, for any other selection S offering maximum choice for µ(T ), S ⊆ T for some T such that µ(T ) = µ(T ) and
T * k ⊆ T ⊆ T k .<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Completeness of the Solutions<h2>text</h2>In the case condition in Eq. ( 6) holds, and if for any selection S there is a k and there are selections T such that T * k ⊆ T ⊆ T k and µ(T ) = µ(S), then those selections T are the only selections offering maximum choice. Therefore, we have a complete list of selections offering maximum choice. This is the case for the typical example of the relation R defined in Section 2.3 and the Lebesgue area measure defined on the unit square [0, 1] 2 . This also holds for any discrete measure with constant non-zero weights, for example, for the counting measure on a finite set with a partial order relation. In the latter case, topologically sorting by increasing values of po and then taking the first n elements results in a set offering maximum choice.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Existence of Solutions of a Different Nature<h2>text</h2>Let us consider again the example in Fig. 3. Here, the set S is not of the form
T * k ∪ A with A ⊆ T k \ T * k .
Nevertheless, it offers maximum choice for m = 3.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>[]<h2>heading</h2>B.3 Diversity<h2>text</h2>As it was discussed in Section 2.2, the concept of maximum choice is functionally related to the concept of diversity, see Def. 3. In Theorem 1 we cannot simply replace choice by diversity. However, by considering only selections of a fixed measure, we obtain the following straightforward corollary of Theorem 1.
Corollary 1. If R is a transitive relation, then for any set of at least k-Pareto optimal elements T k such that µ(T k ) < +∞, we have
div(T k ) = max S∈S,µ(S)=µ(T k ) div(S).
Moreover T k is the unique such maximum. Precisely if S is a selection such that µ(S) = µ(T k ), and div(S) = div(T k ), then S = T k almost everywhere.
C Further Practical Exploration of at least k-Pareto optimal elements<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C.1 Additional Results for Genetic Optimization<h2>text</h2>In Section 5.2, we evaluate the performance of the genetic algorithms using the hypervolume indicator. In this section, we further analyze the behavior of both the state-of-the-art and the proposed algorithms with respect to other metrics. In particular, we study the fraction of solutions dominated by the solution of alternative algorithms and analyze the time complexity of the sorting procedure.
We calculate the percentage of dominated solutions as follows. For a given pair of algorithms algorithm1 and algorithm2, we calculate how many solutions of algorithm2 (dominated algorithm) are dominated by solutions of algorithm1 (dominating algorithm). After that, we average the obtained results among all dominating algorithms to get an average fraction of dominated solutions, denoted by θ. Naturally, lower values of θ indicate better performance. We present the corresponding results in Fig. 14. We notice the following tendencies. NSGA-II and PO-count behave very similarly. For n k = 2, the value of θ for these algorithms is around 20%. After that, it starts increasing and reaches its peak of approximately 45% for n k = 7. Finally, it gradually decreases to 24% for n k = 25. NSGA-III starts at a similar level and reaches its peak of approximately 30% for n k = 5. After that, it decreases below 10% for n k = 7 and stays relatively close to 0 for the larger numbers of knapsacks. These results demonstrate the superiority of NSGA-III over NSGA-II in the case of many-objective optimization. PO-prob starts at around 16%. However, for n k = 4 the value of θ it already almost 0 and does not go up for larger numbers of knapsacks. This shows that the solutions produced by this algorithm are rarely dominated. Thereby, PO-prob is an effective approach for many-objective optimization problems. In Fig. 15, we demonstrate the dependence of sorting time on the population size for values of pop size ranging from 50 to 500. The reported values are the averages over 100 independent executions of one iteration of the corresponding genetic algorithm. From the figure, we can see that PO-prob requires much less time than all other algorithm. The results for NSGA-II and PO-count tend to be very close, as in other experiments. This observation also has theoretical explanation. Indeed, choosing the next generation for NSGA-II and NSGA-III has time complexity of O(N 2 M ) and max{O(N 2 M ), O(N 2 log M -2 N )} respectively where M stands for number of objectives and N is the population size, see [Deb et al., 2002,Deb andJain, 2013]. At the same time, sorting in PO-prob comes down to independent sorting procedures with respect to every objective. The time complexity of this procedure is O(N M log(N )). These results are in line with the theoretical analysis presented in Section 5.1 and prove the computational efficiency of the approximate ranking calculation procedure used in PO-prob.
The maximum choice theorem (Theorem 1) has an intuitive interpretation in the context of genetic algorithms. Assume that the selection step is required to pick a selection of a given maximum size for breeding offspring, and both parents are chosen independently and at random form this selection. Then selections obtained via k-Pareto optimality-based sorting yield most offspring with parents offering choice. Choice here means that every parent is strictly superior to the other with respect to at least one objective, or both have the same values of all objectives.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_11', 'fig_12']<h2>table_ref</h2>[]<h2>heading</h2>C.2 Kendall's τ Rank Correlation Coefficient and Statistical Tests<h2>text</h2>Let us again consider the case of 2 continuous random variables introduced in Section 4.2. Let us assume that X 2 = f (X 1 ) for some increasing function f . For almost all (x, y), either xRy or yRx holds. Thus div(Ω) = 0. Moreover, diversity only depends on the copula which encodes the dependency structure between X 1 and X 2 , see Section 4.2.2 and Appendix B.3. Therefore, a value of div(Ω) close to zero indicates X 1 and X 2 are strongly correlated via an increasing function. It leads to the idea that Kendall's τ rank correlation coefficient [Kendall, 1948] and diversity are strongly related concepts.
Let us consider a sample of n points X = {(x i1 , x i2 ) i∈{1,2,...,n} }. Duplicates almost never occur and the order in which points are drawn has no importance. Therefore, X should be treated like a set. We consider the counting measure # and the relation R define on X. Diversity and choice of X are denoted by div # and cho # .
Kendall's τ correlation coefficient is defined as follows
τ = # con -# dis n 0 ,
where # con is the number of concordant pairs (pairs that do not offer choice in our terminology), # dis is the number of discordant pairs (pairs that do offer choice), and n 0 is the total number of pairs. As duplicates are discarded and the pairs are not ordered, n 0 = n(n -1)/2.
From the above remarks, we have cho # = 2# dis +n and # con +# dis = n 0 . Combining these equalities, we obtain the following relation
τ = n 2 + n -2 cho # n 2 -n .
Dividing the numerator and the denominator by n 2 and then neglecting 1 n , we obtains that approximately
τ ≈ 1 -2 div # .
This result means that the theory developed in this paper can be used for constructing non-parametric statistical tests generalizing Kendall's τ rank correlation coefficient and can be used for testing partial correlation. Below, we illustrate this property by building an indicator for distinguishing between wealthy and non-wealthy states.
A group of states might be considered wealthy if the following two conditions hold.
1. In the group there is no positive correlation between per capita income and the indicator representing education and health.
2. If a state belongs to a group of wealthy states, then all states having higher per capita income and better value of education and health indicator, must also belong to that group. Now, we define the set of all wealthy states as the largest group of states that are wealthy. If Kendall's τ is used to compute correlation, and correlation is considered to be positive if div # < 1 2 , then the elements of the above set can be easily identified. Indeed, Corollary 1 says that the set of wealthy states must be a set of at-least k-Pareto optimal states for the relation higher income and better education and health indicator. For the year 2015foot_9 , we took Gross National Income (GNI) per capita at purchasing power parity (PPP) as the income indicator, and the square root of the education and life expectancy as the education and health indicatorfoot_10 . The scatter plot in Fig. 16 shows the resulting division of states into wealthy and non-wealthy. We can observe that the wealthy states are defined as the states with GNI ≥ 20 000$. This seems perfectly plausible. <h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_13']<h2>table_ref</h2>[]<h2>heading</h2>C.3 Application to Recommender Systems<h2>text</h2>Let us again consider the housing example introduced in Section 2.1. If we aim to provide to the user a full set of possible alternative houses that might fit his preferences, then, according to Theorem 1, sorting available habitations by the increasing value of po is the best strategy. As it was discussed in Section 5.1, in the case of independent components of the underlying composite relations, the computation of po can be simplified by using tools from probability theory. Apart from computational efficience, estimating po in this way has several additional advantages.
• Such sorting results in fewer ties and a meaningful score. Indeed, sorting items x by increasing values of po(x), is the same as sorting by decreasing values of -log(po(x)). The self-information -log(F i (x)) [Jones, 1979], which is additive, indicates how much a characteristic i is valued. In this case, there is no need to introduce any arbitrary coefficients as it is done when sorting by a weighted mean of the characteristics x i .
• If the condition of independence holds, then rarer characteristics get valued more. This makes sense from the economic point of view and is intuitively necessary for maximizing choice.
• If beyond the relation R, there is complete uncertainty about the user's complex needs, tastes and desires, then offering him a selection of maximum choice maximizes the likelihood he finds an appropriate item.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C.4 Constrained Multi-Objective Genetic Algorithms<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C.4.1 Problem Definition<h2>text</h2>A Multiobjective Constrained Optimization Problem (CMOP) is a mathematical problem that is defined as follows [Kumara et al., 2020]:
Minimize f 1 (x), f 2 (x), . . . , f M (x) subject to g i (x) ≤ 0, i ∈ {1, 2, ..., ng}, h j = 0, j ∈ {ng + 1, ng + 2, ..., ng + nh}, L k ≤ x k ≤ U k , h ∈ {1, . . . , D},
where
• f i represents the i-th objective function,
• M is the total number of conflicting objective functions,
• x = (x 1 , x 2 , . . . , x D ) is a solution vector of length D,
• L k and U k are the lower and upper bounds of the search space at the k-th dimension.
Numerically, we consider a constraint h j to be verified iff h j ∈ [-, ]. A solution is feasible iff all ng + nh constraints g i and h j are verified.<h2>publication_ref</h2>['b27']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C.4.2 Problem Re-Definition with Preorder Relations<h2>text</h2>In a more general setting, we can represent a constraint g i ≤ 0 by the preorder relation R gi defined as follows:
xR gi y iff g i (x) ≤ 0 or g i (x) ≤ g i (y). (8
)
And a constraint h j ∈ [a j , b j ] can be represented by the preorder relation R hj defined as follows:
xR hj y iff      h j (x) ∈ [a j , b j ] or h j (y) ≤ h j (x) ≤ a j or b j ≤ h j (x) ≤ h j (y).(9)
Then, the combination of the constraints g i ≤ 0, i ∈ {1, 2, ..., ng} and h j = 0, j ∈ {ng + 1, ng + 2, ..., ng + nh} can be represented by the preorder relation R c defined as follows.
xR c y iff xR g1 y and . . . and xR gng y and xR hng+1 y and . . . and xR h ng+nh y.
The objective consisting in minimizing f i is represented by the preodrer relation R fi :
xR fi y iff f i (x) ≤ f i (y).
Minimization of all M objectives f 1 , . . . , f M is represented by the preorder relation R f defined as follows <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C.4.3 Solution<h2>text</h2>To solve the problem defined above, we can use the standard Adaptive Differential Evolution Algorithm jDE [Noman et al., 2011] with k-Pareto optimality for R cf as a fitness function. For any point x, k-Pareto optimality of x is the likelihood a point drawn at random from the population strictly Pareto dominates x for R cf . Smaller values of po mean better fitness. Under the independence assumption of objectives and constraints, we can easily compute k-Pareto optimality po(x). When saying P ({y|yR = cf x}) = 0, we assume the considered objectives and constraints are not constant on too large sets. Without this simplification, the computation becomes longer, see the derivation below. Any cumulative probability distribution defined above, F i for f i and G i for g i , can be estimated via its empirical cumulative probability distribution. Note, for a population {x 1 , . . . , x k , . . . , x ps } of size ps, and for any real valued function f , the empirical cumulative probability distribution F of f is defined as follows:
F (z) = #({x k |f (x k ) ≤ z}) ps .
In a similar way, P (y|yR hj x) can be estimated by H j * (h j (x)), where H j * (z) is defined as (10)
H j * (z) =
The computation of every Fi can be performed as follows:
• sort the values f (x k ) in increasing order, and store them in an array;
• create two new arrays;
• loop over the sorted values f (x k ); each time a new distinct value f (x k ) is encountered:
append the previously encountered f (x k ) to the first array, append to the second array the loop counter, which is equal to the value of the empirical cumulative probability distribution Fi of the previously encountered value f (x k ).
Thus, retrieving Fi (x) can be performed via a binary lookup with run time O(log ps). Computation of H j * can be performed in the same way. In this case, all three cases of the definition in Eq. ( 10 Finally, it is possible to show that the total run time of the k-Pareto optimality based sorting is O((ng + nh + M )ps log ps).<h2>publication_ref</h2>['b35']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C.5 Exploratory Database Queries<h2>text</h2>Simple database queries q, objectives, and constraints in optimization problems often consist in requiring a continuous attribute to be in a given interval, or a discrete attribute to be equal to a given value. Conceptually, those queries are boolean functions. Complex queries are often conjunctions of the form r = q 1 ∧ q 2 ∧ • • • ∧ q n .
In our formalism, these simple queries translate into simple pre-order relations of the form xRy. Requiring an element to be in an interval can be represented by xR q y iff x is in the desired interval, or x is not situated further from the interval than yfoot_11 . Requiring an attribute to be equal to a given value translates into the relation xR q y if for x the attribute takes the required valuefoot_12 . Complex queries then translate into the composite relations of the form R r = R q1 ∧ R q2 ∧ • • • ∧ R qn . The simple sub-relations R q2 are pre-order relations, and, therefore, R r is also a pre-order relation and is transitive. However, these relations are not partial order relations, as reflexivity does not necessarily hold. A "topological" sorting according to our partial order relation R r can be viewed as a valid fuzzy relaxation of the strict functional query. There are many possible fuzzy relaxations and the problem is to find one that is suitable for a given application. The k-Pareto optimality is one of such fuzzy extensions of the query. It is 0 if all criteria are satisfied, and higher values of k-Pareto optimality indicate worse results. The maximum choice theorem applies here, and the user is offered the maximum choice. This is of particular interest for exploratory queries, such as job search, especially, if there are no items in the database that satisfy all the criteria. Direct brute-force search for selections offering maximum choice is unfeasible as there are too many selections to consider.
Moreover, in the above formalism, one can treat classical optimization objectives in the same way. Maximizing an attribute x can be represented by the relation xRy iff x ≥ y, and the minimization can be represented by the relation ≤. In the above framework, negation can be represented via the relation R -1 defined by xR -1 y iff yRx.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C.6 Scheduling Algorithms<h2>text</h2>In the case of scheduling algorithms, xRy can be given the meaning 'x depends on y'. Then, selections represent sets of tasks that remain to be processed. Having a large choice means having much freedom to parallelize tasks or having flexibility in case the rescheduling is required.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Supplementary Materials A Further Examples for Simple Relations and Measures<h2>text</h2>In the first two supplementary examples, we consider a situation typical in economics or multi-objective optimization. Later, we show how the proposed concepts apply to arbitrary transitive relations.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A.1 Continuous Measures<h2>text</h2>Let us again consider R as defined in Section 2.3. The relation R models preference for small values of x 1 and x 2 . However, instead of assuming X to be a finite subset of R 2 , we now study the unit square [0, 1] × [0, 1] with three continuous measures: the Lebesgue area measure dx 1 dx 2 , as well as 2x 2 dx 1 dx 2 and 4x 1 x 2 dx 1 dx 2 . In each case, the total measure of the unit square equals to one. The Lebesgue area measure represents elements with two uniformly distributed characteristics x 1 and x 2 ; 2x 2 dx 1 dx 2 represents rarefaction of items having small values of x 2 , whereas 4x 1 x 2 dx 1 dx 2 represents rarefaction of items having small values of both x 1 and x 2 .
For each of the cases defined above, in Fig. 8a we show the set of at least k-Pareto optimal elements of measure 0.1, which corresponds to selecting the 10 best percent. All three sets demonstrate the qualitative behaviours expected from sets delimited by indifference curves when the corresponding rarefaction occurs. Indeed, the curve corresponding to the uniform distribution and the Lebesgue area measure dx 1 dx 2 is symmetric. Also, in this case, po(x 1 , x 1 ) = x 1 x 2 , and the sets of at least k-Pareto optimal elements are the sets situated below arcs of hyperbola defined by the equation x 1 x 2 = k, see Sections 4.2 and 4.2.1 for more details. Applying rarefaction with respect to x 2 prioritises smaller values of this characteristic. This is represented by shifting upwards the right part of the hyperbola arc, see the curve for 2x 2 dx 1 dx 2 . Indeed, in this case, the small values of x 2 are observed less often. This results in selecting additional elements with large values of x 1 but relatively small values of x 2 to compensate for this rarefaction. Finally, rarefaction with respect to both x 1 and x 2 results in the fact that the small values of both characteristics are observed less often. Thus, elements with larger values of x 1 and x 2 should be selected to generate a selection of the required measure. It results in the shift of the hyperbola upwards following the direction of the main diagonal, see the curve for 4x 1 x 2 dx 1 dx 2 .  <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2><h2>journal</h2><h2>year</h2>2010<h2>authors</h2> Croft<h2>ref_id</h2>b1<h2>title</h2>Search engines: Information retrieval in practice<h2>journal</h2>Addison-Wesley Reading<h2>year</h2>2010<h2>authors</h2>W B Croft; D Metzler; T Strohman<h2>ref_id</h2>b2<h2>title</h2><h2>journal</h2><h2>year</h2>1957<h2>authors</h2> Dantzig<h2>ref_id</h2>b3<h2>title</h2>Discretevariable extremum problems<h2>journal</h2>Operations research<h2>year</h2>1957<h2>authors</h2>G B Dantzig<h2>ref_id</h2>b4<h2>title</h2><h2>journal</h2><h2>year</h2>2002<h2>authors</h2>Priestley Davey<h2>ref_id</h2>b5<h2>title</h2>Introduction to lattices and order<h2>journal</h2>Cambridge university press<h2>year</h2>2002<h2>authors</h2>B A Davey; H A Priestley<h2>ref_id</h2>b6<h2>title</h2><h2>journal</h2><h2>year</h2>2010<h2>authors</h2> De Loof<h2>ref_id</h2>b7<h2>title</h2>Efficient computation of rank probabilities in posets<h2>journal</h2><h2>year</h2>2010<h2>authors</h2>K De Loof<h2>ref_id</h2>b8<h2>title</h2><h2>journal</h2><h2>year</h2>2013<h2>authors</h2>Deb ; Jain <h2>ref_id</h2>b9<h2>title</h2>An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: solving problems with box constraints<h2>journal</h2>IEEE transactions on evolutionary computation<h2>year</h2>2013<h2>authors</h2>K Deb; H Jain<h2>ref_id</h2>b10<h2>title</h2><h2>journal</h2><h2>year</h2>2002<h2>authors</h2>Deb <h2>ref_id</h2>b11<h2>title</h2>A fast and elitist multiobjective genetic algorithm: Nsga-ii<h2>journal</h2>IEEE transactions on evolutionary computation<h2>year</h2>2002<h2>authors</h2>K Deb; A Pratap; S Agarwal; T Meyarivan<h2>ref_id</h2>b12<h2>title</h2><h2>journal</h2><h2>year</h2>2013<h2>authors</h2>Durante <h2>ref_id</h2>b13<h2>title</h2>A topological proof of sklar's theorem<h2>journal</h2>Applied Mathematics Letters<h2>year</h2>2013<h2>authors</h2>F Durante; J Fernandez-Sanchez; C Sempi<h2>ref_id</h2>b14<h2>title</h2><h2>journal</h2><h2>year</h2>1997<h2>authors</h2>E Knuth<h2>ref_id</h2>b15<h2>title</h2>The Art of Computer Programming<h2>journal</h2>Addison-Wesley<h2>year</h2>1997<h2>authors</h2>E Knuth; D <h2>ref_id</h2>b16<h2>title</h2><h2>journal</h2><h2>year</h2>2013<h2>authors</h2> Halmos<h2>ref_id</h2>b17<h2>title</h2>Measure theory<h2>journal</h2>Springer<h2>year</h2>2013<h2>authors</h2>P R Halmos<h2>ref_id</h2>b18<h2>title</h2><h2>journal</h2><h2>year</h2>2006<h2>authors</h2> Huber<h2>ref_id</h2>b19<h2>title</h2>Fast perfect sampling from linear extensions<h2>journal</h2>Discrete Mathematics<h2>year</h2>2006<h2>authors</h2>M Huber<h2>ref_id</h2>b20<h2>title</h2><h2>journal</h2><h2>year</h2>2016<h2>authors</h2> Ishibuchi<h2>ref_id</h2>b21<h2>title</h2>Performance comparison of nsga-ii and nsga-iii on various manyobjective test problems<h2>journal</h2>IEEE<h2>year</h2>2016<h2>authors</h2>H Ishibuchi; R Imada; Y Setoguchi; Y Nojima<h2>ref_id</h2>b22<h2>title</h2><h2>journal</h2><h2>year</h2>1979<h2>authors</h2> Jones<h2>ref_id</h2>b23<h2>title</h2>Elementary information theory<h2>journal</h2>Clarendon Press<h2>year</h2>1979<h2>authors</h2>D S Jones<h2>ref_id</h2>b24<h2>title</h2><h2>journal</h2><h2>year</h2>1948<h2>authors</h2> Kendall<h2>ref_id</h2>b25<h2>title</h2>Rank correlation methods<h2>journal</h2><h2>year</h2>1948<h2>authors</h2>M G Kendall<h2>ref_id</h2>b26<h2>title</h2><h2>journal</h2><h2>year</h2>2020<h2>authors</h2> Kumara<h2>ref_id</h2>b27<h2>title</h2>Guidelines for real-world multi-objective constrained optimisation competition<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>A Kumara; G Wub; M Alic; Q Luob; R Mallipeddid; P N Suganthane; Swagatam Das; S <h2>ref_id</h2>b28<h2>title</h2><h2>journal</h2><h2>year</h2>2015<h2>authors</h2> Li<h2>ref_id</h2>b29<h2>title</h2>Many-objective evolutionary algorithms: A survey<h2>journal</h2>ACM Computing Surveys (CSUR)<h2>year</h2>2015<h2>authors</h2>B Li; J Li; K Tang; X Yao<h2>ref_id</h2>b30<h2>title</h2><h2>journal</h2><h2>year</h2>1990<h2>authors</h2> Martello<h2>ref_id</h2>b31<h2>title</h2>Knapsack problems: algorithms and computer implementations<h2>journal</h2><h2>year</h2>1990<h2>authors</h2>S Martello<h2>ref_id</h2>b32<h2>title</h2><h2>journal</h2><h2>year</h2>1998<h2>authors</h2> Mitchell<h2>ref_id</h2>b33<h2>title</h2>An introduction to genetic algorithms<h2>journal</h2>MIT press<h2>year</h2>1998<h2>authors</h2>M Mitchell<h2>ref_id</h2>b34<h2>title</h2><h2>journal</h2><h2>year</h2>2011<h2>authors</h2> Noman<h2>ref_id</h2>b35<h2>title</h2>An adaptive differential evolution algorithm<h2>journal</h2>IEEE<h2>year</h2>2011<h2>authors</h2>N Noman; D Bollegala; H Iba<h2>ref_id</h2>b36<h2>title</h2><h2>journal</h2><h2>year</h2>2018<h2>authors</h2> Palakonda<h2>ref_id</h2>b37<h2>title</h2>Pareto dominance-based moea with multiple ranking methods for many-objective optimization<h2>journal</h2>IEEE<h2>year</h2>2018<h2>authors</h2>V Palakonda; S Ghorbanpour; R Mallipeddi<h2>ref_id</h2>b38<h2>title</h2><h2>journal</h2><h2>year</h2>1997<h2>authors</h2>Varian Resnick<h2>ref_id</h2>b39<h2>title</h2>Recommender systems<h2>journal</h2>Communications of the ACM<h2>year</h2>1997<h2>authors</h2>P Resnick; H R Varian<h2>ref_id</h2>b40<h2>title</h2><h2>journal</h2><h2>year</h2>2020<h2>authors</h2> Shang<h2>ref_id</h2>b41<h2>title</h2>A survey on the hypervolume indicator in evolutionary multiobjective optimization<h2>journal</h2>IEEE Transactions on Evolutionary Computation<h2>year</h2>2020<h2>authors</h2>K Shang; H Ishibuchi; L He; L M Pang<h2>ref_id</h2>b42<h2>title</h2><h2>journal</h2><h2>year</h2>1959<h2>authors</h2> Sklar<h2>ref_id</h2>b43<h2>title</h2>Fonctions de repartition an dimensions et leurs marges<h2>journal</h2>Publ. inst. statist. univ<h2>year</h2>1959<h2>authors</h2>M Sklar<h2>ref_id</h2>b44<h2>title</h2><h2>journal</h2><h2>year</h2>1999<h2>authors</h2>Thiele Zitzler<h2>ref_id</h2>b45<h2>title</h2>Multiobjective evolutionary algorithms: a comparative case study and the strength pareto approach<h2>journal</h2>IEEE transactions on Evolutionary Computation<h2>year</h2>1999<h2>authors</h2>E Zitzler; L Thiele<h1>figures</h1><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Points A, B, and C are not dominated by any other point. This means that po(A) = µ({y|yR * A}) = 0 and po(B) = po(C) = po(A) = 0. Point E is dominated by a single point C, that is po(E) = µ({C}) = 1. Finally, points F and D are dominated by two other points each, resulting in po(F ) = µ({C, E}) = 2 and po(D) = µ({A, B}) = 2.8 If R * is transitive, then for any k, T k is a selection.<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Figure 2 :2Figure 1: Illustration of the relation R .<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Figure 3 :3Figure 3: S is a selection that offers maximum choice but cannot be constructed from T k . Point are encoded with their measure and k-Pareto optimality: (µ, po).<h2>figure_data</h2><h2>figure_label</h2>4<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>Figure 4 :4Figure 4: Characterization of hyperbola.<h2>figure_data</h2><h2>figure_label</h2>5<h2>figure_type</h2>figure<h2>figure_id</h2>fig_4<h2>figure_caption</h2>Figure 5 :5Figure 5: Selections of the best 40% according to different sorting criteria.<h2>figure_data</h2><h2>figure_label</h2>7<h2>figure_type</h2>figure<h2>figure_id</h2>fig_5<h2>figure_caption</h2>Figure 7 :7Figure 7: Increase in hypervolume compared to NSGA-II.<h2>figure_data</h2><h2>figure_label</h2>9<h2>figure_type</h2>figure<h2>figure_id</h2>fig_6<h2>figure_caption</h2>Figure 9 :9Figure 9: An illustration of a cone-based relation R a .<h2>figure_data</h2><h2>figure_label</h2>10<h2>figure_type</h2>figure<h2>figure_id</h2>fig_7<h2>figure_caption</h2>Figure 10 :10Figure 10: An illustration of simple partial order relation. The values of po are shown by numbers, and the set of at least 2-Pareto optimal elements T 2 is delimited by a curve.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_8<h2>figure_caption</h2>(a) Sorting by Pareto fronts. (b) Sorting by po.<h2>figure_data</h2><h2>figure_label</h2>11<h2>figure_type</h2>figure<h2>figure_id</h2>fig_9<h2>figure_caption</h2>Figure 11 :11Figure 11: Sorting points of a grid. The equivalence classes (fronts) are represented by numbers.<h2>figure_data</h2><h2>figure_label</h2>12<h2>figure_type</h2>figure<h2>figure_id</h2>fig_10<h2>figure_caption</h2>Figure 12 :12Figure 12: Selections of µ = 0.2 for the set X composed of points in the shaded area.<h2>figure_data</h2><h2>figure_label</h2>14<h2>figure_type</h2>figure<h2>figure_id</h2>fig_11<h2>figure_caption</h2>Figure 14 :14Figure 14: Average percentage of solutions dominated by other algorithms, θ.<h2>figure_data</h2><h2>figure_label</h2>15<h2>figure_type</h2>figure<h2>figure_id</h2>fig_12<h2>figure_caption</h2>Figure 15 :15Figure 15: Sorting duration as a function of population size for 10 knapsacks, n k = 10.<h2>figure_data</h2><h2>figure_label</h2>16<h2>figure_type</h2>figure<h2>figure_id</h2>fig_13<h2>figure_caption</h2>Figure 16 :16Figure 16: Separation between wealthy and non-wealthy states based on div # .<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_14<h2>figure_caption</h2>xR f y iff xR f1 y and . . . and xR f M y. Thus, the above CMOP can be represented by the lexicographic preorder relation xR cf y iff xR * c y or (xR = c y and xR f y), where xR * c y means "xR c y and not yR c x", and xR = c y means "xR c y and yR c x". For the given R cf , constrained Pareto optimal solutions [Kumara et al., 2020] are solutions that are not Pareto dominated by any other solution.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_15<h2>figure_caption</h2>Ppo(x) = P ({y|yR * cf x}), = P ({y|yR cf x} -P ({y|yR = cf x}), = P ({y|yR cf x} -0, = P ({y|yR * c x} ∪ {y|yR = c x and yR f x}), = P ({y|yR * c x}) + P ({y|yR = c x and yR f x}).Thus, if x satisfies all constraints, which means xR = c (0, . . . , 0, a ng+1 , . . . , a ng+nh ), then po(x) = P ({y|yR = c x and yR f x}), = P ({y|yR = c (0, . . . , 0, a ng+1 , . . . , a ng+nh )})P ({y|yR f x}).Now, let F i (z) = P ({y|f i (y) ≤ z}) be the cumulative probability distribution of f i . Then,P ({y|yR f x}) (y|f i (y) ≤ f i (x)). = i∈{1,...,M } F i (f i (x)).Otherwise, if at least one constraint is not satisfied by x, then P ({y|yR = c x}) = 0 and po(x) = P ({y|yR * c x}) = P ({y|yR c x}) = i∈{1,...,ng} G i (g(x)) j∈{ng+1,...,ng+nh} P (y|yR hj x).<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_16<h2>figure_caption</h2>k |z≤hj (x k )≤bj } ps if z < a j , #({x k |aj ≤hj (x k )≤bj ]} ps if a j ≤ z ≤ b j , #({x k |aj ≤hj (x k )≤z} ps if z > b j .<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_17<h2>figure_caption</h2>) are treated separately. Moreover, we have the estimation P ({y|yR = c (0, . . . , 0, a ng+1 , . . . , a ng+nh )<h2>figure_data</h2><h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>cho(A) = (µ × µ)({(x, y) ∈ A 2 |xRy = yRx}).<h2>formula_coordinates</h2>[2.0, 315.0, 608.76, 196.31, 10.31]<h2>formula_id</h2>formula_1<h2>formula_text</h2>po(x) = µ({y|yR * x}).<h2>formula_coordinates</h2>[3.0, 63.0, 309.07, 233.99, 20.69]<h2>formula_id</h2>formula_2<h2>formula_text</h2>T k = {x ∈ X| po(x) ≤ k} 8 .<h2>formula_coordinates</h2>[3.0, 63.0, 456.21, 115.85, 11.22]<h2>formula_id</h2>formula_3<h2>formula_text</h2>(x 1 , x 2 )R (y 1 , y 2 ) iff x 1 ≤ y 1 and x 2 ≤ y 2 , see Fig. 1.<h2>formula_coordinates</h2>[3.0, 63.0, 552.04, 233.99, 9.65]<h2>formula_id</h2>formula_4<h2>formula_text</h2>k that µ(T k ) = m.<h2>formula_coordinates</h2>[4.0, 85.47, 251.83, 79.68, 9.65]<h2>formula_id</h2>formula_5<h2>formula_text</h2>cho(S) = S (µ(S) -2 po(x))dµ(x).(1)<h2>formula_coordinates</h2>[4.0, 103.84, 358.23, 193.16, 17.23]<h2>formula_id</h2>formula_6<h2>formula_text</h2>cho(S) = µ(S) 2 -2(µ × µ)({(x, y) ∈ S 2 |xR * y}) = µ(S) 2 -2(µ × µ)({(x, y) ∈ S × X|yR * x}).<h2>formula_coordinates</h2>[4.0, 67.02, 454.02, 225.96, 26.99]<h2>formula_id</h2>formula_7<h2>formula_text</h2>(µ×µ)({(x, y) ∈ S × X|yR * x} = = S X 1 R * d(µ(y)) dµ(x) = S po(x)dµ(x),<h2>formula_coordinates</h2>[4.0, 67.45, 510.64, 225.1, 41.32]<h2>formula_id</h2>formula_8<h2>formula_text</h2>c(A) = A (µ(A) -2 po(x))dµ(x).<h2>formula_coordinates</h2>[4.0, 107.87, 664.96, 144.26, 17.23]<h2>formula_id</h2>formula_9<h2>formula_text</h2>c(T k ) = max A∈Σ,µ(A)≤µ(T k ) c(A). Moreover, if µ(A) ≤ µ(T k ) and c(A) = c(T k ), then A ⊆ T k almost-everywhere.<h2>formula_coordinates</h2>[4.0, 315.0, 143.48, 234.0, 45.81]<h2>formula_id</h2>formula_10<h2>formula_text</h2>A ∈ Σ, that c(A) ≤ c(T k ) if µ(A) ≤ µ(T k ). As A = (A ∩ T k ) ∪ (A \ T k ) and T k = (A ∩ T k ) ∪ (T k \ A), requiring c(A) ≤ c(T k ) is equivalent to requiring A\T k (µ(A)-2 po(x))dµ(x) ≤ T k \A (µ(T k )-2 po(x))dµ(x).<h2>formula_coordinates</h2>[4.0, 314.45, 463.64, 244.24, 82.76]<h2>formula_id</h2>formula_11<h2>formula_text</h2>(x) > k for x ∈ A \ T k , while po(x) ≤ k for x ∈ T k \ A. Therefore, A\T k (µ(A) -2 po(x))dµ(x) ≤ µ(A \ T k )(µ(A) -2k),<h2>formula_coordinates</h2>[4.0, 315.0, 560.73, 234.0, 59.4]<h2>formula_id</h2>formula_12<h2>formula_text</h2>T k \A (µ(T k ) -2 po(x))dµ(x) ≥ µ(T k \ A)(µ(T k ) -2k). (4) The constraint µ(A) ≤ µ(T k ) means that µ(A \ T k ) ≤ µ(T k \ A). Thus, we have µ(A \ T k )(µ(A) -2k) ≤ µ(T k \ A)(µ(T k ) -2k). (5)<h2>formula_coordinates</h2>[4.0, 314.12, 655.45, 234.88, 85.4]<h2>formula_id</h2>formula_13<h2>formula_text</h2>A ⊂ T k & µ(A) < µ(T k ) =⇒ cho(A) < cho(T k ), (6)<h2>formula_coordinates</h2>[5.0, 68.11, 457.75, 228.89, 9.65]<h2>formula_id</h2>formula_14<h2>formula_text</h2>x 1 , x 2 )R (y 1 , y 2 ) iff y1 2 ≤ x 1 ≤ y 1 and y2 2 ≤ x 2 ≤ y 2 . The sets T k fulfill the requirement of be- ing selections if R * satisfies the weakened transitivity condition xR * y =⇒ po(x) ≤ po(y).<h2>formula_coordinates</h2>[5.0, 63.0, 567.4, 234.0, 47.01]<h2>formula_id</h2>formula_15<h2>formula_text</h2>T * k ∪ A • (1, 1) • (1, 1) • (1, 0) • (4, 0) S<h2>formula_coordinates</h2>[5.0, 266.22, 81.78, 261.47, 660.39]<h2>formula_id</h2>formula_16<h2>formula_text</h2>Proposition 1. For any selection of at least k-Pareto optimal elements T k such that µ(T k ) < +∞ cho(T k ) = µ(T k ) 2 -2 [0,k] xd(po * µ)(x),(7)<h2>formula_coordinates</h2>[5.0, 315.0, 320.19, 234.0, 55.95]<h2>formula_id</h2>formula_17<h2>formula_text</h2>(po * µ)([a, b]) = µ(po -1 ([a, b])).<h2>formula_coordinates</h2>[5.0, 315.0, 397.71, 137.27, 10.31]<h2>formula_id</h2>formula_18<h2>formula_text</h2>ω x R Ω ω y iff X 1 (ω x ) ≤ X 1 (ω y ) and X 2 (ω x ) ≤ X 2 (ω y ).<h2>formula_coordinates</h2>[5.0, 315.21, 491.81, 233.59, 9.65]<h2>formula_id</h2>formula_19<h2>formula_text</h2>x y x 1 x 2 = 1 5 (0, 0) (1, 1) (a)<h2>formula_coordinates</h2>[6.0, 69.88, 75.99, 102.08, 124.24]<h2>formula_id</h2>formula_20<h2>formula_text</h2>T max 2 5 (0, 0) (1, 1) (b) y is preferable to x.<h2>formula_coordinates</h2>[6.0, 184.98, 75.99, 102.08, 124.24]<h2>formula_id</h2>formula_21<h2>formula_text</h2>G : Ω → [0, 1] 2 , ω x → (F 1 (x 1 ), F 2 (x 2 )).<h2>formula_coordinates</h2>[6.0, 359.93, 94.47, 144.15, 23.18]<h2>formula_id</h2>formula_22<h2>formula_text</h2>(G * P )([0, a 1 ] × [0, a 2 ]) = C(a 1 , a 2 ).<h2>formula_coordinates</h2>[6.0, 315.0, 326.35, 166.37, 9.65]<h2>formula_id</h2>formula_23<h2>formula_text</h2>F (x 1 , x 2 ) = C(F 1 (x 1 ), F 2 (x 2 )).<h2>formula_coordinates</h2>[6.0, 315.0, 398.08, 133.93, 9.65]<h2>formula_id</h2>formula_24<h2>formula_text</h2>R Ω P (T k ) = k -k ln(k), cho(T k ) = (k -k ln(k)) 2 -k 2 1 2 -ln k .<h2>formula_coordinates</h2>[6.0, 340.53, 428.93, 203.89, 59.13]<h2>formula_id</h2>formula_25<h2>formula_text</h2>G(T k ) = {(x 1 , x 2 ) ∈ [0, 1] 2 |x 1 x 2 ≤ k}.<h2>formula_coordinates</h2>[6.0, 315.0, 526.69, 234.0, 21.61]<h2>formula_id</h2>formula_26<h2>formula_text</h2>P (po -1 (] -∞, x])) = P (T x ), = (x1,x2)∈[0,1] 2 (G(T x )) = x -x ln(x).<h2>formula_coordinates</h2>[6.0, 332.44, 570.07, 199.13, 40.44]<h2>formula_id</h2>formula_27<h2>formula_text</h2>cho(T k ) = (k -k ln(k)) 2 -2 k 0 x(-ln(x))dx, = (k -k ln(k)) 2 -k 2 (1/2 -ln k) .<h2>formula_coordinates</h2>[6.0, 333.1, 642.1, 197.8, 39.46]<h2>formula_id</h2>formula_28<h2>formula_text</h2>x 1 x 2 min(x 1 , x 2 ) max(x 1 , x 2 ) (x 1 + x 2 )/2 x 1 x 2 (po)<h2>formula_coordinates</h2>[7.0, 69.04, 93.37, 210.91, 96.05]<h2>formula_id</h2>formula_29<h2>formula_text</h2>min(x 1 , x 2 ) max(x 1 , x 2 ) (x 1 + x 2 )/2 x 1 x 2 (po)<h2>formula_coordinates</h2>[7.0, 237.83, 245.24, 52.21, 48.5]<h2>formula_id</h2>formula_30<h2>formula_text</h2>T min a = {(x 1 , x 2 ) ∈ [0, 1] 2 | min(x 1 , x 2 ) ≤ a}, T max a = {(x 1 , x 2 ) ∈ [0, 1] 2 | max(x 1 , x 2 ) ≤ a}.<h2>formula_coordinates</h2>[7.0, 82.09, 456.01, 195.81, 27.9]<h2>formula_id</h2>formula_31<h2>formula_text</h2>x 1 = x 2 .<h2>formula_coordinates</h2>[7.0, 259.9, 619.94, 36.38, 9.65]<h2>formula_id</h2>formula_32<h2>formula_text</h2>P (X i ≤ x i ) - i∈{1,...,m} P (X i = x i ).<h2>formula_coordinates</h2>[7.0, 381.78, 518.23, 154.61, 20.53]<h2>formula_id</h2>formula_33<h2>formula_text</h2>x 2 = f (x 1 ) such that -1 a ≤ df dx1 ≤ 0.<h2>formula_coordinates</h2>[11.0, 63.0, 154.25, 158.41, 14.0]<h2>formula_id</h2>formula_34<h2>formula_text</h2>T * k ⊆ T ⊆ T k .<h2>formula_coordinates</h2>[13.0, 143.69, 554.54, 61.84, 12.55]<h2>formula_id</h2>formula_35<h2>formula_text</h2>T * k ∪ A with A ⊆ T k \ T * k .<h2>formula_coordinates</h2>[13.0, 428.99, 717.67, 120.01, 12.55]<h2>formula_id</h2>formula_36<h2>formula_text</h2>div(T k ) = max S∈S,µ(S)=µ(T k ) div(S).<h2>formula_coordinates</h2>[14.0, 236.49, 162.95, 139.01, 15.72]<h2>formula_id</h2>formula_37<h2>formula_text</h2>τ = # con -# dis n 0 ,<h2>formula_coordinates</h2>[15.0, 265.78, 511.86, 80.44, 23.22]<h2>formula_id</h2>formula_38<h2>formula_text</h2>τ = n 2 + n -2 cho # n 2 -n .<h2>formula_coordinates</h2>[15.0, 259.47, 608.5, 93.06, 23.89]<h2>formula_id</h2>formula_39<h2>formula_text</h2>τ ≈ 1 -2 div # .<h2>formula_coordinates</h2>[15.0, 272.13, 661.58, 67.74, 9.65]<h2>formula_id</h2>formula_40<h2>formula_text</h2>Minimize f 1 (x), f 2 (x), . . . , f M (x) subject to g i (x) ≤ 0, i ∈ {1, 2, ..., ng}, h j = 0, j ∈ {ng + 1, ng + 2, ..., ng + nh}, L k ≤ x k ≤ U k , h ∈ {1, . . . , D},<h2>formula_coordinates</h2>[17.0, 63.0, 146.49, 330.64, 87.08]<h2>formula_id</h2>formula_41<h2>formula_text</h2>• f i represents the i-th objective function,<h2>formula_coordinates</h2>[17.0, 72.96, 266.27, 185.86, 10.32]<h2>formula_id</h2>formula_42<h2>formula_text</h2>xR gi y iff g i (x) ≤ 0 or g i (x) ≤ g i (y). (8<h2>formula_coordinates</h2>[17.0, 250.62, 424.22, 294.14, 24.59]<h2>formula_id</h2>formula_43<h2>formula_text</h2>)<h2>formula_coordinates</h2>[17.0, 544.76, 431.8, 4.24, 8.74]<h2>formula_id</h2>formula_44<h2>formula_text</h2>xR hj y iff      h j (x) ∈ [a j , b j ] or h j (y) ≤ h j (x) ≤ a j or b j ≤ h j (x) ≤ h j (y).(9)<h2>formula_coordinates</h2>[17.0, 229.91, 484.15, 319.09, 41.5]<h2>formula_id</h2>formula_45<h2>formula_text</h2>xR fi y iff f i (x) ≤ f i (y).<h2>formula_coordinates</h2>[17.0, 255.93, 617.01, 100.13, 9.65]<h2>formula_id</h2>formula_46<h2>formula_text</h2>F (z) = #({x k |f (x k ) ≤ z}) ps .<h2>formula_coordinates</h2>[18.0, 247.7, 604.29, 118.84, 22.31]<h2>formula_id</h2>formula_47<h2>formula_text</h2>H j * (z) =<h2>formula_coordinates</h2>[18.0, 201.22, 676.57, 40.4, 15.14]<h1>doi</h1><h1>title</h1>k 2 Q: A Quadratic-Form Response Time and Schedulability Analysis Framework for Utilization-Based Analysis<h1>authors</h1>Jian-Jia Chen; Wen-Hung Huang; Cong Liu<h1>pub_date</h1>2016-09-23<h1>abstract</h1>In this paper, we present a general response-time analysis and schedulability-test framework, called k 2 Q (k to Q). It provides automatic constructions of closed-form quadratic bounds or utilization bounds for a wide range of applications in real-time systems under fixed-priority scheduling. The key of the framework is a k-point schedulability test or a k-point response time analysis that is based on the utilizations and the execution times of k -1 higher-priority tasks. The natural condition of k 2 Q is a quadratic form for testing the schedulability or analyzing the response time. The response time analysis and the schedulability analysis provided by the framework can be viewed as a "blackbox" interface that can result in sufficient utilization-based analysis. Since the framework is independent from the task and platform models, it can be applied to a wide range of applications. We show the generality of k 2 Q by applying it to several different task models. k 2 Q produces better uniprocessor and/or multiprocessor schedulability tests not only for the traditional sporadic task model, but also more expressive task models such as the generalized multi-frame task model and the acyclic task model. Another interesting contribution is that in the past, exponential-time schedulability tests were typically not recommended and most of time ignored due to high complexity. We have successfully shown that exponential-time schedulability tests may lead to good polynomial-time tests (almost automatically) by using the k 2 Q framework. Analogously, a similar concept to test only k points with a different formulation has been studied by us in another framework, called k 2 U, which provides hyperbolic bounds or utilization bounds based on a different formulation of schedulability test. With the quadratic and hyperbolic expressions, k 2 Q and k 2 U frameworks can be used to provide many quantitive features to be measured, like the total utilization bounds, speed-up factors, etc., not only for uniprocessor scheduling but also for multiprocessor scheduling.<h1>sections</h1><h2>heading</h2>Introduction<h2>text</h2>Analyzing the worst-case timing behaviour to ensure the timeliness of embedded systems is essential for building reliable and dependable components in cyber-physical systems. Due to the interaction and integration with external and physical devices, many real-time and embedded systems are expected to handle a large variety of workloads. Towards such dynamics, several formal real-time task models are established to represent these workloads with various characteristics, such as the the generalized multi-frame task model [8], [45] and the self-suspending task model [37]. To analyze the worstcase response time or to ensure the timeliness of the system, for each of these task models, researchers tend to develop dedicated techniques that result in schedulability tests with different computation complexity and accuracy of the analysis. Although many successful results have been developed, after many real-time systems researchers devoted themselves for many years, there does not exist a general framework that can provide efficient and effective analyses for different task models.
Prior to this paper, we have presented a general schedulability analysis framework [19], [20], called k 2 U, that can be applied in uniprocessor scheduling and multiprocessor scheduling, as long as the schedulability condition can be written in a specific form to test only k points. For example, to verify the schedulability of a (constrained-deadline) sporadic real-time task τ k under fixed-priority scheduling in uniprocessor systems, the time-demand analysis (TDA) developed in [35] can be adopted.
The general concept to obtain sufficient schedulability tests in the k 2 Q framework is to test only a subset of time points for verifying the schedulability. This idea is implemented in the k 2 Q framework by providing a k-point last-release schedulability test, which only needs to test k points under any fixed-priority scheduling when checking schedulability of the task with the k th highest priority in the system. Moreover, this concept is further extended to provide a safe upper bound of the worst-case response time. The response time analysis and the schedulability analysis provided by the framework can be viewed as a "blackbox" interface that can result in sufficient utilization-based analysis.
Related Work. There have been several results in the literature with respect to utilization-based, e.g., [13], [30], [32], [33], [38], [39], [47], and non-utilization-based, e.g., [17], [27], schedulability tests for the sporadic real-time task model and its generalizations in uniprocessor systems. Most of the existing utilization-based schedulability analyses focus on the total utilization bound. That is, if the total utilization of the task system is no more than the derived bound, the task system is schedulable by the scheduling policy. For example, the total utilization bounds derived in [16], [30], [39] are mainly for rate-monotonic (RM) scheduling, in which the results in [30] can be extended for arbitrary fixed-priority scheduling. Kuo et al. [32] further improve the total utilization bound by using the notion of divisibility. Lee et al. [33] use linear programming formulations for calculating total utilization bounds when the period of a task can be selected. Moreover, Wu et al. [47] adopt the Network Calculus to analyze the total utilization bounds of several real-time task models.
Bini and Buttazzo [12] propose a framework of schedulability tests that can be tuned to balance the time complexity and the acceptance ratio of the schedulability test for uniprocessor sporadic task systems. The efficient tests in [12] are based on an observation to test whether the parameters of a task set fall into a schedulable region of the fixed-priority scheduling policy. Our strategy and philosophy are simpler than [12]. First, we only look at the parameters of task τ k (the task defined as the k th highest priority) that is under analysis by assuming that the higher-priority tasks are already verified to be schedulable. Second, similar to our recent general schedulability analysis framework k 2 U [20], we also apply the key idea of evaluating only k points. The tunable strategies in [12] consider to examine a subset of the time points for schedulability tests.
Distinct from the results in [12], our objective in this paper is to find closed-form schedulability tests and responsetime analyses that can be independent from task and platform models. We target at sufficient schedulability tests and response time analyses that are not exact but can be calculated efficiently in linear-time or polynomial-time complexity.
Comparison to k 2 U: Even though k 2 Q and k 2 U share the same idea to test and evaluate only k points, they are based on completely different criteria for testing. In k 2 U, all the testings and formulations are based on only the higher-priority task utilizations. In k 2 Q, the testings are based not only on the higher-priority task utilizations, but also on the higher-priority task execution times. The above difference in the formulations results in completely different properties and mathematical closed-forms. The natural condition of k 2 Q is a quadratic form for testing the schedulability or the response time of a task, whereas the natural condition of k 2 U is a hyperbolic form for testing the schedulability of a task.
If one framework were dominated by another or these two frameworks were just with minor difference in mathematical formulations, it wouldn't be necessary to separate and present them as two different frameworks. Both frameworks are in fact needed and have to be applied for different cases. Here, we only shortly explain their differences, advantages, and disadvantages in this paper. For completeness, another document has been prepared in [18] to present the similarity, the difference and the characteristics of these two frameworks in details.
Since the formulation of k 2 U is more restrictive than k 2 Q, its applicability is limited by the possibility to formulate the tests purely by using higher-priority task utilizations without referring to their execution times. There are cases, in which formulating the higher-priority interference by using only task utilizations for k 2 U is troublesome or over-pessimistic. For such cases, further introducing the upper bound of the execution time by using k 2 Q is more precise. Most of the presented cases, except the one in uniprocessor constraineddeadline systems in Appendix B are in the above category. Although k 2 Q is more general, it is not as precise as k 2 U, if we can formulate the schedulability tests into both frameworks with the same parameters. In such cases, the same pseudopolynomial-time (or exponential time) test is used, and the utilization bound or speed-up factor analysis derived from the k 2 U framework is, in general, tighter and better.
In a nutshell, k 2 Q is more general, whereas k 2 U is more precise. If an exact schedulability test can be constructed and the test can be converted into k 2 U, e.g., uniprocessor scheduling for constrained-deadline task sets, then, adopting k 2 U leads to tight results. For example, by using k 2 Q, we can reach the conclusion that the utilization bound for ratemonotonic scheduling is 2-√ 2 ≈ 0.586, which is less precise than the Liu and Layland bound ln 2 ≈ 0.693, a simple implication by using k 2 U. However, if we are allowed to change the execution time and period of a task for different job releases (called acyclic task model in [1]), then the tight utilization bound 2-√ 2 can be easily achieved by using k 2 Q.
Due to the fact the k 2 U is more precise (with respect to the utilization bound) when the exact tests can be constructed, even though k 2 U is more restrictive, both are needed for different cases. Both k 2 U and k 2 Q are general enough to cover a range of spectrum of applications, ranging from uniprocessor systems to multiprocessor systems. For more information and comparisons, please refer to [18].<h2>publication_ref</h2>['b7', 'b44', 'b36', 'b18', 'b19', 'b34', 'b12', 'b29', 'b31', 'b32', 'b37', 'b38', 'b46', 'b16', 'b26', 'b15', 'b29', 'b38', 'b29', 'b31', 'b32', 'b46', 'b11', 'b11', 'b11', 'b19', 'b11', 'b11', 'b17', 'b0', 'b17']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Contributions.<h2>text</h2>The key contribution of this paper is a general schedulability and response-time analysis framework, k 2 Q, that can be easily applied to analyze a number of complex real-time task models, on both uniprocessor and multiprocessor systems. A key novelty of k 2 Q that allows a rather general analysis framework is that we do not specifically seek for the total utilization bound. Instead, we look for the critical value in the specified sufficient schedulability test while verifying the schedulability of task τ k . This critical value of task τ k gives the difficulty of task τ k to be schedulable under the scheduling policy. We present several properties of k 2 Q, which provide a series of closed-form solutions to be adopted for sufficient tests and worst-case response time analyses for real-time task models, as long as a corresponding k-point last-release schedulability test (Definition 2) or a k-point last-release responsetime analysis (Definition 3) can be constructed. The generality of k 2 Q is supported by demonstrating that either new or better results compared to the state-of-the-art can be easily obtained using k 2 Q. Examples include:
• Several utilization-based schedulability and response analyses for uniprocessor sporadic task systems are provided in Section 5 The utilization-based worst-case response-time analysis in Theorem 4 in Section 5 is identical to the response-time analysis by Bini et al. [15] developed in parallel. ≈ 2.823 for implicit-deadline sporadic task systems, which improves upon the existing best speed-up factor 3 presented in [10].
• We provide, to the best of our knowledge, the first polynomial-time worst-case response time analysis for sporadic real-time tasks with jitters [3], [9] in Appendix D.
• We also demonstrate how to convert exponential-time schedulability tests of generalized multi-frame task models [8], [46] to polynomial-time tests by using the k 2 Q framework in Appendix E. • The above results are for task-level fixed-priority scheduling policies. We further explore mode-level fixed-priority scheduling policies by studying the acyclic task model [1] and the multi-mode task model [24]. 1 We conclude a quadratic bound and a utilization bound 2 -√ 2 for RM scheduling policy. The utilization bound is the same as the result in [1]. They can be further generalized to handle more generalized task models, including the digraph task model [44], the recurring real-time task model [6]. This is presented in Appendix F.
The emphasis of this paper is to show the generality of the k 2 Q framework by demonstrating via several task models. The tests and analytical results in the framework are with low complexity, but can still be shown to provide good results through speed-up factor or utilization bound analyses. We also note a somehow surprising finding through developing this framework: in the past, exponential-time schedulability tests were typically not recommended and most of time ignored, as this requires very high complexity. We have successfully shown in this paper that exponential-time schedulability tests may lead to good polynomial-time tests (almost automatically) by using the k 2 Q framework. Therefore, this framework may also open the possibility to re-examine some tests with exponentialtime complexity to improve their applicability.<h2>publication_ref</h2>['b14', 'b9', 'b2', 'b8', 'b7', 'b45', 'b0', 'b23', 'b0', 'b43', 'b5']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Basic Task and Scheduling Models<h2>text</h2>This section presents the sporadic real-time task model, as the basis for our presentations. Even though the framework targets at more general task models, to ease the presentation flow, we will start with the sporadic task models. A sporadic task τ i is released repeatedly, with each such invocation called a job. The j th job of τ i , denoted τ i,j , is released at time r i,j and has an absolute deadline at time d i,j . Each job of any task τ i is assumed to have execution time C i . Here in this paper, whenever we refer to the execution time of a job, we mean for the worst-case execution time of the job, since all the analyses we use are safe by only considering the worst-case execution time. The response time of a job is defined as its finishing time minus its release time. Successive jobs of the same task are required to be executed in sequence. Associated with each task τ i are a period T i , which specifies the minimum time between two consecutive job releases of τ i , and a deadline D i , which specifies the relative deadline of each such job, i.e., d i,j = r i,j + D i . The worst-case response time of a task τ i is the maximum response time among all its jobs. The utilization of a task τ i is defined as
U i = C i /T i .
A sporadic task system τ is an implicit-deadline system if D i = T i holds for each τ i . A sporadic task system τ is a constrained-deadline system if D i ≤ T i holds for each τ i . Otherwise, such a sporadic task system τ is an arbitrarydeadline system.
A task is said schedulable by a scheduling policy if all of its jobs can finish before their absolute deadlines, i.e., the worst-case response time of the task is no more than its relative deadline. A task system is said schedulable by a scheduling policy if all the tasks in the task system are schedulable. A schedulability test expresses sufficient schedulability conditions to ensure the feasibility of the resulting schedule by a scheduling policy.
Throughout the paper, we will focus on fixed-priority preemptive scheduling. That is, each task is associated with a priority level (except in Appendix F). For a uniprocessor system, the scheduler always dispatches the job with the highest priority in the ready queue to be executed. For a multiprocessor system, we consider multiprocessor global scheduling on M identical processors, in which each of them has the same computation power. For global multiprocessor scheduling, there is a global queue and a global scheduler to dispatch the jobs. We consider only global fixed-priority scheduling. At any time, the M -highest-priority jobs in the ready queue are dispatched and executed on these M processors.
Note that the framework is not only limited to the above task and platform models. These terminologies are introduced only for the simplicity of presentation and illustrating some examples.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Speed-Up Factor and Capacity Augmentation Factor:<h2>text</h2>To quantify the error of the schedulability tests or the scheduling policies, the concept of resource augmentation by using speedup factors [43] and the capacity augmentation factors [36] has been adopted. For example, global DM in general does not have good utilization bounds to schedule a set of sporadic tasks on M identical processors, due to "Dhall's effect" [26]. However, if we constrain the total utilization τi Ci M Ti ≤ 1 b , the density
C k + τ i ∈hp(τ k ) Ci M D k ≤ 1
b for each task τ k , and the maximum utilization max τi Ci min{Ti,Di} ≤ 1 b , it is possible to provide the schedulability guarantee of global RM by setting b to 3-1 M [2], [4], [10]. Such a factor b has been recently named as a capacity augmentation factor [36]. Note that the capacity augmentation bound was defined without taking this simple condition
C k + τ i ∈hp(τ k ) Ci M D k ≤ 1
b in [36], as they focus on implicit-deadline systems. For constrained-deadline systems, adding such a new constraint is a natural extension.
An algorithm A is with speed-up factor b: If there exists a feasible schedule for the task system, it is schedulable by algorithm A by speeding up (each processor) to b times as fast as in the original platform (speed). A sufficient schedulability test for scheduling algorithm A is with speed-up factor b: If the task system cannot pass the sufficient schedulability test, the task set is not schedulable by any scheduling algorithm if (each processor) is slowed down to 1 b times of the original platform speed. Note that if the capacity augmentation factor is b, the speed-up factor is also upper-bounded by b.<h2>publication_ref</h2>['b42', 'b35', 'b25', 'b3', 'b9', 'b35', 'b35']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Analysis Flow<h2>text</h2>The framework focuses on testing the schedulability and the response time for a task τ k , under the assumption that the required properties (i.e., worst-case response time or the schedulability) of the higher-priority tasks are already verified and provided. We will implicitly assume that all the higherpriority tasks are already verified and the required properties are already obtained. Therefore, this framework has to be applied for each of the given tasks. To ensure whether a task system is schedulable by the given scheduling policy, the test has to be applied for all the tasks. Of course, the results can be extended to test the schedulability of a task system in linear time complexity or to allow on-line admission control in constant time complexity if the schedulability condition (or with some more pessimistic simplifications) is monotonic. Such extensions are presented only for trivial cases.
We will only present the schedulability test of a certain task τ k , that is analyzed, under the above assumption. For notational brevity, in the framework presentation, we will implicitly assume that there are k-1 tasks, say τ 1 , τ 2 , . . . , τ k-1 with higher-priority than task τ k . We will use hp(τ k ) to denote the set of these k-1 higher-priority tasks, when their orderings do not matter. Moreover, we only consider the cases when k ≥ 2, since k = 1 is pretty trivial.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>k 2 Q<h2>text</h2>This section presents the basic properties of the k 2 Q framework for testing the schedulability of task τ k in a given set of real-time tasks (depending on the specific models given in each application). Before presenting the framework, we first give a simple example to explain the underlying concepts by using an implicit-deadline sporadic task system τ , in which D i = T i for every τ i ∈ τ . The exact schedulability test to verify whether task τ k can meet its deadline under fixedpriority scheduling on uniprocessor systems is to check
∃t with 0 < t ≤ T k and C k + τi∈hp(τ k ) t T i C i ≤ t,(1)
where hp(τ k ) is the set of tasks with higher priority than τ k . Instead of testing all the time points t in the range of 0 and T k , for a sufficient schedulability test, we can greedily only consider to test the time points
( T k Ti -1)T i for τ i ∈ hp(τ k ) and t = T k . If C k + τi∈hp(τ k )
t Ti C i ≤ t holds in one of those k tested time points, then we can conclude that τ k can be feasibly scheduled under this scheduling policy.
To implement to above testing concept, we need two definitions: 1) Definition 1 defines the last release time ordering so that we can formulate the problem with linear algebra, 2) Definition 2 defines an abstracted schedulability test that can be used to model general schedulability tests regardless of the task and platform model. Definition 1 (Last Release Time Ordering). Let π be the last release time ordering assignment as a bijective function π : hp(τ k ) → {1, 2, . . . , k -1} to define the last release time ordering of task τ j ∈ hp(τ k ) in the window of interest. Last release time orderings are numbered from 1 to k -1, i.e., |hp(τ k )|, where 1 is the earliest and k -1 the latest.
The last release time ordering is a very important property in the whole framework. When testing the schedulability or analyzing the worst-case response time of task τ k , we do not need the priority ordering of the higher-priority tasks in hp(τ k ). But, we need to know how to order the k -1 higherpriority tasks so that we can formulate the test with simple and linear arithmetics based on the total order. For the rest of this paper, the ordering of the k -1 higher-priority tasks implicitly refers to their last release time ordering (except explanations regarding the last release time ordering when referring to Example 4). In the k 2 Q framework, we are only interested to test only k time points. More precisely, we are only interested to test whether task τ k can be successfully executed before the last release time of a higher-priority task in the testing window. Therefore, the last release time ordering provides a total order so that we can transform the schedulability tests into the following definition. Definition 2. A k-point last-release schedulability test under a given last release time ordering π of the k -1 higherpriority tasks is a sufficient schedulability test of a fixedpriority scheduling policy, that verifies the existence of t j with j = 1, 2, . . . , k such that
0 ≤ t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ t k and C k + k-1 i=1 α i t i U i + j-1 i=1 β i C i ≤ t j ,(2)
where C k > 0, for i = 1, 2, . . . , k-1, α i > 0, U i > 0, C i ≥ 0, and β i > 0 are dependent upon the setting of the task models and task τ i .
Example 1. Implicit-deadline task systems: For an implicitdeadline sporadic task system τ , suppose that we are interested to test whether task τ k can meet its deadline or not under a fixed-priority scheduling algorithm on a uniprocessor platform. Let |hp(τ k )| be k -1 and the tasks in hp(τ k ) be ordered by
( T k Ti -1)T i non-decreasingly, i.e., t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ t k = T k .
For a specific testing point at time t j for a certain j = 1, 2, . . . , k, the function tj Ti C i (to quantify the workload due to the jobs released by a higher-priority task τ i ∈ hp(τ k )) has two cases: 1) if i < j, due to the definition of t i as ( T k Ti -1)T i and t i ≤ t j ≤ T k , we know that tj Ti C i is upper bounded by
ti Ti C i + C i = t i U i + C i ; 2) if i ≥ j, due to the definition of t i as ( T k Ti -1)T i and t j ≤ t i ≤ T k , we know that tj Ti C i is upper bounded by ti Ti C i = t i U i . 2
By the above analysis, for a given j = 1, 2, . . . , k, we know that
C k + k-1 i=1 tj Ti C i ≤ C k + k-1 i=1 t i U i + j-1 i=1 C i .
Therefore, we know that task τ k is schedulable by the fixedpriority scheduling if there exists j ∈ {1, 2, . . . , k} such that
C k + k-1 i=1 t i U i + j-1 i=1 C i ≤ t j .
In other words, by the specific index rule of the tasks in hp(τ k ) and setting α i = 1 and β i = 1 for every task τ i in hp(τ k ), we reach a concrete example for Definition 2.
A concrete example is provided here for illustrating Example 1.
Example 2. Consider that k = 3 and |hp(τ k )| is 2. For the two tasks in hp(τ k ), let C 1 = 2, U 1 = 0.2, T 1 = 10 and C 2 = 4, U 2 = 0.5, T 2 = 8. Suppose that t 3 = D 3 = T 3 = 36. By the transformation in Example 1, we know that t 1 = 30 and t 2 = 32. The last release time ordering π of {τ 1 , τ 2 } follows the index, i.e., π :
{τ 1 , τ 2 } → {1, 2}. Moreover, α 1 = α 2 = β 1 = β 2 = 1.
Similar to Definition 2, we can also define an abstracted worst-case response time analysis as follows: Definition 3. A k-point last-release response time analysis is a safe response time analysis of a fixed-priority scheduling policy under a given last release time ordering π of the k -1 higher-priority tasks by finding the maximum
t k = C k + k-1 i=1 α i t i U i + k-1 i=1 β i C i ,(3)
with 0 ≤ t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ t k and C k + k-1 i=1 α i t i U i + j-1 i=1 β i C i > t j ,∀j = 1, 2, . . . , k -1,(4)
where C k > 0, α i > 0, U i > 0, C i ≥ 0, and β i > 0 are dependent upon the setting of the task models and task τ i .
Example 3. Response-time for constrained-deadline task systems: Suppose that R k is the exact worst-case response 2 Since t i is an integer multiple of T i , the property
t i T i C i = t i U i holds.
time for task τ k and R k ≤ T k under uniprocessor fixed-priority scheduling. That is, by Eq. (1),
C k + τi∈hp(τ k ) t Ti C i > t for any 0 < t < R k and C k + τi∈hp(τ k ) R k Ti C i = R k .
Similar to Example 1, let |hp(τ k )| be k -1 and the tasks in hp(τ k ) be ordered by ( R k Ti -1)T i non-decreasingly, i.e.,
t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ R k . With the same analysis in Example 1, we know that C k + k-1 i=1 t i U i + j-1 i=1 C i > t j for j = 1, 2, . . . , k -1 and R k ≤ C k + k-1 i=1 t i U i + k-1 i=1 C i .
As a result, by the specific index rule of the tasks in hp(τ k ) and setting α i = 1 and β i = 1 for every task τ i in hp(τ k ), we reach a concrete example for Definition 3.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Important Notes<h2>text</h2>Before presenting the analyses based on Definition 2 and Definition 3, we would like to first explain the important assumptions and the flow to use the analytical results. Throughout the paper, we implicitly assume that t k > 0 when Definition 2 is used. Moreover, we only consider non-trivial cases, in which C k > 0 and 0 < U i ≤ 1 for i = 1, 2, . . . , k-1. The definition of t k depends on how Definition 2 is constructed based on the original schedulability test, usually equal to the length of the interval (of the points to be tested in the original schedulability test), e.g., t k = T k = D k in Example 1. In most of the cases, we can set t k as D k . But, it can also be set to other cases, to be demonstrated in Appendix C for global RM scheduling.
In Definition 2, the k-point last-release schedulability test is a sufficient schedulability test that tests only k time points, defined by the k-1 higher-priority tasks and task τ k . Similarly, in Definition 3, a k-point last-release response time analysis provides a safe response time by only testing whether task τ k has already finished earlier at k -1 points, each defined by a higher-priority task.
In both cases in Definitions 2 and 3, the last release time ordering π is assumed to be given. In some cases, this ordering can be easily obtained. For such cases, all the lemmas in this section can be directly adopted. However, in most of the cases in our demonstrated task models, we have to test all possible last release time orderings and take the worst case. Fortunately, we will show that finding the worst-case ordering is not a difficult problem, which requires to sort the k -1 higherpriority tasks under a simple criteria, in Lemmas 2 and 7. Therefore, for such cases, the lemmas in this section have to be adopted by combining with Lemma 2 or 7.
We first assume that the corresponding coefficients α i and β i in Definitions 2 and 3 are given. How to derive them will be discussed in the following sections. Clearly their values are highly dependent upon the task models and the scheduling policies. Provided that these coefficients α i , β i , C i , U i for every higher-priority task τ i ∈ hp(τ k ) are given, we analyze (1) the response time by finding the extreme case for a given C k (under Definition 3), or (2) the schedulability by finding the extreme case for a given C k and D k . Therefore, the k 2 Q framework provides utilization-based schedulability analyses and response time analyses automatically if the corresponding parameters α i and β i can be defined to ensure that the tests in Definitions 2 and 3 are safe.
k 2 Q can be used by a wide range of applications, as Demonstrated Applications:
Sec. 5: Arbitrary-deadline sporadic tasks Sec. 5:
Multiprocessor RM App. D: Periodic tasks with jitters App. E: Generalized multiframe App. F: Acyclic and Multi-Mode Models long as the users can properly specify the corresponding task properties C i and U i and the constant coefficients α i and β i of every higher-priority task τ i . More precisely, the formulation in Definitions 2 and 3 does not actually care what C i and U i actually mean. When sporadic task models are considered, we will use these two terms as they were defined in Section 2, i.e., C i stands for the execution time and U i is Ci Ti . When we consider more general cases, such as the generalized multiframe and multi-mode task models, we have to properly define the values of U i and C i to apply the framework.
U i , ∀i < k C i , ∀i < k α i , ∀i < k β i , ∀i < k C k t k (for Lemmas 1-
The use cases of k 2 Q can be achieved by using the known schedulability tests (that are in the form of pseudo polynomialtime or exponential-time tests) or some simple modifications of the existing results. We will provide the explanations of the correctness of the selection of the parameters, α i , β i , C i , U i for a higher-priority task τ i to support the correctness of the results. Such a flow actually leads to the elegance and the generality of the framework, which works as long as Definition 2 (Definition 3, respectively) can be successfully constructed for the sufficient schedulability test (response time, respectively) of task τ k in a fixed-priority scheduling policy. The procedure is illustrated in Figure 1. With the availability of the k 2 Q framework, the quadratic bounds or utilization bounds can be automatically derived as long as the safe upper bounds α and β can be safely derived, regardless of the task model or the platforms.
We are not going to present how to systematically and automatically derive these parameters to be applied for the k 2 Q framework. For most of the typical schedulability tests and response time analyses in real-time systems, such a derivation procedure is similar to the automatic parameter generation for the k 2 U in [21].<h2>publication_ref</h2>['b20']<h2>figure_ref</h2>['fig_0']<h2>table_ref</h2>[]<h2>heading</h2>Schedulability Test Framework<h2>text</h2>This section provides five important lemmas for deriving the utilization-based schedulability test based on Definition 2. Lemma 1 is the most general test, whereas Lemmas 3, 4, and 5 work for certain special cases when β i C i ≤ βU i t k for any higher-priority task τ i . Lemma 2 gives the worst-case last release time ordering, which can be used when the last release time ordering for testing task τ k is unknown.
Lemma 1. For a given k-point last-release schedulability test, defined in Definition 2, of a scheduling algorithm, in which 0 < α i , and
0 < β i for any i = 1, 2, . . . , k -1, 0 < t k , k-1 i=1 α i U i ≤ 1, and k-1 i=1 β i C i ≤ t k , task τ k is
schedulable by the fixed-priority scheduling algorithm if the following condition holds
C k t k ≤ 1- k-1 i=1 α i U i - k-1 i=1 (β i C i -α i U i ( k-1 =i β C )) t k .(5)
Proof: We prove this lemma by showing that the condition in Eq. ( 5) leads to the satisfactions of the schedulability conditions listed in Eq. ( 2) by using contrapositive. By taking the negation of the schedulability condition in Eq. ( 2), we know that if task τ k is not schedulable by the scheduling policy, then for each j = 1, 2, . . . , k
C k + k-1 i=1 α i t i U i + j-1 i=1 β i C i > t j .(6)
To enforce the condition in Eq. ( 6), we are going to show that C k must have some lower bound, denoted as C * k . Therefore, if C k is no more than this lower bound, then task τ k is schedulable by the scheduling policy. For the rest of the proof, we replace > with ≥ in Eq. ( 6), as the infimum and the minimum are the same when presenting the inequality with ≥. The unschedulability for satisfying Eq. ( 6) implies that
C k > C * k , where C * k is defined in the optimization problem: min C * k (7a) s.t. C * k + k-1 i=1 α i t * i U i + j-1 i=1 β i C i ≥ t * j , ∀j = 1, 2, . . . , k -1, (7b) t * 1 ≥ 0 (7c) t * j ≥ t * j-1 , ∀j = 2, 3, . . . , k -1,(7d)
C * k + k-1 i=1 α i t * i U i + k-1 i=1 β i C i ≥ t k ,(7e)
where t * 1 , t * 2 , . . . , t * k-1 and C * k are variables, α i , β i , U i , and C i are constants, and t k is a given positive constant. Moreover, it is obvious that relaxing the constraint t * j ≥ t * j-1 for j = 2, 3, . . . , k -1 by using t * j ≥ 0 does not increase the corresponding objective function in the linear programming. Therefore, we have
min C * k (8a) s.t. C * k + k-1 i=1 α i t * i U i + j-1 i=1 β i C i ≥ t * j , ∀j = 1, 2, . . . , k -1, (8b) t * j ≥ 0, ∀j = 1, 2, . . . , k -1,(8c)
C * k + k-1 i=1 α i t * i U i + k-1 i=1 β i C i ≥ t k .(8d)
Let s ≥ 0 be a slack variable such that
C * k = t k + s - ( k-1 i=1 α i t * i U i + k-1 i=1 β i C i ).
Therefore, we can replace the objective function and the constraints with the above equality of C * k . The objective function (i.e., Eq. ( 8a)) is to find the minimum value of
t k + s -( k-1 i=1 α i t * i U i + k-1 i=1 β i C i ) such that Eq. (8b) holds, which is equivalent to t k + s -( k-1 i=1 α i t * i U i + k-1 i=1 β i C i ) + k-1 i=1 α i t * i U i + j-1 i=1 β i C i = t k + s - k-1 i=j β i C i ≥ t * j , ∀j = 1, 2, . . . , k -1.(9)
For notational brevity, let t * k be t k +s. Therefore, the linear programming in Eq. ( 8) can be rewritten as follows:
min t * k -( k-1 i=1 α i U i t * i + k-1 i=1 β i C i ) (10a) s.t. t * k - k-1 i=j β i C i ≥ t * j , ∀1 ≤ j ≤ k -1, (10b) t * j ≥ 0 ∀1 ≤ j ≤ k -1. (10c) t * k ≥ t k (10d)
The remaining proof is to solve the above linear programming to obtain the minimum C * k . Our proof strategy is to solve the linear programming analytically as a function of t * k . This can be imagined as if t * k is given. At the end, we will prove the optimality by considering all possible t * k ≥ t k . This involves three steps:
• Step 1: we analyze certain properties of optimal solutions based on the extreme point theorem for linear programming [40] under the assumption that t * k is given as a constant, i.e., s is known.
• Step 2: we present a specific solution in an extreme point, as a function of t * k . • Step 3: we prove that the above extreme point solution gives the minimum
C * k if k-1 i=1 α i U i ≤ 1.
[Step 1:] After specifying the value t * k as a given constant, the new linear programming without the constraint in Eq. (10d) has only k -1 variables and 2(k -1) constraints. Thus, according to the extreme point theorem for linear programming [40], the linear constraints form a polyhedron of feasible solutions. The extreme point theorem states that either there is no feasible solution or one of the extreme points in the polyhedron is an optimal solution when the objective of the linear programming is finite. To satisfy Eqs. (10b) and (10c), we know that t * j ≤ t * k for j = 1, 2, . . . , k -1, due to t * i ≥ 0, 0 < β, and C i ≥ 0 for i = j, j + 1, . . . , k -1. As a result, the objective of the above linear programming is finite since a feasible solution has to satisfy t
* i ≤ t * k for i = 1, 2, . . . , k -1,.
According to the extreme point theorem, one of the extreme points is the optimal solution of Eq. (10). There are k -1 variables with 2k -2 constraints in Eq. ( 10). An extreme point must have at least k -1 active constraints in Eqs. (10b) and (10c), in which their ≥ are set to equality =.
[Step 2:] One special extreme point solution by setting
t * j > 0 is to put t * k - k-1 i=j β i C i = t * j for every j = 1, 2, . . . , k -1, i.e., ∀1 ≤ i ≤ k -1, t * i+1 -t * i = β i C i ,(11)
which implies that
t * k -t * i = k-1 =i (t * +1 -t * ) = k-1 =i β C(12)
The above extreme point solution is always feasible in the linear programming due to the assumption that
k-1 j=1 β j C j ≤ t k ≤ t * k .
Therefore, in this extreme point solution, the objective function of Eq. ( 10) by rephrasing based on the condition in Eq. ( 12) is
t * k - k-1 i=1 (αiUit * i + βiCi) (13) =t * k - k-1 i=1 αiUi t * k - k-1 =i β C + βiCi (14) =t * k - k-1 i=1 αiUit * k + k-1 i=1 βiCi - k-1 i=1 αiUi k-1 =i β C(15)
which means that
C * k ≥ t * k (1 - k-1 i=1 α i U i ) - k-1 i=1 (β i C i - α i U i ( k-1 =i β C )).
[Step 3:] The rest of the proof shows that other feasible extreme point solutions (that allow t * j to be 0 for some higherpriority task τ j ) are with worse objective values for Eq. (10). Under the assumption that
k-1 i=1 β i C i ≤ t k ≤ t * k , if t * j is set to 0, there are two cases: (1) t * k - k-1 i=j β i C i > 0 or (2) t * k - k-1 i=j β i C i = 0. In the former case, we can simply set t * j to t * k - k-1
i=j β i C i to improve the objective function without introducing any violation of the constraints. In the latter case, the value of t * j can only be set to 0 in any feasible solutions. Therefore, we conclude that any other feasible extreme point solutions for Eq. ( 10) are worse.<h2>publication_ref</h2>['b39', 'b39', 'b9', 'b9']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Note that the above solution of C *<h2>text</h2>k is still a function of t * k . We need to find the minimization of C * k with respect to t * k based on the fact t * k ≥ t k . Due to the assumption that 1 -
k-1 i=1 α i U i ≥ 0 and t * k ≥ t k , we know that t * k (1 - k-1 i=1 α i U i ) ≥ t k (1 - k-1 i=1 α i U i ). Therefore, C * k = t k (1 - k-1 i=1 α i U i ) - k-1 i=1 (β i C i -α i U i ( k-1 =i β C )) when 1 - k-1 i=1 α i U i ≥ 0 and k-1 i=1 β i C i ≤ t k , which concludes the proof.
Lemma 1 can be applied only when the last release time ordering of the k -1 higher-priority tasks is given. We demonstrate the importance of the last release time ordering by using the following example. 3   Example 4.
Consider that k = 3 and |hp(τ k )| is 2. For the two tasks in hp(τ k ), let C 1 = 2, U 1 = 0.2, T 1 = 10 and C 2 = 4, U 2 = 0.5, T 2 = 8. Suppose that t 3 = D 3 = T 3 = 36.
By the transformation in Example 1, we know that α i = 1 and
β i = 1 for i = 1, 2.
There are two last release time orderings. Suppose that π 1 : {τ 1 , τ 2 } → {1, 2} and π 2 : {τ 1 , τ 2 } → {2, 1}. That is, the last release time ordering is τ 1 , τ 2 in π 1 , and the last release time ordering is τ 2 , τ 1 in π 2 . Now, we can use Lemma 1 based on π 1 and π 2 : 3 To demonstrate the impact of the last release time ordering, we use the original task indexes before applying π 1 or π 2 whenever referring to Example 4.
• For π 1 , the schedulability condition in Lemma 1 shows that task τ 3 in Example 4 can meet the deadline if
C 3 ≤ t 3 • (1 -U 1 -U 2 ) -(C 1 -U 1 (C 1 + C 2 ) + C 2 -U 2 C 2 ) = 0.3t 3 -2.8 = 8. • For π 2 , the schedulability condition in Lemma 1 shows that task τ 3 in Example 4 can meet the deadline if C 3 ≤ t 3 • (1 -U 2 -U 1 ) -(C 2 -U 2 (C 2 + C 1 ) + C 1 -U 1 C 1 ) = 0.3t 3 -2.6 = 8.2.
The immediate question is whether both C 3 ≤ 8 based on π 1 and C 3 ≤ 8.2 based on π 2 are safe. When t k = 36, the transformation in Example 1 in fact adopts the last release time ordering π 1 . Therefore, Lemma 1 is only safe under π 1 in this example. As a result, the test in Lemma 1 for the above example is only valid when we apply π 1 .
However, in practice, we usually do not know how these tasks are indexed according to the required last release in the window of interest. It may seem at first glance that we need to test all the possible orderings. Fortunately, with the following lemma, we can safely consider only one specific last release time ordering of the k -1 higher-priority tasks.
Lemma 2. The worst-case ordering π of the k -1 higherpriority tasks under the schedulability condition in Eq. ( 5) in Lemma 1 is to order the tasks in a non-increasing order of βiCi αiUi , in which 0 < α i and 0 < β i for any i = 1, 2, . . . , k -1, 0 < t k .
Proof: This lemma is proved by showing that the schedulability condition in Lemma 1, i.e., 1 -
k-1 i=1 α i U i - k-1 i=1 βiCi t k + αiUi( k-1 =i β C ) t k
, is minimized, when the k -1 higher-priority tasks are indexed in a non-increasing order of βiCi αiUi . Suppose that there are two adjacent tasks τ h and τ h+1 with
β h C h α h U h < β h+1 C h+1 α h+1 U h+1 . Let us now examine the difference of k-1 i=1 αiUi( k-1 =i β C ) t k
by swapping the index of task τ h and task τ h+1 .
It can be easily observed that the other tasks τ i with i = h and i = h + 1 do not change their corresponding values α i U i ( k-1 =i β C ) in both orderings (before and after swapping τ h and τ h+1 ). The difference in the term
α h U h ( k-1 =h β C )+α h+1 U h+1 ( k-1 =h β C
) before and after swapping tasks τ and τ +1 (before -after) is
((α h U h β h+1 C h+1 -α h+1 U h+1 β h C h ) =α h α h+1 U h U h+1 β h+1 C h+1 α h+1 U h+1 - β h C h α h U h > 0.
Therefore, we reach the conclusion that swapping τ h and τ h+1 in the ordering makes the schedulabilty condition more stringent. By applying the above swapping repetitively, we reach the conclusion that ordering the tasks in a non-increasing order of βiCi αiUi has the most stringent schedulability condition in Eq. ( 5).
We again use the configuration in Example 4 to demonstrate the rationale behind Lemma 2. In this example, let us consider that t 3 = T 3 = 23. When t k = 23, the transformation in Example 1 in fact adopts the last release time ordering π 2 , i.e., τ 3 is schedulable if C 3 ≤ 0.3t 3 -2.6 = 4.3. The schedulability condition based on the last release time ordering π 1 , i.e., τ 3 is schedulable if C 3 ≤ 0.3t 3 -2.8 = 4.1, is always worse than that based on π 2 by Lemma 2. Therefore, it is always safe to use π 1 , even though it can be sometimes more pessimistic, e.g., when t 3 is 23.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Different Utilization Bounds<h2>text</h2>The analysis in Lemma 1 uses the execution time and the utilization of the tasks in hp(τ k ) to build an upper bound of C k /t k for schedulability tests. It is also very convenient in real-time systems to build schedulability tests only based on utilization of the tasks. We explain how to achieve that in the following lemmas under the assumptions that 0 < α i ≤ α, and 0 < β i C i ≤ βU i t k for any i = 1, 2, . . . , k -1. These lemmas are useful when we are interested to derive utilization bounds, speed-up factors, resource augmentation factors, etc., for a given scheduling policy by defining the coefficients α and β according to the scheduling policies independently from the detailed parameters of the tasks. Since the property repeats in all the statements, we make a formal definition before presenting the lemmas. Definition 4. Lemmas 3 to 5 are based on the following kpoint last-release schedulability test of a scheduling algorithm, defined in Definition 2, in which 0 < α i ≤ α, and 0
< β i C i ≤ βU i t k for any i = 1, 2, . . . , k -1, 0 < t k , α k-1 i=1 U i ≤ 1, and β k-1 i=1 U i ≤ 1. Lemma 3.
For a given k-point last-release schedulability test of a scheduling algorithm, with the properties in Definition 4, task τ k is schedulable by the scheduling algorithm if the following condition holds
C k t k ≤1 -(α + β) k-1 i=1 Ui + αβ k-1 i=1 Ui( k-1 =i U )(16)
=1 -(α + β) k-1 i=1 Ui + 0.5αβ ( k-1 i=1 Ui) 2 + ( k-1 i=1 U 2 i )(17)
Proof: The condition in Eq. ( 16) comes by reformulating the proof of Lemma 1 with βU i t * k instead of β i C i . All the procedures remain the same, and, therefore, β i C i for task τ i in the right-hand side of Eq. ( 5) can be replaced by βU i .
We focus on the condition in Eq. ( 17) by showing that
k-1 i=1 U i ( k-1 =i U ) = 0.5 ( k-1 i=1 U i ) 2 + ( k-1 i=1 U 2 i ) . This condition clearly holds when k = 2 since U 2 1 = 0.5(U 2 1 + U 2 1
). We consider k ≥ 3. This is due to
k-1 i=1 U i ( k-1 =i U ) = k-1 i=1 U 2 i + k-2 i=1 U i ( k-1 =i+1 U ) = 1 k-1 i=1 U 2 i + 0.5   k-1 i=1 U i 2 - k-1 i=1 U 2 i   = 0.5 ( k-1 i=1 U i ) 2 + ( k-1 i=1 U 2 i ) ,
where = 1 follows from the fact
k-2 i=1 U i ( k-1 =i+1 U ) = k-1 i=2 U i ( i-1 =1 U ) = 0.5 k-1 i=1 U i 2 - k-1 i=1 U 2 i .
Lemma 3 provides a schedulability test based on a quadratic form by using only the utilization of the higherpriority tasks with the properties in Definition 4. The following two lemmas are applicable for testing the utilization bound(s), i.e., the summation of the task utilization.
Lemma 4. For a given k-point last-release schedulability test of a scheduling algorithm, with the properties in Definition 4, task τ k is schedulable by the scheduling algorithm if
k-1 i=1 Ui ≤ k -1 k   α + β -(α + β) 2 -2αβ(1 -C k t k ) k k-1 αβ   .(18)
Proof: This can be formally proved by using the Lagrange Multiplier Method. However, it can also be proved by using a simpler mathematical observation. Suppose that x = k-1 i=1 U i is given. For given α, β, and x, we know that Eq. ( 17) becomes 1 -
(α + β)x + 0.5αβ(x 2 + k-1 i=1 U 2 i ). That is, only the last term 0.5αβ( k-1 i=1 U 2 i ) depends on how U i values are actually assigned. Moreover, k-1 i=1 U 2 i is a well-known convex function with respect to U 1 , U 2 , . . . , U k-1 . That is, ρU 2 i + (1 -ρ)U 2 j ≥ (ρU i + (1 -ρ)U j ) 2 for any 0 ≤ ρ ≤ 1.
Therefore,
k-1 i=1 U 2 i is minimized when U 1 = U 2 = • • • = U k-1 = x k-1 .
Hence, what we have to do is to find the infimum x such that the condition in Eq. ( 17) does not hold. That is,
infimum x s. t. C k t k > 1 -(α + β)x + 0.5αβ x 2 + x 2 k -1 .
This means that as long as
k-1
i=1 U i is no more than such infimum x, the condition in Eq. ( 17) always holds and the schedulability can be guaranteed. Provided that C k t k is given, we can simply solve the above problem by finding the x with
0 = 1 -C k t k -(α + β)x + 0.5αβ k k-1 x 2 .
There are two roots in the above quadratic equation. The smaller root, i.e., the righthand side of Eq. ( 18), is the infimum by definition.
Lemma 5. For a given k-point last-release schedulability test of a scheduling algorithm, with the properties in Definition 4, provided that α + β ≥ 1, then task τ k is schedulable by the scheduling algorithm if
C k t k + k-1 i=1 Ui ≤          k-1 k   α + β -(α + β) 2 -2αβ k k-1 αβ   , if k > (α+β) 2 -1 α 2 +β 2 -1 and α 2 + β 2 > 1 1 + (k-1)((α+β-1)-1 2 (α+β) 2 +0.5) kαβ otherwise(19)
Proof: The proof is similar to the proof of Lemma 4, but slightly more involved. We detail the proof in Appendix A.
By the fact that
(α + β) 2 -2αβ k k-1 = (α + β) 2 -2αβ -2αβ 1 k-1
, which is an increasing function with respect to k, and the fact that k-1 k is a decreasing function with respect to k, we know that the right-hand side of Eq. ( 19) (when α 2 + β 2 > 1) decreases with respect to k. Similarly, the right-hand side of Eq. ( 18) also decreases with respect to k. Therefore, for evaluating the utilization bounds, it is alway safe to take k → ∞ as a safe upper bound. The right-hand side of Eq. ( 18) converges to
α+β-α 2 +β 2 +2αβ C k t k αβ when k → ∞. The right-hand side of Eq. (19) (when α 2 + β 2 > 1) converges to α+β- √ α 2 +β 2 αβ when k → ∞.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Response Time Analysis Framework<h2>text</h2>We now further discuss the utilization-based response-time analysis framework. Lemma 6. For a given k-point response time analysis, defined in Definition 3, of a scheduling algorithm, in which 0
< α i ≤ α, 0 < β i ≤ β for any i = 1, 2, . . . , k -1, 0 < t k and k-1 i=1 α i U i < 1, the response time to execute C k for task τ k is at most C k + k-1 i=1 β i C i - k-1 i=1 α i U i ( k-1 =i β C ) 1 - k-1 i=1 α i U i . (20
)
Proof: The proof is similar to the proof of Lemma 1. The detailed proof is in Appendix A. Not all the last release time orderings are safe for the worstcase response time analysis. Fortunately, similar to Lemma 2, we can safely consider only one specific last release time ordering of the k -1 higher-priority tasks as shown in the following lemma.
Lemma 7. The worst-case ordering π of the k -1 higherpriority tasks under the response bound in Eq. (20) in Lemma 6 is to order the tasks in a non-increasing order of βiCi αiUi , in which 0 < α i and 0 < β i for any i = 1, 2, . . . , k -1, 0 < t k .
Proof: The ordering of the k -1 higher-priority tasks in the indexing rule only matters for the term
k-1 i=1 α i U i ( k-1 =i β C
), which was already proved in the proof of Lemma 2 to be minimized by ordering the tasks in a non-increasing order of βiCi αiUi . Clearly, the minimization of
k-1 i=1 α i U i ( k-1 =i β C
) also leads to the maximization of Eq. (20), which concludes the proof.
As a result, thanks to the help of Lemma 7, we can conclude that π 1 in the example in this subsection is a safe last release time ordering to use Lemma 6 for the worst-case response time analysis.<h2>publication_ref</h2>['b19']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Applications by Using Sporadic Task Models<h2>text</h2>This section demonstrates how to use the k 2 Q framework to derive utilization-based schedulability and responsetime analyses for sporadic task systems in uniprocessor and multiprocessor systems. As sporadic real-time task models are the simplest scenarios that can demonstrate how to use k 2 Q, the content here is merely for explaining how to use the framework, but not for demonstrating the generality or superiority of k 2 Q.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Uniprocessor Constrained-Deadline Systems<h2>text</h2>Theorem 1. Task τ k in a sporadic task system with constrained deadlines is schedulable by the fixed-priority scheduling algorithm if
k-1 i=1 Ci D k ≤ 1 and C k D k ≤ 1 - k-1 i=1 U i - k-1 i=1 C i D k + k-1 i=1 U i ( k-1 =i C ) D k ,(21)
in which the k -1 higher-priority tasks in hp 1 (τ k ) are indexed in a non-increasing order of T i .
Proof: This comes from Lemma 1 and 2 based on the setting α i = 1 and β i = 1 to satisfy Definition 2. 4Theorem 2. Task τ k in a sporadic constrained-deadline task system with is schedulable by the rate-monotonic (RM) scheduling algorithm if
C k D k ≤ 1 -2 k-1 i=1 U i + 0.5 ( k-1 i=1 U i ) 2 + ( k-1 i=1 U 2 i )(22)
or
k-1 i=1 U i ≤ k -1 k   2 -4 - 2k(1 -C k D k ) k -1   (23
)
or
C k D k + k-1 i=1 U i ≤ k-1 k 2 -4 -2k k-1 if k > 3 1 -k-1 2k if k ≤ 3(24)
Proof: Under RM scheduling, we know that
C i = U i T i ≤ U i T k .
Therefore, α can be set to 1 and β can be set to 1 in Definition 4. Eq. ( 22) is due to Lemma 3, Eq. ( 23) is due to Lemma 4, and Eq. ( 24) is due to Lemma 5.
The above result in Theorem 2 leads to the utilization bound 2-√ 2 for implicit-deadline sporadic task systems under RM scheduling. This analysis is less precise than the Liu and Layland bound ln 2 ≈ 0.693, a simple implication by using k 2 U. However, if we are allowed to change the execution time and period of a task for different job releases (called acyclic task model in [1]), then the tight utilization bound 2 -√ 2 can be easily achieved by using k 2 Q, detailed in Appendix F.<h2>publication_ref</h2>['b0']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Uniprocessor Arbitrary-Deadline Systems<h2>text</h2>For a specified fixed-priority scheduling algorithm, let hp(τ k ) be the set of tasks with higher priority than τ k . We now classify the task set hp(τ k ) into two subsets: <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>TABLE I:<h2>text</h2>The αi and βi parameters in our demonstrated task models.
• hp 1 (τ k ) consists of the higher-priority tasks with periods smaller than D k . • hp 2 (τ k ) consists of the higher-priority tasks with periods larger than or equal to D k .
The exact schedulability analysis for arbitrary-deadline task sets under fixed-priority scheduling has been developed in [34]. The schedulability analysis is to use a busy-window concept to evaluate the worst-case response time. That is, we release all the higher-priority tasks together with task τ k at time 0 and all the subsequent jobs are released as early as possible by respecting to the minimum inter-arrival time. The busy window finishes when a job of task τ k finishes before the next release of a job of task τ k . It has been shown in [34] that the worstcase response time of task τ k can be found in one of the jobs of task τ k in the busy window.
For the h-th job of task τ k in the busy window, the finishing time R k,h is the minimum t such that
hC k + k-1 i=1 t T i C i ≤ t,
and, hence, its response time is R k,h -(h -1)T k . The busy window of task τ k finishes on the h-th job if R k,h ≤ hT k .
We can create a virtual sporadic task τ k with execution time
C k = D k T k C k + τi∈hp2(τ k ) C i , relative deadline D k = D k ,
and period T k = D k . For notational brevity, suppose that there are k * -1 tasks in hp 1 (τ k ). We have then the following theorem.
Theorem 3. Task τ k in a sporadic task system is schedulable by the fixed-priority scheduling algorithm if
k * -1 i=1 Ci D k ≤ 1 and C k D k ≤ 1- k * -1 i=1 U i - k * -1 i=1 C i D k + k * -1 i=1 U i ( k * -1 =i C ) D k ,(25)
in which C k = D k T k C k + τi∈hp2(τ k ) C i ,
and the k * -1 higher-priority tasks in hp 1 (τ k ) are indexed in a nondecreasing order of
D k Ti -1 T i .
Proof: The analysis is based on the observation to test whether the busy window can finish within interval length D k , which was also adopted in [22] and [20]. By setting t i = D k Ti -1 T i , and indexing the tasks in a non-decreasing order of t i leads to the satisfaction of Definition 2 with α i = 1 and β i = 1.
Analyzing the schedulability by using Theorem 3 can be good if D k T k is small. However, as the busy window may be stretched when D k T k is large, we further present how to safely estimate the worst-case response time. Suppose that t j = R k,h Tj -1 T j for a higher-priority task τ j . We index the tasks such that the last release ordering π of the k -1 higher-priority tasks is with t j ≤ t j+1 for j = 1, 2, . . . , k -2. Therefore, we know that R k,h is upper bounded by finding the maximum
t k = hC k + k-1 i=1 t i U i + k-1 i=1 C i ,(26)
with 0 ≤ t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ t k and hC k + k-1 i=1 t i U i + j-1 i=1 C i > t j ,∀j = 1, 2, . . . , k -1.(27)
Therefore, the above derivation of R k,h satisfies Definition 3 with α i = 1, and β i = 1 for any higher-priority task τ i . However, it should be noted that the last release time ordering π is actually unknown since R k,h is unknown. Therefore, we have to apply Lemma 7 for such cases to obtain the worst-case release time ordering, i.e., the k -1 higher-priority tasks are ordered in a non-increasing order of their periods.
Lemma 8. Suppose that
k-1 i=1 U i ≤ 1.
Then, for any h ≥ 1 and C k > 0, we have
R k,h ≤ hC k + k-1 i=1 C i - k-1 i=1 U i ( k-1 =i C ) 1 - k-1 i=1 U i , (28
)
where the k -1 higher-priority tasks are ordered in a nonincreasing order of their periods.
Proof: This comes from the above discussions with α i = 1, β i = 1 by applying Lemmas 6 and 7 when
k-1 i=1 U i < 1. The case when k-1 i=1 U i = 1 has a safe upper bound R k,h = ∞ in Eq. (28). Theorem 4. Suppose that k i=1 U i ≤ 1. The worst-case response time of task τ k is at most R k ≤ C k + k-1 i=1 C i - k-1 i=1 U i ( k-1 =i C ) 1 - k-1 i=1 U i , (29
)
where the k -1 higher-priority tasks are ordered in a nonincreasing order of their periods.
Proof: This can be proved by showing that R k,h -(h -1)T k is maximized when h is 1, where R k,h is derived by using Lemma 8. The first-order derivative of R k,h -(h -1)T k with respect to h is
C k 1-k-1 i=1 Ui -T k = C k -(1-k-1 i=1 Ui)T k 1-k-1 i=1 Ui
. There are two cases:
Case 1: If k i=1 U i < 1, then C k -(1-k-1 i=1 Ui)T k 1-k-1 i=1 Ui < C k -U k T k 1-k-1 i=1 Ui = 0. Therefore, R k,h -(h -1)
T k is a decreasing function of h. Therefore, the response time is maximized when h is 1.
Case 2: If k i=1 U i = 1, then we know that C k -(1-k-1 i=1 Ui)T k 1-k-1 i=1 Ui = 0. Therefore, R k,h -(h -1)T k remains the same regardless of h.
Therefore, for both cases, the worst-case response time of task τ k can be safely bounded by Eq. ( 29). Moreover, since the worst case happens when h = 1, we do not have to check the length of the busy window, and we reach our conclusion.
Corollary 1. Task τ k in a sporadic task system is schedulable by the fixed-priority scheduling algorithm if
k i=1 U i ≤ 1 and C k D k ≤ 1 - k-1 i=1 U i - k-1 i=1 C i D k + k-1 i=1 U i ( k-1 =i C ) D k ,(30)
where the k -1 higher-priority tasks are ordered in a nonincreasing order of their periods.<h2>publication_ref</h2>['b33', 'b33', 'b21', 'b19']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Remarks:<h2>text</h2>The utilization-based worst-case response-time analysis in Theorem 4 is analytically tighter than the best known result, R k ≤
C k + k-1 i=1 Ci-k-1 i=1 UiCi 1-k-1 i=1 Ui
, by Bini et al. [14]. Lehoczky [34] also provides the total utilization bound of RM scheduling for arbitrary-deadline systems. The analysis in [34] is based on the Liu and Layland analysis [39]. The resulting utilization bound is a function of ∆ = max τi { Di Ti }. When ∆ is 1, it is an implicit-deadline system. The utilization bound in [34] has a closed-form when ∆ is an integer. However, calculating the utilization bound for non-integer ∆ is done asymptotically for k = ∞ with a complicated analysis.<h2>publication_ref</h2>['b13', 'b33', 'b33', 'b38', 'b33']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Multiprocessor Implicit-Deadline Systems<h2>text</h2>We now present how to use k 2 Q to analyze the schedulability for implicit-deadline sporadic task systems under global rate-monotonic (global RM) scheduling. Here, we start from the pseudo-polynomial-time schedulability test by Guan et al. [29] that we only have to consider M -1 tasks with carryin jobs, for constrained-deadline (hence, also for implicitdeadline) task sets. More precisely, we can define two different time-demand functions, depending on whether task τ i is with a carry-in job or not: 5
W carry i (t) = C i 0 < t < C i C i + t-Ci Ti C i otherwise,(31)
and
W normal i (t) = t T i C i .(32)
Moreover, we can further over-approximate W carry i (t), since W carry i (t) ≤ W normal i (t)+C i . Therefore, a sufficient schedulability test for testing task τ k with k > M for global RM is to verify whether
∃0 < t ≤ T k , C k + ( τi∈T C i ) + ( k-1 i=1 W normal i (t)) M ≤ t. (33
)
5 This is an over-approximation of the linear function used by Guan et al. [29].
for all T ⊆ hp(τ k ) with |T | = M -1.
This leads to the following theorem by using Lemma 1.
Theorem 5. Task τ k in a sporadic implicit-deadline task system is schedulable by global RM on M processors if
k-1 i=1 C i ≤ M T k and U k ≤ 1- τ i ∈T Ci M T k - k-1 i=1 Ui M - k-1 i=1 Ci M T k + k-1 i=1 (Ui k-1 =i C ) M 2 T k . (34
)
by indexing the k -1 higher-priority tasks in a non-decreasing order of ( T k Ti -1)T i for every τ i ∈ hp(τ k ) and by putting the M -1 higher-priority tasks with the largest execution times into T . Proof: It is not necessary to enumerate all T ⊆ T with |T | = M -1 if we can construct the task set T ⊆ hp(τ k ) with the maximum τi∈T C i . To use k 2 Q, we are certain about which tasks should be put into the carry-in task set T by assuming that C i and T i are both given. That is, we simply have to put the M -1 higher-priority tasks with the largest execution times into T . This can be imagined as if we increase the execution time of task
τ k from C k to C k = C k + τ i ∈T Ci M . Moreover, we have α i = 1 M and β i = 1 M for every task τ i ∈ hp(τ k ) in this case.
Therefore, based on the test in Eq. ( 33), we have the last release time ordering defined by indexing the k -1 higherpriority tasks in a non-decreasing order of ( T k Ti -1)T i for every τ i ∈ hp(τ k ). By adopting Lemma 1 with α i = 1 M and
β i = 1 M , we know that task τ k is schedulable by global RM if k-1 i=1 Ci M ≤ T k and C k + τ i ∈T C i M T k ≤ 1- k-1 i=1 Ui M - k-1 i=1 Ci M T k + k-1 i=1 (Ui k-1 =i C ) M 2 T k .(35)
By reorganizing the above inequality, we reach the conclusion.
We can always take the pessimistic last release time ordering in Lemma 2, for concluding the following theorem. Theorem 6. Task τ k in a sporadic implicit-deadline task system is schedulable by global RM on M processors if the condition in Eq. (34) holds by indexing the k-1 higher-priority tasks in a non-increasing order of T i , for every τ i ∈ hp(τ k ).
Proof: This is proved based on the same argument in Theorem 5 by adopting Lemmas 1 and 2.
We can of course revise the statement in Theorems 5 and 6 by adopting Lemma 3 and Lemma 4 to construct schedulability tests by using only the utilization of the higher-priority tasks.<h2>publication_ref</h2>['b28', 'b28']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Evaluation Results<h2>text</h2>We conduct experiments using synthesized task sets for evaluating the tests in Theorem 5 and Theorem 6. We first generated a set of sporadic tasks. The cardinality of the task set was 5 times the number of processors, i.e., 40 tasks on 8 multiprocessor systems. The UUniFast-Discard method [23] was adopted to generate a set of utilization values with the given goal. We used the approach suggested by Davis et al. [25] to generate the task periods according to a uniform distribution in the range of the logarithm of the task periods (i.e., log-uniform distribution). The order of magnitude p to control the period values between the largest and smallest periods is parameterized in evaluations, (e.g., 1 -10ms for p = 1, 1 -100ms for p = 2, etc.). We evaluate these tests in uniprocessor systems with p ∈ [1,2,3]. The execution time was set accordingly, i.e., C i = T i U i . Tasks' relative deadlines were equal to their periods.
The evaluated tests for n tasks in T with n ≥ M are:
• BCL: the linear-time test in Theorem 4 in [11].
• FF: the pseudo-polynomial-time forced-forward (FF) analysis in Eq. ( 5) in [7]. • BAK: the O(n 3 ) test in Theorem 11 in [5].
• Guan: the pseudo-polynomial-time response time analysis [29]. 34) in Theorem 5. This requires to sort the higher-priority tasks to define the proper last release ordering and the M -1 carry-in jobs; therefore, the time complexity is O(n 2 log n) for a task set with n tasks. • QB-BC2 (from k 2 Q): Eq. ( 34) in Theorem 6 by always using the worst-case release time ordering, which is the reverse order of the given priority assignment. The schedulability test can be implemented in O(n log M ) time complexity by using proper data structures, provided that the RM priority order is given. 6   Figure 2 depicts the result of the performance comparison. In all the cases, we can see that QB-BC is superior to all the other polynomial-time tests. QB-BC2 is slightly worse than QB-BC but the time complexity is lower. Since QB-BC and QB-BC2 are designed from a more pessimistic test than the analysis by Guan et al. [29] in pseudo-polynomial time, they are worse. But, we note that there is a significant gap in time complexity between QB-BC, QB-BC2, and Guan. Overall, the tests derived by using the k 2 Q framework perform reasonably well with their low time complexity.
• QB-BC (from k 2 Q): Eq. (<h2>publication_ref</h2>['b22', 'b24', 'b0', 'b1', 'b2', 'b10', 'b6', 'b4', 'b28', 'b28']<h2>figure_ref</h2>['fig_3']<h2>table_ref</h2>[]<h2>heading</h2>Conclusion and Extensions<h2>text</h2>In this paper, we present a general response-time analysis and schedulability-test framework, called k 2 Q. Thanks to the independence upon the task and platform models in the framework, k 2 Q can be viewed as a "block-box" interface that can result in sufficient utilization-based analyses for a wide range of applications in real-time systems under fixedpriority scheduling. We believe that the k 2 Q framework has high potential to be adopted to solve several other problems for analyzing other task models in real-time systems with fixedpriority scheduling. The framework can be used, once the 6 The time complexity is mainly due to the calculation of T to get the M -1 tasks with the maximum carry-in execution time since the other operations can be done in O(1) time complexity by using proper data structures to calculate the values when we intend to test task τ k+1 after task τ k . Specifically, due to the predefined last release time ordering, when we intend to test task τ k+1 after task τ k , we only have to insert task τ k to be indexed as 1 and updating
from k-1 i=1 (U i k-1 =i C ) M 2 T k to k i=1 (U i k =i C ) M 2 T k
(under the new ordering) takes only constant time complexity. Finding task set T can be implemented by using a min heap to store the M -1 tasks in T . When we move from testing task τ k (when k ≥ M ) to task τ k+1 , we need to compare whether C k is larger than the minimum execution time of the tasks in the heap. If no, we keep the same task set T ; if yes, we pop out the task with the minimum execution time in the heap, and insert task τ k into the heap. By using the heap, this operation requires time complexity O(log M ). Calculating C k+1 from C k with the help of the heap can be done in O(1) time complexity.
corresponding k-point last-release scheduling test or response time analysis can be constructed. Moreover, our proposed frameworks, k 2 U and k 2 Q, provide a solid mathematical foundation for deriving polynomialtime utilization-based schedulability tests and response time analyses almost automatically. That is, utilization-based analyses are almost automatically derived if the schedulability tests can be formulated in the scope of the frameworks. We have demonstrated several applications in this paper. Some models have introduced pretty high dynamics, but we can still handle the response time analysis and schedulability test with proper constructions so that the k 2 Q framework is applicable. Therefore, with the presented approach, some difficult schedulability test and response time analysis problems may be solved by building a good (or exact) exponential-time test and using the approximation in the k 2 Q framework. With the quadratic and hyperbolic expressions, k 2 Q and k 2 U frameworks can be used to provide many quantitive features to be measured, like the total utilization bounds, speed-up factors, etc., not only for uniprocessor scheduling but also for multiprocessor scheduling.
When adopting k 2 Q for schedulability tests, we assume that t k is specified in Lemma 1. In this paper, we do not explore how to configure the best value of t k and its last release time ordering π such that the resulting quadratic form is the best. Therefore, the combination of k 2 Q/k 2 U and the tunable approach by Bini and Buttazzo [12] can be an interesting future research direction, as this can potentially balance the schedulability test and the time complexity for concrete applications. Appendix A: Proofs Proof of Lemma 5. Similar to the proof of Lemma 4, we only have to consider the cases when U i is set to x k-1 to make the schedulability condition the most difficult, where x = k-1 i=1 U i . Suppose that C k t k is y. Then, we are looking for the infimum x + y such that y > 1 -(α + β)x + 0.5αβ(x 2 + x 2 k-1 ). To solve this, we start with y = 1-(α+β)x+0.5αβ(x 2 + x 2 k-1 ). Our objective becomes to minimize
H(x) = x + 1 - (α + β)x + 0.5αβ(x 2 + x 2 k-1 ). By finding dH(x) dx = 1 -(α + β) + kαβx k-1 = 0, we know that x = (k-1)(α+β-1)<h2>publication_ref</h2>['b11']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>kαβ<h2>text</h2>. Therefore,
y =1 -x α + β - 0.5kαβ (k-1)(α+β-1) kαβ k -1 =1 - (k -1)(α + β -1) kαβ (0.5(α + β + 1)) =1 - 0.5(k -1) (α + β) 2 -1 kαβ . (36
)
Since α+β ≥ 1, we know that x ≥ 0. Whether we should take the above solution only depends on whether y ≥ 0 or not. If y ≥ 0, then we can conclude the solution directly; otherwise, if y < 0, we should set y to 0. That is, by reorganizing Eq. ( 36) (under the assumption α > 0 and β > 0), examining whether 2 -1 due to the assumption α+β ≥ 1. Therefore, there are two cases:
y < 0 is equivalent to testing (1 -1 k ) (α + β) 2 -1 > 2αβ, which implies to test whether α 2 +β 2 -1 > 1 k (α + β) 2 -1 . If α 2 + β 2 ≤ 1, then y ≥ 0 since α 2 + β 2 -1 ≤ 0 ≤ 1 k (α + β)
Case 1: If α 2 + β 2 > 1 and k > (α+β) 2 -1 α 2 +β 2 -1
, then, for such a case y derived from Eq. ( 36) is negative. We should set y to 0. The remaining procedure here is the same as in solving the quadratic equation in the proof of Lemma 4 by setting C k t k to 0. This leads to the first condition in Eq. (19).
Case 2: If α 2 + β 2 ≤ 1 or k ≤ (α+β) 2 -1 α 2 +β 2 -1
, then, we have the conclusion that y ≥ 0 and x ≥ 0. We just have to sum up the above derived x and y. This leads to the second condition in Eq. ( 19) directly.
Proof of Lemma 6. Definition 3 leads to the following optimization problem:
sup C k + k-1 i=1 α i t * i U i + k-1 i=1 β i C i (37a) such that C k + k-1 i=1 α i t * i U i + j-1 i=1 β i C i > t * j , ∀j = 1, . . . , k -1, (37b) t * j ≥ 0, ∀j = 1, . . . , k -1,(37c)
where t * 1 , t * 2 , . . . , t * k-1 and are variables, α i , β i , U i , C i for higher-priority task τ i and C k are constants. For the rest of the proof, we replace > with ≥ in Eq. ( 37), as the supermum and the maximum are the same when presenting the inequality with ≥. We can also further drop the condition t * j ≥ 0, which just makes the resulting solution more pessimistic. This results in the following linear programming, which has a safe upper bound of Eq. ( 37),
maximize C k + k-1 i=1 α i t * i U i + k-1 i=1 β i C i (38a) such that C k + k-1 i=1 α i t * i U i + j-1 i=1 β i C i ≥ t * j , ∀j = 1, . . . , k -1. (38b)
The linear programming in Eq. ( 38) (by replacing > with ≥ and supremum with maximum) has k -1 variables and k -1 constraints. Like the proof of Lemma 1, we again adopt the extreme point theorem for linear programming [40] to solve the linear programming. Suppose that t
† 1 , t † 2 , . . . , t † k-1
is a feasible solution for the linear programming in (38) and
t = max{t † 1 , t † 2 , . . . , t † k-1 }.
By the satisfaction of Eq. (38b), we know that
C k + t k-1 i=1 α i U i + k-1 i=1 β i C i ≥ C k + k-1 i=1 α i t † i U i + k-1 i=1 β i C i ≥ t.
As a result, we have t ≤
C k + k-1 i=1 βiCi 1-k-1 i=1 αiUi . That is, any feasible solution of Eq. (38) has t * j ≤ C k + k-1 i=1 βiCi 1-k-1 i=1 αiUi for any j = 1, 2, . . . , k -1. Under the assumption that k-1 i=1 α i U i < 1 and 0 ≤ k-1 i=1 β i C i ,
the above linear programming has a bounded objective function.
The only extreme point solution is to put
C k + k-1 i=1 α i t * i U i + j-1 i=1 β i C i = t * j for every j = 1, 2, . . . , k -1.
Since the objective function is bounded, by the extreme point theorem [40], we know that this extreme point solution is the optimal solution for the linear programming in Eq. (38). For such a solution, we know that
∀j = 2, 3, . . . , k -1, t * j -t * j-1 = β j-1 C j-1 .(39)
and
t * 1 = C k + k-1 i=1 α i U i (t * 1 + i-1 =0 β C ),(40)
where β 0 and C 0 are defined as 0 for notational brevity. Therefore, we know that
t * 1 = C k + k-1 i=1 α i U i ( i-1 =0 β C ) 1 - k-1 i=1 α i U i .(41)
Clearly, the above extreme point solution is always feasible when
k-1 i=1 α i U i < 1.
Therefore, in this extreme point solution, the objective function of the linear programming is
t * 1 + k-1 i=1 β i C i = C k + k-1 i=1 α i U i ( i-1 =0 β C ) 1 -k-1 i=1 α i U i + k-1 i=1 β i C i(42)
= C k + k-1 i=1 β i C i -k-1 i=1 α i U i ( k-1 =i β C ) 1 -k-1 i=1 α i U i(43)
which concludes the proof.
uniprocessor systems, the time-demand analysis (TDA) developed in [35] can be adopted. That is, if
∃t with 0 < t ≤ D k and C k + τi∈hp(τ k ) t T i C i ≤ t,(44)
then task τ k is schedulable under the fixed-priority scheduling algorithm, where hp(τ k ) is the set of tasks with higher priority than τ k , D k , C k , and T i represent τ k 's relative deadline, worst-case execution time, and period, respectively. For a constrained-deadline task τ k , the schedulability test in Eq. ( 44) is equivalent to the verification of the existence of 0 < t ≤ D k such that
C k + τi∈hp2(τ k ) C i + τi∈hp1(τ k ) t T i C i ≤ t.(45)
We can then create a virtual sporadic task τ k with execution time
C k = C k + τi∈hp2(τ k ) C i , relative deadline D k = D k , and period T k = D k .
It is clear that the schedulability test to verify the schedulability of task τ k under the interference of the higher-priority tasks hp 1 (τ k ) is the same as that of task τ k under the interference of the higher-priority tasks hp(τ k ).
For notational brevity, suppose that there are k * -1 tasks in hp 1 (τ k ).
Theorem 7. Task τ k in a sporadic task system with constrained deadlines is schedulable by the fixed-priority scheduling algorithm if
k * -1 i=1 Ci D k ≤ 1 and C k D k ≤ 1- k * -1 i=1 U i - k * -1 i=1 C i D k + k * -1 i=1 U i ( k * -1 =i C ) D k ,(46)
in which the k * -1 higher-priority tasks in hp 1 (τ k ) are indexed in a non-decreasing order of
D k Ti -1 T i .
Proof:
Setting t i = D k Ti
-1 T i , and indexing the tasks in a non-decreasing order of t i leads to the satisfaction of Definition 2 with α i = 1 and β i = 1.
Corollary 2. Task τ k in a sporadic task system with implicit deadlines is schedulable by the RM scheduling algorithm if Lemmas 1, 3, 4, or 5 holds by setting C k t k as U k , α = 1, and β = 1.
The above result in Corollary 2 leads to the utilization bound 2 -√ 2 (by using Lemma 5 with α = 1 and β = 1) for RM scheduling, which is worse than the existing Liu and Layland bound ln 2 [39].<h2>publication_ref</h2>['b18', 'b39', 'b37', 'b39', 'b37', 'b34', 'b38']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Appendix C: Multiprocessor DM/RM Scheduling<h2>text</h2>This part demonstrates how to use the k 2 Q framework for multiprocessor global fixed-priority scheduling. We consider that the system has M identical processors. For global fixedpriority scheduling, there is a global queue and a global scheduler to dispatch jobs. We demonstrate the applicability for constrained-deadline and implicit-deadline sporadic systems under global fixed-priority scheduling. Specifically, we will present how to apply the framework to obtain speed-up and capacity augmentation factors for global DM and global RM.
The success of the scheme depends on a corresponding exponential-time test. Here we will use the property to be presented in Lemma 9, based on the forced-forward algorithm proposed by Baruah et al. [7] to characterize the workload of higher-priority tasks. The method in [7] to analyze fixedpriority scheduling is completely different from ours, as they rely on the demand bound functions of the tasks.
The following lemma provides a sufficient test based on the observations by Baruah et al. [7]. The construction of the following lemma is based on a minor change of the forcedforward algorithm.
Lemma 9. Let ∆ max k be max k-1 j=1 {U j , C k D k }.
Task τ k in a sporadic task system with constrained deadlines is schedulable by a global fixed-priority (workload conserving) scheduling algorithm on M processors if
∀y ≥ 0, (∀0 ≤ ω i ≤ T i , ∀τ i ∈ hp(τ k )) , ∃t with 0 < t ≤ D k + y such that ∆ max k • (D k + y) + k-1 i=1 ω i • U i + t-ωi Ti C i M ≤ t.
Proof: This is proved by contrapositive. If task τ k is not schedulable by the global fixed-priority scheduling, we will show that there exist y ≥ 0 and 0
≤ ω i ≤ T i such that for all 0 < t ≤ D k + y, the condition ∆ max k • (D k + y) + k-1 i=1 ωi•Ui+ t-ω i T i Ci M > t holds.
The proof is mainly based on the forced-forward algorithm for the analysis of global DM by Baruah et al. in [7], by making some further annotations.
If τ k is not schedulable by global DM, let z 0 be the first time at which task τ k misses its absolute deadline, i.e., z 0 . Let z 1 be the arrival time of this job of task τ k . For notational brevity, let this job be J 1 , which arrives at time z 1 and has not yet been finished at time z 0 . By definition, we know that z 0 -z 1 is D k . Due to the fixed-priority and workload-conserving scheduling policy and the constraineddeadline setting, removing (1) all the other jobs of task τ k (except the one arriving at time z 1 ), (2) all the jobs arriving no earlier than z 0 , and (3) lower-priority jobs does not change the unschedulability of job J 1 . Therefore, the rest of the proof only considers the jobs from τ 1 , τ 2 , . . . , τ k . Now, we expand the window of interest by using a slightly different algorithm from that proposed in [7], also illustrated with the notation in Figure 3, as in Algorithm 1. The difference is only in the setting "strictly less than (z -1 -z ) • Û units", whereas the setting in [7] uses "strictly less than (z -1 -z ) • s units" for a certain s. For notationaly brevity, Û is the utilization of the task that generates job J .
Algorithm 1 (Revised) Forced-Forward Algorithm 1: for ← 2, 3, ... do 2:
let J denote a job that arrives at some time-instant z < z -1 ; -has an absolute deadline after z -1 ; -has not completed execution by z -1 ; and has executed for strictly less than (z -1 -z ) • Û units over the interval [z , z -1 ), where Û is the utilization of the task that generates job J .<h2>publication_ref</h2>['b6', 'b6', 'b6', 'b6', 'b6', 'b6']<h2>figure_ref</h2>['fig_5']<h2>table_ref</h2>[]<h2>heading</h2>3:<h2>text</h2>if there is no such a job then 4:
← ( -1); break;
Suppose that the forced-forward algorithm terminates with equals to * . We now examine the schedule in the interval (z * , z 0 ]. Since J 1 , J 2 , . . . , J * belong to τ 1 , τ 2 , . . . , τ k , we know that Û ≤ ∆ max k for = 1, 2, . . . , * . Let σ be the total length of the time interval over (z , z -1 ] during which J is executed. By the choice of J , it follows that
σ < (z -1 -z ) • Û ≤ (z -1 -z ) • ∆ max k .
Moreover, all the M processors execute other higher-priority jobs (than J ) at any other time points in the interval (z , z -1 ] at which J is not executed. Therefore, we know that the maximum amount of time from z * to z 0 , in which not all the M processors execute certain jobs, is at most
* =1 σ < * =1 (z -1 -z ) • ∆ max k = (z 0 -z * ) • ∆ max k .
Up to here, the treatment is almost identical to that in "Observation 1" in [7]. The following analysis becomes different as we do not intend to use the demand bound function. Now, we replace job J 1 with another job J 1 with inflated execution time in the above schedule, where J 1 is released at time z * with absolute deadline z 0 and execution time (z 0 -z * ) • ∆ max k . According to the above analysis, J 1 cannot be finished before z 0 in the above schedule. For each task τ i for i = 1, 2, . . . , k -1, in the above schedule, there may be one carry-in job, denoted as J i , of τ i (under the assumption of the schedulability of a higher-priority task τ i ) that arrives at time r i with r i < z * and r i + T i > z * . Let d i be the next released time of task τ i after z * , i.e., d i = r i + T i .
According to the termination condition in the construction of z * , if J i exists, we know that at least (z * -r i ) • U i amount of execution time has been executed before z * , and the remaining execution time of job J i to be executed after z * is at most (d i -z * ) • U i . If J i does not exist, then d i is set to z * for notational brevity. Therefore, the amount of workload W i (t) of all the released jobs of task τ i for i = 1, 2, . . . , k -1 to be executed in time interval (z * , z * + t) is at most
W i (t) = (d i -z * ) • U i + t -(d i -z * ) T i C i .(47)
The assumption of the unschedulability of job J 1 (due to the unschedulability of job J 1 ) under the global fixed-priority scheduling implies that J 1 cannot finish its computation at any time between z * and z 0 . This leads to the following
(necessary) condition ∆ max k (z 0 -z * ) + τ i ∈hp(τ k ) W i (t) M > t for all 0 < t ≤ z * -z 0 for the unschedulability of job J 1 .
Therefore, by the existence of y = z 1 -z * (with y ≥ 0) and ω i = d i -z * (with 0 ≤ ω i ≤ T i ) for i = 1, 2, . . . , k -1 to enforce the above necessary condition, we reach the conclusion of the proof by contrapositive. That is, task τ k is schedulable if, for all y ≥ 0 and any combination of 0 ≤ ω i ≤ T i for i = 1, 2, . . . , k -1, there exists 0
< t ≤ D k + y with ∆ max k (D k + y) + τ i ∈hp(τ k ) ωi•Ui+ t-ω i T i Ci M ≤ t.
The schedulability condition in Lemma 9 may look at the first glance strange. We briefly explain the logical meaning (but informally) here. Suppose we would like to know whether a job of task τ k arrived at time r k can be finished before/at time r k + D k . To better quantify the interference from the higher-priority tasks, we would like to account for the higherpriority jobs arrived prior to r k . The variable y defines the extension of the window of interest from [r k , r k + D k ) to [r k -y, r k +D k ). The variable ω i defines the maximum residual execution time ω i T i of a carry-in job of task τ i that arrives before r k -y and should be executed in the window of interest, i.e., [r k -y, r k + D k ). If the residual workload is at most ω i T i , the next job can be released at time r k -y + ω i , as shown in the proof. Task τ k is schedulable by the global fixedpriority scheduling, if, for any combinations of y ≥ 0 and 0 ≤ ω i ≤ T i , ∀τ i ∈ hp(τ k ), we can always finish the inflated workload ∆ max k • (D k + y) of task τ k and the higher-priority workload in the window of interest. For formal explanations, please refer to the formal proof of Lemma 9.
Note that the schedulability condition in Lemma 9 requires to test all possible y ≥ 0 and all possible settings of 0 ≤ ω i ≤ T i for the higher-priority tasks τ i with i = 1, 2, . . . , k -1. Therefore, it needs exponential time (for all the possible combinations of ω i ). 7 However, we are not going to directly use the test in Lemma 9 in the paper. We will only use this test to construct the corresponding k-point schedulability test under Definition 2.
We present the corresponding polynomial-time schedulability tests for global fixed-priority scheduling. More specifically, we will also analyze the capacity augmentation factors of these tests for global RM and global DM in Corollaries 3 and 4, respectively.
Theorem 8. Let U max k be max k j=1 U j . Task τ k in a sporadic task system with implicit deadlines is schedulable by global RM on M processors if
U max k ≤ 1 - 2 M k-1 i=1 U i + 0.5 M 2 ( k-1 i=1 U i ) 2 + ( k-1 i=1 U 2 i ) (48) or k-1 j=1 U j M ≤ k -1 k 2 -2 + 2U max k k k -1 .(49)
Proof: We will show that the schedulability condition in the theorem holds for all possible settings of y and ω i s. Suppose that y and ω i for i = 1, 2, . . . , k -1 are given, in which y ≥ 0 and 0 ≤ ω i ≤ T i . Let t k be T k + y. Now, we set t i to ω i + T k +y-ωi Ti T i for i = 1, 2, . . . , k -1 and reindex the tasks such that t 1 ≤ t 2 ≤ . . . ≤ t k . Therefore, if i < j, we know that
ω i • U i + tj -ωi Ti C i ≤ ω i • U i + ( ti-ωi Ti + 1)C i = t i U i + C i . If i ≥ j, then ω i • U i + tj -ωi Ti C i ≤ ω i • U i + ( ti-ωi Ti )C i = t i U i .
The sufficient schedulability condition in Lemma 9 under the given y and ω i s is to verify the existence of t j ∈ {t 1 , t 2 , . . . t k } such that
U max k (T k + y) + k-1 i=1 ω i • U i + tj -ωi Ti C i M (50) ≤U max k t k + k-1 i=1 U i t i + j-1 i=1 C i M ≤ t j .(51)
By the definition of global RM scheduling (i.e., T k ≥ T i ), we can conclude that 
C i = U i T i ≤ U i T k ≤ U i (T k +y) = U i t k for i = 1, 2, . . . ,
U max k t k + k-1 i=1 α i U i t i + j-1 i=1 βU i t k ≤ t j ,(52)
where α i = 1 M and β ≤ 1 M for i = 1, 2, . . . , k -1. Therefore, we reach the conclusion of the schedulability conditions in Eqs. ( 48) and (49) by Lemma 3 and Lemma 4 under given y and ω i s, respectively.
The schedulability test in Eq. ( 52) is independent from the settings of y and ω i s. However, the setting of y and ω i s affects how the k -1 higher-priority tasks are indexed. Fortunately, it can be observed that the schedulability tests in Eqs. ( 48) and (49) are completely independent upon the indexing of the higher-priority tasks. Therefore, no matter how y and ω i s are set, the schedulability conditions in Eqs. ( 48) and ( 49) are the corresponding results from the k 2 Q framework. As a result, we can reach the conclusion.
Corollary 3. The capacity augmentation factor of global RM for a sporadic system with implicit deadlines is 3+ 
∆ max k ≤ 1 - 1 M k-1 i=1 U i + C i D k + 1 M 2 k-1 i=1 U i ( k-1 =i C D k ) ,(53
) where the k -1 higher-priority tasks are ordered in a nonincreasing order of their periods.
Proof: This is due to a similar proof to that of Theorem 8 and Lemma 9, by applying Lemma 1 with
t k = D k + y, α i = 1
M , and β i = 1 M , under the worst-case last release time ordering, βiCi αiUi = T i non-increasingly, in Lemma 2. Therefore, for a given y ≥ 0, if
∆ max k ≤ 1 -1 M k-1 i=1 U i + Ci D k +y + 1 M 2 k-1 i=1 U i ( k-1 =i C D k +y )
, task τ k is schedulable by the global fixed-scheduling. By the assumption 8 This comes from the simple algebra property that for any two vectors a and b of size is minimized when y is 0. As a result, the above schedulability condition is the worst when y is 0.
k-1 i=1 U i ≤ M , we know that C i i =1 U M ≤ C i . Therefore 8 , - 1 M k-1 i=1 C i D k + y + 1 M 2 k-1 i=1 U i ( k-1 =i C D k + y ) = 1 D k + y 1 M - k-1 i=1 C i + k-1 i=1 C i i =1 U M
(k -1) there is k-1 i=1 a i k-1 =i b = k-1 i=1 b i i =1 a . x z J z -1 J -1 J3 z2 J2 z1 J1 z0 • • • • • • • • • •
Corollary 4. The capacity augmentation factor and the speedup factor of global DM by using Theorem 9 for a sporadic system with constrained deadlines is 3.
Proof: If k i=1 U i ≤ M or k-1 i=1 Ci D k ≤ M is violated,
the capacity augmentation factor is already 1. Therefore, we focus on the case that task τ k does not pass the schedulability condition in Eq. (53). That is,
∆ max k >1 - 1 M k-1 i=1 U i + C i D k + 1 M 2 k-1 i=1 U i ( k-1 =i C D k ) ≥1 - 1 M k-1 i=1 U i + C i D k .
This means that the unschedulability of task τ k under global DM implies that either
∆ max k > 1 3 , 1 M k-1 i=1 U i > 1 3 , or 1 M k-1 i=1 Ci D k > 1 3
, by the pigeonhole principle. Therefore, we conclude the factor 3.<h2>publication_ref</h2>['b6']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Remarks:<h2>text</h2>The utilization bound in Eq. ( 49) is analytically better than the best known utilization-based schedulability test
k j=1 U j ≤ M 2 (1-U max k )+U max k for global RM by Bertogna et al. [10], since (1-x) 2 ≤ 2 - √ 2 + 2x when 0 ≤ x ≤ 1.
The capacity augmentation factor 2.823 in Corollary 3 is weaker than the result 2.668 by Lundberg [41]. However, we would like to point out the incompleteness in the proof in [41]. In the proof of the extreme task set, the argument in Page 150 in [41] concludes that task τ n is more difficult to be schedulable due to the increased interference of task τ n-1 to task τ n after the transformation. The argument was not correctly proved and can be optimistic since the increased interference has to be analyzed in all time points in the analysis window, whereas the analysis in [41] only considers the interference in a specific interval length. Without analyzing the resulting interference in all time points in the analysis window, task τ n after transformation may still have chance to finish earlier due to the potential reduction of the interference at earlier time points.
The speed-up factor 3 provided in Corollary 4 is asymptotically the same as the result by Baruah et al. [7]. The speed-up factor 3 in [7] requires a pseudo polynomial-time test, whereas we show that a simple test in Eq. (53) can already yield the speed-up factor 3 in O(k log k) time complexity. We limit our attention here for the global RM/DM scheduling. Andersson et al. [2] propose the RM-US[ς] algorithm, which gives the highest priority to tasks τ i s with U i > ς, and otherwise assigns priorities by using RM. Our analysis here can also be applied for the RM-US[ς] algorithm with some modifications in the proofs by setting ς = 2 3+ √ 7 ≈ 0.3542.<h2>publication_ref</h2>['b40', 'b40', 'b40', 'b40', 'b6', 'b6', 'b1']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Appendix D: Response-Time for Periodic Tasks with Jitters<h2>text</h2>A periodic task (with jitters) τ i is defined by its period T i , its execution time C i , its relative deadline D i and, its jitter L i . The jitter problem arises when we consider some flexibility to delay the job arrival for a certain bounded length L i . Such a problem has also been studied in the literature, such as [3], [9].
We focus on uniprocessor fixed-priority scheduling here. The busy window concept has also been used for the schedulability analysis of fixed-priority scheduling [3]. For the h-th job of task τ k in the busy window, the finishing time R k,h is the minimum t such that
hC k + k-1 i=1 t + L i T i C i ≤ t.
The h-th job of task τ k arrives at time max{(h-1)T k -L k , 0}, and, hence, its response time is
R k,h -max{(h-1)T k -L k , 0}.
The busy window of task τ k finishes on the h-th
job if R k,h ≤ max{hT k -L k , 0}.
Lemma 10. R k,h is upper bounded by finding the maximum
t k = hC k + k-1 i=1 L i U i + k-1 i=1 t i U i + k-1 i=1 C i ,(54)
among all possible least release time orderings of the k -1 higher-priority tasks with 0
≤ t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ t k and hC k + k-1 i=1 LiUi + k-1 i=1 tiUi + j-1 i=1
Ci > tj,∀j = 1, 2, . . . , k -1.
(55)
Proof: Suppose that R k,h is known. Then, the last release of task τ i in the busy window before R k,h is at time
t i = ( R k,h +Li Ti -1)T i -L i .
We can index the k-1 higher-priority tasks by t i non-decreasingly. By the definition of t i , we know that (
R k,h +Li Ti -1)C i = ti+Li Ti T i U i = t i U i + L i U i .
That is, the accumulative workload of task τ i from 0 to t i is exactly L i U i + t i U i . Due to the indexing rule, and the known R k,h we can now conclude the conditions in Eqs. ( 54) and (55).
However, it should be noted that the last release time ordering is actually unknown since R k,h is unknown. Therefore, we have to consider all possible last release time orderings.
Lemma 11. Suppose that k-1 i=1 U i ≤ 1. Then, for any h ≥ 1 and C k > 0, we have R k,h ≤ hC k + k-1 i=1 (C i + L i U i ) - k-1 i=1 U i ( k-1 =i C ) 1 - k-1 i=1 U i , (56
) where the k -1 higher-priority tasks are ordered in a nonincreasing order of their periods.
Proof: This comes from the above discussions with α i = 1, β i = 1 in Lemma 10 by applying Lemmas 6 and 7 when
k-1 i=1 U i < 1. The case when k-1 i=1 U i = 1 has a safe upper bound R k,h = ∞ in Eq. (56). Theorem 10. Suppose that k i=1 U i ≤ 1. The worst-case response time of task τ k is at most R k ≤ k-1 i=1 (Ci + LiUi) -k-1 i=1 Ui( k-1 =i C ) 1 -k-1 i=1 Ui ,(57)
+ max h * C k 1 -k-1 i=1 Ui , (h * + 1)C k 1 -k-1 i=1 Ui -h * T k + L k
where the k -1 higher-priority tasks are ordered in a nonincreasing order of their periods and h * = L k T k + 1.
Proof: By the definition of h * , we know that
(h * -1)T k - L k ≤ 0, whereas h * T k -L k > 0. When h ≤ h * , we know that max {(h * -1)T k -L k , 0} is 0. Therefore, R k,h -0 is maximized when h is set to h * for any h ≤ h * . The first-order derivative of R k,h -((h -1)T k -L k ) with respect to h when h ≥ h * + 1 is C k 1-k-1 i=1 Ui -T k = C k -(1-k-1 i=1 Ui)T k 1-k-1 i=1 Ui
. Similar to the proof of Theorem 4, we know that setting R k,h -(h -1)T k is maximized when h is set to h * + 1 for any h ≥ h * + 1.
As a result, we only have to evaluate the two cases by setting h as h * or h * + 1. One of them is the worst-case response time. The formulation in Eq. (57) simply compares these two response times.<h2>publication_ref</h2>['b2', 'b8', 'b2']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Appendix E: Schedulability for Generalized Multi-Frame<h2>text</h2>A generalized multi-frame real-time task τ i with m i frames is defined as a task with an array (C i,0 , D i,0 , T i,0 , C i,1 , D i,1 , T i,1 , . . . , C i,mi-1 , D i,mi-1 , T i,mi-1 ) of different execution times, different relative deadlines, and the minimal inter-arrival time of the next frame [8], [46]. The execution time of the j-th job of task τ i is defined as C i,(j mod mi) . If a job of the j-th frame of task τ i is released at time t, the earliest time that task τ i can release the next (j + 1) mod m i frame is at time t + T j . Here, we consider only constrained-deadline cases, in which D i,j ≤ T i,j .
Takada and Sakamura [46] provide an exact test with exponential-time complexity for such a problem under tasklevel fixed-priority scheduling, in which each task is assigned with one static priority level. For this section, we will focus on such a setting. Specifically, we are interested in analyzing the schedulability of the h-th frame of task τ k under the given task priority ordering. It was shown that the critical instant theorem of periodic task systems by Liu and Layland [39] does not work anymore for generalized multi-frame systems. Fortunately, as shown in Theorem 2 in [46], the critical instant of the h-th frame of task τ k is to release a certain frame of a higher priority task τ i at the same time, and the subsequent frames of task τ i as early as possible. In fact, this problem has been recently proved to be co-NP hard in the strong sense in [45].
For completeness, the test with exponential time complexity by Takada and Sakamura [46] will be presented in Lemma 12 by using our notation. We define rbf (i, q, t) as the maximum workload of task τ i released within an interval length t starting from the q-th frame. That is,
rbf (i, q, t) = θ(i,q,t) j=q C i,(j mod mi) ,(58)
where θ(i, q, t) is the smallest such that j=q T i,(j mod mi) ≥ t. That is, θ(i, q, t) is the last frame released by task τ i before t under the critical instant starting with the q-th frame of task τ i .
Lemma 12. The h-th frame of task τ k in a generalized multiframe task system with constrained deadlines is schedulable by a fixed-priority scheduling on a uniprocessor system if
∀q i ∈ {0, 1, . . . , m i -1} , ∀i = 1, 2, . . . , k -1 ∃0 < t ≤ D k,h , C k,h + k-1 i=1 rbf (i, q i , t) ≤ t.
Proof: This is a reformulation of the test by Takada and Sakamura [46].
Since the test in Lemma 12 requires exponential-time complexity, an approximation by using Maximum Interference Function (MIF) was proposed in [46] to provide a sufficient test efficiently. 9 Instead of testing all possible combinations of q i , a simple strategy is to use rbf (i, t) = max mi-1 q=0 rbf (i, q, t) to approximate the test in Lemma 12. This results in a pseudopolynomial-time test. Guan et al. [28] have recently provided proofs to show that such an approximation is with a speed-up factor of 2.
We will use a different way to build our analysis by constructing the k-point last-release schedulability test in Definition 2. The idea is very simple. If task τ i starts with its q-th frame, we can find and define t i,q = θ(i,q,D k,h )-1 j=q T i,(j mod mi) as the last release time of task τ i before D k,h . Therefore, for the given D k,h , we are only interested in these m i last release times of task τ i . We need a safe function for estimating their workload. More precisely, we want to find two constants U i,k,h and C i,k,h such that U i,k,h • t i,q ≥ rbf (i, q, t i,q ) and U i,k,h • t i,q + C i,k,h ≥ rbf (i, q, t i,q + ) = rbf (i, q, D k,h ). This means that no matter which frame of task τ i is the first frame in the critical instant, we can always bound the workload from task τ i by using U i,k,h • t + C i,k,h for the points that we are interested to test. According to the above discussions, we can set U i,k,h = max q=0,1,...,mi-1 max rbf (i, q, t i,q ) t i,q
C i,k,h = max q=0,1,...,mi-1 rbf (i, q, D k,h ) -U i,k,h • t i,q (60)
Now, we can reorganize the schedulability test to link to Definition 2.
Lemma 13. The h-th frame of task τ k in a generalized multiframe task system with constrained deadlines is schedulable by a fixed-priority scheduling on a uniprocessor system if the following condition holds: For any last release ordering π of 9 The reason why this is not an exact test was recently provided by Stigge and Wang in [45].
the k -1 higher-priority tasks with t 1,q1 ≤ t 2,q2 ≤ • • • ≤ t k-1,q k-1 ≤ t k = D k,h , there exists t j,qj such that
C k,h + k-1 i=1 U i,k,h • t i,qi + j-1 i=1 C i,k,h ≤ t j,qj ,(61)
where U i,k,h and C i,k,h are defined in Eqs. (59) and (60), respectively.
Proof: This comes from the above discussions and Lemma 12.
The choice of q i only affects the last release ordering of the k -1 higher-priority tasks, but it does not change the constants U i,k,h and C i,k,h . Therefore, Lemma 13 satisfies Definition 2 by removing the indexes q, but the worst-case last release time ordering has to be considered.
Theorem 11. Suppose that k-1 i=1 C i,k,h < D k,h . The h-th frame of task τ k in a generalized multiframe task system with constrained deadlines is schedulable by a scheduling algorithm on a uniprocessor system if the following condition holds
C k,h D k,h ≤ 1- k-1 i=1 U i,k,h - k-1 i=1 (C i,k,h -U i,k,h ( k-1 =i C ,k,h )) D k,h ,(62
) in which the k -1 higher-priority tasks are indexed in a nonincreasing order of C i,k,h U i,k,h of task τ i .
Proof: The schedulability test (under a specified last release time ordering π) in Lemma 13 satisfies Definition 2 with α i = β i = 1. Therefore, we can apply Lemmas 1 and 2 to reach the conclusion of the proof.
A naïve implementation to calculate t i,q and rbf (i, q, t) requires O(m i ) time complexity. Therefore, calculating U i,k,h and C i,k,h requires O(m 2 i ) time complexity with such an implementation. It can be calculated with better data structures and implementations to achieve O(m i ) time complexity. That is, we first calculate t i,0 and rbf (i, 0, t i,0 ) in O(m i ) time complexity. The overall time complexity to calculate the following t i,q and rbf (i, q, t i,q ) can be done in O(m i ) by using simple algebra. Therefore, the time complexity of the schedulability test in Theorem 11 is O( k-1 i=1 m i + k log k). Remarks: Although we focus ourselves on generalized multiframe task systems in this section, it can be easily seen that our approach can also be adopted to find polynomial-time tests based on the request bound function presented in [28]. Since such functions have been shown useful for schedulability tests in more generalized task models, including the digraph task model [44], the recurring real-time task model [6], etc., the above approach can also be applied for such models by quantifying the utilization (e.g., U i,k,h ) and the execution time (e.g., C i,k,h ) of task τ i based on the limited options of the last release times before the deadline (e.g., D k,h in generalized multi-frame) to be tested.
framework to obtain the polynomial-time schedulability test in the following theorems.
Theorem 12. For a given task τ i and a task mode τ k,h under testing, let C max i = max τi,j ∈hp(τ k,h ) C i,j and U max i = max τi,j ∈hp(τ k,h ) Ci,j Ti,j . Suppose that there are k -1 tasks with higher-priority modes than task mode τ k,h . Let ∆ max k be max{max τ k,j ∈hp(τ k,h ) { C k,j T k,j }, C k,h D k,h }. Suppose that k-1 i=1 C max i < D k,h . Task τ k,h in a multi-mode task system with constrained deadlines is schedulable by a mode-level fixed-priority scheduling algorithm on a uniprocessor system if k-1 i=1 U max i ≤ 1 and
∆ max k ≤ 1- k-1 i=1 U max i - k-1 i=1 (C max i -U max i ( k-1 =i C max )) D k,h ,(65
) in which the k -1 higher-priority tasks are indexed in a nonincreasing order of
C max i U max i of task τ i .
Proof: Suppose that t i is given for each task τ i . For the given t i , we can define the last release time ordering, in which t 1 ≤ t 2 ≤ . . . ≤ t k-1 ≤ D k,h + y = t k . By Lemma 15, we can pessimistically rephrase the schedulability test in Lemma 14 to verify whether there exists t j ∈ {t 1 , t 2 , . . . , t k } such that
∆ max k (D k,h +y)+ k-1 i=1 U max i t i + j-1 i=1 C max i ≤ t j .
Therefore, we reach Definition 2 with α i = 1 and β i = 1, and, hence, can apply Lemmas 1 and 2. . Moreover, with the same argument in the proof of Theorem 9, we know that the resulting schedulability condition is the worst if y is 0 under the assumption
k-1 i=1 U max i ≤ 1.
Theorem 13. For a given task τ i and a task mode τ k,h under testing, let U max i = max τi,j ∈hp(τ k,h ) Ci,j Ti,j . Suppose that there are k -1 tasks with higher-priority modes than task mode τ k,h . Let U max k be max{max τ k,j ∈hp(τ k,h ) { C k,j T k,j },
C k,h
T k,h }. Suppose that k i=1 U max i ≤ 1. Task τ k,h in a multi-mode task system with implicit deadlines is schedulable by the modelevel RM scheduling algorithm on a uniprocessor system if the following condition holds
U max k ≤ 1-2 k-1 i=1 U max i +0.5 ( k-1 i=1 U max i ) 2 + ( k-1 i=1 (U max i ) 2 ) , (66) or k-1 i=1 U max i ≤ k -1 k 2 -2 + 2U max k k k -1 ,(67)
or
U max k + k-1 i=1 U max i ≤ k-1 k 2 -4 -2k k-1 , if k > 3 1 -(k-1) 2k otherwise.
(68)
Proof: Due to the RM property, we know that T i,j ≤ T k,h if τ i,j is in hp(τ k,h ). Therefore, C max i ≤ T k,h U max i . We now reach the conditions in Lemmas 3, 4, and 5 with α = 1 and β = 1. The three conditions are directly from these lemmas. Note that the above tests assume implicitly that the tasks in hp(τ k,h ) are already tested to be schedulable under the scheduling policy. Therefore, we have to apply the results in Theorems 12 and 13 by testing all the task modes from the highest priority to the lowest priority. The following theorem provides the utilization bound for testing the whole task set in linear time for mode-level RM scheduling.
Corollary 5. Suppose that k i=1 U i ≤ 1. A system with k acyclic tasks with utilization U 1 , U 2 , . . . , U k is schedulable by the mode-level RM scheduling algorithm on a uniprocessor system if
k i=1 U i ≤ k-1 k 2 -4 -2k k-1 , if k > 3 1 -(k-1) 2k otherwise. (69) or 0 ≤ 1 -U k -2 k-1 i=1 U i + 0.5 ( k-1 i=1 U i ) 2 + ( k-1 i=1 U 2 i ) , (70
)
where task τ k is the task with the minimum utilization among the k tasks.
Proof: This comes with similar arguments in Theorem 13. Adopting the utilization bound in Eq. (69) does not need to consider the ordering of the tasks. However, the quadratic bound in Eq. (70) changes for different settings of U k . Let k i=1 U i be a given constant H 1 and k i=1 U 2 i be H 2 . Then, the quadratic bound in Eq. ( 70) becomes 1 -2H 1 + U k + 0.5H 2 1 + 0.5H 2 -0.5U 2 k . The first order derivative of the quadratic bound with respect to U k is 1 -U k , which implies that the minimum U k leads to the worst condition in the quadratic bound in Eq. (70) under the condition U k ≤ 1.
The result in Eq. (69) in Corollary 5 is the same as the utilization bound 2 -√ 2 (when k → ∞) in [1], [31] for such a task model. Our results here are more generic than [1] and can also be easily applied for any (mode-level or task-level) fixed-priority scheduling.<h2>publication_ref</h2>['b7', 'b45', 'b45', 'b38', 'b45', 'b44', 'b45', 'b45', 'b45', 'b27', 'b44', 'b27', 'b43', 'b5', 'b0', 'b30', 'b0']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Appendix B: Quadratic Bound for Uniprocessor Constrained-Deadline Tasks<h2>text</h2>To verify the schedulability of a (constrained-deadline) sporadic real-time task τ k under fixed-priority scheduling in<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Appendix F: Acyclic and Multi-Mode Tasks<h2>text</h2>This section considers the acyclic model, proposed in [1]. That is, each task τ i is specified only by its utilization U i . An instance of task τ i can have different worst-case execution times and different relative deadlines. If an instance of task τ i arrives at time t with execution time C i,t , its relative deadline is Ci,t Ui , and the next instance of task τ i can only be released after t + Ci,t Ui .
For systems with known modes, we can also define a multi-mode task system. A multi-mode task τ i with m i modes is denoted by a set of triplet:
to specify the worst-case execution time C i,j , the minimum inter-arrival time T i,j , and the relative deadline D i,j of the corresponding task mode τ i,j . For a multi-mode task τ i , when a job of mode τ i,j is released at time t, this job has to be finished no later than its absolute deadline at time t + D i,j , and the next release time of task τ i is no earlier than t + T i,j . We only consider systems with constrained deadlines, in which D i,j ≤ T i,j . This model is studied in our paper [31]. The difference between this model and the generalized multi-frame model is that the system can switch arbitrarily among any two modes if the temporal seperation constraints in the multi-mode model are respected.
We will focus on mode-level fixed-priority scheduling on uniprocessor scheduling in this section. Suppose that we are testing whether task τ k,h can be feasibly scheduled. Let hp(τ k,h ) be the set of task mode τ k,h and the other task modes with higher priority than task mode τ k,h . It is important to note that τ k,h is also in hp(τ k,h ) for the simplicity of presentation. For notational brevity, we assume that there are k -1 tasks with higher-priority task modes than τ k,h . Moreover, for the rest of this section, we implicitly assume that the tasks in hp(τ k,h ) \ {τ k,h } are schedulable by the mode-level fixedpriority scheduling algorithm under testing.
Let load(i, t) be the maximum workload of task τ i (by considering all the task modes τ i,j ∈ hp(τ k,h )) released from 0 to t such that the next mode can be released at time t. That is, let n i,q be a non-negative integer to denote the number of times that task mode τ i,q is released, in which
q n i,q T i,q ≤ t}.
We denote C max i (τ k,h ) the maximum mode execution time among the higher-priority modes of task τ i than task mode τ k,h , i.e., max τi,j ∈hp(τ k,h ) C i,j for a given index i. Similarly, we denote U max i (τ k,h ) the maximum mode utilization among the higher-priority modes of task τ i than task mode τ k,h , i.e., max τi,j ∈hp(τ k,h ) Ci,j Ti,j for a given index i. For notational brevity, since we define C max i (τ k,h ) and U max i (τ k,h ) by referring to τ k,h , we will simplify the notation by using C max i and U max i , respectively, for the rest of this section.
Suppose that the last release of task τ i in the window of interest is at t i . We define the request bound function for such a case as follows:
Lemma 14. Task τ k,h in a multi-mode task system with constrained deadlines is schedulable by a mode-level fixed-priority scheduling algorithm on uniprocessor systems if
Proof: This is proved by contrapositive. Suppose that τ k,h misses its deadline firstly at time d k,h . We know that this job of τ k,h arrives to the system at time a 1 = d k,h -D k,h . Due to the scheduling policy, we know that the processor is busy executing jobs in hp(τ k,h ) (recall that τ k,h is also in hp(τ k,h )) from a 1 to d k,h . Let a 0 be the last time point in the above schedule before a 1 such that the processor is idle or executing any job with lower priority than τ k,h . It is clear that a 0 is well-defined. Therefore, the processor executes only jobs in hp(τ k,h ) from a 0 to d k,h . We denote a 1 -a 0 as y. Let the last release of the task modes of task τ i before d k,h be a 0 + t i . If task τ i does not have any release, we set t i to 0. Therefore, we have 0
Without loss of generality, we set a 0 to 0. Therefore, the workload requested by the modes of task τ i at any time t before (and at) t i is no more than load(i, t). Moreover, the workload released by the modes of task τ i at any time t after t i is no more than load(i, t i ) + C max i . As a result, the function rbf (i, t i , t) defined in Eq. ( 63) is a safe upper bound of the workload requested by task τ i upt to time t.
By the definition of the task model, task mode τ k,h and the other higher-priority task modes of task τ k may also release some workload before a 1 , in which the workload is upper bounded by load(k, y). The assumption of non-schedulability of task τ k,h in the above schedule and the busy execution in the interval (a 0 , d k,h ] implies that the above workload at any point t in the interval (a 0 , d k,h ] is larger than t. Therefore, we know that
which concludes the proof by contrapositive.
Evaluating load(i, t) is in fact equivalent to the unbounded knapsack problem (UKP). The UKP problem is to select some items in a collection of items with specified weights and profits so that the total weight of the selected items is less than or equal to a given limit (called knapsack) and total profit of the selected items is maximized, in which an item can be selected unbounded multiple times. The definition of load(i, t) is essentially an unbounded knapsack problem, by considering t as the knapsack constraint, the minimum inter-arrival time (period) of a task mode as the weight of an item, and the execution time of a task mode as the profit of the item. 
Proof: It has been shown in [42] that an upper bound of the above unbounded knapsack problem is max τi,j ∈hp(τ k,h ) { Ci,j Ti,j } • t = U max i • t, which completes the proof.
By Lemma 14 and Lemma 15, we can now apply the k 2 Q<h2>publication_ref</h2>['b0', 'b30', 'b41']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>A utilization bound for aperiodic tasks and priority driven scheduling<h2>journal</h2>IEEE Trans. Computers<h2>year</h2>2004<h2>authors</h2>T F Abdelzaher; V Sharma; C Lu<h2>ref_id</h2>b1<h2>title</h2>Static-priority scheduling on multiprocessors<h2>journal</h2><h2>year</h2>2001<h2>authors</h2>B Andersson; S K Baruah; J Jonsson<h2>ref_id</h2>b2<h2>title</h2>Applying new scheduling theory to static priority pre-emptive scheduling<h2>journal</h2>Software Engineering Journal<h2>year</h2>1993-09<h2>authors</h2>N Audsley; A Burns; M Richardson; K Tindell; A Wellings<h2>ref_id</h2>b3<h2>title</h2>Multiprocessor EDF and deadline monotonic schedulability analysis<h2>journal</h2><h2>year</h2>2003<h2>authors</h2>T P Baker<h2>ref_id</h2>b4<h2>title</h2>An analysis of fixed-priority schedulability on a multiprocessor<h2>journal</h2>Real-Time Systems<h2>year</h2>2006<h2>authors</h2>T P Baker<h2>ref_id</h2>b5<h2>title</h2>The non-cyclic recurring real-time task model<h2>journal</h2><h2>year</h2>2010-12-03<h2>authors</h2>S K Baruah<h2>ref_id</h2>b6<h2>title</h2>Improved multiprocessor global schedulability analysis<h2>journal</h2>Real-Time Systems<h2>year</h2>2010<h2>authors</h2>S K Baruah; V Bonifaci; A Marchetti-Spaccamela; S Stiller<h2>ref_id</h2>b7<h2>title</h2>Generalized multiframe tasks<h2>journal</h2>Real-Time Systems<h2>year</h2>1999<h2>authors</h2>S K Baruah; D Chen; S Gorinsky; A K Mok<h2>ref_id</h2>b8<h2>title</h2>Jitter concerns in periodic task systems<h2>journal</h2><h2>year</h2>1997<h2>authors</h2>S K Baruah; D Chen; A K Mok<h2>ref_id</h2>b9<h2>title</h2>New schedulability tests for real-time task sets scheduled by deadline monotonic on multiprocessors<h2>journal</h2><h2>year</h2>2005<h2>authors</h2>M Bertogna; M Cirinei; G Lipari<h2>ref_id</h2>b10<h2>title</h2>New schedulability tests for real-time task sets scheduled by deadline monotonic on multiprocessors<h2>journal</h2>Springer<h2>year</h2>2006<h2>authors</h2>M Bertogna; M Cirinei; G Lipari<h2>ref_id</h2>b11<h2>title</h2>Schedulability analysis of periodic fixed priority systems<h2>journal</h2>IEEE Trans. Computers<h2>year</h2>2004<h2>authors</h2>E Bini; G C Buttazzo<h2>ref_id</h2>b12<h2>title</h2>Rate monotonic analysis: the hyperbolic bound<h2>journal</h2>Computers, IEEE Transactions on<h2>year</h2>2003<h2>authors</h2>E Bini; G C Buttazzo; G M Buttazzo<h2>ref_id</h2>b13<h2>title</h2>A responsetime bound in fixed-priority scheduling with arbitrary deadlines<h2>journal</h2>IEEE Transactions on Computers<h2>year</h2>2009<h2>authors</h2>E Bini; T H C Nguyen; P Richard; S K Baruah<h2>ref_id</h2>b14<h2>title</h2>A quadratic-time response time upper bound with a tightness property<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>E Bini; A Parri; G Dossena<h2>ref_id</h2>b15<h2>title</h2>New strategies for assigning real-time tasks to multiprocessor systems<h2>journal</h2><h2>year</h2>1995<h2>authors</h2>A Burchard; J Liebeherr; Y Oh; S H Son<h2>ref_id</h2>b16<h2>title</h2>Approximate schedulability analysis<h2>journal</h2><h2>year</h2>2002<h2>authors</h2>S Chakraborty; S Künzli; L Thiele<h2>ref_id</h2>b17<h2>title</h2>Evaluate and compare two utilization-based schedulability-test frameworks for realtime systems<h2>journal</h2>CoRR<h2>year</h2>2015<h2>authors</h2>J.-J Chen; W.-H Huang; C Liu<h2>ref_id</h2>b18<h2>title</h2>k 2 U : A general framework from k-point effective schedulability analysis to utilization-based tests<h2>journal</h2>RTSS<h2>year</h2>2015<h2>authors</h2>J.-J Chen; W.-H Huang; C Liu<h2>ref_id</h2>b19<h2>title</h2>k2U: A general framework from k-point effective schedulability analysis to utilization-based tests<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>J.-J Chen; W.-H Huang; C Liu<h2>ref_id</h2>b20<h2>title</h2>Automatic parameter derivations in k 2 U framework<h2>journal</h2>CoRR<h2>year</h2>2016<h2>authors</h2>J.-J Chen; W.-H Huang; C Liu<h2>ref_id</h2>b21<h2>title</h2>Quantifying the suboptimality of uniprocessor fixed priority pre-emptive scheduling for sporadic tasksets with arbitrary deadlines<h2>journal</h2><h2>year</h2>2009<h2>authors</h2>R Davis; T Rothvoß; S Baruah; A Burns<h2>ref_id</h2>b22<h2>title</h2>Improved priority assignment for global fixed priority pre-emptive scheduling in multiprocessor real-time systems<h2>journal</h2>Real-Time Systems<h2>year</h2>2011<h2>authors</h2>R I Davis; A Burns<h2>ref_id</h2>b23<h2>title</h2>Schedulability tests for tasks with variable rate-dependent behaviour under fixed priority scheduling<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>R I Davis; T Feld; V Pollex; F Slomka<h2>ref_id</h2>b24<h2>title</h2>Efficient exact schedulability tests for fixed priority real-time systems<h2>journal</h2>Computers, IEEE Transactions on<h2>year</h2>2008<h2>authors</h2>R I Davis; A Zabos; A Burns<h2>ref_id</h2>b25<h2>title</h2>On a real-time scheduling problem<h2>journal</h2>Operations Research<h2>year</h2>1978<h2>authors</h2>S K Dhall; C L Liu<h2>ref_id</h2>b26<h2>title</h2>A fully polynomial-time approximation scheme for feasibility analysis in static-priority systems with arbitrary relative deadlines<h2>journal</h2><h2>year</h2>2005<h2>authors</h2>N Fisher; S K Baruah<h2>ref_id</h2>b27<h2>title</h2>Approximate response time analysis of real-time task graphs<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>N Guan; C Gu; M Stigge; Q Deng; W Yi<h2>ref_id</h2>b28<h2>title</h2>New response time bounds for fixed priority multiprocessor scheduling<h2>journal</h2><h2>year</h2>2009<h2>authors</h2>N Guan; M Stigge; W Yi; G Yu<h2>ref_id</h2>b29<h2>title</h2>A better polynomial-time schedulability test for real-time fixed-priority scheduling algorithms<h2>journal</h2><h2>year</h2>1997<h2>authors</h2>C.-C Han; H Ying Tyan<h2>ref_id</h2>b30<h2>title</h2>Techniques for schedulability analysis in mode change systems under fixed-priority scheduling<h2>journal</h2>RTCSA<h2>year</h2>2015<h2>authors</h2>W.-H Huang; J.-J Chen<h2>ref_id</h2>b31<h2>title</h2>Efficient online schedulability tests for real-time systems<h2>journal</h2>Software Engineering, IEEE Transactions on<h2>year</h2>2003<h2>authors</h2>T.-W Kuo; L.-P Chang; Y.-H Liu; K.-J Lin<h2>ref_id</h2>b32<h2>title</h2>Enhanced utilization bounds for qos management<h2>journal</h2>IEEE Trans. Computers<h2>year</h2>2004<h2>authors</h2>C.-G Lee; L Sha; A Peddi<h2>ref_id</h2>b33<h2>title</h2>Fixed priority scheduling of periodic task sets with arbitrary deadlines<h2>journal</h2><h2>year</h2>1990<h2>authors</h2>J P Lehoczky<h2>ref_id</h2>b34<h2>title</h2>The rate monotonic scheduling algorithm: Exact characterization and average case behavior<h2>journal</h2><h2>year</h2>1989<h2>authors</h2>J P Lehoczky; L Sha; Y Ding<h2>ref_id</h2>b35<h2>title</h2>Analysis of federated and global scheduling for parallel real-time tasks<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>J Li; J Chen; K Agrawal; C Lu; C Gill; A Saifullah<h2>ref_id</h2>b36<h2>title</h2>Task scheduling with self-suspensions in soft real-time multiprocessor systems<h2>journal</h2><h2>year</h2>2009<h2>authors</h2>C Liu; J Anderson<h2>ref_id</h2>b37<h2>title</h2>Bursty-interference analysis techniques for analyzing complex real-time task models<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>C Liu; J.-J Chen<h2>ref_id</h2>b38<h2>title</h2>Scheduling algorithms for multiprogramming in a hard-real-time environment<h2>journal</h2>Journal of the ACM (JACM)<h2>year</h2>1973<h2>authors</h2>C L Liu; J W Layland<h2>ref_id</h2>b39<h2>title</h2>Linear and nonlinear programming<h2>journal</h2>Springer<h2>year</h2>2008<h2>authors</h2>D G Luenberger; Y Ye<h2>ref_id</h2>b40<h2>title</h2>Analyzing fixed-priority global multiprocessor scheduling<h2>journal</h2><h2>year</h2>2002<h2>authors</h2>L Lundberg<h2>ref_id</h2>b41<h2>title</h2>Knapsack problems: algorithms and computer implementations<h2>journal</h2>John Wiley & Sons, Inc<h2>year</h2>1990<h2>authors</h2>S Martello; P Toth<h2>ref_id</h2>b42<h2>title</h2>Optimal time-critical scheduling via resource augmentation<h2>journal</h2><h2>year</h2>1997<h2>authors</h2>C Phillips; C Stein; E Torng; J Wein<h2>ref_id</h2>b43<h2>title</h2>The digraph real-time task model<h2>journal</h2><h2>year</h2>2011-04-14<h2>authors</h2>M Stigge; P Ekberg; N Guan; W Yi<h2>ref_id</h2>b44<h2>title</h2>Hardness results for static priority real-time scheduling<h2>journal</h2><h2>year</h2>2012<h2>authors</h2>M Stigge; W Yi<h2>ref_id</h2>b45<h2>title</h2>Schedulability of generalized multiframe task sets under static priority assignment<h2>journal</h2><h2>year</h2>1997<h2>authors</h2>H Takada; K Sakamura<h2>ref_id</h2>b46<h2>title</h2>On schedulability bounds of static priority schedulers<h2>journal</h2><h2>year</h2>2005<h2>authors</h2>J Wu; J Liu; W Zhao<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Fig. 1 :1Fig. 1: The k 2 Q framework.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>•We use the same example in Example 4 by setting C 3 = 8 to demonstrate how to use Lemma 6. By the transformation in Example 3, we know that α i = 1 and β i = 1 for i = 1, 2. Now, we can use Lemma 6 based on π 1 and π 2 (defined in Example 4) to calculate the worst-case response time:• For π 1 , the response-time analysis in Lemma 6 shows that the response time of task τ 3 in Example 4 is upper bounded by C3+C1+C2-U1(C1+C2)For π 2 , the response-time analysis in Lemma 6 shows that the response time of task τ 3 in Example 4 is upper bounded by C3+C1+C2-U2(C2+C1)<h2>figure_data</h2><h2>figure_label</h2>111<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>α i = 1 β i = 1 = 1 M β i = 1 M111Theorems 3 and 4 Multiprocessor Global RM/DM for Sporadic Tasks α i Theorems 5, 6, 8,and 9 Uniprocessor Periodic Tasks with Jitters α i = 1 β i = 1 Theorem 10 Uniprocessor Generalized Multi-Frame, Acyclic, and Mode-Change Tasks α i = 1 β i = 1 Theorems 11, 12, and 13.<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>Fig. 2 :2Fig. 2: Acceptance ratio comparison on implicit-deadline 8 multiprocessor systems.<h2>figure_data</h2><h2>figure_label</h2>17229<h2>figure_type</h2>figure<h2>figure_id</h2>fig_4<h2>figure_caption</h2>1 b 7 . 2 ≈ 2 Theorem 9 .17229and U max k ≤ max τi U i ≤ 1 b . The right-hand side of Eq. (49) converges to 2 -2 + U max k when k → ∞ Therefore, by Eq. (49), we can guarantee the schedulability of taskτ k if 1 b ≤ 2 -2 + 2 b . This is equivalent to solving x = 2 -√ 2 + 2x, which holds when x = 3 -√ Therefore, we reach the conclusion of the capacity augmentation factor 3+√ 7 Let ∆ max k be max k-1 j=1 {U j , C k D k }.Task τ k in a sporadic task system with constrained deadlines is schedulable by a global fixed-priority scheduling on M processors ifk i=1 U i ≤ M ,<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_5<h2>figure_caption</h2>Fig. 3 :3Fig.3: Notation of the forced-forward algorithm for the analysis of global fixed-priority scheduling.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>k -1. Hence, we can safely reformulate the sufficient test by verifying whether there exists t j ∈ {t 1 , t 2 , . . . t k } such that<h2>figure_data</h2><h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>U i = C i /T i .<h2>formula_coordinates</h2>[3.0, 154.09, 450.24, 50.45, 9.65]<h2>formula_id</h2>formula_1<h2>formula_text</h2>C k + τ i ∈hp(τ k ) Ci M D k ≤ 1<h2>formula_coordinates</h2>[3.0, 369.74, 195.48, 90.15, 17.3]<h2>formula_id</h2>formula_2<h2>formula_text</h2>C k + τ i ∈hp(τ k ) Ci M D k ≤ 1<h2>formula_coordinates</h2>[3.0, 362.89, 268.25, 92.97, 17.3]<h2>formula_id</h2>formula_3<h2>formula_text</h2>∃t with 0 < t ≤ T k and C k + τi∈hp(τ k ) t T i C i ≤ t,(1)<h2>formula_coordinates</h2>[4.0, 57.62, 140.5, 243.45, 27.94]<h2>formula_id</h2>formula_4<h2>formula_text</h2>( T k Ti -1)T i for τ i ∈ hp(τ k ) and t = T k . If C k + τi∈hp(τ k )<h2>formula_coordinates</h2>[4.0, 48.96, 209.97, 252.11, 31.85]<h2>formula_id</h2>formula_5<h2>formula_text</h2>0 ≤ t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ t k and C k + k-1 i=1 α i t i U i + j-1 i=1 β i C i ≤ t j ,(2)<h2>formula_coordinates</h2>[4.0, 105.69, 663.2, 195.38, 47.5]<h2>formula_id</h2>formula_6<h2>formula_text</h2>( T k Ti -1)T i non-decreasingly, i.e., t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ t k = T k .<h2>formula_coordinates</h2>[4.0, 319.0, 113.83, 252.1, 25.05]<h2>formula_id</h2>formula_7<h2>formula_text</h2>ti Ti C i + C i = t i U i + C i ; 2) if i ≥ j, due to the definition of t i as ( T k Ti -1)T i and t j ≤ t i ≤ T k , we know that tj Ti C i is upper bounded by ti Ti C i = t i U i . 2<h2>formula_coordinates</h2>[4.0, 319.0, 205.26, 252.1, 49.5]<h2>formula_id</h2>formula_8<h2>formula_text</h2>C k + k-1 i=1 tj Ti C i ≤ C k + k-1 i=1 t i U i + j-1 i=1 C i .<h2>formula_coordinates</h2>[4.0, 361.1, 272.51, 210.0, 14.6]<h2>formula_id</h2>formula_9<h2>formula_text</h2>C k + k-1 i=1 t i U i + j-1 i=1 C i ≤ t j .<h2>formula_coordinates</h2>[4.0, 385.05, 316.64, 120.01, 30.8]<h2>formula_id</h2>formula_10<h2>formula_text</h2>{τ 1 , τ 2 } → {1, 2}. Moreover, α 1 = α 2 = β 1 = β 2 = 1.<h2>formula_coordinates</h2>[4.0, 319.0, 480.47, 252.1, 20.61]<h2>formula_id</h2>formula_11<h2>formula_text</h2>t k = C k + k-1 i=1 α i t i U i + k-1 i=1 β i C i ,(3)<h2>formula_coordinates</h2>[4.0, 375.27, 590.24, 195.83, 30.32]<h2>formula_id</h2>formula_12<h2>formula_text</h2>with 0 ≤ t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ t k and C k + k-1 i=1 α i t i U i + j-1 i=1 β i C i > t j ,∀j = 1, 2, . . . , k -1,(4)<h2>formula_coordinates</h2>[4.0, 319.0, 626.95, 252.1, 47.5]<h2>formula_id</h2>formula_13<h2>formula_text</h2>t i T i C i = t i U i holds.<h2>formula_coordinates</h2>[4.0, 499.21, 738.33, 71.89, 13.23]<h2>formula_id</h2>formula_14<h2>formula_text</h2>C k + τi∈hp(τ k ) t Ti C i > t for any 0 < t < R k and C k + τi∈hp(τ k ) R k Ti C i = R k .<h2>formula_coordinates</h2>[5.0, 48.96, 69.79, 252.11, 31.63]<h2>formula_id</h2>formula_15<h2>formula_text</h2>t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ R k . With the same analysis in Example 1, we know that C k + k-1 i=1 t i U i + j-1 i=1 C i > t j for j = 1, 2, . . . , k -1 and R k ≤ C k + k-1 i=1 t i U i + k-1 i=1 C i .<h2>formula_coordinates</h2>[5.0, 48.96, 130.79, 252.1, 36.16]<h2>formula_id</h2>formula_16<h2>formula_text</h2>U i , ∀i < k C i , ∀i < k α i , ∀i < k β i , ∀i < k C k t k (for Lemmas 1-<h2>formula_coordinates</h2>[5.0, 357.84, 132.79, 43.84, 46.4]<h2>formula_id</h2>formula_17<h2>formula_text</h2>0 < β i for any i = 1, 2, . . . , k -1, 0 < t k , k-1 i=1 α i U i ≤ 1, and k-1 i=1 β i C i ≤ t k , task τ k is<h2>formula_coordinates</h2>[5.0, 319.0, 728.69, 252.1, 23.05]<h2>formula_id</h2>formula_18<h2>formula_text</h2>C k t k ≤ 1- k-1 i=1 α i U i - k-1 i=1 (β i C i -α i U i ( k-1 =i β C )) t k .(5)<h2>formula_coordinates</h2>[6.0, 55.14, 85.78, 245.93, 30.32]<h2>formula_id</h2>formula_19<h2>formula_text</h2>C k + k-1 i=1 α i t i U i + j-1 i=1 β i C i > t j .(6)<h2>formula_coordinates</h2>[6.0, 105.69, 211.19, 195.38, 30.79]<h2>formula_id</h2>formula_20<h2>formula_text</h2>C k > C * k , where C * k is defined in the optimization problem: min C * k (7a) s.t. C * k + k-1 i=1 α i t * i U i + j-1 i=1 β i C i ≥ t * j , ∀j = 1, 2, . . . , k -1, (7b) t * 1 ≥ 0 (7c) t * j ≥ t * j-1 , ∀j = 2, 3, . . . , k -1,(7d)<h2>formula_coordinates</h2>[6.0, 48.96, 340.86, 252.11, 105.85]<h2>formula_id</h2>formula_21<h2>formula_text</h2>C * k + k-1 i=1 α i t * i U i + k-1 i=1 β i C i ≥ t k ,(7e)<h2>formula_coordinates</h2>[6.0, 73.23, 450.52, 227.83, 25.0]<h2>formula_id</h2>formula_22<h2>formula_text</h2>min C * k (8a) s.t. C * k + k-1 i=1 α i t * i U i + j-1 i=1 β i C i ≥ t * j , ∀j = 1, 2, . . . , k -1, (8b) t * j ≥ 0, ∀j = 1, 2, . . . , k -1,(8c)<h2>formula_coordinates</h2>[6.0, 56.4, 578.44, 244.66, 51.12]<h2>formula_id</h2>formula_23<h2>formula_text</h2>C * k + k-1 i=1 α i t * i U i + k-1 i=1 β i C i ≥ t k .(8d)<h2>formula_coordinates</h2>[6.0, 73.23, 633.37, 227.83, 25.0]<h2>formula_id</h2>formula_24<h2>formula_text</h2>C * k = t k + s - ( k-1 i=1 α i t * i U i + k-1 i=1 β i C i ).<h2>formula_coordinates</h2>[6.0, 48.96, 680.8, 252.1, 25.39]<h2>formula_id</h2>formula_25<h2>formula_text</h2>t k + s -( k-1 i=1 α i t * i U i + k-1 i=1 β i C i ) such that Eq. (8b) holds, which is equivalent to t k + s -( k-1 i=1 α i t * i U i + k-1 i=1 β i C i ) + k-1 i=1 α i t * i U i + j-1 i=1 β i C i = t k + s - k-1 i=j β i C i ≥ t * j , ∀j = 1, 2, . . . , k -1.(9)<h2>formula_coordinates</h2>[6.0, 48.96, 72.56, 522.14, 676.99]<h2>formula_id</h2>formula_26<h2>formula_text</h2>min t * k -( k-1 i=1 α i U i t * i + k-1 i=1 β i C i ) (10a) s.t. t * k - k-1 i=j β i C i ≥ t * j , ∀1 ≤ j ≤ k -1, (10b) t * j ≥ 0 ∀1 ≤ j ≤ k -1. (10c) t * k ≥ t k (10d)<h2>formula_coordinates</h2>[6.0, 326.86, 178.01, 244.25, 93.56]<h2>formula_id</h2>formula_27<h2>formula_text</h2>C * k if k-1 i=1 α i U i ≤ 1.<h2>formula_coordinates</h2>[6.0, 420.94, 447.88, 93.31, 14.11]<h2>formula_id</h2>formula_28<h2>formula_text</h2>* i ≤ t * k for i = 1, 2, . . . , k -1,.<h2>formula_coordinates</h2>[6.0, 447.88, 608.56, 123.23, 12.55]<h2>formula_id</h2>formula_29<h2>formula_text</h2>t * j > 0 is to put t * k - k-1 i=j β i C i = t * j for every j = 1, 2, . . . , k -1, i.e., ∀1 ≤ i ≤ k -1, t * i+1 -t * i = β i C i ,(11)<h2>formula_coordinates</h2>[6.0, 319.0, 708.68, 252.1, 42.54]<h2>formula_id</h2>formula_30<h2>formula_text</h2>t * k -t * i = k-1 =i (t * +1 -t * ) = k-1 =i β C(12)<h2>formula_coordinates</h2>[7.0, 98.85, 74.92, 202.21, 30.55]<h2>formula_id</h2>formula_31<h2>formula_text</h2>k-1 j=1 β j C j ≤ t k ≤ t * k .<h2>formula_coordinates</h2>[7.0, 48.96, 120.87, 252.1, 25.15]<h2>formula_id</h2>formula_32<h2>formula_text</h2>t * k - k-1 i=1 (αiUit * i + βiCi) (13) =t * k - k-1 i=1 αiUi t * k - k-1 =i β C + βiCi (14) =t * k - k-1 i=1 αiUit * k + k-1 i=1 βiCi - k-1 i=1 αiUi k-1 =i β C(15)<h2>formula_coordinates</h2>[7.0, 56.74, 171.72, 244.33, 88.27]<h2>formula_id</h2>formula_33<h2>formula_text</h2>C * k ≥ t * k (1 - k-1 i=1 α i U i ) - k-1 i=1 (β i C i - α i U i ( k-1 =i β C )).<h2>formula_coordinates</h2>[7.0, 48.96, 266.63, 252.1, 26.95]<h2>formula_id</h2>formula_34<h2>formula_text</h2>k-1 i=1 β i C i ≤ t k ≤ t * k , if t * j is set to 0, there are two cases: (1) t * k - k-1 i=j β i C i > 0 or (2) t * k - k-1 i=j β i C i = 0. In the former case, we can simply set t * j to t * k - k-1<h2>formula_coordinates</h2>[7.0, 48.96, 331.08, 252.1, 56.14]<h2>formula_id</h2>formula_35<h2>formula_text</h2>k-1 i=1 α i U i ≥ 0 and t * k ≥ t k , we know that t * k (1 - k-1 i=1 α i U i ) ≥ t k (1 - k-1 i=1 α i U i ). Therefore, C * k = t k (1 - k-1 i=1 α i U i ) - k-1 i=1 (β i C i -α i U i ( k-1 =i β C )) when 1 - k-1 i=1 α i U i ≥ 0 and k-1 i=1 β i C i ≤ t k , which concludes the proof.<h2>formula_coordinates</h2>[7.0, 48.96, 468.88, 252.11, 61.41]<h2>formula_id</h2>formula_36<h2>formula_text</h2>Consider that k = 3 and |hp(τ k )| is 2. For the two tasks in hp(τ k ), let C 1 = 2, U 1 = 0.2, T 1 = 10 and C 2 = 4, U 2 = 0.5, T 2 = 8. Suppose that t 3 = D 3 = T 3 = 36.<h2>formula_coordinates</h2>[7.0, 48.96, 587.63, 252.1, 31.57]<h2>formula_id</h2>formula_37<h2>formula_text</h2>β i = 1 for i = 1, 2.<h2>formula_coordinates</h2>[7.0, 48.96, 631.47, 79.41, 9.65]<h2>formula_id</h2>formula_38<h2>formula_text</h2>C 3 ≤ t 3 • (1 -U 1 -U 2 ) -(C 1 -U 1 (C 1 + C 2 ) + C 2 -U 2 C 2 ) = 0.3t 3 -2.8 = 8. • For π 2 , the schedulability condition in Lemma 1 shows that task τ 3 in Example 4 can meet the deadline if C 3 ≤ t 3 • (1 -U 2 -U 1 ) -(C 2 -U 2 (C 2 + C 1 ) + C 1 -U 1 C 1 ) = 0.3t 3 -2.6 = 8.2.<h2>formula_coordinates</h2>[7.0, 328.96, 69.11, 242.14, 75.4]<h2>formula_id</h2>formula_39<h2>formula_text</h2>k-1 i=1 α i U i - k-1 i=1 βiCi t k + αiUi( k-1 =i β C ) t k<h2>formula_coordinates</h2>[7.0, 328.56, 371.57, 242.54, 30.41]<h2>formula_id</h2>formula_40<h2>formula_text</h2>β h C h α h U h < β h+1 C h+1 α h+1 U h+1 . Let us now examine the difference of k-1 i=1 αiUi( k-1 =i β C ) t k<h2>formula_coordinates</h2>[7.0, 328.56, 423.77, 242.54, 32.71]<h2>formula_id</h2>formula_41<h2>formula_text</h2>α h U h ( k-1 =h β C )+α h+1 U h+1 ( k-1 =h β C<h2>formula_coordinates</h2>[7.0, 319.0, 515.96, 178.84, 14.11]<h2>formula_id</h2>formula_42<h2>formula_text</h2>((α h U h β h+1 C h+1 -α h+1 U h+1 β h C h ) =α h α h+1 U h U h+1 β h+1 C h+1 α h+1 U h+1 - β h C h α h U h > 0.<h2>formula_coordinates</h2>[7.0, 346.84, 547.57, 196.42, 36.96]<h2>formula_id</h2>formula_43<h2>formula_text</h2>< β i C i ≤ βU i t k for any i = 1, 2, . . . , k -1, 0 < t k , α k-1 i=1 U i ≤ 1, and β k-1 i=1 U i ≤ 1. Lemma 3.<h2>formula_coordinates</h2>[8.0, 48.96, 299.0, 252.1, 49.08]<h2>formula_id</h2>formula_44<h2>formula_text</h2>C k t k ≤1 -(α + β) k-1 i=1 Ui + αβ k-1 i=1 Ui( k-1 =i U )(16)<h2>formula_coordinates</h2>[8.0, 55.5, 385.74, 245.56, 27.06]<h2>formula_id</h2>formula_45<h2>formula_text</h2>=1 -(α + β) k-1 i=1 Ui + 0.5αβ ( k-1 i=1 Ui) 2 + ( k-1 i=1 U 2 i )(17)<h2>formula_coordinates</h2>[8.0, 70.41, 416.44, 230.66, 26.87]<h2>formula_id</h2>formula_46<h2>formula_text</h2>k-1 i=1 U i ( k-1 =i U ) = 0.5 ( k-1 i=1 U i ) 2 + ( k-1 i=1 U 2 i ) . This condition clearly holds when k = 2 since U 2 1 = 0.5(U 2 1 + U 2 1<h2>formula_coordinates</h2>[8.0, 48.96, 518.36, 252.1, 39.14]<h2>formula_id</h2>formula_47<h2>formula_text</h2>k-1 i=1 U i ( k-1 =i U ) = k-1 i=1 U 2 i + k-2 i=1 U i ( k-1 =i+1 U ) = 1 k-1 i=1 U 2 i + 0.5   k-1 i=1 U i 2 - k-1 i=1 U 2 i   = 0.5 ( k-1 i=1 U i ) 2 + ( k-1 i=1 U 2 i ) ,<h2>formula_coordinates</h2>[8.0, 73.36, 563.97, 203.3, 103.77]<h2>formula_id</h2>formula_48<h2>formula_text</h2>k-2 i=1 U i ( k-1 =i+1 U ) = k-1 i=2 U i ( i-1 =1 U ) = 0.5 k-1 i=1 U i 2 - k-1 i=1 U 2 i .<h2>formula_coordinates</h2>[8.0, 59.48, 674.19, 241.59, 32.38]<h2>formula_id</h2>formula_49<h2>formula_text</h2>k-1 i=1 Ui ≤ k -1 k   α + β -(α + β) 2 -2αβ(1 -C k t k ) k k-1 αβ   .(18)<h2>formula_coordinates</h2>[8.0, 320.53, 124.76, 253.63, 40.33]<h2>formula_id</h2>formula_50<h2>formula_text</h2>(α + β)x + 0.5αβ(x 2 + k-1 i=1 U 2 i ). That is, only the last term 0.5αβ( k-1 i=1 U 2 i ) depends on how U i values are actually assigned. Moreover, k-1 i=1 U 2 i is a well-known convex function with respect to U 1 , U 2 , . . . , U k-1 . That is, ρU 2 i + (1 -ρ)U 2 j ≥ (ρU i + (1 -ρ)U j ) 2 for any 0 ≤ ρ ≤ 1.<h2>formula_coordinates</h2>[8.0, 319.0, 221.92, 252.11, 62.19]<h2>formula_id</h2>formula_51<h2>formula_text</h2>k-1 i=1 U 2 i is minimized when U 1 = U 2 = • • • = U k-1 = x k-1 .<h2>formula_coordinates</h2>[8.0, 319.0, 284.19, 252.1, 25.52]<h2>formula_id</h2>formula_52<h2>formula_text</h2>infimum x s. t. C k t k > 1 -(α + β)x + 0.5αβ x 2 + x 2 k -1 .<h2>formula_coordinates</h2>[8.0, 332.09, 343.98, 225.93, 37.36]<h2>formula_id</h2>formula_53<h2>formula_text</h2>k-1<h2>formula_coordinates</h2>[8.0, 449.09, 387.93, 14.6, 6.12]<h2>formula_id</h2>formula_54<h2>formula_text</h2>0 = 1 -C k t k -(α + β)x + 0.5αβ k k-1 x 2 .<h2>formula_coordinates</h2>[8.0, 319.0, 433.17, 158.93, 14.37]<h2>formula_id</h2>formula_55<h2>formula_text</h2>C k t k + k-1 i=1 Ui ≤          k-1 k   α + β -(α + β) 2 -2αβ k k-1 αβ   , if k > (α+β) 2 -1 α 2 +β 2 -1 and α 2 + β 2 > 1 1 + (k-1)((α+β-1)-1 2 (α+β) 2 +0.5) kαβ otherwise(19)<h2>formula_coordinates</h2>[8.0, 320.19, 519.5, 259.5, 87.64]<h2>formula_id</h2>formula_56<h2>formula_text</h2>(α + β) 2 -2αβ k k-1 = (α + β) 2 -2αβ -2αβ 1 k-1<h2>formula_coordinates</h2>[8.0, 328.96, 650.55, 242.14, 31.63]<h2>formula_id</h2>formula_57<h2>formula_text</h2>α+β-α 2 +β 2 +2αβ C k t k αβ when k → ∞. The right-hand side of Eq. (19) (when α 2 + β 2 > 1) converges to α+β- √ α 2 +β 2 αβ when k → ∞.<h2>formula_coordinates</h2>[9.0, 48.96, 68.34, 252.1, 46.86]<h2>formula_id</h2>formula_58<h2>formula_text</h2>< α i ≤ α, 0 < β i ≤ β for any i = 1, 2, . . . , k -1, 0 < t k and k-1 i=1 α i U i < 1, the response time to execute C k for task τ k is at most C k + k-1 i=1 β i C i - k-1 i=1 α i U i ( k-1 =i β C ) 1 - k-1 i=1 α i U i . (20<h2>formula_coordinates</h2>[9.0, 48.96, 206.3, 252.1, 77.32]<h2>formula_id</h2>formula_59<h2>formula_text</h2>)<h2>formula_coordinates</h2>[9.0, 296.91, 264.04, 4.15, 8.64]<h2>formula_id</h2>formula_60<h2>formula_text</h2>k-1 i=1 α i U i ( k-1 =i β C<h2>formula_coordinates</h2>[9.0, 59.48, 636.8, 84.31, 14.11]<h2>formula_id</h2>formula_61<h2>formula_text</h2>k-1 i=1 α i U i ( k-1 =i β C<h2>formula_coordinates</h2>[9.0, 70.88, 673.49, 84.31, 14.11]<h2>formula_id</h2>formula_62<h2>formula_text</h2>k-1 i=1 Ci D k ≤ 1 and C k D k ≤ 1 - k-1 i=1 U i - k-1 i=1 C i D k + k-1 i=1 U i ( k-1 =i C ) D k ,(21)<h2>formula_coordinates</h2>[9.0, 329.42, 227.08, 241.68, 52.39]<h2>formula_id</h2>formula_63<h2>formula_text</h2>C k D k ≤ 1 -2 k-1 i=1 U i + 0.5 ( k-1 i=1 U i ) 2 + ( k-1 i=1 U 2 i )(22)<h2>formula_coordinates</h2>[9.0, 333.7, 385.45, 237.4, 30.32]<h2>formula_id</h2>formula_64<h2>formula_text</h2>k-1 i=1 U i ≤ k -1 k   2 -4 - 2k(1 -C k D k ) k -1   (23<h2>formula_coordinates</h2>[9.0, 340.68, 433.94, 226.27, 33.53]<h2>formula_id</h2>formula_65<h2>formula_text</h2>)<h2>formula_coordinates</h2>[9.0, 566.95, 447.88, 4.15, 8.64]<h2>formula_id</h2>formula_66<h2>formula_text</h2>C k D k + k-1 i=1 U i ≤ k-1 k 2 -4 -2k k-1 if k > 3 1 -k-1 2k if k ≤ 3(24)<h2>formula_coordinates</h2>[9.0, 326.05, 488.79, 245.06, 31.41]<h2>formula_id</h2>formula_67<h2>formula_text</h2>C i = U i T i ≤ U i T k .<h2>formula_coordinates</h2>[9.0, 319.0, 531.67, 252.1, 20.61]<h2>formula_id</h2>formula_68<h2>formula_text</h2>hC k + k-1 i=1 t T i C i ≤ t,<h2>formula_coordinates</h2>[10.0, 124.46, 375.87, 101.1, 30.32]<h2>formula_id</h2>formula_69<h2>formula_text</h2>C k = D k T k C k + τi∈hp2(τ k ) C i , relative deadline D k = D k ,<h2>formula_coordinates</h2>[10.0, 48.96, 452.72, 252.1, 25.72]<h2>formula_id</h2>formula_70<h2>formula_text</h2>k * -1 i=1 Ci D k ≤ 1 and C k D k ≤ 1- k * -1 i=1 U i - k * -1 i=1 C i D k + k * -1 i=1 U i ( k * -1 =i C ) D k ,(25)<h2>formula_coordinates</h2>[10.0, 48.96, 514.92, 252.11, 61.48]<h2>formula_id</h2>formula_71<h2>formula_text</h2>in which C k = D k T k C k + τi∈hp2(τ k ) C i ,<h2>formula_coordinates</h2>[10.0, 48.96, 585.32, 188.85, 14.37]<h2>formula_id</h2>formula_72<h2>formula_text</h2>D k Ti -1 T i .<h2>formula_coordinates</h2>[10.0, 144.69, 613.22, 53.72, 13.7]<h2>formula_id</h2>formula_73<h2>formula_text</h2>t k = hC k + k-1 i=1 t i U i + k-1 i=1 C i ,(26)<h2>formula_coordinates</h2>[10.0, 381.72, 238.73, 189.38, 30.32]<h2>formula_id</h2>formula_74<h2>formula_text</h2>with 0 ≤ t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ t k and hC k + k-1 i=1 t i U i + j-1 i=1 C i > t j ,∀j = 1, 2, . . . , k -1.(27)<h2>formula_coordinates</h2>[10.0, 319.0, 275.41, 252.1, 47.5]<h2>formula_id</h2>formula_75<h2>formula_text</h2>k-1 i=1 U i ≤ 1.<h2>formula_coordinates</h2>[10.0, 434.8, 409.55, 48.38, 14.11]<h2>formula_id</h2>formula_76<h2>formula_text</h2>R k,h ≤ hC k + k-1 i=1 C i - k-1 i=1 U i ( k-1 =i C ) 1 - k-1 i=1 U i , (28<h2>formula_coordinates</h2>[10.0, 336.18, 438.58, 230.77, 29.75]<h2>formula_id</h2>formula_77<h2>formula_text</h2>)<h2>formula_coordinates</h2>[10.0, 566.95, 448.75, 4.15, 8.64]<h2>formula_id</h2>formula_78<h2>formula_text</h2>k-1 i=1 U i < 1. The case when k-1 i=1 U i = 1 has a safe upper bound R k,h = ∞ in Eq. (28). Theorem 4. Suppose that k i=1 U i ≤ 1. The worst-case response time of task τ k is at most R k ≤ C k + k-1 i=1 C i - k-1 i=1 U i ( k-1 =i C ) 1 - k-1 i=1 U i , (29<h2>formula_coordinates</h2>[10.0, 319.0, 515.15, 252.1, 100.63]<h2>formula_id</h2>formula_79<h2>formula_text</h2>)<h2>formula_coordinates</h2>[10.0, 566.95, 596.19, 4.15, 8.64]<h2>formula_id</h2>formula_80<h2>formula_text</h2>C k 1-k-1 i=1 Ui -T k = C k -(1-k-1 i=1 Ui)T k 1-k-1 i=1 Ui<h2>formula_coordinates</h2>[10.0, 408.73, 685.28, 158.11, 20.03]<h2>formula_id</h2>formula_81<h2>formula_text</h2>Case 1: If k i=1 U i < 1, then C k -(1-k-1 i=1 Ui)T k 1-k-1 i=1 Ui < C k -U k T k 1-k-1 i=1 Ui = 0. Therefore, R k,h -(h -1)<h2>formula_coordinates</h2>[10.0, 319.0, 716.34, 252.1, 36.02]<h2>formula_id</h2>formula_82<h2>formula_text</h2>Case 2: If k i=1 U i = 1, then we know that C k -(1-k-1 i=1 Ui)T k 1-k-1 i=1 Ui = 0. Therefore, R k,h -(h -1)T k remains the same regardless of h.<h2>formula_coordinates</h2>[11.0, 48.96, 83.83, 252.11, 40.95]<h2>formula_id</h2>formula_83<h2>formula_text</h2>k i=1 U i ≤ 1 and C k D k ≤ 1 - k-1 i=1 U i - k-1 i=1 C i D k + k-1 i=1 U i ( k-1 =i C ) D k ,(30)<h2>formula_coordinates</h2>[11.0, 55.23, 202.73, 245.83, 50.88]<h2>formula_id</h2>formula_84<h2>formula_text</h2>C k + k-1 i=1 Ci-k-1 i=1 UiCi 1-k-1 i=1 Ui<h2>formula_coordinates</h2>[11.0, 140.59, 314.2, 93.99, 20.03]<h2>formula_id</h2>formula_85<h2>formula_text</h2>W carry i (t) = C i 0 < t < C i C i + t-Ci Ti C i otherwise,(31)<h2>formula_coordinates</h2>[11.0, 73.06, 563.66, 228.01, 26.99]<h2>formula_id</h2>formula_86<h2>formula_text</h2>W normal i (t) = t T i C i .(32)<h2>formula_coordinates</h2>[11.0, 124.68, 606.88, 176.38, 23.23]<h2>formula_id</h2>formula_87<h2>formula_text</h2>∃0 < t ≤ T k , C k + ( τi∈T C i ) + ( k-1 i=1 W normal i (t)) M ≤ t. (33<h2>formula_coordinates</h2>[11.0, 49.69, 686.51, 250.64, 33.42]<h2>formula_id</h2>formula_88<h2>formula_text</h2>)<h2>formula_coordinates</h2>[11.0, 296.91, 711.3, 4.15, 8.64]<h2>formula_id</h2>formula_89<h2>formula_text</h2>for all T ⊆ hp(τ k ) with |T | = M -1.<h2>formula_coordinates</h2>[11.0, 319.0, 58.12, 165.03, 9.68]<h2>formula_id</h2>formula_90<h2>formula_text</h2>k-1 i=1 C i ≤ M T k and U k ≤ 1- τ i ∈T Ci M T k - k-1 i=1 Ui M - k-1 i=1 Ci M T k + k-1 i=1 (Ui k-1 =i C ) M 2 T k . (34<h2>formula_coordinates</h2>[11.0, 319.0, 112.36, 252.1, 55.31]<h2>formula_id</h2>formula_91<h2>formula_text</h2>)<h2>formula_coordinates</h2>[11.0, 567.37, 159.89, 3.73, 7.77]<h2>formula_id</h2>formula_92<h2>formula_text</h2>τ k from C k to C k = C k + τ i ∈T Ci M . Moreover, we have α i = 1 M and β i = 1 M for every task τ i ∈ hp(τ k ) in this case.<h2>formula_coordinates</h2>[11.0, 319.0, 306.69, 252.1, 37.36]<h2>formula_id</h2>formula_93<h2>formula_text</h2>β i = 1 M , we know that task τ k is schedulable by global RM if k-1 i=1 Ci M ≤ T k and C k + τ i ∈T C i M T k ≤ 1- k-1 i=1 Ui M - k-1 i=1 Ci M T k + k-1 i=1 (Ui k-1 =i C ) M 2 T k .(35)<h2>formula_coordinates</h2>[11.0, 319.0, 401.7, 252.7, 67.96]<h2>formula_id</h2>formula_94<h2>formula_text</h2>• QB-BC (from k 2 Q): Eq. (<h2>formula_coordinates</h2>[12.0, 58.92, 222.34, 115.9, 10.55]<h2>formula_id</h2>formula_95<h2>formula_text</h2>from k-1 i=1 (U i k-1 =i C ) M 2 T k to k i=1 (U i k =i C ) M 2 T k<h2>formula_coordinates</h2>[12.0, 48.96, 661.63, 163.92, 17.22]<h2>formula_id</h2>formula_96<h2>formula_text</h2>H(x) = x + 1 - (α + β)x + 0.5αβ(x 2 + x 2 k-1 ). By finding dH(x) dx = 1 -(α + β) + kαβx k-1 = 0, we know that x = (k-1)(α+β-1)<h2>formula_coordinates</h2>[14.0, 48.96, 157.74, 252.1, 46.97]<h2>formula_id</h2>formula_97<h2>formula_text</h2>y =1 -x α + β - 0.5kαβ (k-1)(α+β-1) kαβ k -1 =1 - (k -1)(α + β -1) kαβ (0.5(α + β + 1)) =1 - 0.5(k -1) (α + β) 2 -1 kαβ . (36<h2>formula_coordinates</h2>[14.0, 82.26, 211.9, 214.65, 83.8]<h2>formula_id</h2>formula_98<h2>formula_text</h2>)<h2>formula_coordinates</h2>[14.0, 296.91, 280.44, 4.15, 8.64]<h2>formula_id</h2>formula_99<h2>formula_text</h2>y < 0 is equivalent to testing (1 -1 k ) (α + β) 2 -1 > 2αβ, which implies to test whether α 2 +β 2 -1 > 1 k (α + β) 2 -1 . If α 2 + β 2 ≤ 1, then y ≥ 0 since α 2 + β 2 -1 ≤ 0 ≤ 1 k (α + β)<h2>formula_coordinates</h2>[14.0, 48.96, 355.45, 252.1, 47.99]<h2>formula_id</h2>formula_100<h2>formula_text</h2>Case 1: If α 2 + β 2 > 1 and k > (α+β) 2 -1 α 2 +β 2 -1<h2>formula_coordinates</h2>[14.0, 48.96, 409.57, 179.5, 16.01]<h2>formula_id</h2>formula_101<h2>formula_text</h2>Case 2: If α 2 + β 2 ≤ 1 or k ≤ (α+β) 2 -1 α 2 +β 2 -1<h2>formula_coordinates</h2>[14.0, 48.96, 467.67, 171.96, 16.01]<h2>formula_id</h2>formula_102<h2>formula_text</h2>sup C k + k-1 i=1 α i t * i U i + k-1 i=1 β i C i (37a) such that C k + k-1 i=1 α i t * i U i + j-1 i=1 β i C i > t * j , ∀j = 1, . . . , k -1, (37b) t * j ≥ 0, ∀j = 1, . . . , k -1,(37c)<h2>formula_coordinates</h2>[14.0, 59.1, 573.23, 241.97, 82.84]<h2>formula_id</h2>formula_103<h2>formula_text</h2>maximize C k + k-1 i=1 α i t * i U i + k-1 i=1 β i C i (38a) such that C k + k-1 i=1 α i t * i U i + j-1 i=1 β i C i ≥ t * j , ∀j = 1, . . . , k -1. (38b)<h2>formula_coordinates</h2>[14.0, 323.82, 72.75, 247.28, 53.78]<h2>formula_id</h2>formula_104<h2>formula_text</h2>† 1 , t † 2 , . . . , t † k-1<h2>formula_coordinates</h2>[14.0, 513.3, 187.17, 57.3, 13.91]<h2>formula_id</h2>formula_105<h2>formula_text</h2>t = max{t † 1 , t † 2 , . . . , t † k-1 }.<h2>formula_coordinates</h2>[14.0, 319.14, 209.92, 111.77, 13.91]<h2>formula_id</h2>formula_106<h2>formula_text</h2>C k + t k-1 i=1 α i U i + k-1 i=1 β i C i ≥ C k + k-1 i=1 α i t † i U i + k-1 i=1 β i C i ≥ t.<h2>formula_coordinates</h2>[14.0, 319.0, 238.37, 262.13, 30.32]<h2>formula_id</h2>formula_107<h2>formula_text</h2>C k + k-1 i=1 βiCi 1-k-1 i=1 αiUi . That is, any feasible solution of Eq. (38) has t * j ≤ C k + k-1 i=1 βiCi 1-k-1 i=1 αiUi for any j = 1, 2, . . . , k -1. Under the assumption that k-1 i=1 α i U i < 1 and 0 ≤ k-1 i=1 β i C i ,<h2>formula_coordinates</h2>[14.0, 319.0, 275.12, 252.1, 65.23]<h2>formula_id</h2>formula_108<h2>formula_text</h2>C k + k-1 i=1 α i t * i U i + j-1 i=1 β i C i = t * j for every j = 1, 2, . . . , k -1.<h2>formula_coordinates</h2>[14.0, 329.51, 358.83, 241.59, 23.15]<h2>formula_id</h2>formula_109<h2>formula_text</h2>∀j = 2, 3, . . . , k -1, t * j -t * j-1 = β j-1 C j-1 .(39)<h2>formula_coordinates</h2>[14.0, 335.26, 430.29, 235.84, 12.69]<h2>formula_id</h2>formula_110<h2>formula_text</h2>t * 1 = C k + k-1 i=1 α i U i (t * 1 + i-1 =0 β C ),(40)<h2>formula_coordinates</h2>[14.0, 370.51, 458.04, 200.59, 30.55]<h2>formula_id</h2>formula_111<h2>formula_text</h2>t * 1 = C k + k-1 i=1 α i U i ( i-1 =0 β C ) 1 - k-1 i=1 α i U i .(41)<h2>formula_coordinates</h2>[14.0, 368.96, 521.05, 202.14, 29.74]<h2>formula_id</h2>formula_112<h2>formula_text</h2>k-1 i=1 α i U i < 1.<h2>formula_coordinates</h2>[14.0, 357.2, 565.19, 67.03, 14.11]<h2>formula_id</h2>formula_113<h2>formula_text</h2>t * 1 + k-1 i=1 β i C i = C k + k-1 i=1 α i U i ( i-1 =0 β C ) 1 -k-1 i=1 α i U i + k-1 i=1 β i C i(42)<h2>formula_coordinates</h2>[14.0, 326.14, 610.4, 244.96, 25.0]<h2>formula_id</h2>formula_114<h2>formula_text</h2>= C k + k-1 i=1 β i C i -k-1 i=1 α i U i ( k-1 =i β C ) 1 -k-1 i=1 α i U i(43)<h2>formula_coordinates</h2>[14.0, 378.22, 638.04, 192.88, 24.55]<h2>formula_id</h2>formula_115<h2>formula_text</h2>∃t with 0 < t ≤ D k and C k + τi∈hp(τ k ) t T i C i ≤ t,(44)<h2>formula_coordinates</h2>[15.0, 53.94, 85.7, 247.12, 27.94]<h2>formula_id</h2>formula_116<h2>formula_text</h2>C k + τi∈hp2(τ k ) C i + τi∈hp1(τ k ) t T i C i ≤ t.(45)<h2>formula_coordinates</h2>[15.0, 83.38, 201.37, 217.68, 27.94]<h2>formula_id</h2>formula_117<h2>formula_text</h2>C k = C k + τi∈hp2(τ k ) C i , relative deadline D k = D k , and period T k = D k .<h2>formula_coordinates</h2>[15.0, 48.96, 248.39, 252.1, 23.2]<h2>formula_id</h2>formula_118<h2>formula_text</h2>k * -1 i=1 Ci D k ≤ 1 and C k D k ≤ 1- k * -1 i=1 U i - k * -1 i=1 C i D k + k * -1 i=1 U i ( k * -1 =i C ) D k ,(46)<h2>formula_coordinates</h2>[15.0, 55.14, 350.58, 245.93, 54.52]<h2>formula_id</h2>formula_119<h2>formula_text</h2>D k Ti -1 T i .<h2>formula_coordinates</h2>[15.0, 182.66, 423.98, 53.72, 13.7]<h2>formula_id</h2>formula_120<h2>formula_text</h2>Setting t i = D k Ti<h2>formula_coordinates</h2>[15.0, 99.47, 452.88, 81.82, 13.7]<h2>formula_id</h2>formula_121<h2>formula_text</h2>Lemma 9. Let ∆ max k be max k-1 j=1 {U j , C k D k }.<h2>formula_coordinates</h2>[15.0, 319.0, 167.08, 189.87, 14.48]<h2>formula_id</h2>formula_122<h2>formula_text</h2>∀y ≥ 0, (∀0 ≤ ω i ≤ T i , ∀τ i ∈ hp(τ k )) , ∃t with 0 < t ≤ D k + y such that ∆ max k • (D k + y) + k-1 i=1 ω i • U i + t-ωi Ti C i M ≤ t.<h2>formula_coordinates</h2>[15.0, 320.94, 220.21, 259.6, 43.42]<h2>formula_id</h2>formula_123<h2>formula_text</h2>≤ ω i ≤ T i such that for all 0 < t ≤ D k + y, the condition ∆ max k • (D k + y) + k-1 i=1 ωi•Ui+ t-ω i T i Ci M > t holds.<h2>formula_coordinates</h2>[15.0, 319.0, 297.08, 252.1, 41.01]<h2>formula_id</h2>formula_124<h2>formula_text</h2>σ < (z -1 -z ) • Û ≤ (z -1 -z ) • ∆ max k .<h2>formula_coordinates</h2>[16.0, 84.26, 95.87, 181.5, 13.14]<h2>formula_id</h2>formula_125<h2>formula_text</h2>* =1 σ < * =1 (z -1 -z ) • ∆ max k = (z 0 -z * ) • ∆ max k .<h2>formula_coordinates</h2>[16.0, 69.13, 175.51, 215.59, 32.2]<h2>formula_id</h2>formula_126<h2>formula_text</h2>W i (t) = (d i -z * ) • U i + t -(d i -z * ) T i C i .(47)<h2>formula_coordinates</h2>[16.0, 69.07, 463.4, 232.0, 23.22]<h2>formula_id</h2>formula_127<h2>formula_text</h2>(necessary) condition ∆ max k (z 0 -z * ) + τ i ∈hp(τ k ) W i (t) M > t for all 0 < t ≤ z * -z 0 for the unschedulability of job J 1 .<h2>formula_coordinates</h2>[16.0, 48.96, 542.0, 252.1, 26.62]<h2>formula_id</h2>formula_128<h2>formula_text</h2>< t ≤ D k + y with ∆ max k (D k + y) + τ i ∈hp(τ k ) ωi•Ui+ t-ω i T i Ci M ≤ t.<h2>formula_coordinates</h2>[16.0, 48.96, 627.55, 252.1, 31.63]<h2>formula_id</h2>formula_129<h2>formula_text</h2>U max k ≤ 1 - 2 M k-1 i=1 U i + 0.5 M 2 ( k-1 i=1 U i ) 2 + ( k-1 i=1 U 2 i ) (48) or k-1 j=1 U j M ≤ k -1 k 2 -2 + 2U max k k k -1 .(49)<h2>formula_coordinates</h2>[16.0, 319.0, 379.52, 252.1, 74.68]<h2>formula_id</h2>formula_130<h2>formula_text</h2>ω i • U i + tj -ωi Ti C i ≤ ω i • U i + ( ti-ωi Ti + 1)C i = t i U i + C i . If i ≥ j, then ω i • U i + tj -ωi Ti C i ≤ ω i • U i + ( ti-ωi Ti )C i = t i U i .<h2>formula_coordinates</h2>[16.0, 319.0, 548.4, 252.1, 50.47]<h2>formula_id</h2>formula_131<h2>formula_text</h2>U max k (T k + y) + k-1 i=1 ω i • U i + tj -ωi Ti C i M (50) ≤U max k t k + k-1 i=1 U i t i + j-1 i=1 C i M ≤ t j .(51)<h2>formula_coordinates</h2>[16.0, 339.89, 629.53, 231.21, 55.24]<h2>formula_id</h2>formula_132<h2>formula_text</h2>C i = U i T i ≤ U i T k ≤ U i (T k +y) = U i t k for i = 1, 2, . . . ,<h2>formula_coordinates</h2>[16.0, 319.0, 707.67, 251.44, 19.92]<h2>formula_id</h2>formula_133<h2>formula_text</h2>U max k t k + k-1 i=1 α i U i t i + j-1 i=1 βU i t k ≤ t j ,(52)<h2>formula_coordinates</h2>[17.0, 93.21, 85.98, 207.85, 30.79]<h2>formula_id</h2>formula_134<h2>formula_text</h2>∆ max k ≤ 1 - 1 M k-1 i=1 U i + C i D k + 1 M 2 k-1 i=1 U i ( k-1 =i C D k ) ,(53<h2>formula_coordinates</h2>[17.0, 48.96, 464.72, 252.11, 39.14]<h2>formula_id</h2>formula_135<h2>formula_text</h2>t k = D k + y, α i = 1<h2>formula_coordinates</h2>[17.0, 48.96, 550.18, 252.1, 20.61]<h2>formula_id</h2>formula_136<h2>formula_text</h2>∆ max k ≤ 1 -1 M k-1 i=1 U i + Ci D k +y + 1 M 2 k-1 i=1 U i ( k-1 =i C D k +y )<h2>formula_coordinates</h2>[17.0, 50.16, 586.78, 250.91, 33.16]<h2>formula_id</h2>formula_137<h2>formula_text</h2>k-1 i=1 U i ≤ M , we know that C i i =1 U M ≤ C i . Therefore 8 , - 1 M k-1 i=1 C i D k + y + 1 M 2 k-1 i=1 U i ( k-1 =i C D k + y ) = 1 D k + y 1 M - k-1 i=1 C i + k-1 i=1 C i i =1 U M<h2>formula_coordinates</h2>[17.0, 48.96, 621.04, 252.1, 99.99]<h2>formula_id</h2>formula_138<h2>formula_text</h2>(k -1) there is k-1 i=1 a i k-1 =i b = k-1 i=1 b i i =1 a . x z J z -1 J -1 J3 z2 J2 z1 J1 z0 • • • • • • • • • •<h2>formula_coordinates</h2>[17.0, 94.28, 55.5, 474.45, 695.9]<h2>formula_id</h2>formula_139<h2>formula_text</h2>Proof: If k i=1 U i ≤ M or k-1 i=1 Ci D k ≤ M is violated,<h2>formula_coordinates</h2>[17.0, 338.92, 218.58, 232.18, 15.23]<h2>formula_id</h2>formula_140<h2>formula_text</h2>∆ max k >1 - 1 M k-1 i=1 U i + C i D k + 1 M 2 k-1 i=1 U i ( k-1 =i C D k ) ≥1 - 1 M k-1 i=1 U i + C i D k .<h2>formula_coordinates</h2>[17.0, 321.1, 271.52, 240.01, 64.34]<h2>formula_id</h2>formula_141<h2>formula_text</h2>∆ max k > 1 3 , 1 M k-1 i=1 U i > 1 3 , or 1 M k-1 i=1 Ci D k > 1 3<h2>formula_coordinates</h2>[17.0, 320.19, 351.25, 250.91, 28.52]<h2>formula_id</h2>formula_142<h2>formula_text</h2>k j=1 U j ≤ M 2 (1-U max k )+U max k for global RM by Bertogna et al. [10], since (1-x) 2 ≤ 2 - √ 2 + 2x when 0 ≤ x ≤ 1.<h2>formula_coordinates</h2>[17.0, 319.0, 416.53, 252.1, 28.97]<h2>formula_id</h2>formula_143<h2>formula_text</h2>hC k + k-1 i=1 t + L i T i C i ≤ t.<h2>formula_coordinates</h2>[18.0, 116.1, 223.07, 117.83, 30.32]<h2>formula_id</h2>formula_144<h2>formula_text</h2>R k,h -max{(h-1)T k -L k , 0}.<h2>formula_coordinates</h2>[18.0, 175.03, 271.39, 126.04, 9.65]<h2>formula_id</h2>formula_145<h2>formula_text</h2>job if R k,h ≤ max{hT k -L k , 0}.<h2>formula_coordinates</h2>[18.0, 48.96, 282.35, 252.1, 20.61]<h2>formula_id</h2>formula_146<h2>formula_text</h2>t k = hC k + k-1 i=1 L i U i + k-1 i=1 t i U i + k-1 i=1 C i ,(54)<h2>formula_coordinates</h2>[18.0, 87.35, 327.2, 213.71, 30.32]<h2>formula_id</h2>formula_147<h2>formula_text</h2>≤ t 1 ≤ t 2 ≤ • • • ≤ t k-1 ≤ t k and hC k + k-1 i=1 LiUi + k-1 i=1 tiUi + j-1 i=1<h2>formula_coordinates</h2>[18.0, 54.66, 373.97, 246.21, 43.93]<h2>formula_id</h2>formula_148<h2>formula_text</h2>t i = ( R k,h +Li Ti -1)T i -L i .<h2>formula_coordinates</h2>[18.0, 48.96, 453.24, 252.1, 25.9]<h2>formula_id</h2>formula_149<h2>formula_text</h2>R k,h +Li Ti -1)C i = ti+Li Ti T i U i = t i U i + L i U i .<h2>formula_coordinates</h2>[18.0, 77.58, 491.36, 189.4, 14.67]<h2>formula_id</h2>formula_150<h2>formula_text</h2>Lemma 11. Suppose that k-1 i=1 U i ≤ 1. Then, for any h ≥ 1 and C k > 0, we have R k,h ≤ hC k + k-1 i=1 (C i + L i U i ) - k-1 i=1 U i ( k-1 =i C ) 1 - k-1 i=1 U i , (56<h2>formula_coordinates</h2>[18.0, 48.96, 584.87, 252.11, 66.37]<h2>formula_id</h2>formula_151<h2>formula_text</h2>k-1 i=1 U i < 1. The case when k-1 i=1 U i = 1 has a safe upper bound R k,h = ∞ in Eq. (56). Theorem 10. Suppose that k i=1 U i ≤ 1. The worst-case response time of task τ k is at most R k ≤ k-1 i=1 (Ci + LiUi) -k-1 i=1 Ui( k-1 =i C ) 1 -k-1 i=1 Ui ,(57)<h2>formula_coordinates</h2>[18.0, 48.96, 58.15, 522.14, 693.59]<h2>formula_id</h2>formula_152<h2>formula_text</h2>+ max h * C k 1 -k-1 i=1 Ui , (h * + 1)C k 1 -k-1 i=1 Ui -h * T k + L k<h2>formula_coordinates</h2>[18.0, 353.88, 103.43, 197.62, 25.1]<h2>formula_id</h2>formula_153<h2>formula_text</h2>(h * -1)T k - L k ≤ 0, whereas h * T k -L k > 0. When h ≤ h * , we know that max {(h * -1)T k -L k , 0} is 0. Therefore, R k,h -0 is maximized when h is set to h * for any h ≤ h * . The first-order derivative of R k,h -((h -1)T k -L k ) with respect to h when h ≥ h * + 1 is C k 1-k-1 i=1 Ui -T k = C k -(1-k-1 i=1 Ui)T k 1-k-1 i=1 Ui<h2>formula_coordinates</h2>[18.0, 319.0, 175.81, 252.1, 100.41]<h2>formula_id</h2>formula_154<h2>formula_text</h2>rbf (i, q, t) = θ(i,q,t) j=q C i,(j mod mi) ,(58)<h2>formula_coordinates</h2>[19.0, 102.6, 75.3, 198.46, 31.18]<h2>formula_id</h2>formula_155<h2>formula_text</h2>∀q i ∈ {0, 1, . . . , m i -1} , ∀i = 1, 2, . . . , k -1 ∃0 < t ≤ D k,h , C k,h + k-1 i=1 rbf (i, q i , t) ≤ t.<h2>formula_coordinates</h2>[19.0, 80.07, 205.36, 189.89, 44.23]<h2>formula_id</h2>formula_157<h2>formula_text</h2>C k,h + k-1 i=1 U i,k,h • t i,qi + j-1 i=1 C i,k,h ≤ t j,qj ,(61)<h2>formula_coordinates</h2>[19.0, 354.89, 86.95, 216.21, 30.79]<h2>formula_id</h2>formula_158<h2>formula_text</h2>C k,h D k,h ≤ 1- k-1 i=1 U i,k,h - k-1 i=1 (C i,k,h -U i,k,h ( k-1 =i C ,k,h )) D k,h ,(62<h2>formula_coordinates</h2>[19.0, 320.19, 297.09, 253.95, 38.91]<h2>formula_id</h2>formula_159<h2>formula_text</h2>∆ max k ≤ 1- k-1 i=1 U max i - k-1 i=1 (C max i -U max i ( k-1 =i C max )) D k,h ,(65<h2>formula_coordinates</h2>[21.0, 48.96, 201.84, 252.11, 38.91]<h2>formula_id</h2>formula_160<h2>formula_text</h2>C max i U max i of task τ i .<h2>formula_coordinates</h2>[21.0, 131.83, 250.37, 64.74, 18.15]<h2>formula_id</h2>formula_161<h2>formula_text</h2>∆ max k (D k,h +y)+ k-1 i=1 U max i t i + j-1 i=1 C max i ≤ t j .<h2>formula_coordinates</h2>[21.0, 87.35, 332.88, 213.71, 14.11]<h2>formula_id</h2>formula_162<h2>formula_text</h2>k-1 i=1 U max i ≤ 1.<h2>formula_coordinates</h2>[21.0, 124.01, 388.73, 60.6, 14.11]<h2>formula_id</h2>formula_163<h2>formula_text</h2>C k,h<h2>formula_coordinates</h2>[21.0, 275.92, 441.98, 15.98, 6.79]<h2>formula_id</h2>formula_164<h2>formula_text</h2>U max k ≤ 1-2 k-1 i=1 U max i +0.5 ( k-1 i=1 U max i ) 2 + ( k-1 i=1 (U max i ) 2 ) , (66) or k-1 i=1 U max i ≤ k -1 k 2 -2 + 2U max k k k -1 ,(67)<h2>formula_coordinates</h2>[21.0, 48.96, 509.91, 258.84, 85.93]<h2>formula_id</h2>formula_165<h2>formula_text</h2>U max k + k-1 i=1 U max i ≤ k-1 k 2 -4 -2k k-1 , if k > 3 1 -(k-1) 2k otherwise.<h2>formula_coordinates</h2>[21.0, 48.96, 614.89, 250.91, 31.84]<h2>formula_id</h2>formula_166<h2>formula_text</h2>k i=1 U i ≤ k-1 k 2 -4 -2k k-1 , if k > 3 1 -(k-1) 2k otherwise. (69) or 0 ≤ 1 -U k -2 k-1 i=1 U i + 0.5 ( k-1 i=1 U i ) 2 + ( k-1 i=1 U 2 i ) , (70<h2>formula_coordinates</h2>[21.0, 319.0, 169.49, 252.1, 80.26]<h2>formula_id</h2>formula_167<h2>formula_text</h2>)<h2>formula_coordinates</h2>[21.0, 566.95, 230.16, 4.15, 8.64]<h1>doi</h1><h1>title</h1>SC-OTGM: SINGLE-CELL PERTURBATION MODELING BY SOLVING OPTIMAL MASS TRANSPORT ON THE MANIFOLD OF GAUSSIAN MIXTURES<h1>authors</h1>Andac Demir; Elizaveta Solovyeva; James Boylan; Mei Xiao; Fabrizio Serluca; Sebastian Hoersch; Jeremy Jenkins; Murthy Devarakonda; Bulent Kiziltan<h1>pub_date</h1><h1>abstract</h1>Influenced by recent breakthroughs in Large Language Models (LLMs), single-cell foundation models are emerging. While these models demonstrate successful performance in cell type clustering, phenotype classification, and gene perturbation response prediction, it remains to be seen if a simpler model could achieve comparable or better results, especially with limited data. This is important, as the quantity and quality of single-cell data typically fall short of the standards in textual data used for training LLMs. Single-cell sequencing often suffers from technical artifacts, dropout events, and batch effects. These challenges are compounded in a weakly supervised setting, where the labels of cell states can be noisy, further complicating the analysis. To tackle these challenges, we present sc-OTGM, streamlined with less than 500K parameters, making it approximately 100x more compact than the foundation models, offering an efficient alternative. sc-OTGM is an unsupervised model grounded in the inductive bias that the scRNAseq data can be generated from a combination of the finite multivariate Gaussian distributions. The core function of sc-OTGM is to create a probabilistic latent space utilizing a Gaussian mixture model (GMM) as its prior distribution and distinguish between distinct cell populations by learning their respective marginal probability density functions (PDFs). It uses a Hit-and-Run Markov chain sampler to determine the optimal transport (OT) plan across these PDFs within the GMM framework. We evaluated our model against a CRISPR-mediated perturbation dataset, called CROP-seq, consisting of 57 one-gene perturbations. Our results demonstrate that sc-OTGM is effective in cell state classification, aids in the analysis of differential gene expression, and ranks genes for target identification through a recommender system. It also predicts the effects of single-gene perturbations on downstream gene regulation and generates synthetic scRNA-seq data conditioned on specific cell states.<h1>sections</h1><h2>heading</h2>INTRODUCTION<h2>text</h2>The molecular mechanisms that drive diseases are complex, often reflected in the high-dimensional profiles of gene expression. Conducting detailed analyses of these gene expression matrices-across various cell types, disease states, and control versus experimental subjects-is essential to understand disease progression and identify targets for potential drug interventions. scRNA-seq technology facilitates the detailed profiling of transcriptomes across a vast range of cells, from thousands to millions, allowing for the exploration of cellular heterogeneity and the understanding of disease pathogenesis at the single-cell level.
In recent years, Perturb-seq has emerged as a powerful high-throughput method that combines CRISPR-based genetic perturbation with scRNA-seq Dixit et al. (2016); Adamson et al. (2016). It enables the analysis of how specific gene modulations impact gene expression across numerous individual cells. Within this framework, CRISPRi and CRISPRa allows for targeted downregulation and upregulation of gene expression, respectively, offering insights into gene interactions and regulatory networks at the granularity of single-cells.
A significant hurdle in this research is that cellular responses to genetic perturbations are highly heterogeneous Elsasser (1984); Rubin (1990). This variability arises from differences in mRNA and protein levels Sonneveld et al. (2020), cell states Kramer et al. (2022), and the microenvironment among single-cells Snijder et al. (2009). Given the heterogeneity of potential perturbations, and the complexity of possible cell states, understanding the inherent data geometries and distributions of distinct cell populations becomes crucial for effective analyses. sc-OTGM employs a GMM to parametrize the marginal PDFs of diverse cell states and states within a reduced subspace. Not all subpopulations are well modeled by Gaussian distributions, and some subpopulations may cover large regions of feature space due to skewed class proportions and need further subdivisions. However, the use of GMM as priors enables the detection of local, high-density regions of phenotype space and effectively incorporates an inductive bias to overcome the limitations of data quantity and quality. This approach aligns with some recent methodologies in single-cell genomics, where Gaussian mixture priors are adopted to address the challenges of noise and data scarcity inherent in scRNA-seq datasets Wen et al. (2023); Xu et al. (2023); Grønbech et al. (2020); Slack et al. (2008).
While mixture modeling is not a new concept in scRNA-seq data analysis, we break new ground from following perspective: First, sc-OTGM effectively learns the OT plan that facilitates the mapping from one cell population to another on the manifold of Gaussian mixtures. To capture the transformation from unperturbed to perturbed cell states, the model employs a Hit-and-Run Markov chain sampler. This sampler guarantees a globally optimal solution to generate samples from the target distribution and offers faster convergence than small-step random walks Smith (1996). Second, modeling the PDF of perturbations allows us to identify perturbed genes and predict changes in the expression of other genes following perturbation. Third, sc-OTGM provides a scalable and unified framework for cell state classification, differential gene expression analysis, gene ranking for target identification, perturbation response prediction, and the generation of synthetic scRNA-seq data by sampling from the posteriors of Gaussian components. It offers improved efficiency with reduced runtime and memory requirements compared to LLMs and Variational Autoencoder (VAE) variants, while maintaining competitive accuracy.<h2>publication_ref</h2>['b9', 'b0', 'b10', 'b19', 'b23', 'b14', 'b22', 'b29', 'b31', 'b11', 'b20']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>RELATED WORK<h2>text</h2>Recent advances in single-cell transcriptomics have led to the development of models such as Geneformer Theodoris et al. (2023), scGPT Cui et al. (2023), and scBERT Yang et al. (2022), which utilize masked language modeling (MLM) to learn gene embeddings. However, their effectiveness, particularly in zero-shot learning and in addressing batch effects, is still questioned Kedzierska et al. (2023); Boiarsky et al. (2023). Additionally, there have been significant developments in generative models, including Compositional Perturbation Autoencoder (CPA) Lotfollahi et al. (2021), which uses distinct encoder networks for cell states and perturbations, and further advancements by Lopez et al. (2023) and SAMS-VAE Bereket & Karaletsos (2023), which focus on disentangling representations with causal semantics for perturbation analysis. Tejada-Lapuerta et al. (2023) has criticized these approaches for oversimplifying complex causal relationships. Further details on these models and their comparative analyses can be found in the Appendix A.1.<h2>publication_ref</h2>['b25', 'b6', 'b32', 'b13', 'b4', 'b17', 'b16', 'b3', 'b24']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>DATASET<h2>text</h2>To evaluate the performance of sc-OTGM, we used the CROP-seq dataset from in-vitro experiments on human-induced pluripotent stem cell (iPSC)-derived neurons subjected to genetic perturbations Tian et al. (2021). These perturbations were executed via CRISPRi, enabling targeted gene knockdown to investigate its effects on neuronal survival and oxidative stress. Using the rank genes groups method from scanpy package Wolf et al. (2018) for differential expression analysis, we scrutinized the effects of knocking down 185 genes identified as potentially relevant to neuronal health and disease states. Of these, only 57 genes met our significance threshold (adjusted p-value less than 0.05), indicating a significant alteration in expression levels post-perturbation. These findings are summarized in Table 2, which includes the genes that presented significant differential expression, highlighting their potential roles in neuronal function and susceptibility to oxidative stress-a key factor in the pathogenesis of neurodegenerative diseases. Additional information regarding the pre-processing procedures can be found in Appendix A.4. Raw published data is available from the Gene Expression Omnibus under accession code GSE152988.
z i x proj,i π µ k b Σ k b + e k h k * z pi N K K
Figure 1: sc-OTGM represented as a generative graphical model.
We define the complete generative model for sc-OTGM as illustrated in Figure 1. Let X proj denote the gene expression matrix, with rows representing individual cells and columns representing features in a reduced-dimensional space. The gene expression profile of cell i is X proj, i . π represents the cluster probabilities. Each π k , where k specifies a particular cell state, indicates the prior probability of the k-th component in the mixture, subject to k π k = 1. The latent variable z i ∈ R K determining the component generating each data point x proj, i , is one-hot encoded and follows a categorical distribution parameterized by π. For each Gaussian component in the mixture model, µ k b ∈ R m and Σ k b ∈ R m×m define the mean and covariance matrix for unperturbed cells of a specific cell type, respectively. We specify perturbations and heterogeneous cellular responses as multivariate Gaussiandistributed variables e ∼ N (µ ke , Σ ke ) and h ∼ N (µ km , Σ km ), respectively. We model perturbation as a dynamic system, where the cell outputs an impulse response function h, when presented with a brief perturbation signal e. The convolution of these variables represents the combined effect on the latent state z pi , which is also distributed as a multivariate Gaussian:
z pi = (e k * h k )(z) = +∞ -∞ e k (τ )h k (z-τ )dτ ∼ N (µ ke +µ km , Σ ke +Σ km ) ∼ N (µ kp , Σ kp ),(1)
In the proposed generative mixture model, the joint probability distribution for observed data X, and latent variables Z i , E, and H conditioned on π, µ, Σ is formulated as:
p(X, Z i , E, H | π, µ, Σ) = N i=1 p(z i |π)p(x proj,i |z i , µ, Σ) N i=1 e * h (2) = N i=1 K k=1 π z ik k N (w bi ; µ k b , Σ k b ) z ik N (w pi ; µ kp , Σ kp ) ,(3)
where Z ∈ {0, 1} N ×K denotes latent class indicators for N cells across K cell states, E ∈ R N ×m captures perturbation effects, and H ∈ R N ×m represents the cellular responses to these perturbations. Each datum w bi , w pi ∈ R m is drawn independently and identically distributed (i.i.d.) from their respective marginal PDFs. To prevent numerical instability due to arithmetic underflow or overflow during likelihood calculations in the E-step of the Expectation-Maximization (EM) algorithm, we use the log-sum-exp trick. This method transforms the product of Gaussian probabilities into a sum, ensuring more stable computations. Let L denote p(X,
Z i , E, H | π, µ, Σ): log L = N i=1 K k=1 z ik log π k + z ik log N (w bi ; µ k b , Σ k b ) + log N (w pi ; µ kp , Σ kp )(4)
The Maximum-a-Posteriori (MAP) parameter updates for the GMM via the EM algorithm are detailed in Algorithm 3. To address numerical instability in Σ's inversion due to its near singularity or nonpositive semi-definiteness, Tikhonov regularization is applied Alberti et al. (2021). See Appendix A.6 for additional details.<h2>publication_ref</h2>['b26', 'b30', 'b1']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_1']<h2>heading</h2>PLANNING OPTIMAL TRANSPORT VIA HIT-AND-RUN MARKOV CHAIN SAMPLER<h2>text</h2>OT problems, central to measuring the cost of optimally transporting mass from one distribution to another, are traditionally solved via the Monge (1781) and Kantorovich (1942) formulations, which, however, scale poorly for large datasets due to their reliance on linear programming (LP) Bunne et al. (2023). A breakthrough by Cuturi (2013) introduces entropic regularization into OT, resulting in the Sinkhorn algorithm, which significantly reduces computational complexity, enabling efficient largescale applications. This methodological advancement, detailed in Section A.2.3, represents a pivotal shift towards practical OT computation in machine learning. While OT is conventionally represented by a scalar value indicating the minimum cost required for such transport under specific constraints, sc-OTGM conceptualizes OT as a distribution to focus on the distribution of transportation costs and paths rather than summarizing these costs into a single scalar. A distribution-based approach encapsulates more information about the transport process, such as the variance of transport costs, providing not just the minimum cost but also how costs are distributed across different transport paths. Additionally, it provides a more robust measure of similarity between distributions, as it does not collapse the transport problem into a single metric but rather considers the entire cost landscape, potentially mitigating the influence of outliers.
We model the latent states of unperturbed and perturbed cells as Gaussian distributions, with unperturbed cells described by X = N (µ k b , Σ k b ) and perturbed cells by Y = N (µ ky , Σ ky ). MAP estimates for µ k b , Σ k b , µ ky , Σ ky were derived using the EM Algorithm within a GMM framework.
To quantify the perturbation effect, we introduce a distribution, Z = N (µ kp , Σ kp ), resulting from the linear displacement between X and Y . Specifically, Z is characterized by
µ kp = µ ky -µ k b and Σ kp = Σ k b + Σ ky -2Σ cross .
Z captures the OT cost distribution required to transition between these states. The coupling (joint distribution) of X and Y is unknown, therefore we approximate Σ cross via Hit-and-Run Markov Chain Monte Carlo (MCMC). This generates a Markov chain that, in its stable state, converges to the uniform distribution over a convex polytope van Valkenhoef & Tervonen (2015), and under certain regularity conditions, converges in distribution to the target distribution Smith (1996). The steps to compute Σ cross are elaborated in Algorithm 1. For recursive updates of Σ cross we use follow-the-leader (FTL) strategy which is prominently used in online density estimation and active learning Azoury & Warmuth (2001); Dasgupta & Hsu (2007). Details on the implementation and synthetic data experiments are provided in the Appendix A.8.
sc-OTGM samples transportation paths (vectors) directly from the OT landscape, rooted in the perturbation distribution Z, within a dimensionally reduced subspace: x path ∼ N (µ kp , Σ kp ), which effectively captures the essence of gene expression dynamics under perturbation. The high-dimensional gene expression profiles corresponding to these paths are reconstructed via inverse PCA, expressed as:
x ′′ path = x path V T m
, where V m represents the matrix comprising the top m eigenvectors derived from the covariance matrix of the pre-processed gene expression data. See Appendix A.4 and A.5 for more details. For each gene expression profile within the OT landscape, we derive a parametric representation, as follows:
µ gene,i = the i th element of (µ kp V T m ) and σ 2 gene,i = the i th diagonal entry of (V m Σ kp V T m ),
where µ gene,i denotes the degree of change in a gene's expression in response to perturbation, indicating potential activation or suppression of the gene. σ gene,i quantifies the confidence in these expression changes, providing insights into the variability of our computations. While biological systems are often complex and nonlinear, under specific conditions or within certain ranges, linear approximations can provide valuable insights and simplify modeling efforts. van Someren et al. (2000) presents a methodology for modeling genetic networks that employs clustering to tackle the dimensionality problem and a linear model to infer the regulatory interactions. By analyzing the covariance matrix Σ kp of the perturbation distribution, we can explore the interconnected behavior between genes, examining how they co-vary or influence each other. Consider the scenario where the expression level of gene i changes, denoted as ∆X i . The covariance matrix Σ kp , upon transformation via V m , becomes
Σ ′′ kp = V m Σ kp V T m .
The expected change in gene j's expression level, ∆X j , resulting from a change in gene i, is linearly approximated as follows:
∆X j = Σ ′′ kp,ij Σ ′′ kp,ii ∆X i (5)
Algorithm 1 Estimation of Cross-Covariance Matrix via Hit-and-Run Markov Chain Monte Carlo 1: Input: Domains D X and D Y with non-zero density for X and Y , number of iterations N , mean vectors µ X , µ Y , covariance matrices Σ X , Σ Y , and confidence interval for the bounds α. 2: Output: Estimated cross-covariance matrix Σ XY .
3: Initialize (x (0) , y (0) ) uniformly from D X × D Y . 4: Initialize Σ (0)
XY as a random matrix, ensuring it is symmetric and positive definite. 5: Compute the z-score, z, for the specified confidence interval α using the inverse of the standard normal CDF:
z = Φ -1 1+α 2 . 6: for i = 1 to N do 7: Calculate non-zero density bounds [a X , b X ] for X: 8: Generate a random unit direction d X in X's space. 9: Normalize d X to unit length: d X,normalized = d X ∥d X ∥ 10: Project x (i-1) onto d X,normalized : p X,projection = x (i-1) • d X,normalized 11: Compute standard deviation σ X,projection = d T X,normalized Σ X d X,normalized 12: Determine [a X , b X ] using p X,projection ± z • σ X,projection 13: Calculate non-zero density bounds [a Y , b Y ] for Y : 14: Generate a random unit direction d Y in Y 's space. 15: Normalize d Y to unit length: d Y,normalized = d Y ∥d Y ∥ 16: Project y (i-1) onto d Y,normalized : p Y,projection = y (i-1) • d Y,normalized 17: Compute standard deviation σ Y,projection = d T Y,normalized Σ Y d Y,normalized 18: Determine [a Y , b Y ] using p Y,projection ± z • σ Y,projection 19: Update Σ XY : 20: Sample x (i) ∼ Uniform(a X , b X ) and Sample y (i) ∼ Uniform(a Y , b Y ) 21: ∆x (i) = x (i) -µ X , ∆y (i) = y (i) -µ Y 22: Σ (i) XY = i i+1 Σ (i-1) XY + i (i+1) 2 ∆x (i) (∆y (i) ) T
▷ FTL Online Density Estimation 23: end for<h2>publication_ref</h2>['b12', 'b5', 'b7', 'b28', 'b21', 'b8', 'b27']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>RESULTS<h2>text</h2>Not all cells receiving the CRISPR components will have the target gene successfully knockeddown, and even among those that do, the degree of knockdown can vary substantially due to differences in Cas9 activity, guide RNA efficiency, and individual cell responses. Additionally, the downstream effects of knocking down a particular gene leads to compensatory mechanisms in the cell, altering the expression of other genes. This necessitates benchmarking computational models based on their accuracy to identify knockeddown genes and predict subsequent changes in the expression of other genes. We evaluated statistical models for differential gene expression in CRISPRi experiments, including the Mann-Whitney U test, t-test, and sc-OTGM, as shown in Table 1. We assessed how often each method placed the true knockeddown gene within the top-k results. Lower p-values from the Mann-Whitney U test and t-test correlate with higher rankings. sc-OTGM performs exceptionally well in Top-1 accuracy, showing its effectiveness in identifying the most likely perturbed gene. The performance advantage of sc-OTGM decreases as the ranking threshold increases. In our experiments, we investigated the impact of 57 single-gene knockdowns, featuring BIN1 as a case study in the paper. The dataset for this case study, comprising 382 cells (315 control and 67 target), was split into an 80-20 train-test ratio. The extremely limited size of the dataset presented significant challenges in training our model from scratch and fine-tuning existing foundation models for the CRISPRi experiment. Our results confirm that sc-OTGM accurately predicts the direction of expression level changes among DEGs, distinguishing between upregulated and downregulated genes following BIN1 knockdown. However, the differences between the magnitudes of model predictions and actual values highlight the potential advantages of nonlinear models in capturing the complex dynamics of gene regulation. Furthermore, based on sc-OTGM's ranking, a cutoff of 100 is set to select the top predicted DEGs.
Predicted DEGs are obtained from the top of the ranked gene list, and compared against the list of known DEGs. We conducted Fisher's exact test which yielded a p-Value of 6.39e-08, significantly below the standard threshold of 0.05. This confirms a strong statistical correlation between known DEGs after BIN1 knockdown and those predicted by sc-OTGM. While this analysis focuses on BIN1, Table 3 presents results for all targeted gene knockdowns, showing p-Values well below 0.05, validating the performance on DEG enrichment. Additionally, sc-OTGM demonstrated a mean accuracy of 75.2% ± 15.4% and F1-score of 0.79 ± 0.14, predicting the directionality of expression changes (upregulation or downregulation) in DEGs.  scGPT introduces a variant of masked language modeling (MLM) that mimics the auto-regressive generation in natural language processing, where the masked genes are iteratively predicted according to the model's confidence. Geneformer completely abandons the precise expression levels of genes. Instead, it models the rank of gene expressions and constructs sequences of genes according to their relative expression levels within each cell. The assessment of Geneformer and scGPT revealed significant limitations in their zero-shot performance Kedzierska et al. (2023). In various tasks, particularly in cell type annotation, these models are outperformed by simpler models, such as scVI Lopez et al. (2018) and strategies focusing on highly variable genes (HVGs). Kedzierska et al. ( 2023) also highlighted that aligning the pretraining dataset's tissue of origin with the target task did not consistently improve scGPT's performance, and broader pretraining datasets sometimes resulted in decreased effectiveness. Additionally, both Geneformer and scGPT showed inadequate handling of batch effects in zero-shot settings. Comparisons of scBERT and scGPT with L1-regularized logistic regression in cell type annotation under limited training data suggest that logistic regression performs more accurately, questioning the complexity needed for cell type annotation and the efficacy of MLM in learning gene embeddings Boiarsky et al. (2023). The failure of these models to accurately predict gene expression in zero-shot or limited data scenarios emphasizes the necessity for advancements in model design and training methodologies. CellPLM aggregates gene embeddings since gene expressions are bag-of-word features and leverages spatially-resolved transcriptomic (SRT) data in pre-training to facilitate learning cell-cell relations and introduce a Gaussian mixture prior as an additional inductive bias to overcome data limitation Wen et al. (2023).
The field has also seen advancements in generative modeling. Compositional Perturbation Autoencoder (CPA) has been developed to learn embeddings for both cell states and perturbations Lotfollahi et al. (2021). This is achieved within a unified framework, utilizing input data comprising two main components: gene expression data and perturbation data, such as drug types and dosages. CPA employs distinct encoder networks for cell states and perturbations, respectively. These encoders map input data into a latent space, from which the decoder network reconstructs the expected cellular response to a specific perturbation. Building on that concept, Lopez et al. (2023) leverages sparse mechanism shift assumption in order to learn disentangled representations with a causal semantic to the analysis of single-cell genomics data with genetic or chemical perturbations. However, in single-cell transcriptomics, such a setting might be overly simplistic. Similarly, SAMS-VAE adopts the idea to disentangle cellular latent spaces into basal and perturbation latent variables Bereket & Karaletsos (2023). Specifically, the proposed method models the latent state of perturbed samples as a combination of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention. These global variables are formulated as the point-wise product between latent perturbation variables and a binary mask. Tejada-Lapuerta et al. ( 2023) argues that disentanglement simplifies the problem of recovering meaningful causal representations assuming independence among latent variables by the use of mean field approximation to ease the computation of variational inference.<h2>publication_ref</h2>['b13', 'b15', 'b4', 'b29', 'b17', 'b16', 'b3']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_0', 'tab_3']<h2>heading</h2>A.2 OPTIMAL TRANSPORT<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A.2.1 MONGE'S FORMULATION<h2>text</h2>Monge's formulation of the OT problem seeks a mapping T : X → Y that transports a mass distribution µ on a space X to a mass distribution ν on a space Y in the most cost-effective way. The cost of transporting a unit mass from point x ∈ X to point y ∈ Y is given by a cost function c(x, y).
The goal is to minimize the total transportation cost:
min T X c(x, T (x)) dµ(x)
subject to the constraint that T # µ = ν, where T # µ is the pushforward measure of µ by T , ensuring that the mass distribution after transportation is ν.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A.2.2 KANTOROVICH'S RELAXATION<h2>text</h2>Kantorovich's formulation relaxes Monge's problem by considering a probabilistic coupling π in the product space X × Y , which represents a joint distribution of source and target points that respects the marginal distributions µ and ν. The problem is formulated as:
min π∈Π(µ,ν) X×Y c(x, y) dπ(x, y)
where Π(µ, ν) is the set of all couplings π with marginals µ on X and ν on Y . This formulation is more flexible than Monge's because it allows for mass splitting, making it possible to find solutions in situations where Monge's problem has none.
The Kantorovich problem leads to a dual formulation that expresses the OT cost as:
sup (f,g)∈Φ X f (x) dµ(x) + Y g(y) dν(y)
where Φ consists of all pairs of functions (f, g) such that f (x)+g(y) ≤ c(x, y) for all (x, y) ∈ X ×Y .
The p-Wasserstein distance between µ and ν for p ≥ 1 is derived from Kantorovich's problem with the cost function c(x, y) = ∥x -y∥ p , providing a metric on the space of probability measures:
W p (µ, ν) = min π∈Π(µ,ν) X×Y ∥x -y∥ p dπ(x, y) 1/p
This distance measures the minimal amount of work required to transform the distribution µ into the distribution ν under the given cost function.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A.2.3 REGULARIZED TRANSPORT WITH SINKHORN'S ALGORITHM<h2>text</h2>The Sinkhorn distance introduces an entropic regularization to the OT problem, making it computationally more tractable by allowing the use of efficient algorithms. The entropic regularization term, ϵ KL(π∥µ ⊗ ν), where ϵ is a positive regularization parameter and KL denotes the Kullback-Leibler divergence between π and the product distribution µ ⊗ ν, adds strong convexity to the optimization problem. This convexity ensures that the smoothness of the objective function allows for the application of gradient-based optimization methods that are known to converge quickly.   <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A.4 PRE-PROCESSING<h2>text</h2>Cell Filtering Cells expressing fewer than θ g genes are excluded. Formally, cell i is removed if its gene count j 1 {Xij >0} is less than θ g , where X ij denotes the expression level of gene j in cell i, and 1 represents the indicator function.
Gene Filtering Similarly, genes expressed in fewer than θ c cells are discarded. A gene j is eliminated if i 1 {Xij >0} falls below θ c .<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Normalization and Logarithmic Scaling<h2>text</h2>The gene expression dataset undergoes normalization to equalize expression levels across cells, parameterized by counts per cell after. Postnormalization, logarithmic scaling is applied:
X ′ ij = log(1 + X ij )
, where X ij represents the normalized expression of gene j in cell i.
Identification of Highly Variable Genes Genes are deemed highly variable based on their mean expression µ j and dispersion σ 2 j , constrained within specified thresholds. A gene j qualifies as highly variable if it meets the criteria θ µ,min < µ j < θ µ,max and σ 2 j > θ σ . Scaling Subsequently, gene expression levels are standardized, ensuring zero mean and unit variance for each gene:
X ′ ij = Xij -Xj σj
, where X j and σ j denote the mean and standard deviation of gene j's expression, respectively.
Final Steps After the selection of highly variable genes, further scaling is applied:
X ′′ ij = min(max(X ′ ij , -θ max ), θ max ).
This step ensures that the data range remains within the predefined limits, specified by θ max .<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A.5 PROJECTION TO REDUCED SUBSPACE<h2>text</h2>In downstream analysis, Principal Component Analysis (PCA) is utilized to reduce the dimensionality of a gene expression matrix, X ′′ ∈ R N ×d , comprising N cells and d genes. This process involves the eigendecomposition of the covariance matrix Σ of pre-processed data X ′′ , to compute eigenvectors v i and their corresponding eigenvalues λ i , satisfying the equation Σv i = λ i v i . The top m eigenvectors, selected based on the magnitude of their eigenvalues, are represented as a matrix V m ∈ R d×m . The data X ′′ is then projected onto the lower-dimensional subspace defined by V m , resulting in the projected data X proj = X ′′ V m , where X proj ∈ R N ×m and m ≪ d.
Algorithm 2 K-S Test for Each Principal Component of a High-Dimensional Sample against a Gaussian Reference Distribution 1: Null Hypothesis H 0 : The data for principal component i follows the reference distribution f (x).
2: Significance Level α: Threshold (α = 0.05) to decide on the null hypothesis based on the K-S statistic. 3: for each principal component i in x proj ∈ R N ×m do 4: Extract all data points in dimension i to form S i = {x 1,i proj , x 2,i proj , . . . , x N,i proj }<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>5:<h2>text</h2>Compute mean µ i and variance σ 2 i of S i 6:
Define the Gaussian reference distribution f (x) = N (x; µ i , σ 2 i )
7:
Sort S i in ascending order 8:
for j = 1 to N do 9:
F emp (x j,i proj ) = j N ▷ Empirical Cumulative Distribution Function (ECDF) 10: F ref (x j,i proj ) = x j,i proj -∞ f (x) dx ▷ CDF of the Reference Gaussian Distribution 11: D j,i = |F emp (x j,i proj ) -F ref (x j,i proj )| ▷ K-S Statistic 12:
end for 13:
D i = max j D j,i 14:
if D i is greater than the critical value at significance α for dimension i then end if 19: end for To determine if GMM is appropriate to model the scRNA-seq data, we apply Kolmogorov-Smirnov (K-S) Test to all principal components as detailed in Algorithm 2. If the K-S test suggests that the data is not drawn from a single Gaussian, this can be an initial hint (though not definitive proof) that a GMM might be suitable. Although this approach may not capture the true multivariate nature of the data or account for inter-feature dependencies, it is useful in identifying the presence of multiple modes or clusters in the data.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A.6 MAP PARAMETER ESTIMATION<h2>text</h2>Algorithm 3 MAP Parameter Estimation for the GMM via the EM Algorithm 1: Input: {k, x}. k denotes the number of Gaussian components. x ∈ R N ×m denotes N samples, each with m dimensions after PCA. 2: Output: {π 1:k , µ 1:k , Σ 1:k } MAP parameter estimates, where π k is the prior probability, µ k ∈ R m is the mean vector, and Σ k ∈ R m×m is the covariance matrix of component k.
3: Initialize π i , µ i , Σ i for i = 1 : k. 4: repeat 5:
for i = 1 : k do ▷ Iterate through each Gaussian component in the mixture.
6:
for j = 1 : N do ▷ Calculate log-likelihood & posterior for each sample.
7:
log P (x j | π i ) = -m 2 log(2π) -1 2 log |Σ i | -1 2 (x j -µ i ) T Σ -1 i (x j -µ i ) 8:
a i,j = log P (x j | π i ) + log P (π i )
9:
log P (x j ) = log k l=1 exp(a l,j -max l a l,j ) + max l a l,j ▷ Log-Sum-Exp trick 10:
P (π i | x j ) = exp(a i,j -log P (x j )) ▷ E-Step 11:
end for 12:
µ i = N j=1 P (πi|xj )xj N j=1 P (πi|xj )
13:
Σ i = N j=1 P (πi|xj )(xj -µi)(xj -µi) T N j=1 P (πi|xj )
▷ M-Step 14:
π i = 1 N N j=1 P (π i | x j )
▷ Update the priors for each component 15:
α = 0.01 × mean(diag(Σ i ))
16:
Σ i = Σ i + αI ▷ Tikhonov regularization 17:
end for 18: until Convergence or maximum number of EM iterations We introduce a bias to stabilize the covariance matrix by adding a scaled identity matrix:
Σ = Σ + αI (6)
where α is a small positive regularization parameter and I is the identity matrix of the same dimension as Σ. The choice α = 0.01 × mean(diag(Σ i )) is a heuristic wherein the covariance matrix is regularized by adding 1% of its average variance to its diagonal. This represents an arbitrary yet small perturbation. Benefits of covariance regularization include:
• Invertibility: When the number of dimensions is close to or exceeds the number of data points, Σ might be singular. Regularization ensures its invertibility. • Stability: For ill-conditioned matrices, their inversion can be highly sensitive to slight changes in the data. Regularization stabilizes the inversion process. The condition number is a commonly used measure to gauge the stability of a matrix, especially when it comes to measuring how a matrix will amplify errors in problems involving matrix inversion. The condition number of a matrix A in terms of its norm is defined as:
κ(A) = ∥A∥ • ∥A -1 ∥
For the 2-norm (or Euclidean norm), it can be expressed in terms of the singular values σ max and σ min of A as: κ 2 (A) = σ max σ min Given a linear system Ax = b: If κ(A) ≈ 1, then A is well-conditioned. For small relative changes in b or δA, the corresponding changes in the solution x are also small. If κ(A) ≫ 1, then A is ill-conditioned. Minor relative perturbations in b or δA can lead to significant variations in the solution x.
• Eigenvalue Shrinkage: The regularization effectively increases each eigenvalue of Σ by α, beneficial when there is a need to dampen the influence of high variance variables.  We implemented this code to simulate the performance of Hit-and-Run sampling in estimating the cross-covariance matrix between two distributions. Two sets of parameters (means and covariances) for two different multivariate normal distributions are used to generate two sets of samples: base samples and noise. base samples are generated using the first set of parameters (mean1, cov1) by sampling from the corresponding multivariate normal distribution. Similarly, noise is generated using the second set of parameters (mean2, cov2). To create a correlation between these two sets of samples, the function uses a correlation factor. This factor determines how much of the final synthetic data (correlated samples) will be influenced by the base samples versus the noise. This introduces a controlled amount of dependence between the two sets of samples, simulating correlated data. By controlling the correlation factor, the code can simulate different degrees of correlation between the two sets of data. This synthetic data is then used in the code to evaluate the performance of the Hit-and-Run sampler in estimating the cross-covariance of correlated distributions. It evaluates the accuracy of the estimation by comparing it to a ground truth and measures performance using Root Mean Square Error (RMSE) and Frobenius norm.
# Calculate the variance of the projection variance_projection = np.dot( direction_normalized.T, np.dot(cov, direction_normalized) ) # Calculate the standard deviation of the projection std_deviation = np.sqrt(variance_projection) # Find the z-scores for the specified confidence interval z_score = norm.ppf((1 + confidence) / 2)   <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2><h2>text</h2>Source code and documentation are available at: https://github.com/Novartis/scOTGM.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2><h2>text</h2>import numpy as np from scipy.stats import multivariate_normal, norm import matplotlib.pyplot as plt def generate_random_params(dimension: int) -> tuple:
""" Generate random means and covariances for a given dimension.
Args: dimension (int): The dimensionality of the mean and covariance.
Returns: tuple: A tuple containing the mean vector and covariance matrix.   <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>A multiplexed single-cell crispr screening platform enables systematic dissection of the unfolded protein response<h2>journal</h2>Cell<h2>year</h2>2016<h2>authors</h2>Britt Adamson; Marco Thomas M Norman; Min Y Jost; James K Cho; Yuwen Nuñez; Jacqueline E Chen; Luke A Villalta; Max A Gilbert; Marco Y Horlbeck;  Hein<h2>ref_id</h2>b1<h2>title</h2>Learning the optimal tikhonov regularizer for inverse problems<h2>journal</h2>Advances in Neural Information Processing Systems<h2>year</h2>2021<h2>authors</h2>Ernesto De Giovanni S Alberti; Matti Vito; Luca Lassas; Matteo Ratti;  Santacesaria<h2>ref_id</h2>b2<h2>title</h2>Relative loss bounds for on-line density estimation with the exponential family of distributions<h2>journal</h2>Machine learning<h2>year</h2>2001<h2>authors</h2>S Katy; Manfred K Azoury;  Warmuth<h2>ref_id</h2>b3<h2>title</h2>Modelling cellular perturbations with the sparse additive mechanism shift variational autoencoder<h2>journal</h2><h2>year</h2>2023<h2>authors</h2>Michael Bereket; Theofanis Karaletsos<h2>ref_id</h2>b4<h2>title</h2>A deep dive into single-cell rna sequencing foundation models<h2>journal</h2>bioRxiv<h2>year</h2>2023<h2>authors</h2>Rebecca Boiarsky; M Nalini; Alejandro Singh; Gad Buendia; David Getz;  Sontag<h2>ref_id</h2>b5<h2>title</h2>Learning single-cell perturbation responses using neural optimal transport<h2>journal</h2>Nature Methods<h2>year</h2>2023<h2>authors</h2>Charlotte Bunne; Stefan G Stark; Gabriele Gut; Jacobo Sarabia; Del Castillo; Mitch Levesque; Kjong-Van Lehmann; Lucas Pelkmans; Andreas Krause; Gunnar Rätsch<h2>ref_id</h2>b6<h2>title</h2>scgpt: towards building a foundation model for single-cell multi-omics using generative ai<h2>journal</h2>bioRxiv<h2>year</h2>2023<h2>authors</h2>Haotian Cui; Chloe Wang; Hassaan Maan; Kuan Pang; Fengning Luo; Bo Wang<h2>ref_id</h2>b7<h2>title</h2>Sinkhorn distances: Lightspeed computation of optimal transport<h2>journal</h2>Advances in neural information processing systems<h2>year</h2>2013<h2>authors</h2>Marco Cuturi<h2>ref_id</h2>b8<h2>title</h2>On-line estimation with the multivariate gaussian distribution<h2>journal</h2>Springer<h2>year</h2>2007<h2>authors</h2>Sanjoy Dasgupta; Daniel Hsu<h2>ref_id</h2>b9<h2>title</h2>Perturb-seq: dissecting molecular circuits with scalable single-cell rna profiling of pooled genetic screens<h2>journal</h2>cell<h2>year</h2>2016<h2>authors</h2>Atray Dixit; Oren Parnas; Biyu Li; Jenny Chen; Charles P Fulco; Livnat Jerby-Arnon; Nemanja D Marjanovic; Danielle Dionne; Tyler Burks; Raktima Raychowdhury<h2>ref_id</h2>b10<h2>title</h2>Outline of a theory of cellular heterogeneity<h2>journal</h2>Proceedings of the National Academy of Sciences<h2>year</h2>1984<h2>authors</h2>M Walter;  Elsasser<h2>ref_id</h2>b11<h2>title</h2>scvae: variational auto-encoders for single-cell gene expression data<h2>journal</h2>Bioinformatics<h2>year</h2>2020<h2>authors</h2>Maximillian Fornitz Christopher Heje Grønbech; Pascal N Vording; Casper Kaae Timshel; Tune H Sønderby; Ole Pers;  Winther<h2>ref_id</h2>b12<h2>title</h2>On the transfer of masses (in russian)<h2>journal</h2>Doklady Akademii Nauk<h2>year</h2>1942<h2>authors</h2> Kantorovich<h2>ref_id</h2>b13<h2>title</h2>Assessing the limits of zero-shot foundation models in single-cell biology<h2>journal</h2>bioRxiv<h2>year</h2>2023<h2>authors</h2>Zofia Kasia; Lorin Kedzierska; Ava Pardis Crawford; Alex X Amini;  Lu<h2>ref_id</h2>b14<h2>title</h2>Multimodal perception links cellular state to decision-making in single cells<h2>journal</h2>Science<h2>year</h2>2022<h2>authors</h2>Bernhard A Kramer; Jacobo Sarabia Del Castillo; Lucas Pelkmans<h2>ref_id</h2>b15<h2>title</h2>Deep generative modeling for single-cell transcriptomics<h2>journal</h2>Nature methods<h2>year</h2>2018<h2>authors</h2>Romain Lopez; Jeffrey Regier; Michael I Michael B Cole; Nir Jordan;  Yosef<h2>ref_id</h2>b16<h2>title</h2>Learning causal representations of single cells via sparse mechanism shift modeling<h2>journal</h2>PMLR<h2>year</h2>2023<h2>authors</h2>Romain Lopez; Natasa Tagasovska; Stephen Ra; Kyunghyun Cho; Jonathan Pritchard; Aviv Regev<h2>ref_id</h2>b17<h2>title</h2>Compositional perturbation autoencoder for single-cell response modeling<h2>journal</h2>BioRxiv<h2>year</h2>2021<h2>authors</h2>Anna Klimovskaia Lotfollahi; Carlo De Susmelj; Yuge Donno; Ignacio L Ji; Alexander Ibarra; Nafissa Wolf; Fabian J Yakubova; David Theis;  Lopez-Paz<h2>ref_id</h2>b18<h2>title</h2>Mémoire sur la théorie des déblais et des remblais<h2>journal</h2>Mem. Math. Phys. Acad. Royale Sci<h2>year</h2>1781<h2>authors</h2> Gaspard Monge<h2>ref_id</h2>b19<h2>title</h2>The significance of biological heterogeneity<h2>journal</h2>Cancer and Metastasis Reviews<h2>year</h2>1990<h2>authors</h2>Harry Rubin<h2>ref_id</h2>b20<h2>title</h2>Characterizing heterogeneous cellular responses to perturbations<h2>journal</h2>Proceedings of the National Academy of Sciences<h2>year</h2>2008<h2>authors</h2>Elisabeth D Michael D Slack; Lani F Martinez; Steven J Wu;  Altschuler<h2>ref_id</h2>b21<h2>title</h2>The hit-and-run sampler: a globally reaching markov chain sampler for generating arbitrary multivariate distributions<h2>journal</h2><h2>year</h2>1996<h2>authors</h2>L Robert;  Smith<h2>ref_id</h2>b22<h2>title</h2>Population context determines cell-to-cell variability in endocytosis and virus infection<h2>journal</h2>Nature<h2>year</h2>2009<h2>authors</h2>Berend Snijder; Raphael Sacher; Pauli Rämö; Eva-Maria Damm; Prisca Liberali; Lucas Pelkmans<h2>ref_id</h2>b23<h2>title</h2>Heterogeneity in mrna translation<h2>journal</h2>Trends in cell biology<h2>year</h2>2020<h2>authors</h2>Stijn Sonneveld; M P Bram; Marvin E Verhagen;  Tanenbaum<h2>ref_id</h2>b24<h2>title</h2>Causal machine learning for single-cell genomics<h2>journal</h2><h2>year</h2>2023<h2>authors</h2>Alejandro Tejada-Lapuerta; Paul Bertin; Stefan Bauer; Hananeh Aliee; Yoshua Bengio; Fabian J Theis<h2>ref_id</h2>b25<h2>title</h2>Transfer learning enables predictions in network biology<h2>journal</h2>Nature<h2>year</h2>2023<h2>authors</h2>Christina V Theodoris; Ling Xiao; Anant Chopra; Zeina R Al Mark D Chaffin; Matthew C Sayed; Helene Hill; Elizabeth M Mantineo; Zexian Brydon; X Zeng;  Shirley Liu<h2>ref_id</h2>b26<h2>title</h2>Genome-wide crispri/a screens in human neurons link lysosomal failure to ferroptosis<h2>journal</h2>Nature neuroscience<h2>year</h2>2021<h2>authors</h2>Ruilin Tian; Anthony Abarientos; Jason Hong; Sayed Hadi Hashemi; Rui Yan; Nina Dräger; Kun Leng; Mike A Nalls; Andrew B Singleton; Ke Xu<h2>ref_id</h2>b27<h2>title</h2>Linear modeling of genetic networks from experimental data<h2>journal</h2>Citeseer<h2>year</h2>2000<h2>authors</h2>Eugene P Van Someren; Lodewyk Fa Wessels; Marcel Jt Reinders<h2>ref_id</h2>b28<h2>title</h2>hitandrun:" hit and run" and" shake and bake" for sampling uniformly from convex shapes<h2>journal</h2>CRAN<h2>year</h2>2015<h2>authors</h2>G Van Valkenhoef; T Tervonen<h2>ref_id</h2>b29<h2>title</h2>Cellplm: Pre-training of cell language model beyond single cells<h2>journal</h2>bioRxiv<h2>year</h2>2023<h2>authors</h2>Hongzhi Wen; Wenzhuo Tang; Xinnan Dai; Jiayuan Ding; Wei Jin; Yuying Xie; Jiliang Tang<h2>ref_id</h2>b30<h2>title</h2>Scanpy: large-scale single-cell gene expression data analysis<h2>journal</h2>Genome biology<h2>year</h2>2018<h2>authors</h2>Alexander Wolf; Philipp Angerer; Fabian J Theis<h2>ref_id</h2>b31<h2>title</h2>Graph embedding and gaussian mixture variational autoencoder network for end-to-end analysis of single-cell rna sequencing data<h2>journal</h2>Cell Reports methods<h2>year</h2>2023<h2>authors</h2>Junlin Xu; Jielin Xu; Yajie Meng; Changcheng Lu; Lijun Cai; Xiangxiang Zeng; Ruth Nussinov; Feixiong Cheng<h2>ref_id</h2>b32<h2>title</h2>scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data<h2>journal</h2>Nature Machine Intelligence<h2>year</h2>2022<h2>authors</h2>Fan Yang; Wenchuan Wang; Fang Wang; Yuan Fang; Duyu Tang; Junzhou Huang; Hui Lu; Jianhua Yao<h1>figures</h1><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Figure2shows both sc-OTGM's perturbation distribution and a confusion matrix for cell state classification. Here, clustering around the center suggests minimal impact on most genes, whereas known differentially expressed genes (DEGs) (blue points) significantly diverge. sc-OTGM accurately ranked BIN1 as a knockeddown gene, placing it at the top of the recommendation list. In the presented confusion matrix, cells are classified as either Control-indicating no CRISPR-induced changes-or Perturbed-where changes are expected. True negatives (56) correctly identify cells as Control; false positives (18) erroneously label cells as Perturbed; false negatives (5) miss the classification of Perturbed cells as Control; and true positives (8) correctly detect Perturbed cells. Additionally, we used in silico perturbation response predictions to analyze gene expression changes between BIN1-knockdown and control cells, as shown in Figure3.<h2>figure_data</h2><h2>figure_label</h2>23<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Figure 2 :Figure 3 :23Figure 2: Perturbation distribution from the OT landscape, and ranking genes for target identification through a recommender system.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>min π∈Π(µ,ν) X×Y c(x, y) dπ(x, y) + ϵ KL(π∥µ ⊗ ν) A.3 DATASET STATISTICS<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_5<h2>figure_caption</h2>for i in range(1, num_iterations + 1):# Hit-and-Run sampling for X direction_x = np.random.randn(len(mean1)) lb_x, ub_x = get_non_zero_density_bounds( cov1, x_current, direction_x ) x_current = np.random.uniform(lb_x, ub_x) # Hit-and-Run sampling for Y direction_y = np.random.randn(len(mean2)) lb_y, ub_y = get_non_zero_density_bounds( cov2, y_current, direction_y ) y_current = np.random.uniform(lb_y, ub_y) # Sequential update on cross-covariance matrix deviation_x = x_current -mean1 deviation_y = y_current -mean2 cross_covariance = (i / (i + 1)) * cross_covariance + ( i / (i + 1) ** 2 ) * np.outer(deviation_x, deviation_y) # Update rmse values and frobenius_norms rmse = np.sqrt(np.mean((cross_covariance -ground_truth) ** 2)) rmse_values.append(rmse) fro_norm = np.linalg.norm(cross_covariance -ground_truth, "fro") normalized_fro_norm = fro_norm / np.sqrt((dimension ** 2)) frobenius_norms.append(normalized_fro_norm) return cross_covariance, rmse_values, frobenius_norms cross-covariance for a given dimension with an imposed correlation factor. Args: dimension (int): The dimensionality of the distribution. num_iterations (int, optional): Number of iter. for sampling. Default is 1000. correlation_factor (float, optional): Factor to impose correlation between the two distributions. Default is 0. Returns: list: List of RMSE values over iterations and list of Frobenius norms for iterations. """ mean1, cov1 = generate_random_params(dimension) mean2, cov2 = generate_random_params(dimension) # Generate correlated samples _, _, correlated_samples = generate_non_independent_data( mean1, cov1, mean2, cov2, num_iterations, correlation_factor ) # Calculate the ground truth cross-covariance matrix ground_truth = np.cov(correlated_samples, rowvar=False) _, rmse, frobenius_norms = estimate_cross_covariance( mean1, cov1, mean2, cov2, ground_truth, num_iterations ) return rmse, frobenius_norms if __name__ == "__main__": dimensions = [100] num_iterations = 1000correlation_factors = [0.0, 0.1, 0.2, 0.5] # Dictionary to store results for each correlation factor all_results = {} # Compute results for each correlation factor and dimension for factor in correlation_factors: all_results[factor] = { dim: main(dim, num_iterations, factor) for dim in dimensions } plt.figure(figsize=(16, 5)) lines = [] # To store line objects for the legend labels = [] # To store label strings for the legend for i, factor in enumerate(correlation_factors): plt.subplot(2, 4, i + 1) # Retrieve results for this correlation factor results = all_results[factor]<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>table<h2>figure_id</h2>tab_0<h2>figure_caption</h2>Benchmarking differential gene expression analysis techniques. The highest performance is highlighted in bold, the best baseline method is underlined. . Expression Analysis Top-1 Acc.  Top-5 Acc.  Top-10 Acc.  Top-50 Acc.  Top-100 Acc. <h2>figure_data</h2>DiffMann-Whitney U test0.370.400.420.580.60t-test0.390.670.700.790.86sc-OTGM0.560.680.740.820.91<h2>figure_label</h2>2<h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>Quantitative Analysis of Gene Expression Alterations Post-CRISPRi Knockdown<h2>figure_data</h2>Genelog 2 (fold change)p-ValueAdjusted p-ValueNum. Control CellsNum. Targeted CellsTUBB4A-5.601.30 × 10 -29 1.06 × 10 -25 31786ATP1A3-5.571.11 × 10 -25 9.08 × 10 -22 35487KIFAP3-5.341.50 × 10 -26 1.23 × 10 -22 358100MAPT-5.132.26 × 10 -23 1.84 × 10 -19 368101CASP3-4.731.53 × 10 -20 1.25 × 10 -16 30783APEX1-4.728.08 × 10 -18 6.60 × 10 -14 34676COX10-4.565.61 × 10 -14 3.28 × 10 -10 9755NDUFS8-4.521.57 × 10 -15 1.29 × 10 -11 35273ZNF292-4.364.91 × 10 -13 4.01 × 10 -933960Continued on next page<h2>figure_label</h2>2<h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>Continued from previous page<h2>figure_data</h2>Genelog 2 (fold change)p-ValueAdjusted p-ValueNum. Control CellsNum. Targeted CellsGSTA4-4.123.74 × 10 -14 3.06 × 10 -10 33773STX1B-4.013.34 × 10 -16 2.72 × 10 -12 23572OPTN-3.954.06 × 10 -16 3.32 × 10 -12 33888SOD1-3.843.71 × 10 -15 3.03 × 10 -11 36485NDUFV1-3.728.89 × 10 -11 5.80 × 10 -732062CALB1-3.624.02 × 10 -78.21 × 10 -45064EEF2-3.602.61 × 10 -12 5.32 × 10 -936763BIN1-3.573.12 × 10 -11 2.55 × 10 -731567SCFD1-3.562.38 × 10 -63.60 × 10 -424932PON2-3.506.35 × 10 -11 5.18 × 10 -79974BAX-3.453.23 × 10 -14 2.63 × 10 -10 23086SCAPER-3.071.37 × 10 -91.12 × 10 -531171CYB561-3.061.15 × 10 -51.57 × 10 -313832AKAP9-3.001.13 × 10 -99.21 × 10 -636679VPS35-2.971.38 × 10 -83.76 × 10 -533465PRNP-2.928.65 × 10 -92.35 × 10 -525372AP2A2-2.872.51 × 10 -11 2.05 × 10 -733299SOD2-2.806.16 × 10 -85.03 × 10 -518861BECN1-2.797.42 × 10 -51.89 × 10 -214827SNCB-2.713.35 × 10 -89.12 × 10 -514468CDH11-2.711.55 × 10 -61.58 × 10 -39761ELOVL5-2.661.36 × 10 -91.11 × 10 -517392NTRK2-2.662.21 × 10 -41.62 × 10 -223731DAP-2.658.90 × 10 -83.63 × 10 -415670EIF4G1-2.491.17 × 10 -61.63 × 10 -328168TRPM7-2.465.46 × 10 -74.46 × 10 -310085COASY-2.371.06 × 10 -74.32 × 10 -4120101TRAP1-2.341.82 × 10 -77.44 × 10 -420686CYP46A1-2.325.03 × 10 -74.11 × 10 -316386PARP1-2.252.66 × 10 -79.08 × 10 -432088FOXRED1-2.254.29 × 10 -51.78 × 10 -213953AFG3L2-2.242.13 × 10 -68.69 × 10 -327479RAB7A-2.149.64 × 10 -83.94 × 10 -434495PPP2R2B-2.138.95 × 10 -68.84 × 10 -332477RGS2-2.101.73 × 10 -51.09 × 10 -211878AMFR-2.081.61 × 10 -44.88 × 10 -29669MRPL10-2.067.03 × 10 -51.34 × 10 -212769ANO10-1.941.86 × 10 -58.91 × 10 -314592DMXL1-1.946.13 × 10 -52.15 × 10 -210789HYOU1-1.912.91 × 10 -51.83 × 10 -217786HTT-1.899.44 × 10 -73.85 × 10 -3200125ECHS1-1.882.57 × 10 -66.12 × 10 -333296CYCS-1.851.45 × 10 -63.04 × 10 -336565CEP63-1.843.53 × 10 -51.92 × 10 -215887FARP1-1.806.95 × 10 -53.14 × 10 -232774FRMD4A-1.727.80 × 10 -42.36 × 10 -324667RPL6-1.663.29 × 10 -92.69 × 10 -536887PFN1-1.502.87 × 10 -52.93 × 10 -236676<h2>figure_label</h2>3<h2>figure_type</h2>table<h2>figure_id</h2>tab_3<h2>figure_caption</h2>Quantitative Analysis of Gene Perturbation Responses in CRISPRi Experiments. This table presents the effectiveness of sc-OTGM in identifying known differentially expressed genes (DEGs) following targeted gene knockdowns. Statistical validation is provided through p-Values obtained from Fisher's exact test, confirming a significant correlation between predicted and known DEGs for all targeted gene knockdown experiments, well below the significance threshold of 0.05. The columns labeled 'Accuracy (%)' and 'F1-score' evaluate sc-OTGM's performance to accurately predict the direction of expression changes-either upregulation or downregulation-in DEGs. The mean accuracy of 75.2% ± 15.4% and mean F1-score of 0.79 ± 0.14 show the model's performance in predicting gene expression dynamics.<h2>figure_data</h2>GeneDEG Enrichment Perturbation Response Predictionp-ValueAcc. (%)F1-scoreTUBB4A7.37 × 10 -6100.01.00ATP1A33.12 × 10 -2078.70.85KIFAP36.39 × 10 -1087.50.67MAPT9.01 × 10 -4100.01.00CASP36.93 × 10 -6100.01.00APEX12.61 × 10 -16100.01.00COX101.43 × 10 -1683.70.87NDUFS82.29 × 10 -3682.70.83ZNF2924.91 × 10 -1665.40.72GSTA45.50 × 10 -2189.50.92STX1B4.95 × 10 -3872.10.76OPTN1.51 × 10 -6100.01.00SOD11.16 × 10 -788.90.90NDUFV14.33 × 10 -3073.90.79CALB11.38 × 10 -440.00.46EEF25.08 × 10 -2992.20.92BIN16.39 × 10 -888.90.89SCFD11.59 × 10 -4256.40.66PON28.41 × 10 -5553.60.60BAX1.27 × 10 -3078.30.83SCAPER1.48 × 10 -3187.20.89CYB5615.66 × 10 -3360.40.66AKAP93.69 × 10 -14100.01.00VPS359.36 × 10 -1480.00.85PRNP3.95 × 10 -5359.40.72AP2A22.48 × 10 -5775.40.82SOD22.01 × 10 -790.50.91BECN16.22 × 10 -473.10.75SNCB6.79 × 10 -4187.70.89CDH111.77 × 10 -1466.70.70ELOVL52.28 × 10 -1492.30.93NTRK24.50 × 10 -2958.70.61DAP1.27 × 10 -4581.70.86EIF4G13.00 × 10 -2476.40.84TRPM73.66 × 10 -1466.70.80COASY1.51 × 10 -683.30.84TRAP12.11 × 10 -3580.00.89CYP46A14.56 × 10 -450.00.33PARP15.51 × 10 -2457.70.67FOXRED11.49 × 10 -2575.80.78AFG3L21.01 × 10 -1575.00.81RAB7A1.21 × 10 -1283.30.84PPP2R2B3.10 × 10 -3563.90.75RGS25.03 × 10 -3063.00.68AMFR4.06 × 10 -1262.50.74MRPL101.11 × 10 -2243.60.57Continued on next page<h2>figure_label</h2>3<h2>figure_type</h2>table<h2>figure_id</h2>tab_4<h2>figure_caption</h2>Continued from previous page<h2>figure_data</h2>GeneDEG Enrichment Perturbation Response Predictionp-ValueAcc. (%)F1-scoreANO101.38 × 10 -440.00.46DMXL16.72 × 10 -3366.70.76HYOU17.43 × 10 -3555.30.65HTT8.81 × 10 -2461.10.66ECHS15.44 × 10 -971.40.79CYCS7.80 × 10 -377.80.80CEP632.80 × 10 -1475.00.77FARP11.35 × 10 -2283.30.88FRMD4A8.53 × 10 -3183.10.84RPL62.74 × 10 -3667.70.77PFN13.91 × 10 -1680.00.85A.8 IMPLEMENTATION OF HIT-AND-RUN MARKOV CHAIN SAMPLER<h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_6<h2>figure_caption</h2># Plotting RMSE values for dim, (rmse, _) in results.items():(line,) = plt.plot(rmse, label=f"d={dim}", linewidth=1) if i == 0: # Only add to legend for the first subplot lines.append(line) labels.append(f"d={dim}")<h2>figure_data</h2>plt.title(f"$\\rho$: {factor}", fontsize=12)plt.xlabel("# of iterations", fontsize=10)plt.ylabel("RMSE", fontsize=10)plt.xlim(0, num_iterations)plt.grid(True)for i, factor in enumerate(correlation_factors):plt.subplot(2, 4, i + 5)# Retrieve results for this correlation factorresults = all_results[factor]# Plotting Frobenius normsfor dim, (_, fro_norm) in results.items():plt.plot(fro_norm, label=f"d={dim}", linewidth=1)plt.title(f"$\\rho$: {factor}", fontsize=12)plt.xlabel("# of iterations", fontsize=10)plt.ylabel("Frobenius norm", fontsize=10)plt.xlim(0, num_iterations)plt.grid(True)plt.figlegend(lines,labels,loc="upper center",ncol=len(dimensions),fontsize=10,)plt.tight_layout(rect=[0, 0, 1, 0.95])plt.show()<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>z i x proj,i π µ k b Σ k b + e k h k * z pi N K K<h2>formula_coordinates</h2>[3.0, 194.97, 119.21, 217.7, 88.88]<h2>formula_id</h2>formula_1<h2>formula_text</h2>z pi = (e k * h k )(z) = +∞ -∞ e k (τ )h k (z-τ )dτ ∼ N (µ ke +µ km , Σ ke +Σ km ) ∼ N (µ kp , Σ kp ),(1)<h2>formula_coordinates</h2>[3.0, 112.98, 400.86, 391.69, 26.29]<h2>formula_id</h2>formula_2<h2>formula_text</h2>p(X, Z i , E, H | π, µ, Σ) = N i=1 p(z i |π)p(x proj,i |z i , µ, Σ) N i=1 e * h (2) = N i=1 K k=1 π z ik k N (w bi ; µ k b , Σ k b ) z ik N (w pi ; µ kp , Σ kp ) ,(3)<h2>formula_coordinates</h2>[3.0, 139.07, 480.16, 365.6, 65.5]<h2>formula_id</h2>formula_3<h2>formula_text</h2>Z i , E, H | π, µ, Σ): log L = N i=1 K k=1 z ik log π k + z ik log N (w bi ; µ k b , Σ k b ) + log N (w pi ; µ kp , Σ kp )(4)<h2>formula_coordinates</h2>[3.0, 141.41, 622.96, 363.25, 57.86]<h2>formula_id</h2>formula_4<h2>formula_text</h2>µ kp = µ ky -µ k b and Σ kp = Σ k b + Σ ky -2Σ cross .<h2>formula_coordinates</h2>[4.0, 108.0, 320.41, 395.0, 21.28]<h2>formula_id</h2>formula_5<h2>formula_text</h2>x ′′ path = x path V T m<h2>formula_coordinates</h2>[4.0, 123.39, 478.18, 64.44, 14.27]<h2>formula_id</h2>formula_6<h2>formula_text</h2>µ gene,i = the i th element of (µ kp V T m ) and σ 2 gene,i = the i th diagonal entry of (V m Σ kp V T m ),<h2>formula_coordinates</h2>[4.0, 121.03, 533.77, 369.95, 12.85]<h2>formula_id</h2>formula_7<h2>formula_text</h2>Σ ′′ kp = V m Σ kp V T m .<h2>formula_coordinates</h2>[4.0, 182.65, 670.5, 79.75, 14.2]<h2>formula_id</h2>formula_8<h2>formula_text</h2>∆X j = Σ ′′ kp,ij Σ ′′ kp,ii ∆X i (5)<h2>formula_coordinates</h2>[4.0, 265.35, 704.08, 239.32, 30.15]<h2>formula_id</h2>formula_9<h2>formula_text</h2>3: Initialize (x (0) , y (0) ) uniformly from D X × D Y . 4: Initialize Σ (0)<h2>formula_coordinates</h2>[5.0, 112.98, 131.22, 205.57, 24.42]<h2>formula_id</h2>formula_10<h2>formula_text</h2>z = Φ -1 1+α 2 . 6: for i = 1 to N do 7: Calculate non-zero density bounds [a X , b X ] for X: 8: Generate a random unit direction d X in X's space. 9: Normalize d X to unit length: d X,normalized = d X ∥d X ∥ 10: Project x (i-1) onto d X,normalized : p X,projection = x (i-1) • d X,normalized 11: Compute standard deviation σ X,projection = d T X,normalized Σ X d X,normalized 12: Determine [a X , b X ] using p X,projection ± z • σ X,projection 13: Calculate non-zero density bounds [a Y , b Y ] for Y : 14: Generate a random unit direction d Y in Y 's space. 15: Normalize d Y to unit length: d Y,normalized = d Y ∥d Y ∥ 16: Project y (i-1) onto d Y,normalized : p Y,projection = y (i-1) • d Y,normalized 17: Compute standard deviation σ Y,projection = d T Y,normalized Σ Y d Y,normalized 18: Determine [a Y , b Y ] using p Y,projection ± z • σ Y,projection 19: Update Σ XY : 20: Sample x (i) ∼ Uniform(a X , b X ) and Sample y (i) ∼ Uniform(a Y , b Y ) 21: ∆x (i) = x (i) -µ X , ∆y (i) = y (i) -µ Y 22: Σ (i) XY = i i+1 Σ (i-1) XY + i (i+1) 2 ∆x (i) (∆y (i) ) T<h2>formula_coordinates</h2>[5.0, 108.5, 166.73, 350.46, 231.67]<h2>formula_id</h2>formula_11<h2>formula_text</h2>min T X c(x, T (x)) dµ(x)<h2>formula_coordinates</h2>[9.0, 254.92, 718.96, 102.16, 17.23]<h2>formula_id</h2>formula_12<h2>formula_text</h2>min π∈Π(µ,ν) X×Y c(x, y) dπ(x, y)<h2>formula_coordinates</h2>[10.0, 242.74, 184.42, 126.53, 17.23]<h2>formula_id</h2>formula_13<h2>formula_text</h2>sup (f,g)∈Φ X f (x) dµ(x) + Y g(y) dν(y)<h2>formula_coordinates</h2>[10.0, 218.58, 273.2, 167.37, 17.23]<h2>formula_id</h2>formula_14<h2>formula_text</h2>W p (µ, ν) = min π∈Π(µ,ν) X×Y ∥x -y∥ p dπ(x, y) 1/p<h2>formula_coordinates</h2>[10.0, 198.43, 345.84, 214.64, 27.18]<h2>formula_id</h2>formula_15<h2>formula_text</h2>X ′ ij = log(1 + X ij )<h2>formula_coordinates</h2>[12.0, 303.54, 133.26, 87.89, 12.33]<h2>formula_id</h2>formula_16<h2>formula_text</h2>X ′ ij = Xij -Xj σj<h2>formula_coordinates</h2>[12.0, 166.61, 215.33, 58.91, 14.6]<h2>formula_id</h2>formula_17<h2>formula_text</h2>X ′′ ij = min(max(X ′ ij , -θ max ), θ max ).<h2>formula_coordinates</h2>[12.0, 108.0, 246.9, 396.0, 24.76]<h2>formula_id</h2>formula_18<h2>formula_text</h2>F emp (x j,i proj ) = j N ▷ Empirical Cumulative Distribution Function (ECDF) 10: F ref (x j,i proj ) = x j,i proj -∞ f (x) dx ▷ CDF of the Reference Gaussian Distribution 11: D j,i = |F emp (x j,i proj ) -F ref (x j,i proj )| ▷ K-S Statistic 12:<h2>formula_coordinates</h2>[12.0, 108.5, 537.94, 396.17, 57.58]<h2>formula_id</h2>formula_19<h2>formula_text</h2>D i = max j D j,i 14:<h2>formula_coordinates</h2>[12.0, 108.5, 596.16, 97.89, 22.39]<h2>formula_id</h2>formula_20<h2>formula_text</h2>3: Initialize π i , µ i , Σ i for i = 1 : k. 4: repeat 5:<h2>formula_coordinates</h2>[13.0, 112.98, 205.34, 145.38, 30.69]<h2>formula_id</h2>formula_21<h2>formula_text</h2>log P (x j | π i ) = -m 2 log(2π) -1 2 log |Σ i | -1 2 (x j -µ i ) T Σ -1 i (x j -µ i ) 8:<h2>formula_coordinates</h2>[13.0, 112.98, 246.98, 357.57, 21.92]<h2>formula_id</h2>formula_22<h2>formula_text</h2>P (π i | x j ) = exp(a i,j -log P (x j )) ▷ E-Step 11:<h2>formula_coordinates</h2>[13.0, 108.5, 284.79, 395.5, 19.73]<h2>formula_id</h2>formula_23<h2>formula_text</h2>µ i = N j=1 P (πi|xj )xj N j=1 P (πi|xj )<h2>formula_coordinates</h2>[13.0, 154.82, 303.72, 87.08, 20.19]<h2>formula_id</h2>formula_24<h2>formula_text</h2>Σ i = N j=1 P (πi|xj )(xj -µi)(xj -µi) T N j=1 P (πi|xj )<h2>formula_coordinates</h2>[13.0, 154.82, 324.91, 144.46, 20.19]<h2>formula_id</h2>formula_25<h2>formula_text</h2>π i = 1 N N j=1 P (π i | x j )<h2>formula_coordinates</h2>[13.0, 154.82, 346.11, 105.68, 14.56]<h2>formula_id</h2>formula_26<h2>formula_text</h2>α = 0.01 × mean(diag(Σ i ))<h2>formula_coordinates</h2>[13.0, 154.82, 361.89, 115.4, 9.65]<h2>formula_id</h2>formula_27<h2>formula_text</h2>Σ i = Σ i + αI ▷ Tikhonov regularization 17:<h2>formula_coordinates</h2>[13.0, 108.5, 372.82, 395.5, 19.73]<h2>formula_id</h2>formula_28<h2>formula_text</h2>Σ = Σ + αI (6)<h2>formula_coordinates</h2>[13.0, 281.4, 437.12, 223.27, 11.48]<h2>formula_id</h2>formula_29<h2>formula_text</h2>κ(A) = ∥A∥ • ∥A -1 ∥<h2>formula_coordinates</h2>[13.0, 280.43, 589.99, 87.01, 10.81]<h1>doi</h1><h1>title</h1>sustain.AI: a Recommender System to analyze Sustainability Reports<h1>authors</h1>Maren Pielka; David Leonhard; Tobias Deußer; Tim Dilmaghani; Bernd Kliem; Rüdiger Loitz; Milad Morad; Christian Temath; Thiago Bell; Robin Stenzel; Rafet Sifa; Lars Hillebrand; Tim Dil- Maghani<h1>pub_date</h1>2023-05-26<h1>abstract</h1>We present sustain.AI, an intelligent, context-aware recommender system that assists auditors and financial investors as well as the general public to efficiently analyze companies' sustainability reports. The tool leverages an end-to-end trainable architecture that couples a BERT-based encoding module with a multi-label classification head to match relevant text passages from sustainability reports to their respective law regulations from the Global Reporting Initiative (GRI) standards. We evaluate our model on two novel German sustainability reporting data sets and consistently achieve a significantly higher recommendation performance compared to multiple strong baselines. Furthermore, sustain.AI is publicly available for everyone at https://sustain.ki.nrw/.<h1>sections</h1><h2>heading</h2>INTRODUCTION<h2>text</h2>In the face of climate change and environmental degradation, our society's expectations of sustainable and responsible entrepreneurial action have increased continuously over the past years. Legislators worldwide and particularly in the EU become increasingly aware of the situation and have taken concrete political measures to enforce corporate social responsibility (CSR). In 2014 the EU approved the Non-Financial Reporting Directive (NFRD) which forces large companies to extend their reporting on policies, risks and key performance indicators regarding sustainability and social Figure 1: A screenshot of the sustain.AI recommender tool. After selecting a specific regulatory requirement from one of the categories, the system predicts the most relevant segments of a provided sustainability report. On the right side, the recommended segments are highlighted in the rendered report, fostering an efficient sustainability analysis. matters. Beginning in 2024 the NFRD will be updated with the stricter CSR-Directive, which applies to around 50,000 European companies and includes a wider catalog of reporting requirements covering environmental, social and governance aspects. The majority of these requirements are based on the popular regulatory framework from the Global Reporting Initiative (GRI). Its universal reporting standards provide a detailed set of indicators that address a company's impact on the economy, environment and people.
In light of these more comprehensive and rigorous sustainability regulations and the public's growing interest in corporate social responsibility, it is of vital importance to make the disclosed information easily accessible and comparable. However, manually retrieving and analyzing the published reports concerning specific GRI-Indicators is practically infeasible, especially considering that the documents often span around a hundred pages or more. This is particularly true for the auditing domain, where auditors spend hours to assure a report's compliance related to said CSR standards.
Hence, we introduce sustain.AI, a sophisticated, context-aware recommender system that utilizes modern techniques of natural language processing (NLP) and machine learning to process and analyze uploaded sustainability reports. Concretely, interested users like consumers or investors can query the recommender engine for specific GRI-indicators, e.g. the company's emissions (see Figure 1), and the engine returns and renders the most relevant document segments related to the query. Thus, stakeholders are able to quickly assess investment risks and opportunities arising from social and environmental issues and to evaluate the sustainability performance of companies. Similarly, auditors significantly benefit from the automated matching of concrete regulatory requirements to the relevant text passages. In fact, a large part of the sustainability report audit is about ensuring the completeness and correctness of the report according to the specified GRI standards.
Our recommender system builds on a BERT-based [3] encoding module followed by a non-linear multi-label classification head. Both components are trained jointly in an end-to-end fashion leveraging weighted random sampling (WRS) to counter the significant class label imbalance. We evaluate the model on two novel German sustainability reporting data sets while consistently outperforming a large set of strong baselines by more than 10 percentage points in mean average precision.
sustain.AI is released to the public as a KI-NRW demonstrator, which is available at https://sustain.ki.nrw/. First user tests have already promised significant efficiency gains for the analysis of sustainability reports in the context of auditing. Moreover, the continuous use in production will further improve the system's recommendation capabilities due to the integration of human feedback, e.g. in the form of correcting wrong predictions.<h2>publication_ref</h2>['b2']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>RELATED WORK<h2>text</h2>Before continuing with the description of the inner workings of sustain.AI, we take a look at prior accomplishments of other researchers related to this work.
In terms of facilitating the audit of annual financial statements, [16] presented the Automated List Inspection (ALI) tool, a recommender system that ranks textual elements of financial documents to associated requirements of predefined regulatory frameworks like IFRS (International financial reporting standards) or HGB (Handelsgesetzbuch). For the ranking task, the authors used classical NLP techniques like Tf-Idf (Term frequency-Inverse document frequency), latent semantic indexing, neural networks and logistic regression (LR) with the combination of the first and last methods giving the best performance. In a follow-up work, [12] improved ALI by utilizing a pre-trained BERT [3] language model as the backbone to encode text segments. Our architecture extends this approach by including weighted random sampling in the training process which speeds up the model convergence time and improves the overall performance. Concerning a more granular information extraction approach related to automatic consistency checks of financial disclosures, [8] introduced KPI-Check, a BERT-based system that makes use of a tailored named entity and relation extraction model [7] to automatically detect and validate semantically equivalent key performance indicators in financial reports.
When it comes to the NLP-based analysis of sustainability or Corporate Social Responsibility (CSR) reports, different aspects have been researched. [6] and [5] addressed the problem of automatically evaluating the GRI-and ESG 1 -accordance of CSR-reports. Both applied unsupervised text similarity measures building on GloVe (Global Vectors for word representation) embeddings. Similarly, [2] leveraged the language model RoBERTa [9] to predict 1 Environmental, Social and Governance factors. the relevance of sustainability reports according to the sustainable development goals in the USA. Specifically targeted for the banking sector, [11] developed a rule-based named entity recognition approach to estimate an index that displays the level of compliance of the climate-related financial disclosures with the TCFDfoot_0 recommendations.<h2>publication_ref</h2>['b15', 'b11', 'b2', 'b7', 'b6', 'b5', 'b4', 'b1', 'b8', 'b10']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>METHODOLOGY<h2>text</h2>In this section, we formally define the problem of matching text segments within documents to relevant legal requirements before turning to the in-depth analysis of our proposed architecture which is visualized in Figure 2.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_0']<h2>table_ref</h2>[]<h2>heading</h2>Problem Formulation<h2>text</h2>Given a sustainability report consisting of 𝑁 distinct text segments S, e.g. paragraphs, titles, tables or diagrams, and a set of 𝑀 regulatory checklist requirements R, our goal is to identify all semantically relevant text segments for each requirement. Since the number of requirements 𝑀 is static, but each document has a different length (number of text segments) 𝑁 , we initially model the described matching task from a segment-to-requirements perspective as a multi-label classification problem. Formally, for every 𝑠 𝑖 ∈ S our recommender model assigns relevance scores to all 𝑟 𝑗 ∈ R.
However, from the users' point of view, the reverse direction of getting relevant segment recommendations for a specific requirement 𝑟 𝑗 (requirement-to-segments perspective) is far more beneficial. This is especially true because a significant amount of text segments within a sustainability report is unrelated to concrete requirements in R. This is why, based on the assigned relevance scores, our model ranks the text segments per requirement in descending order and subsequently recommends the top 𝐾 relevant text blocks to the user.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Document Parsing<h2>text</h2>Before we focus on the actual recommender module, the core component of sustain.AI, we briefly touch upon the non-negligible task of document parsing. The large majority of publicly available sustainability reports are published as PDF documents, an inherently difficult format to convert into a structured machine-readable form like XML or JSON. The latter is particularly true for scanned PDF reports that only contain image information.
To solve this issue our system utilizes a custom PDF parser (see Figure 2), that is capable of parsing machine-created as well as scanned PDFs with arbitrarily complex formattings. The parser leverages a refined image segmentation technique by combining the powerful object detection network Faster R-CNN [14] with the density-based clustering algorithm DBSCAN [4]. It is also trained to recognize specific elements of a document, such as footers, headers or pagination. For further details about the parser's functionality, we refer to [1].
After the successful PDF parsing we apply some basic textual preprocessing in the form of removing line break hyphens and filtering out irrelevant text segment types like footer, header and table of contents. Our final set of considered segments S consists of titles, paragraphs, enumerations, tables and diagrams.<h2>publication_ref</h2>['b13', 'b3', 'b0']<h2>figure_ref</h2>['fig_0']<h2>table_ref</h2>[]<h2>heading</h2>Recommender System<h2>text</h2>Considering a parsed and processed sustainability report, we use a pretrained BERT [3] model to individually encode each text segment 𝑠 𝑖 ∈ S.
Formally, we first apply WordPiece [15] tokenization to transform an examplary input segment 𝑠 into a sequence of sub-word tokens 𝑡 = ([CLS], 𝑡 1 , . . . , 𝑡 𝑛 , [SEP]). Note [CLS] denotes a BERTspecific special token that aggregates the content of the entire segment while [SEP] simply highlights the end of the sequence.
Passing 𝑡 to the BERT model with pretrained parameters 𝑾 bert yields a sequence of contextual token embeddings 𝒉 [CLS] , 𝒉 1 , . . . , 𝒉 𝑛 , 𝒉 [SEP] , where 𝒉 [CLS] represents the aggregated context hidden state for the whole segment 𝑠.
Subsequently, we employ a multi-layer perceptron (MLP) with trainable parameters 𝑾 mlp to predict relevance probabilities ŷ = [ ŷ1 , . . . , ŷ𝑀 ] ∈ R 𝑀 for all requirements in R. The classifying MLP consists of a fully-connected hidden layer followed by dropout and ReLU (Rectified Linear Unit) activation functions and a sigmoidal output layer.
During training, we jointly optimize and finetune the parameters of the BERT model 𝑾 bert and the classification layer 𝑾 mlp to minimize the Binary Cross Entropy (BCE) loss between target labels 𝒚 and predicted probabilities ŷ.
Finally, after assigning relevance scores over requirements for all 𝑠 𝑖 ∈ S, we sort the segments for each requirement 𝑟 𝑗 in descending order in order to recommend the top 𝐾 relevant text blocks. <h2>publication_ref</h2>['b2', 'b14']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>EXPERIMENTS<h2>text</h2>In the following sections, we introduce our two custom data sets of German sustainability reports, define our evaluation metrics, discuss the overall training setup, describe the competing baseline methods, and finally, evaluate results.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Data<h2>text</h2>We train and evaluate our algorithms on two novel sustainability reporting data sets. The first data set, named GRI, consists of 92 published sustainability reports from major German companies. The reports have been sourced in PDF format from the companies' websites. After the parsing step domain experts from the auditing industry annotated all text segments in accordance to the requirements of the Global Reporting Initiative (GRI) standards. Concretely, we consider the 89 indicators of the GRI topic standards which cover the three main categories, economy, environment and social that are further split into granular topics like anti-corruption, energy consumption and human rights assessment. The annotation work load was equally split among three auditors which were supervised by a senior auditor. In multiple iterations, the created requirement labels have been validated and refined via double-checking randomly selected sample annotations and a qualitative inspection of the false positive and negative model predictions.
The second data set, named DNK, leverages the public sustainability reporting database from the German Sustainability Code 3 (DNK). The platform is used by the majority of German companies to annually disclose their sustainability activities with respect to 33 requirements from 20 DNK criteria, e.g. usage of natural resources and human rights. The categories and their requirements cover most of the GRI topics but are generally less granular. In contrast to the PDF documents of the GRI data set, the DNK reports in HTML format follow a predefined structure where each section of text segments answers a distinct requirement. Since the requirement descriptions precede their respective sections we can automatically retrieve the ground truth annotations from the HTML during the parsing process.
Table 1 displays descriptive statistics for both data sets. Due to the smaller amount of training documents, the greater document size and the annotation sparsity, we consider the GRI data set the harder challenge for our models. We separately train, optimize and 3 https://www.deutscher-nachhaltigkeitskodex.de/Home/Database. <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_0']<h2>heading</h2>Evaluation Metrics<h2>text</h2>We quantitatively evaluate all models by calculating modified mean sensitivity (MS) and mean average precision (MAP) scores for the top 𝐾 recommendations. While MAP punishes the lower ranked recommendations of relevant segments, MS only considers whether the relevant segments are contained in the set of recommendations. For a single document and a concrete requirement 𝑟 𝑗 the modified sensitivity S(𝐾) from [16] and the average precision AP(𝐾) are respectively defined as:
S(𝐾) = |top 𝐾 recommendations ∩ 𝐿 annotations| min(𝐾, 𝐿) ,(1)
AP(𝐾) = 1 min(𝐾, 𝐿) 𝐾 ∑︁ 𝑖=1 (P(𝑖) • rel(𝑖)) ,(2)
where 𝐿 denotes the number of relevant segment annotations, rel(𝑖) indicates whether the 𝑖 th recommendation is relevant (rel(𝑖) = 1) or not (rel(𝑖) = 0), and
P(𝑖) = |top 𝑖 recommendations ∩ 𝐿 annotations| 𝑖(3)
represents the precision score considering the top 𝑖 recommendations. Averaging S(𝐾) and AP(𝐾) over all checklist requirements 𝑟 𝑗 ∈ R and documents yields the subsequently reported mean sensitivity MS(𝐾) and mean average precision MAP(𝐾) metrics.<h2>publication_ref</h2>['b15']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Training Setup<h2>text</h2>In this section, we shed light on the training process and the hyperparameter optimization of sustain.AI. For all evaluated models we conduct an exhaustive grid search comparing various parameter combinations based on their validation set MAP(3) performance to determine the best training setup. Table 2 highlights the explored ranges and respective best values of sustain.AI's tuned model parameters. 4 https://github.com/LarsHill/dnk-dataset.  As encoding backbone we employ a BERT BASE model, published by the MDZ Digital Library team (dbmdz) 5 . It mirrors the architectural setup of the English BERT BASE counterpart 6 and is pre-trained on a large corpus of German books, news reports and Wikipedia articles. We train our model and all neural network based baselines via gradient descent utilizing the AdamW [10] optimizer with a linear warmup of 10% and a linearly decaying learning rate schedule. Additionally, we apply weight decay of 0.01 and gradient clipping with a maximum value of 1. We also analyze different learning rates, batch sizes, levels of dropout regularization, and MLP hidden dimensions, as can be seen in Table 2. For all training runs we set a random seed of 42 and fix the maximum number of epochs to 15 while applying early stopping with a patience of 3 epochs.
Due to the small percentage of annotated segments 𝑠 in the GRI data set (9%, see Table 1) we employ weighted random sampling (WRS) with replacement to expose these relevant segments more frequently during training. Concretely, we alter the originally uniform sampling probability of each segment to the normalized inverse frequency of relevant + or irrelevantoccurrences in the training set.
Figure 3 showcases the benefits of integrating WRS into the model training process for the GRI data set. We achieve a much faster training convergence and thus, save a considerable amount of training time and compute power benefitting from early stopping. At the same time, our model's MAP(3) score on the validation set increases by 3 percentage points.<h2>publication_ref</h2>['b9']<h2>figure_ref</h2>['fig_2']<h2>table_ref</h2>['tab_1', 'tab_1', 'tab_0']<h2>heading</h2>Baselines<h2>text</h2>We compare sustain.AI's end-to-end recommender model from Section 3.3 with 4 competing baseline architectures. For a fair comparison, all baselines make use of weighted random sampling concerning the imbalanced GRI data set.
First, we utilize word frequency-based Tf-Idf [13] representations that have been fitted on our respective training corpora. Prior to training, all segments have been preprocessed in terms of lowercasing, punctuation-and digit removal as well as stemming. The resulting 8000 dimensional segment vectors are then used as input for an ensemble of one-vs-rest binary logistic regression (LR) classifiers. Each classifier is trained for a specific requirement 𝑟 and a maximum of 100 iterations using the "liblinear" solver from the scikit-learn python library. Second, we pass the same Tf-Idf representations into an MLP with one hidden layer of dimensionality 1024. In contrast to the binary logistic regression heads, the MLP performs multi-label classification and predicts the relevant requirements simultaneously. We find an optimal batch size of 64 and a learning rate of 1𝑒-3.
Third, we exchange the Tf-Idf input vectors with frozen contextual embeddings from sustain.AI's BERT model. As classifiers we evaluate the previously defined MLP and a GRU (Gated Recurrent Unit). While the MLP takes BERT's CLS output embedding as input, the bidirectional GRU processes the resulting token representations of the frozen BERT model. Specifically, the last/first hidden state of the forward/backward GRU are concatenated and passed to a sigmoidal output layer. Optimal settings are obtained with a hidden size of 512 neurons, a batch size of 8 and a learning rate of 1𝑒-5.<h2>publication_ref</h2>['b12']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Results<h2>text</h2>We evaluate and compare sustain.AI and all baseline methods on the previously specified hold out test set for both the GRI and DNK data. Table 3 reports mean sensitivity (MS) and mean average precision (MAP) scores for the top 3 and top 5 recommendations.
First, it can be seen that the overall DNK performance across all methods is much better compared to the GRI data. This was expected, considering the reduced number of requirements and the larger amount of training documents and annotations.
Second, we find that the application of weighted random sampling (WRS) during training significantly improves the test set performance of our model. Compared to the version without WRS all metrics have increased by more than 6 percentage points. To enable a fair comparison we apply WRS during the training process of all baseline methods. Also, WRS is solely employed for the GRI data, since the DNK reports do not exhibit any annotation scarcity.
Finally, the results in Table 3 show the overall superiority of sustain.AI's end-to-end architecture, outperforming all baselines by a large margin.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_2', 'tab_2']<h2>heading</h2>CONCLUSION AND FUTURE WORK<h2>text</h2>We presented sustain.AI, an interactive, AI-powered tool for the semi-automated analysis of German sustainability reports. Our transformer-based model achieves promising results both on the well-structured DNK data set and on the real-world GRI data, compared to a number of strong baselines. Qualitative exploration of the results also suggests that it is indeed helpful in analyzing those long documents. The tool is planned to be deployed on an online platform soon and will then be openly accessible to the public.
Future work includes improving the current model with additional annotated data, which can easily be inferred from the user feedback we will collect through the tool. We also plan to extend the framework to English reports, as currently only the processing of German documents is possible. Another idea for improvement is to extract specific numeric key performance indicators from the reports, such as different types of CO 2 emissions, water consumption or indicators for social welfare.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>A clustering backed deep learning approach for document layout analysis<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Rhys Agombar; Max Luebbering; Rafet Sifa<h2>ref_id</h2>b1<h2>title</h2>A RoBERTa Approach for Automated Processing of Sustainability Reports<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Beyza Merih Angin;  Taşdemir; Arda Cenk; Gökcan Yılmaz; Mert Demiralp; Pelin Atay; Gökhan Angin;  Dikmener<h2>ref_id</h2>b2<h2>title</h2>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova<h2>ref_id</h2>b3<h2>title</h2>A densitybased algorithm for discovering clusters in large spatial databases with noise<h2>journal</h2><h2>year</h2>1996<h2>authors</h2>Martin Ester; Hans-Peter Kriegel; Jörg Sander; Xiaowei Xu<h2>ref_id</h2>b4<h2>title</h2>Mining company sustainability reports to aid financial decision-making<h2>journal</h2>AAAI<h2>year</h2>2020<h2>authors</h2>Tushar Goel; Palak Jain; Ishan Verma; Lipika Dey; Shubham Paliwal<h2>ref_id</h2>b5<h2>title</h2>Natural Language Processing Methods for Scoring Sustainability Reports-A Study of Nordic Listed Companies<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Marcelo Gutierrez-Bustamante; Leonardo Espinosa-Leal<h2>ref_id</h2>b6<h2>title</h2>KPI-BERT: A joint Named Entity Recognition and Relation Extraction Model for Financial Reports<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Lars Hillebrand; Tobias Deußer; Tim Dilmaghani; Bernd Kliem; Rüdiger Loitz; Christian Bauckhage; Rafet Sifa<h2>ref_id</h2>b7<h2>title</h2>Towards automating Numerical Consistency Checks in Financial Reports<h2>journal</h2><h2>year</h2>2022<h2>authors</h2>Lars Hillebrand; Tobias Deußer; Tim Dilmaghani; Bernd Kliem; Rüdiger Loitz; Christian Bauckhage; Rafet Sifa<h2>ref_id</h2>b8<h2>title</h2>RoBERTa: A Robustly Optimized BERT Pretraining Approach<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov<h2>ref_id</h2>b9<h2>title</h2>Decoupled Weight Decay Regularization<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Ilya Loshchilov; Frank Hutter<h2>ref_id</h2>b10<h2>title</h2>Application of text mining to the analysis of climate-related disclosures<h2>journal</h2>International Review of Financial Analysis<h2>year</h2>2022<h2>authors</h2>Angel-Ivan Moreno; Teresa Caminero<h2>ref_id</h2>b11<h2>title</h2>ALiBERT: improved automated list inspection (ALI) with BERT<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Rajkumar Ramamurthy; Maren Pielka; Robin Stenzel; Christian Bauckhage; Rafet Sifa; Tim Khameneh; Ulrich Warning; Bernd Kliem; Rüdiger Loitz<h2>ref_id</h2>b12<h2>title</h2>Using tf-idf to determine word relevance in document queries<h2>journal</h2><h2>year</h2>2003<h2>authors</h2>Juan Ramos<h2>ref_id</h2>b13<h2>title</h2>Faster r-cnn: Towards real-time object detection with region proposal networks<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Kaiming Shaoqing Ren; Ross He; Jian Girshick;  Sun<h2>ref_id</h2>b14<h2>title</h2>Japanese and korean voice search<h2>journal</h2><h2>year</h2>2012<h2>authors</h2>Mike Schuster; Kaisuke Nakajima<h2>ref_id</h2>b15<h2>title</h2>Towards automated auditing with machine learning<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Rafet Sifa; Anna Ladi; Maren Pielka; Rajkumar Ramamurthy; Lars Hillebrand; Birgit Kirsch; David Biesner; Robin Stenzel; Thiago Bell; Max Lübbering<h1>figures</h1><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Figure 2 :2Figure 2: Schematical visualization of the recommender system and the data flow in sustain.AI. A custom PDF parser processes the raw sustainability reports. After some textual clean-ups, a fine-tuned BERT model encodes individual text segments that are subsequently matched to relevant regulatory requirements.<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Figure 3 :3Figure 3: Positive impact of weighted random sampling (WRS) on training convergence and validation performance. We report the mean average precision considering the top 3 recommendations (MAP(3)) with and without WRS.<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>table<h2>figure_id</h2>tab_0<h2>figure_caption</h2>Properties of our GRI and DNK data sets. We display the number of requirements and documents, the average number of segments per document, the average percentage of segments assigned to at least one requirement, and the average number of matched segments per requirement.<h2>figure_data</h2>Data setGRI DNK# requirements8933# documents921779# segments 𝑠 per document972242% segments 𝑠 matched9100# matched segments 𝑠 per requirement 2.77.3<h2>figure_label</h2>2<h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>Evaluated hyperparameter configurations of sustain.AI. The best configuration on the validation set is highlighted in boldface.<h2>figure_data</h2>HyperparameterConfigurationsMLP hidden dimensions None, 512, 1024, 2048Dropout0.0, 0.1, 0.3, 0.5,Batch size2, 4, 8, 16Learning rate1𝑒 -4, 1𝒆-5, 1𝑒 -6evaluate our algorithms on both data sets to verify this hypothesis,investigating how well sustain.AI handles different sizes of trainingdata and number of labels. For our GRI and DNK experiments, weemploy fixed training, validation and testing splits of 65-15-20 and70-15-15, respectively.As a contribution to the open-source community and for furtherresearch concerning German sustainability reports we make theDNK data set publicly available 4 .<h2>figure_label</h2>3<h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>Test set results for the recommendation of relevant segments in GRI and DNK sustainability reports. sustain.AI outperforms all competing baselines in top 3/5 mean sensitivity (MS) and mean average precision (MAP).<h2>figure_data</h2>in %GRIDNKModelMSMAPMSMAP35353535Tf-Idf + LR24.3 33.7 17.1 19.5 70.8 66.3 66.8 59.0Tf-Idf + MLP33.0 39.8 22.6 24.2 77.4 77.8 74.8 71.8BERT frozen + MLP 28.4 36.1 21.0 22.4 75.2 70.6 73.5 66.4BERT frozen + GRU 28.1 36.8 20.5 22.1 84.0 80.2 83.0 77.2sustain.AI no WRS35.5 44.2 28.4 30.5 90.3 87.8 89.7 86.1sustain.AI WRS48.0 53.8 35.9 37.0-----= not applicable, since weighted random sampling (WRS) is only applied on GRI data.<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>S(𝐾) = |top 𝐾 recommendations ∩ 𝐿 annotations| min(𝐾, 𝐿) ,(1)<h2>formula_coordinates</h2>[4.0, 84.25, 418.66, 210.33, 20.76]<h2>formula_id</h2>formula_1<h2>formula_text</h2>AP(𝐾) = 1 min(𝐾, 𝐿) 𝐾 ∑︁ 𝑖=1 (P(𝑖) • rel(𝑖)) ,(2)<h2>formula_coordinates</h2>[4.0, 77.52, 445.33, 217.06, 24.75]<h2>formula_id</h2>formula_2<h2>formula_text</h2>P(𝑖) = |top 𝑖 recommendations ∩ 𝐿 annotations| 𝑖(3)<h2>formula_coordinates</h2>[4.0, 85.14, 516.7, 209.44, 18.77]<h1>doi</h1>10.1145/3594536.3595131<h1>title</h1>paper2repo: GitHub Repository Recommendation for Academic Papers<h1>authors</h1>Huajie Shao; Dachun Sun; Jiahao Wu; Zecheng Zhang; Aston Zhang; Shuochao Yao; Shengzhong Liu; Tianshi Wang; Chao Zhang; Tarek Abdelzaher<h1>pub_date</h1>2020-04-13<h1>abstract</h1>GitHub has become a popular social application platform, where a large number of users post their open source projects. In particular, an increasing number of researchers release repositories of source code related to their research papers in order to attract more people to follow their work. Motivated by this trend, we describe a novel item-item cross-platform recommender system, paper2repo, that recommends relevant repositories on GitHub that match a given paper in an academic search system such as Microsoft Academic. The key challenge is to identify the similarity between an input paper and its related repositories across the two platforms, without the benefit of human labeling. Towards that end, paper2repo integrates text encoding and constrained graph convolutional networks (GCN) to automatically learn and map the embeddings of papers and repositories into the same space, where proximity offers the basis for recommendation. To make our method more practical in real life systems, labels used for model training are computed automatically from features of user actions on GitHub. In machine learning, such automatic labeling is often called distant supervision. To the authors' knowledge, this is the first distant-supervised cross-platform (paper to repository) matching system. We evaluate the performance of paper2repo on real-world data sets collected from GitHub and Microsoft Academic. Results demonstrate that it outperforms other state of the art recommendation methods.<h1>sections</h1><h2>heading</h2>INTRODUCTION<h2>text</h2>This paper proposes a novel item-item recommender system that matches papers with related code repositories (on GitHub [8]) based on a joint embedding of both into the same space, where shorter distances imply a higher degree of relevance. The joint embedding is novel in considering both text descriptions (of papers and repositories) as well as their relations (to other papers and repositories) expressed in appropriately defined graphs.
The work is motivated by the growing popularity of GitHub as a platform for collaboration on open source projects and ideas. In recent years, more researchers from academia and industry have shared the source code of their research with others on GitHub. A particularly clear example of that trend in computer science is research on machine learning and data mining, where links are often provided to open source repositories in published papers. This trend is expected to grow, as it is attributed to the increasingly computation-intensive nature of modern scientific discovery. Advances in science are increasingly assisted by complex computing systems and algorithms. Reproducibility, therefore, calls for availing the research community not only of the published scientific results but also of the software responsible for producing them. In the foreseeable future, readers will frequently want to find the source code needed to repeat the experiments when searching for papers of interest on academic search systems, such as Google Scholar and Microsoft Academicfoot_0 . Our new cross-platform recommender system, paper2repo, addresses this need by automatically finding repositories relevant to a paper, even if pointers to the code were not included in the manuscript.
In the past few years, cross-platform recommender systems [3,9,29] attracted a lot of attention. Cross-platform recommender systems refer to those that leverage information from multiple different platforms to recommend items for users. Existing work mainly adopts transfer learning to enrich users' preference models with auxiliary information from multiple platforms. For example, Elkahky et al. [3] developed a multi-view deep learning model to jointly learn users' features from their preferred items in different domains of Microsoft service. Such prior work typically uses supervised learning to infer user preferences from their past behavior. In contrast, this paper designs a new item-item cross-platform recommender system, paper2repo, that matches repositories on GitHub to papers in the academic search system, without the benefit of prior access history or explicit labeling. Instead, we use joint embedding to quickly find repositories related to papers of interest, thereby seamlessly connecting Microsoft Academic with GitHub (acquired by Microsoft in June, 2018 2 ).
A simple candidate solution to our matching problem might be to search for keywords from paper titles in the database of repositories on GitHub. However, it may not recommend sufficiently diverse items to users. For instance, a user who is interested in the textCNN repository 3 may like the Transformer repository 4 as well, because both of them are related to natural language processing. Such a comprehensive set of relevant repositories is hard to infer from paper titles and, in fact, will often not be included in the text of the manuscript either.
Deep neural networks have recently been used to implement recommender systems. Among the recent advances, a semi-supervised graph convolutional network (GCN) [6,26,27] has been widely used for recommendation because it can exploit both the graph structure and input features together. However, past work applied the approach to single platforms only. Since papers and repositories live on two different platforms, their embeddings generated by traditional GCN are not in the same space. Our contribution lies in extending this approach to joint embedding that bridges the two platforms. The embedding requires constructing appropriate graphs for papers and repositories as inputs to the GCN, as well as selecting the right text features for each. We address these challenges with the following core techniques:
Context graphs: Relations between papers can be described by a citations graph. Relations between repositories are less obvious. In this paper, we leverage the tags and user starring to connect repositories together. There exists an edge between two repositories if there are overlapped keywords between them, or if they are starred by the same users. In addition, term frequency-inverse document frequency (TF-IDF) [16] is adopted to extract the important tags to construct the repository-repository context graph.
Constrained GCN model with text encoding. To map the embeddings of papers and repositories to the same space, we develop a joint model that incorporates a text encoding technique into the constrained GCN model based on their contents and graph structures. Since some research papers explicitly name (in the manuscript) their corresponding repositories of source code on GitHub, these papers could be used as bridge papers to connect the two cross platforms. Accordingly, we propose a constrained GCN model to minimize the distance between the embeddings of bridge papers and their original repositories. Moreover, we leverage a text encoding technique with convolutional neural networks (CNN) to encode information from the abstracts of papers, and the descriptions and tags of repositories as input features of the constrained GCN model. As a result, the proposed constrained GCN with text encoding can We conduct experiments to evaluate the performance of the proposed paper2repo system on real-world data sets collected from GitHub and Microsoft Academic. The evaluation results demonstrate that the proposed paper2repo outperforms other compared recommendation methods. It can achieve about 10% higher recommendation accuracy than other methods for the top 10 recommended candidates. To sum up, we make the following contributions to applications, methodology, and experimentation, respectively:
• Application: We develop a novel item-item cross-platform recommender system, paper2repo, that can automatically recommend repositories on GitHub to a query paper in the academic search system. • Methodology: We propose a joint model that incorporates the text encoding technique into the constrained GCN model to jointly learn and map the embeddings of papers and repositories from two different platforms into the same space. • Experimentation: Evaluation results demonstrate that the proposed paper2repo produces better recommendations than other state of the art methods. The rest of the paper is organized as follows. Section 2 presents the motivation, background, and formulation for our paper2repo problem. In Section 3, we introduce the overall architecture of the paper2repo recommender system. Section 4 evaluates its performance on real-world data sets. Section 5 summarizes related work. Finally, we conclude the paper in Section 6.<h2>publication_ref</h2>['b7', 'b2', 'b8', 'b28', 'b2', 'b5', 'b25', 'b26', 'b15']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>PRELIMINARIES<h2>text</h2>In this section, we first introduce relevant features of GitHub, and use a real-world example to motivate our work. We then review traditional graph convolutional networks (GCNs) that constitute the analytical basis for our solution approach. Finally, we formulate the problem of GitHub repository recommendation and offer the main insight behind the adopted solution.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Background and Motivation<h2>text</h2>GitHub is a social application platform, where users can collaborate on open source projects and share code with others. Users can create different repositories (i.e., digital directories) to store files and source code for their research projects. Fig. 1 illustrates an example of the Tensorflow repository for deep learning. The repository contains basic information such as descriptions, tags (also called topics), and user starring. Descriptions refer to a couple of sentences that describe the repository. Tags are keywords used to classify a repository, including its purpose and subject area. Starring a repository represents an expression of interest from a user in the repository. Much like browser bookmarks, stars give users improved shortcut-like access to repository information.
Consider a simple example to motivate features of our solution approach based on real-world data sets collected from GitHub and Microsoft Academic. Fig. 2 illustrates selected repositories and papers in the two different platforms. Repository R 1 is the original open source code for paper P 1 , ResNeXt [25], which presents a highly modularized network architecture for image classification developed by Facebook Research. The pair of R 1 and P 1 could be used as a bridge to connect the GitHub platform and Microsoft Academic platform. In addition, we find that users who star the repository R 1 also star certain other repositories related to image classification and deep neural networks. We collect 540 users who star repository R 1 and discover that 241 of them (such as user U 1 ) star R 4 as well. Thus, we infer that the two repositories starred by many of the same users are likely related to each other. We can also find repositories in the same subarea or topic that have similar tags. Take repository R 2 and R 3 , for example. These two repositories are related to image classification and they have the same tag "ImageNet". In this paper, we use both user starring and tags to construct a context graph for repositories.
In the papers domain, related papers are connected together via citation relationships. As illustrated in It has more than 100 citing/cited papers. Most of them are also involved in the same or similar areas as P 1 . For example, paper P 1 cites paper P 2 , P 3 , and P 4 that are related to image classification using deep neural networks. Paper P 3 cites P 2 and P 4 because they share the same research topics on deep residual networks. Hence, related papers can be identified from the paper citation graph. We further find that the more edges (connections) exist among papers, the more related they are. These observations form the basis of constructing the context graph for publications.
Finally, we feed both the context graphs (describing relations) and content information (describing nodes), such as raw paper abstracts and repository descriptions, into the graph convolutional neural networks framework to perform joint embedding.<h2>publication_ref</h2>['b24']<h2>figure_ref</h2>['fig_0', 'fig_2']<h2>table_ref</h2>[]<h2>heading</h2>Graph Convolutional Networks<h2>text</h2>Graph convolutional networks (GCNs) [2,12] have been widely used in classification and recommendation systems because they encode both the graph neighborhood and content information of nodes to be classified or recommended. The basic idea of GCNs is to learn the embeddings of nodes based on both their input features, X, and graph structures. The graph structure is often denoted by an adjacency matrix, A. A multi-layer GCN forward model could be expressed by:
H (l +1) = σ D-1 2 Ã D-1 2 H (l ) W (l ) ,(1)
where Ã = A + I N , A is the adjacency matrix for the input graph structure and I N is the identity matrix, Dii = j Ãij , W (l ) is the weight matrix to be trained, σ (.) is the activation function such as ReLU, H (l ) is the hidden layer, l is the layer index and H (0) = X is the input features of nodes. Because the embeddings of papers and repositories generated by traditional GCNs for the two different platforms are not in the same space, it is necessary for us to jointly train the corresponding neural networks subject to a constraint that aligns the projections of related papers and repositories onto the same (or neighboring) points in the embedding space. The above challenge leads to the following research problem formulation.<h2>publication_ref</h2>['b1', 'b11']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Problem Statement and Solution Idea<h2>text</h2>Suppose there are N p papers in an academic search system, such as Microsoft Academic, and N r repositories on GitHub. Some papers explicitly name their corresponding original repositories of source code on GitHub. We call these papers bridge papers. They can be used to connect the two platforms. For instance, paper P 1 in Fig. 2 is a bridge paper that connects to its original repository R 1 on GitHub. Each paper has an abstract, while each repository has tags and descriptions. Our main task is to learn high-quality embeddings of papers and repositories that can be used for recommendation across the two different platforms based on their content information and graph structures.
To this end, we construct two undirected context graphs, one for the paper platform and one for the repository platform, respectively. These graphs are introduced below:
• Paper-citation graph: We model the papers and their references as an undirected paper-citation graph, because only the connection between two papers is required in this work.
We adopt an adjacency matrix with binary measurements to denote the graph structure. In this graph, each paper, p i , is a node. Each node has an abstract as its input attributes. • Repository-repository context graph: Since there is no direct citation relationship between repositories, it is more difficult to construct the repository-repository context graph. Motivated by the example in Section 2.1, we leverage tags and user starring to construct this graph. Specifically, as previously motivated in Fig. 2, we add a binary edge between two repositories if they are starred together by at least one user or share at least one term whose TF-IDF score is over a threshold (e.g., 0.3) in their description or tags. While binary edges ignore information available on the degree of overlap (in tags or starring users), we find that the simplification is adequate, especially that (at the end of the day) our embedding also considers full text of paper abstracts, repository descriptions, and tags.
After obtaining these two context graphs, we can combine the graph structures with the input content information of papers and repositories to generate high-quality embeddings. We can then compute similarity scores between neighboring paper and repository embeddings to recommend highly related repositories to a query paper. Namely, the top (i.e., nearest) repository candidates will be recommended to the query papers. Below, we elaborate this general idea in more detail.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2', 'fig_2']<h2>table_ref</h2>[]<h2>heading</h2>MODEL<h2>text</h2>In this section, we first introduce the overall architecture of the proposed paper2repo recommender system, then present techniques for model training.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>paper2repo Architecture<h2>text</h2>The key contribution of this paper lies in developing a joint embedding that incorporates both text encoding and constrained GCN to generate embeddings that take into account similarity (between repositories and papers) in both text features and context graph structures.
Fig. 3 shows the overall architecture of the proposed paper2repo system. There are two main modules: text encoding and constrained GCN. The first module tries to develop a text encoding technique to encode the content information of papers and repositories into vectors as inputs of the constrained GCN model. The second module implements the GCN model with an added constraint. The constraint specifies that repositories explicitly mentioned in paper descriptions must be mapped to the same place as the referring paper, thereby linking the mappings of repositories and papers into a single consistent space. We detail these two modules in the following.
Text encoding. Fig. 4 shows the detailed framework of using text encoding to learn the features from both the tags and descriptions of repositories on GitHub. In general, the descriptions of repositories are limited and simple, so it is difficult to learn the overall features of repositories from them. Hence, we add the tags from repositories to enrich the descriptions, because tags often extract some key information about the topics of the repositories on GitHub. Therefore, we design the text encoding module to encode the features of both tags and descriptions. As illustrated in Fig. 4, there are four steps for description encoding and tag encoding of a repository: We first use Step 1 and Step 2 to encode the descriptions of a repository as shown in Fig. 4. Let x i ∈ R k denote the k-dimensional word vector of the i-th word in the description. The word vector comes from the pre-trained embeddings of Wiki words offered by the GloVe algorithm [15]. The length of each description is fixed to n (padded or cropped where necessary). So a description can be defined by
x 1:n = x 1 ⊕ x 2 ⊕ . . . ⊕ x n ,(2)
where ⊕ is the concatenation operator that concatenates all the embeddings of words for a sentence. Then we apply filters w ∈ R hk for the convolution operation in order to learn new features, where h is the window size of words for filters. Each filter computes the convolution to generate a new feature, c i , from a possible window of words x i:i+h-1 , which can be expressed by
c i = f (w.x i:i+h-1 + b),(3)
where b is a bias parameter and f (.) is a non-linear activation function such as ReLU. As the window slides from x 1:h to x n-h+1:n , the filter yields a feature map
c = [c 1 , c 2 , . . . , c n-h+1 ].(4)
After obtaining the feature map, a max-over-time pooling operation is adopted to get the maximum value, max{c}, of each output feature map as illustrated in Step 2.
Next, we implement Step 3 to encode tags. Since there is no sequence for tags, we leverage fully connected layers to learn their features. For each tag, we first use the fastText trick [11] to get its word representation via merging the embeddings of each word. Here, fastText is a simple text encoding technique where word embeddings are averaged into a single vector. As illustrated in Fig. 4, the embedding of "machine learning" would be denoted by a new word vector by adding the word vectors of "machine" and "learning". We then apply fully connected layers to produce the new features whose dimensions are aligned with the number of feature maps for description encoding. After that, feature fusion is adopted in Step 4 to add the new features of tags generated in Step 3 to the produced features in Step 2. Finally, the new features, v j , integrated with description encoding and tag encoding are input of the constrained GCN model. In order to improve the stability of model training, we adopt batch normalization to normalize the new features.
Similarly, for abstract encoding of a paper, we can propose the same methodology to learn the new features as the description encoding of a repository in Step 1 and Step 2. For brevity, we will not describe it in detail.
Constrained GCN model. Since papers and repositories are in two different platforms, the generated embeddings by the traditional GCN are not in the same space. Thus, we propose a constrained GCN model to constrain the embeddings to the same space. Specifically, it leverages the general GCN as the forward propagation model in Equation (1) and minimizes the distance between the embeddings of some bridge papers and their original repositories as a constraint. We use the cosine similarity to measure the distance. In order to compute the cosine similarity distance for the embeddings, we normalize their embeddings as p i and r j , respectively. Let p ′ i and r ′ i denote the normalized embeddings of the i-th bridge paper and the corresponding original repository, respectively. Then, for each pair of bridge paper and repository, the corresponding constraint could be expressed as:
1 -p ′ i ⊤ r ′ i ≤ ϵ,(5)
Paper abstract<h2>publication_ref</h2>['b14', 'b10', 'b0']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Repository tags & descriptions Text Encoding<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Text encoding<h2>text</h2>Constrained GCN<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Hidden Layers GCN Input<h2>text</h2>Embedding Layer Score Ranking  where ϵ is a small error term, such as 0.001.
In this paper, we adopt weighted approximate-rank pairwise (WARP) loss [24] to train the paper2repo model in order to recommend the target repositories to a query paper at the top of the ranked candidates. Let L be the WARP loss for the labeled pairs of papers and repositories during training, and m be the number of pairs for bridge papers and their original repositories in the training data. Then, the constrained GCN model is defined as:
min L subject to m i=1 (1 -p ′ i ⊤ r ′ i ) ≤ ϵ.(6)
In the above (6), the WARP loss function L is defined by:
L = n k L rank ∆ p ⊤ r + |∆ -p ⊤ r + + p ⊤ r n k | + rank ∆ p ⊤ r + ,(7)
where (0 < ∆ < 1) is the margin hyper-parameter; n k denotes the number of negative examples in a batch during model training; r + and r n k denote the representations of positive and negative repositories, respectively; |t | + is the positive part of t. In addition, L(.) and rank ∆ p ⊤ r + will be introduced below. First, L(.) in ( 7) is a transformation function which transforms the rank into a loss, defined as
L(K) = K j=1 1 j ,(8)
where K denotes the position of a positive example in the ranked list.
In addition, rank ∆ p ⊤ r + is the margin-penalized rank of positive examples, defined as:
rank ∆ p ⊤ r + = n k I ∆ -p ⊤ r + + p ⊤ r n k > 0 ,(9)
where I (.) is the indicator function that outputs 1 or 0. After defining the WARP loss, the next step is to solve the above non-linear optimization model in (6). One possible solution is to transform it into a dual problem. Based on the method of Lagrange multipliers, we can rewrite the above constrained model (6) as:
min L + λ m i=1 (1 -p ′ i ⊤ r ′ i ).(10)
It is very hard to obtain a closed-form solution that minimizes the objective function (10) due to the nonlinear nature of the optimization problem. Instead, we feed this objective function as a new loss function to the neural network. Convergence of training of this neural network to a solution that meets both the original loss function, L (left term in objective function 10), and the joint embedding constraint (right term in objective function 10), requires that the two terms have comparable weights. Otherwise, learning gradients will favor optimizing the larger term and ignore the smaller. Unfortunately, the loss function L dynamically drops during training, making it difficult to choose the hyper-parameter, λ, to create gradients that properly trade off the loss function, L, and the constraint error. To address this issue, we replace λ in the second term with the varying WARP loss, L, and normalize the constraint error instead of using the total error. Accordingly, our model can be formulated as:
min 1 + C e L,(11)
where C e is the average constraint error, defined as:
C e = 1 2m m i=1 (1 -p ′ i ⊤ r ′ i ),(12)
Note that, C e ∈ [0, 1]. This is because the cosign similarity for any two vectors, p ′ i ⊤ r ′ i , ranges between 1 and -1. Thus, each term in the summation above ranges between 0 and 2. The entire summation ranges between 0 and 2m, and the normalization leads to a value of C e between 0 and 1. Using the mean constraint error, C e , helps keep the scale of the two terms in the objective function (11) comparable. While hypothetically, we can even remove the constant, 1, from objective function (11), we find that keeping it improves numeric stability and convergence.
In the new formulation, we no longer need to dynamically adjust the hyper-parameter, λ, for model training. The constrained optimization problem in ( 6) can be solved by minimizing the new loss function (11) by feeding it to the graph neural network.<h2>publication_ref</h2>['b23', 'b5', 'b9', 'b10', 'b10', 'b10']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Model Training<h2>text</h2>We summarize the main notations used in our model in Table 1. The output of our trained network is the ranked inner products of the embeddings for pairs of papers and repositories. The closer the embeddings of a paper and a repository, the higher their inner product, and the higher the ranking. Given that output, one can identify the top ranked recommended repositories for a given paper (or the top ranked papers for a given repository). Training computes the weights and biases of each convolutional neural network layer, the weights and biases of each fully connected layer (for tag encoding), and the weights and biases of filters for the convolution operation. Besides, we need to tune the number of filters used for text encoding, the filter window h, the number of fully connected layers for tag encoding of repositories, the number of hidden layers of neural networks, and the dimension of output embeddings of each node as well as the margin hyper-parameter for WARP loss. We set the output dimension of representations (embeddings) of each paper and repository equal in order to compute their similarity scores. We then select positive and negative samples as follows:
• Positive samples: We use the bridge papers and the corresponding repositories as labeled training data. Let p i be the i-th paper and r j be its highly related repositories. Such pairs, (p i , r j ), constitute positive samples. We further hypothesize that if users, who star repository A, also star repository B more often than C, then (on average) B is more related than C to repository A. Thus, besides bridge repositories, we collect the top T related repositories, ranked by the frequency they are starred by users who star the original bridge repository. In order to get more training data, we also sample some one-hop neighbors of bridge papers combined with the corresponding bridge repositories to be positive examples  (at most T ). Different values of T represent how liberally one interprets "related" (and will result in different recommender performance). We can think of this method as a form of distant supervision; an imperfect heuristic to label more samples than what can be inferred from explicit bridge references. It is important to note that we do not use the same heuristic for recommender evaluation. Rather, as we describe in a later section, we use Mechanical Turk workers to evaluate resulting recommendation accuracy, allowing us to understand the efficacy of our distant supervision framework. • Negative samples: We also introduce negative examples, referring to repositories that are not highly related to a query paper. In this work, we randomly sample n k negative examples of repositories across the entire graph to train the model. We expect the similarity scores of positive examples to be larger than those of negative examples.
Ã = A p + I N , D = j Ãi j , H (0) = X p ; 3 p i ← ReLU D-1 2 Ã D-1 2 H (l ) W (l ) ; 4 p i ← p i / ∥p i ∥ 2 ; 5 /* For repo embeddings */ 6 Ã = A r + I N , D = j Ãi j , H (0) = X r ; 7 r j ← ReLU D-1 2 Ã D-1 2 H (l ) W (l ) ; 8 r j ← r j /
We briefly summarize the proposed constrained GCN algorithm in Algorithm 1. In this algorithm, the first l layers learn the hidden vectors (embeddings) of each paper and repository using the GCN algorithm in (1). Then we use the normalized embeddings of paper p i , and repository r j to compute their similarity scores. Additionally, we try to minimize the distance between the embeddings of bridge papers and their original repositories as an added constraint.<h2>publication_ref</h2>['b0']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_0']<h2>heading</h2>EXPERIMENTS<h2>text</h2>We carry out experiments to evaluate the performance of the proposed paper2repo on the real-world data sets collected from GitHub and Microsoft Academic. Further, we show that the proposed model performs well on larger data sets without any hyper-parameter tuning. In addition, we conduct ablation experiments to explore how design choices impact performance. Finally, we conduct a case study to evaluate the effectiveness of the proposed method.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Datasets<h2>text</h2>We collected a repository data set and a research paper data set from GitHub and Microsoft Academic, respectively. Microsoft Academic provides an API for users to query the detailed entities of academic papers, such as paper title, abstract, and venue names. For a proof of concept, we query the top 20 venue names and 8 journals of computer science, such as KDD, WWW, ACL, ICML, NIPS, CVPR and TKDE, to retrieve the entities of 59, 404 raw papers from year 2010 to 2018. After that, we query for the titles of these papers to collect some original open source repositories through the GitHub API. We obtain about 2, 427 original repositories corresponding to the named papers. We define bridge papers as those for which we found a matching repository (which we call the bridge repository). In addition, we collect about 8, 000 popular repositories from users who star the bridge repositories. After data cleaning and preprocessing, we have 32, 029 research papers and 7, 571 repositories, including 2, 107 bridge repositories. In our experiments, we evaluate the performance of the proposed model on both small and full data sets as illustrated in Table 2. Testing and ground truth estimation. To evaluate the accuracy of our recommender system, its results need to be compared to ground truth (that is not part of training data). For that purpose, we asked human graders on Amazon Mechanical Turk (AMT) to evaluate produced recommendations. Specifically, for each pair to be evaluated, we provided the graders with (i) the paper title, abstract, and (ii) the description, ReadMe (if available) and URL the corresponding repository on GitHub. The graders were asked to grade similarity on a three point scale: Score âĂĲ2âĂİ meant that the paper and repository were highly related; score âĂĲ1âĂİ meant that they were somewhat related; and score âĂĲ0âĂİ meant that the pair was not related. Three graders were used per pair.
After labelling, we considered those pairs that received a score of 1 or 2 from all graders to be correct matches. Pairs that received a score of 1 or 0 from all graders (and received at least one zero) were considered to be incorrect matches. Finally, pairs where graders disagreed, receiving scores 0 and 2 together (about 7% result), were graded by an additional grader. An average was then computed. If the average score was less than 1, the pair was considered unrelated (an incorrect match). Otherwise, it was considered a correct match.
It remains to describe how we partition data into the training set and test set. We already described how positive and negative training pairs were selected. It is tempting to try and choose test papers and repositories at random. However, due to the large number of papers and repositories available on the respective platforms, this random sampling leads to a predominance of unrelated pairs, unless the test set is very large (which would be very expensive to label). Clearly, restricting testing to predominantly unrelated papers and repositories would artificially reduce the recommender's ability to find matches. In other words, the test set has to include related pairs. Hence, we first selected 580 bridge papers and their one-hop neighbors not present in the training set. We then included their repositories as well as the two hop neighborhood of those repositories according to the context graph. Thus, for each paper in the test set, we included some repositories that are likely related to different degrees, and hundreds of repositories that are likely not related (i.e., repositories related to other papers). This allowed for a much richer diversity in the test set and Mechanical Turk outputs. It is important to underscore that the criteria above were used merely for paper and repository selection for inclusion in the test set. The labeling of the degree of match for pairs returned by the recommender was done strictly by human graders and not by machine heuristics.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_2']<h2>heading</h2>Experimental Settings<h2>text</h2>Baseline methods. We compare the proposed paper2repo with the algorithms below:
• NSCR [23]: This is a cross-domain recommendation framework that combines deep fully connected layers with graph Laplacian to recommend items from information domain to social domains. • KGCN [21]: This method leverages knowledge graph and graph convolutional neural networks to recommend interested items to users. • CDL [20]: This is a hybrid recommendation algorithm, which jointly performs deep representation learning for the content information and collaborative filtering for the ratings matrix. • NCF [7]: This is a neural collaborative filtering model for recommendation, which combines the matrix factorization (MF) and MLP to learn the user-item relationship.
• LINE [18]: This is a graph embedding algorithm that uses BFS to learn the embedding of nodes in the graph with unsupervised learning. In order to better perform LINE, we construct the entire graph of papers and repositories via the bridge papers and their original repositories. • MF [19]: This method is widely used for traditional recommender systems due to its good performance for dense data.
It is a supervised algorithm, so we use 50% positive examples as training data and the remaining data as testing data.
• BPR [17]: This is an optimized MF model with Bayesian analysis for implicit recommendation. It is a supervised model and we use the same method as MF to do experiments.
Evaluation measures. In order to evaluate the performance of the proposed paper2repo recommender system, this paper adopts three commonly used information retrieval measures: HR@K (hit ratio), MAP@K (mean average precision), MRR@K (mean reciprocal rank).  In general, HR is used to measure the accuracy about the number of correct repositories recommended by the paper2repo system, while MAP and MRR measure the accuracy of the rank of recommended items.
Model architecture and parameters. In order to compare the performance of our proposed paper2repo to the other recommender algorithms, we tune the hyper-parameters during model training. We use 90% of training data to train the model and the remaining training data as validation data. After extensive experiments (not shown), we obtain the best parameters for our model below: we set the number of layers to 2 for the graph convolutional neural networks and the size of each layer to 256. The number of fully connected layers is 2 for tags encoding of repositories. The length of each abstract from papers, and description of repositories is fixed to 200 and 50, respectively. For paper abstract encoding, the filter window (h) is 2, 3, 5, 7 with 64 feature maps each. For repository descriptions encoding, its filter windows (h) is 2, 4 with 64 and 32 feature maps, respectively. The pre-trained embeddings of words produced by GloVe are used with size of 200. We set the learning rate to 0.0005. For training, we set the number of positive repositories, T , to 6 given a paper. In addition, we randomly sample 44 negative samples to train the model. We set the margin hyper-parameter, ∆, to 0.5 in this experiment. For testing, we sample 50 examples include T positive examples and the remaining 50 -T negative example to measure the three metrics above. Our evaluation results are obtained by averaging 3 runs per point.<h2>publication_ref</h2>['b22', 'b20', 'b19', 'b6', 'b17', 'b18', 'b16']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Performance Comparison<h2>text</h2>To understand dependence on scale, we start with a subset of 11, 272 papers and 7, 516 repositories, including 1, 386 pairs of bridge papers and original repositories, to conduct experiments (later we shall use the full data set). We compare the performance of the proposed paper2repo with the competing methods. Fig. 5 illustrates the comparison results of different methods as K increases with three measures: HR@K, MRR@K and MAP@K. We can observe that the proposed paper2repo outperforms the other recommendation algorithms for all the three metrics. This is because our paper2repo encodes both the contents and graph structures of papers and repositories to learn their embeddings. In addition, paper2repo performs better than the cross-domain NCSR algorithm for two reasons. First of all, NCSR leverages spectral embeddings method without using node features in social domain, while our model encodes both content information and graph structures. Secondly, the items in our data sets have more attributes (keywords and tags) than that (20 attributes) in the NCSR paper, making it hard to train the NCSR model. In Fig. 5(c), we also find that the MAP of paper2repo slightly drops. This reason is that more positive repositories are recommended but not at the top of ranking list, leading to lower MAP. Moreover, we can observe that CDL and NCF do not perform well in our case.
The main reason is that the number of positive examples for model training is very small such that they suffer from data sparsity as the traditional MF. Besides, KGCN does not work very well because it only adopts one hot embedding without taking node features into account in the paper domain.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_6', 'fig_6']<h2>table_ref</h2>[]<h2>heading</h2>Performance on Larger Data Sets<h2>text</h2>We conduct another experiment to evaluate the performance of the paper2repo on larger data sets. We do not otherwise perform any data-set-specific tuning. We change the number of papers from 11, 272 to 32, 029, and the number of pairs of bridge papers and their original repositories from 1, 386 to 2, 107. Fig. 6 illustrates the comparison results for two different sizes of data sets. From this figure, it can be observed that the proposed paper2repo performs better on larger data sets than that on small data sets in terms of MAP and MRR, and their hit ratios are comparable. The is because our model can learn more information from different nodes in a large-scale graph.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_7']<h2>table_ref</h2>[]<h2>heading</h2>Ablation Studies<h2>text</h2>Effect of Pre-trained Embeddings. We first explore how the settings of pre-trained embeddings impact the performance of the proposed paper2repo. We compare three different cases: (i) fixed pre-trained embeddings; (ii) pre-trained embeddings are used as initialization (not fixed); (iii) fixed and not fixed embeddings are concatenated together. Table 3 illustrates the comparison results for these three cases when K = 10. We can see that when the pretrained embeddings are fixed, it performs better than the other two cases. The main reason is that the pre-trained embeddings are produced from the large Wikipedia corpus by GloVe. The performance of concatenated embeddings seems not very good, because it is more difficult to train the model with more complicated networks. Effect of top T positive repositories . Next, we explore how the number of positive repositories in training, T , influences the performance of paper2repo. In our experiment, the number of positive repositories, T , varies from 3 to 7 with step 1. From Table 4, it can be seen that the performance of paper2repo gradually boosts as T increases from 3 to 6. When T = 6 and 7, their performance are close to each other. Thus, we can find that paper2repo has a better performance when the number of positive repositories in training is large in some degree. In this paper, we use T = 6 to do experiments because most of papers in the testing data labelled by graders have at most 6 positive examples.
Effect of the Margin Hyper-parameter. We also study the impact of the margin hyper-parameter on the performance of the paper2repo. We change the margin parameter, ∆, from 0.1 to 0.7 with step 0.1 while keeping the other hyper-parameters unchanged.  We further discuss the advantages and disadvantages of pape2repo.
According to our experiments, we discover that the recommended repositories are relevant to a query paper when there exists substantial user co-starring between bridge repositories and other repositories or when there are multiple overlapped tags between them. The main reason is that two repositories starred by many of the same users or that have multiple overlapped keywords are very likely to involve similar research topics. However, when only few users star both repositories, the recommendation performance is not very good. For example, when two repositories are only starred by a couple of users together, it is hard to judge whether these two repositories are similar or not. As a result, the recommended repositories seem not to be very relevant to the query papers. The need to find a sufficient number of bridge repositories is another limitation. Extensions of this joint embedding framework to domains with no natural analogue to bridge papers/repositories can be a good topic for future investigation. Cold start is one of the most important research topics in recommender systems. As we know, when users release new source code, their repositories have very little user starring in the beginning. Thus, in practice, cold start is an issue similar to lack of sufficient co-starring, discussed above. Our pape2repo is able to partially deal with cold start because, as mentioned in Section 2.3, we construct the repository-repository graph using tags as well. Even if a repository has very few stars, we can still use its tags to construct the repository graph. Therefore, we can still recommend some repositories to the query papers, although the qualify of recommendation will be impacted.
The accuracy of our evaluation results is impacted by the accuracy of estimating ground truth. Due to budget constraints, we used three Mechanical Turk graders per item, when they coincided. Grading reliability can be improved by increasing the graders and performing grader training.
Other limitations include the general applicability of repository recommendations. While software libraries are becoming an increasingly important research enabler, some fields (including theory) are less likely to benefit from this system. The system is also less relevant to research enabled by artifacts not in the public domain. One may also argue that authors will tend to cite relevant repositories as a best practice. This is true, but our recommender system can also uncover subsequently created repositories that impact a given paper, such as novel implementations of relevant libraries, or algorithm implementations on new hardware platforms and accelerators. This is the same reason why one might not exclusively rely on citations in a paper to find other relevant papers, especially if the paper in question is a few years old.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_3', 'tab_4']<h2>heading</h2>RELATED WORK<h2>text</h2>This section reviews related work on recommender systems that use deep learning and graph neural networks, especially for crossdomain recommendations.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Cross-domain Recommendation<h2>text</h2>In order to deal with data sparsity and cold start issues in recommender systems, recent research proposed cross-domain recommendations [3,9,10,23] to enrich user preferences with auxiliary information from other platforms. Cross-domain recommendations get rich information on items that users prefer buying based on their history on multiple platforms. For example, some users like to purchase furniture on Walmart and buy clothes on Amazon. So we can recommend furniture to the users on Amazon next time, which can improve the diversity of recommendations. Motivated by this observation, a multi-view deep learning model was proposed by Elkahky et al. [3] to jointly learn the users' features from their past preferred items on different domains. Following this work, Lian et al. [13] further developed a CCCFNet model that integrates collaborative filtering and content-based filtering in a unified framework to address the data sparsity problem. However, these algorithms mainly adopt the idea of transfer learning to boost users' preferences via jointly learning users' items on multiple platforms or domains. In addition, Wang et al. [23] proposed a user-item crossdomain framework, NSCR, to recommend items from information domains to users on social media. However, a limitation is that NSCR is a supervised learning algorithm. In contrast, we propose a item-item cross-domain recommendation framework with a distantsupervised learning model (without human labelling for training).<h2>publication_ref</h2>['b2', 'b8', 'b9', 'b22', 'b2', 'b12', 'b22']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Recommendation with Graph Neural Networks<h2>text</h2>In recent years, graph neural networks have been widely proposed in recommender systems [1,5,14,22,27,28] because they can encode both node information and graph structure. For instance, Fan et al. [5] developed a new framework, called GraphRec, to jointly capture interactions and opinions in the user-item graph. In addition, Shaohua et al. [4] proposed a meta-path guided heterogeneous Graph Neural Network for intent recommendation on an e-commerce platform. However, these efforts apply graph neural networks to recommender sytems on a single platform. Different from prior work, we propose a novel item-item crossdomain recommendation framework that automatically recommends related repositories on GitHub to a query paper in an academic search system, helping users find their repositories of interest quickly by a joint embeddings across platforms.<h2>publication_ref</h2>['b0', 'b4', 'b13', 'b21', 'b26', 'b27', 'b4', 'b3']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>CONCLUSIONS AND FUTURE WORK<h2>text</h2>This paper developed an item-item cross-platform recommender system, paper2repo that can automatically recommend repositories on GitHub matching a specified paper in the academic search system. We proposed a joint model that incorporates a text encoding technique into a constrained GCN formulation to generate joint embeddings of papers and repositories from the two different platforms. Specifically, the text encoding technique was leveraged to learn sequence information from paper abstracts and descriptions/tags of repositories. In order to map the representations of papers and repositories onto the same space, we adopted a constrained GCN model that forces the embeddings of bridge papers and their corresponding repositories to be equal as a constraint. Finally, we conducted experiments to evaluate the performance of paper2repo on real-world data sets. Our evaluation results demonstrated that the proposed paper2repo systems can achieve a higher recommendation accuracy than prior methods. In the future, we can extend our method to consider other entities (such as venues and authors), in addition to papers, to construct the knowledge graph. We can improve text embedding (e.g., by use of phrase embedding instead of word embedding). We can also investigate generalizations of our joint embedding, especially to domains with no analogue to bridge papers to serve as mapping constraints.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>An E icient Adaptive Transfer Neural Network for Social-aware Recommendation<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Chong Chen; Min Zhang; Chenyang Wang; Weizhi Ma; Minming Li; Yiqun Liu; Shaoping Ma<h2>ref_id</h2>b1<h2>title</h2>Convolutional neural networks on graphs with fast localized spectral filtering<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>Michaël Defferrard; Xavier Bresson; Pierre Vandergheynst<h2>ref_id</h2>b2<h2>title</h2>A multi-view deep learning approach for cross domain user modeling in recommendation systems<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Ali Mamdouh; Elkahky ; Yang Song; Xiaodong He<h2>ref_id</h2>b3<h2>title</h2>Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Junxiong Shaohua Fan; Xiaotian Zhu; Chuan Han; Linmei Shi; Biyu Hu; Yongliang Ma;  Li<h2>ref_id</h2>b4<h2>title</h2>Graph Neural Networks for Social Recommendation<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Wenqi Fan; Yao Ma; Qing Li; Yuan He; Eric Zhao; Jiliang Tang; Dawei Yin<h2>ref_id</h2>b5<h2>title</h2>Representation learning on graphs: Methods and applications<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Rex William L Hamilton; Jure Ying;  Leskovec<h2>ref_id</h2>b6<h2>title</h2>Neural collaborative filtering<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Xiangnan He; Lizi Liao; Hanwang Zhang; Liqiang Nie; Xia Hu; Tat-Seng Chua<h2>ref_id</h2>b7<h2>title</h2>Open Source Repository Recommendation in Social Coding<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Jyun-Yu Jiang; Pu-Jen Cheng; Wei Wang<h2>ref_id</h2>b8<h2>title</h2>Social recommendation with cross-domain transferable knowledge<h2>journal</h2>IEEE Transactions on Knowledge and Data Engineering<h2>year</h2>2015<h2>authors</h2>Meng Jiang; Peng Cui; Xumin Chen; Fei Wang; Wenwu Zhu; Shiqiang Yang<h2>ref_id</h2>b9<h2>title</h2>Little is much: Bridging cross-platform behaviors through overlapped crowds<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>Meng Jiang; Peng Cui; Nicholas Jing Yuan; Xing Xie; Shiqiang Yang<h2>ref_id</h2>b10<h2>title</h2>Bag of tricks for efficient text classification<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>Armand Joulin; Edouard Grave; Piotr Bojanowski; Tomas Mikolov<h2>ref_id</h2>b11<h2>title</h2>Semi-supervised classification with graph convolutional networks<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>N Thomas; Max Kipf;  Welling<h2>ref_id</h2>b12<h2>title</h2>CCCFNet: a content-boosted collaborative filtering neural network for cross domain recommender systems<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Jianxun Lian; Fuzheng Zhang; Xing Xie; Guangzhong Sun<h2>ref_id</h2>b13<h2>title</h2>Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Namyong Park; Andrey Kan; Xin ; Luna Dong; Tong Zhao; Christos Faloutsos<h2>ref_id</h2>b14<h2>title</h2>Glove: Global vectors for word representation<h2>journal</h2><h2>year</h2>2014<h2>authors</h2>Jeffrey Pennington; Richard Socher; Christopher Manning<h2>ref_id</h2>b15<h2>title</h2>Using tf-idf to determine word relevance in document queries<h2>journal</h2><h2>year</h2>2003<h2>authors</h2>Juan Ramos<h2>ref_id</h2>b16<h2>title</h2>BPR: Bayesian personalized ranking from implicit feedback<h2>journal</h2><h2>year</h2>2009<h2>authors</h2>Steffen Rendle; Christoph Freudenthaler; Zeno Gantner; Lars Schmidt-Thieme<h2>ref_id</h2>b17<h2>title</h2>Line: Large-scale information network embedding<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Jian Tang; Meng Qu; Mingzhe Wang; Ming Zhang; Jun Yan; Qiaozhu Mei<h2>ref_id</h2>b18<h2>title</h2>Deep content-based music recommendation<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>Aaron Van Den Oord; Sander Dieleman; Benjamin Schrauwen<h2>ref_id</h2>b19<h2>title</h2>Collaborative deep learning for recommender systems<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Hao Wang; Naiyan Wang; Dit-Yan Yeung<h2>ref_id</h2>b20<h2>title</h2>Knowledge graph convolutional networks for recommender systems<h2>journal</h2>ACM<h2>year</h2>2019<h2>authors</h2>Hongwei Wang; Miao Zhao; Xing Xie; Wenjie Li; Minyi Guo<h2>ref_id</h2>b21<h2>title</h2>KGAT: Knowledge Graph Attention Network for Recommendation<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Xiang Wang; Xiangnan He; Yixin Cao; Meng Liu; Tat-Seng Chua<h2>ref_id</h2>b22<h2>title</h2>Item silk road: Recommending items from information domains to social users<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Xiang Wang; Xiangnan He; Liqiang Nie; Tat-Seng Chua<h2>ref_id</h2>b23<h2>title</h2>Wsabie: Scaling up to large vocabulary image annotation<h2>journal</h2><h2>year</h2>2011<h2>authors</h2>Jason Weston; Samy Bengio; Nicolas Usunier<h2>ref_id</h2>b24<h2>title</h2>Aggregated residual transformations for deep neural networks<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Saining Xie; Ross Girshick; Piotr Dollár; Zhuowen Tu; Kaiming He<h2>ref_id</h2>b25<h2>title</h2>How powerful are graph neural networks?<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Keyulu Xu; Weihua Hu; Jure Leskovec; Stefanie Jegelka<h2>ref_id</h2>b26<h2>title</h2>Graph Convolutional Neural Networks for Web-Scale Recommender Systems<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Rex Ying; Ruining He; Kaifeng Chen; Pong Eksombatchai; William L Hamilton; Jure Leskovec<h2>ref_id</h2>b27<h2>title</h2>Hierarchical Temporal Convolutional Networks for Dynamic Recommender Systems<h2>journal</h2>ACM<h2>year</h2>2019<h2>authors</h2>Jiaxuan You; Yichen Wang; Aditya Pal; Pong Eksombatchai; Chuck Rosenburg; Jure Leskovec<h2>ref_id</h2>b28<h2>title</h2>Cross-domain novelty seeking trait mining for sequential recommendation<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Fuzhen Zhuang; Yingmin Zhou; Fuzheng Zhang; Xiang Ao; Xing Xie; Qing He<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Figure 1 :1Figure 1: An example of the tensorflow repository jointly learn and map the embeddings of papers and repositories from the two platforms into the same space.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Fig 2, paper P 1 , called ResNeXt, develops a deep residual networks for image classification.<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Figure 2 :2Figure 2: A real-world example of repository-repository context graph on GitHub and paper citation graph on Microsoft Academic. In this figure, each orange circle represents a repository while each green circle represent a paper. In addition, repository R 1 is the original open source code of paper P 1 , which could be used to bridge the two different platforms.<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>•Step 1: convolution with multiple filters; • Step 2: max-over-time pooling; • Step 3: transformation with fully connected layers; • Step 4: feature fusion.<h2>figure_data</h2><h2>figure_label</h2>34<h2>figure_type</h2>figure<h2>figure_id</h2>fig_5<h2>figure_caption</h2>Figure 3 :Figure 4 :34Figure 3: Overall architecture of the paper2repo system. There are two main modules: text encoding and constrained GCN model. The inputs of the paper2repo are the content information and graph structures of papers and repositories.<h2>figure_data</h2><h2>figure_label</h2>5<h2>figure_type</h2>figure<h2>figure_id</h2>fig_6<h2>figure_caption</h2>Figure 5 :5Figure 5: Performance comparison of different methods as the number of candidates K increases.<h2>figure_data</h2><h2>figure_label</h2>6<h2>figure_type</h2>figure<h2>figure_id</h2>fig_7<h2>figure_caption</h2>Figure 6 :6Figure 6: Performance comparison of different sizes of data sets using same parameters.<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>table<h2>figure_id</h2>tab_0<h2>figure_caption</h2>Main notations in our model. Features of papers and repos X p , X r , adjacency matrix A p , A r for paper-citation graph and repository-repository graph output : Embeddings of paper p i and repository r j<h2>figure_data</h2>Notations Descriptionsp ii-th paper among all the papersr j p ′ i r ′ i Lj-th repositories among all the repositories i-th bridge paper bridge (original) repository of i-th bridge paper WARP lossC eMean of the constraint error∆Margin hyper-parameter for WARP lossTNumber of top related repositories for a paperAlgorithm 1: Constrained GCN algorithm1 /* For paper embeddings*/2<h2>figure_label</h2>2<h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>Information summary about paper and repositories data sets.<h2>figure_data</h2>Data # papers # repositories # bridge papersSmall11, 2727, 5161, 386Large32, 0297, 5712, 107<h2>figure_label</h2>3<h2>figure_type</h2>table<h2>figure_id</h2>tab_3<h2>figure_caption</h2>Performance comparison for different settings of pre-trained embeddings when K = 10, T = 6.<h2>figure_data</h2>Pre-trained embeddings MAP MRRHRfixed0.399 0.460 0.466not fixed0.330 0.364 0.367fixed & not fixed0.332 0.374 0.374<h2>figure_label</h2>4<h2>figure_type</h2>table<h2>figure_id</h2>tab_4<h2>figure_caption</h2>Performance comparison for different T when K = 10.<h2>figure_data</h2>Top T34567MAP 0.269 0.323 0.350 0.399 0.388MRR 0.291 0.36 0.390 0.460 0.438HR0.409 0.414 0.461 0.466 0.432<h2>figure_label</h2>5<h2>figure_type</h2>table<h2>figure_id</h2>tab_5<h2>figure_caption</h2>Performance comparison for different margin parameters when K = 10.<h2>figure_data</h2>∆0.10.20.30.40.50.60.7MAP 0.394 0.392 0.394 0.399 0.349 0.307 0.274MRR 0.436 0.456 0.440 0.460 0.400 0.345 0.312HR0.446 0.500 0.466 0.466 0.412 0.381 0.310<h2>figure_label</h2>5<h2>figure_type</h2>table<h2>figure_id</h2>tab_6<h2>figure_caption</h2>illustrates the evaluation results for different margin parameters when K = 10. We can observe that the performance of paper2repo gradually increases as ∆ rises from 0.1 to 0.4, and then gradually drops as ∆ increases from 0.4 to 0.7. This is to say, when ∆ = 0.4, it performs best among them. The main reason is that as margin ∆ is too small, it is hard to separate the positive and negative examples. At the same time, a larger ∆ may result in higher loss during training. Effect of Number of Bridge Papers. Finally, we explore how the number of bridge papers and repositories affect the recommendation performance of paper2repo. Table6illustrates the comparison results under different ratios of 1, 386 bridge papers in the small data set. We can observe from it that the evaluation metrics, HR, MAP and HRR, gradually rise as the number of bridge papers increases. This is because more bridge papers and repositories can make the embeddings of the similar nodes closer to each other in the graph.<h2>figure_data</h2><h2>figure_label</h2>6<h2>figure_type</h2>table<h2>figure_id</h2>tab_7<h2>figure_caption</h2>Performance comparison for different number of bridge papers when K = 10.<h2>figure_data</h2>Ratio0.40.60.81.0MAP 0.298 0.314 0.341 0.399MRR 0.325 0.364 0.391 0.460HR0.339 0.369 0.379 0.4664.6 Discussion of paper2repo<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>H (l +1) = σ D-1 2 Ã D-1 2 H (l ) W (l ) ,(1)<h2>formula_coordinates</h2>[3.0, 113.86, 631.02, 180.19, 12.92]<h2>formula_id</h2>formula_1<h2>formula_text</h2>x 1:n = x 1 ⊕ x 2 ⊕ . . . ⊕ x n ,(2)<h2>formula_coordinates</h2>[4.0, 390.29, 120.67, 167.92, 9.89]<h2>formula_id</h2>formula_2<h2>formula_text</h2>c i = f (w.x i:i+h-1 + b),(3)<h2>formula_coordinates</h2>[4.0, 395.55, 205.41, 162.65, 10.4]<h2>formula_id</h2>formula_3<h2>formula_text</h2>c = [c 1 , c 2 , . . . , c n-h+1 ].(4)<h2>formula_coordinates</h2>[4.0, 394.91, 258.65, 163.29, 10.4]<h2>formula_id</h2>formula_4<h2>formula_text</h2>1 -p ′ i ⊤ r ′ i ≤ ϵ,(5)<h2>formula_coordinates</h2>[4.0, 412.09, 697.94, 146.12, 13.88]<h2>formula_id</h2>formula_5<h2>formula_text</h2>min L subject to m i=1 (1 -p ′ i ⊤ r ′ i ) ≤ ϵ.(6)<h2>formula_coordinates</h2>[5.0, 111.13, 621.88, 182.92, 41.54]<h2>formula_id</h2>formula_6<h2>formula_text</h2>L = n k L rank ∆ p ⊤ r + |∆ -p ⊤ r + + p ⊤ r n k | + rank ∆ p ⊤ r + ,(7)<h2>formula_coordinates</h2>[5.0, 86.7, 683.94, 207.35, 27.7]<h2>formula_id</h2>formula_7<h2>formula_text</h2>L(K) = K j=1 1 j ,(8)<h2>formula_coordinates</h2>[5.0, 413.09, 412.61, 145.12, 28.67]<h2>formula_id</h2>formula_8<h2>formula_text</h2>rank ∆ p ⊤ r + = n k I ∆ -p ⊤ r + + p ⊤ r n k > 0 ,(9)<h2>formula_coordinates</h2>[5.0, 353.68, 484.26, 204.53, 21.67]<h2>formula_id</h2>formula_9<h2>formula_text</h2>min L + λ m i=1 (1 -p ′ i ⊤ r ′ i ).(10)<h2>formula_coordinates</h2>[5.0, 387.47, 568.61, 170.73, 28.67]<h2>formula_id</h2>formula_10<h2>formula_text</h2>min 1 + C e L,(11)<h2>formula_coordinates</h2>[6.0, 142.15, 153.5, 151.89, 9.78]<h2>formula_id</h2>formula_11<h2>formula_text</h2>C e = 1 2m m i=1 (1 -p ′ i ⊤ r ′ i ),(12)<h2>formula_coordinates</h2>[6.0, 129.32, 180.32, 164.73, 28.67]<h2>formula_id</h2>formula_12<h2>formula_text</h2>Ã = A p + I N , D = j Ãi j , H (0) = X p ; 3 p i ← ReLU D-1 2 Ã D-1 2 H (l ) W (l ) ; 4 p i ← p i / ∥p i ∥ 2 ; 5 /* For repo embeddings */ 6 Ã = A r + I N , D = j Ãi j , H (0) = X r ; 7 r j ← ReLU D-1 2 Ã D-1 2 H (l ) W (l ) ; 8 r j ← r j /<h2>formula_coordinates</h2>[6.0, 320.59, 288.91, 233.42, 85.45]<h1>doi</h1>10.1145/3366423.3380145<h1>title</h1>printf: Preference Modeling Based on User Reviews with Item Images and Textual Information via Graph Learning<h1>authors</h1>Jyun-Yu Jiang; Ming-Hao Juan; Pu-Jen Cheng<h1>pub_date</h1>2023-08-19<h1>abstract</h1>Nowadays, modern recommender systems usually leverage textual and visual contents as auxiliary information to predict user preference. For textual information, review texts are one of the most popular contents to model user behaviors. Nevertheless, reviews usually lose their shine when it comes to top-N recommender systems because those that solely utilize textual reviews as features struggle to adequately capture the interaction relationships between users and items. For visual one, it is usually modeled with naive convolutional networks and also hard to capture high-order relationships between users and items. Moreover, previous works did not collaboratively use both texts and images in a proper way. In this paper, we propose printf, preference modeling based on user reviews with item images and textual information via graph learning, to address the above challenges. Specifically, the dimensionbased attention mechanism directs relations between user reviews and interacted items, allowing each dimension to contribute different importance weights to derive user representations. Extensive experiments are conducted on three publicly available datasets. The experimental results demonstrate that our proposed printf consistently outperforms baseline methods with the relative improvements for NDCG@5 of 26.80%, 48.65%, and 25.74% on Amazon-Grocery, Amazon-Tools, and Amazon-Electronics datasets, respectively. The in-depth analysis also indicates the dimensions of review representations definitely have different topics and aspects, assisting the validity of our model design.<h1>sections</h1><h2>heading</h2>INTRODUCTION<h2>text</h2>In the era of information overload, recommender systems play a crucial role in satisfying users and keeping them engaged by providing personalized recommendations. With the increasing demand for customized contents on modern E-commerce and entertainment platforms, such as Yahoo News, Amazon Shopping, and Yelp, the effectiveness of recommendations is limited by existing user-item interactions and model capacity because of data sparsity. To address the sparsity issues, previous studies leverage higher-order relationships to mitigate the gaps between users and items. For example, as one of the most widely adopted and successful techniques, collaborative filtering (CF) [32] assumes low-rank relations between users and items, thereby establishing their relevance based on past interactions. Specifically, CF assumes that users with similar preferences will also consume similar items, thereby identifying items for recommendation [13,21,29,31,45,48]. In recent years, graph-based models, such as knowledge graph learning [1,39,41,42,47] and graph convolution networks (GCN) [10,12,38,46], are powerful approaches to improve the accuracy and scalability of collaborative filtering in recommendation systems by representing users and items as nodes in a graph with edges of their interactions. In other words, the graph structure enables the capability of capturing highorder relations among users and items beyond direct interactions.
For most modern e-commerce applications, besides structured data like product categories and brands, the majority of items are also accompanied by crucial unstructured data, such as product titles and images. Specifically, textual and image contents are the most popular unstructured data formats. However, the potential of those unstructured data has not been fully unleashed. For instance, conventional methods tackle texts and images with large-scale neural language models [7,25] and convolutional neural networks [16] or transformer-based visual models [8], respectively, but different data formats are modeled independently so that their knowledge cannot be aligned with each other. Moreover, to the best of our knowledge, none of the previous studies focus on simultaneously incorporating image and textual contents.
Among various textual contents, user reviews can become valuable resources to model their preferences and derive user representations [2,5,11,20,43] because detailed user opinions and item attributes can alleviate the data sparsity [33,49]. However, although some studies like RGCL [34] utilize reviews as edge features in graph-based models, the relations between reviews and item contents are not considered. This represents an unprecedented opportunity to improve the effectiveness of recommendation systems by leveraging both user reviews and item contents to generate more accurate personalized results.
The attention mechanism [37] is one of the most popular ways to model user interests from interacted items [51]. Previous studies like AGTM [14] also utilize attention to conquer the semantic gap between users and items, further differentiating the importance of interacted items toward the user. However, the previous attention mechanisms are based on vector similarities, where each latent dimension of item representations shares the same weight in computations. In other words, they lose the flexibility to learn distinct similarities of diversified topics within multiple dimensions of different semantics. Therefore, specifying different attention weights for each latent dimension can be a more suitable approach for assigning importance weights with better explanatory power.
In this paper, we present a novel framework, preference model based on user reviews with item images and textual information via graph learning (printf), which considers item images and textual contents as the primary source of item representations and leverages user reviews to determine user representations. Our framework consists of three sub-modules, including (1) Cross-Modality Item Modeling (CMIM), (2) Review-Aware User Modeling (RAUM), and (3) User-Item Embedding Propagation and Interaction Modeling (EPIM). CMIM fine-tunes our cross-modality feature encoder to extract image and text representations for items. RAUM leverages user reviews to distill user interests and preferences. User reviews are treated as an indicator to derive user embeddings. Compared to previous methods, instead of using a vector-wise inner product to compute the attention scores, we propose to compute multiple dimension-wise attention scores to learn diversified topics. EPIM employs the state-of-the-art graph convolutional network to propagate user-item embeddings and model high-order connectivity of user-item interactions.
To conclude, the main contributions of this work can be summarized as follows:
• To the best of our knowledge, we are the first study to innovatively incorporate unstructured contents, such as user reviews, and the images and titles of items, as supplementary sources of multi-modality information.<h2>publication_ref</h2>['b31', 'b12', 'b20', 'b28', 'b30', 'b44', 'b47', 'b0', 'b38', 'b40', 'b41', 'b46', 'b9', 'b11', 'b37', 'b45', 'b6', 'b24', 'b15', 'b7', 'b1', 'b4', 'b10', 'b19', 'b42', 'b32', 'b48', 'b33', 'b36', 'b50', 'b13']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>• The novel dimension-based attention technique enables us<h2>text</h2>to learn varied topics within multiple embedding dimensions while modeling the connection between review embeddings and item embeddings.
• Our proposed framework, printf, generates precise recommendations based on high-order relationships in user-item interactions through graph networks. Through rigorous experiments, we showcase the significance and potency of our suggested printf, leading to the significant relative improvements for NDCG@5 of 26.80%, 48.65%, and 25.74% on the Amazon-Grocery, Amazon-Tools, and Amazon-Electronics datasets, respectively.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>RELATED WORK 2.1 Graph-Based Recommendations<h2>text</h2>In recent years, graph neural networks (GNNs) have shown promising results in various recommendation tasks. For instance, GraphRec [9] incorporates user-item interactions and item contents to learn a joint user-item representation via graph convolutional neural networks (GCNs) and propagate messages between connected nodes to predict ratings. PinSage [46] uses a two-level GNN architecture to capture the structural and content-based information of items, and performs neighborhood aggregation for top-N recommendations. To further reduce the model complexity and improve the performance, LightGCN [12] is the state-of-the-art GNN-based model that minimizes the number of computational resources, only applies graph convolutional operations without non-linear activation to learn user and item embeddings. These previous studies demonstrate the effectiveness of GNNs in top-N tasks by leveraging high-order relationships successfully.<h2>publication_ref</h2>['b8', 'b45', 'b11']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Content-Based Recommendations<h2>text</h2>2.2.1 Item Content Utilization. Item content features usually come in unstructured forms, such as item titles, descriptions, and images. To the best of our knowledge, the majority of studies such as TPR [6] and AGTM [14] specifically focus on modeling unstructured item textual contents. TPR models token-level information between users and items by constructing an enormous knowledge graph including tokens, items, and users nodes. However, the information provided by the token-level is limited; therefore, AGTM tries to leverage transformer-based language models [7,28,37] to extract item textual contents at the sentence-level. Even though both methods utilized item titles and descriptions, there is no previous model dedicated to model item textual contents and images simultaneously. However, in the field of vision-language (V-L) tasks, they have reached notable improvement and success in retrieving the information from (image,text) pairs. Vision-Language Representation Learning In recent years, the existing work on Vision-Language (V-L) representation learning can be classified into 3 main categories. The first category [4,27] involves learning separate uni-modal encoders for images and texts and utilizing a contrastive loss to pre-train on large, noisy web data. They perform exceptionally well on image-text retrieval tasks but lack the capacity to model more intricate interactions between image and text for other V-L tasks. The second category [19,24] utilizes transformer-based multi-modal encoders to model interactions between image and text features, which are highly effective in downstream V-L tasks that require intricate reasoning. However, these methods often require high-resolution input images and pre-trained object detectors. Although some recent approaches aim to improve inference speed by eliminating object detectors, they result in lower performance. Therefore, hybrid models, such as ALBEF [18] and BLIP [17], have emerged as a third category, which unifies the first two categories to create robust uni-modal and multimodal representations with superior performance on both retrieval and reasoning tasks. Furthermore, most of these recent approaches do not require object detectors, which were previously a significant bottleneck for many conventional methods.
2.2.2 User Review Utilization. Historical reviews have been widely used to improve the learning of user and item embeddings in the field of recommendation systems [2,5,11,20,33,34,43,49]. Generally speaking, the users can be represented as the reviews they have written, and the items can be represented as the reviews they have received. For instance, DeepCoNN [50] utilized the TextCNN [3] to encode features and generate user and item embeddings for rating prediction. In order to differentiate the importance of different reviews, the attention mechanism [37] has also been introduced to improve review-based recommendation performance.
In some studies such as NARRE [2], after the feature extraction by CNN, attention is employed to select important reviews in learning user and item representations, improving model interpretability. HUITA [44] designs a three-hierarchy attention mechanism to leverage word-level, sentence-level, and review-level information in user reviews. We can also concatenate all reviews generated by a single user into a long document, for instance, NRCA [22] integrates the document-level and review-level modeling for learning better representations. Last but not least, previous studies such as RGCL [34] also combine the review information during the propagation of the graph-based model. It employs user reviews to interact with the corresponding users and items, and the rating-specific weights, to further get better representations for rating prediction tasks. In summary, from the aforementioned previous works, it is evident that user reviews play a crucial role in recommendation scenarios.<h2>publication_ref</h2>['b5', 'b13', 'b6', 'b27', 'b36', 'b3', 'b26', 'b18', 'b23', 'b17', 'b16', 'b1', 'b4', 'b10', 'b19', 'b32', 'b33', 'b42', 'b48', 'b49', 'b2', 'b36', 'b1', 'b43', 'b21', 'b33']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>PRINTF FOR PREFERENCE MODELING 3.1 Overview<h2>text</h2>Referring to Figure 1, printf is composed of three main components, including (1) Cross-Modality Item Modeling (CMIM), (2) Review-Aware User Modeling (RAUM), and (3) User-Item Embedding Propagation and Interaction Modeling (EPIM). CMIM fine-tunes our cross-modality feature encoder to extract image and text representations for items. RAUM leverages user reviews to distill user interests and preferences. User reviews are treated as an indicator to derive user embeddings. Compared to previous methods, instead of using a vector-wise inner product operator to compute the attention scores, we propose to compute multiple dimension-wise attention scores to reflect diversified topics between user reviews and item contents. EPIM employs the state-of-the-art graph convolutional network to propagate user-item embeddings and model high-order connectivity of user-item interactions. Problem Statement. Suppose we have a user set U, an item set I, and the review matrix R, where the entry 𝑟 𝑢,𝑖 specifies the review written by the user 𝑢 ∈ U toward the item 𝑖 ∈ I. The observedinteractions list can be designated as E = {(𝑢, 𝑖, 𝑟 𝑢,𝑖 )|𝑢 ∈ U, 𝑖 ∈ I, 𝑟 𝑢,𝑖 ∈ R}, where each (𝑢, 𝑖, 𝑟 𝑢,𝑖 ) tuple represents an interaction between the user 𝑢 and the item 𝑖 with the corresponding review 𝑟 𝑢,𝑖 . Moreover, we have a set of item text contents T , and a set of item image contents M. For each item 𝑖 ∈ I, its corresponding textual and image contents are denoted as 𝑡 𝑖 ∈ T and 𝑚 𝑖 ∈ M respectively. The goal of our proposed model is to learn a function that can predict how likely a user will interact with an unseen item given interaction data E and item contents {T , M}. Note that since the review feature 𝑟 𝑢,𝑖 implies the positive interaction between the user 𝑢 ∈ U and the item 𝑖 ∈ I, the review feature of the given user toward the item 𝑟 𝑢,𝑖 will be not be fed into the model while making the prediction, i.e., inference stage.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_0']<h2>table_ref</h2>[]<h2>heading</h2>Cross-Modality Item Modeling (CMIM)<h2>text</h2>3.2.1 Cross-Modality Feature Alignment Encoder. Inspired by the ALBEF [18] model, we propose to leverage the image-text contrastive loss [15] and an additional multi-modal encoder to obtain aligned image and text representations. Specifically, a 12layer visual transformer [8] and a 6-layer BERT model [7] derive the image and text embeddings h 𝑣 𝑖 and h 𝑡 𝑖 . In the learning stage, the contrastive loss will align embeddings to each other while an additional 6-layer cross-attentive multi-modal encoder will enforce their multi-modality with self-supervised learning. The detailed loss functions can be found in Section 3.5.<h2>publication_ref</h2>['b17', 'b14', 'b7', 'b6']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Item Embedding Compressor.<h2>text</h2>Graph networks favor using lower-dimensional embeddings to model high-order connectivity [40] while higher-dimensional embeddings usually lead to severe over-fitting [26]. For instance, both LightGCN [12] and NGCF [40] employ a fixed embedding dimension size of 64 to achieve the state-of-the-art performance at the time. However, most of the Transformer-based text and image encoders tend to derive embeddings with a higher dimension number, e.g., 768 of BERT embeddings. To connect the dots from Transformers to GCN, here we propose an item embedding compressor based on Auto-Encoder (AE) [30] to reduce the number of embedding dimensions.
Formally, given the original image and text embeddings h 𝑣 𝑖 and h 𝑡 𝑖 of an item 𝑖 ∈ I, we can have AEs as:
e 𝑣 𝑖 = F 𝑣 encoder (h 𝑣 𝑖 ), ĥ𝑣 𝑖 = F 𝑣 decoder (e 𝑣 𝑖 ), e 𝑡 𝑖 = F 𝑡 encoder (h 𝑡 𝑖 ), ĥ𝑡 𝑖 = F 𝑡 decoder (e 𝑡 𝑖 ),(1)
where e 𝑣 𝑖 ∈ R 𝑑 ′ and e 𝑡 𝑖 ∈ R 𝑑 ′ are the dimension reduced image and text embeddings; 𝑑 ′ is the reduced embedding dimension size, which is set to 64 in this work following previous studies [12,40]; F 𝑣 encoder (•) and F 𝑣 decoder (•) as the AE of image embeddings are both two 2-layer multi-layer perceptron (MLP) modules. Similarly, two 2-layer MLP modules F 𝑡 encoder (•) and F 𝑡 decoder (•) are applied for the AE of text embeddings.
Last but not least, the embedding of the item 𝑖 as the input of graph networks can be derived by concatenating the dimensionreduced item image and text embeddings together as:
e (0) 𝑖 = concat(e 𝑣 𝑖 , e 𝑡 𝑖 )(2)<h2>publication_ref</h2>['b39', 'b25', 'b11', 'b39', 'b29', 'b11', 'b39']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Review-Aware User Modeling (RAUM)<h2>text</h2>To preserve the information from item contents during propagation and to avoid the semantic gap between items and users, we propose utilizing user review features as auxiliary knowledge to overcome this issue. This approach can ultimately assist in modeling users better and help bridge the semantic gap between user and item embeddings via user reviews, leading to a more robust and explainable recommendation.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>3.3.1<h2>text</h2>User Review Feature Extraction. To consider the sentencelevel and paragraph-level semantics, the Sentence-Transformer [28] is used as an encoder to extract contextualized global semantics from user reviews. For the review 𝑟 𝑢,𝑖 written by a user 𝑢 toward an item 𝑖, we can derive the contextualized representation h 𝑟 𝑢,𝑖 as the review embedding.<h2>publication_ref</h2>['b27']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Review Embedding<h2>text</h2>Compressor. As we mentioned in section 3.2, we favor downsizing the embedding dimension for better usage in the training phase. Therefore, here we employ a similar approach to do dimension reduction via an AE. The formula can be defined in this fashion:
e 𝑟 𝑢,𝑖 = F 𝑟 encoder (h 𝑟 𝑢,𝑖 ), ĥ𝑟 𝑢,𝑖 = F 𝑟 decoder (e 𝑟 𝑢,𝑖 ),(3)
where e 𝑟 𝑢,𝑖 ∈ R 𝑑 ′ is the dimension reduced review embedding; 𝑑 ′ is the reduced embedding dimension size (i.e., 64 in this work); F 𝑟 encoder (•) and F 𝑟 decoder (•) as two 2-layer MLP modules are the AE of review embeddings.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Dimension-based<h2>text</h2>Review-Aware User Modeling. In this paper, we assume each dimension of the review embedding represents different perspectives and aspects of the user's feedback on the interacted item. In order to establish the connection between reviews and items, we first generate the average review embeddings of items, and calculate the dimension-based relations between user reviews and the corresponding item products as:
D = R𝑇 e (0) 𝑖 (4)
where R ∈ R | I | ×𝑑 is the average review embedding of the item over valid reviews written by users; D ∈ R 𝑑 ×𝑑 is the cross-relation matrix that captures the relationship between the dimensions of the review embeddings and the item embeddings. The matrix can also be treated as a projection matrix from the latent space of user reviews toward the space of item contents. Once we have the relationship matrix between reviews and items, we can determine the dimension-based attention weight of each review towards the interacted items by multiplying the relationship matrix with the user review. Eventually, we utilize this pre-computed dimensionbased attention weight to determine the significance of different interacted items by the weighted sum of the initial item embeddings for initializing user representations.
e (0) 𝑢 = ∑︁ 𝑖 ∈ N 𝑢 exp(e 𝑟 𝑢,𝑖 × D) 𝑗 ∈ N 𝑢 exp(e 𝑟 𝑢,𝑗 × D) ⊙ e (0) 𝑖(5)
where × denotes matrix multiplication, ⊙ represents element-wise multiplication (dot), and e
(0)
𝑢 denotes the user embedding for user 𝑢 as the input of the graph network.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>User-Item Embedding Propagation and Interaction Modeling (EPIM)<h2>text</h2>For the user-item embedding propagation phase, we leverage the structure of the graph convolutional network [12] to learn user and item embeddings via pair-wise interactions.<h2>publication_ref</h2>['b11']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Message<h2>text</h2>Passing and Message Aggregation. We empirically follow message passing and aggregation methods similar to previous works [12,36] to adopt mean aggregation in each graph convolutional layer. The formula is defined as follows:
x
(𝑙+1) 𝑖→𝑢 = e (𝑙 ) 𝑖 √︁ |N 𝑢 ||N 𝑖 | , x (𝑙+1) 𝑢→𝑖 = e (𝑙 ) 𝑢 √︁ |N 𝑢 ||N 𝑖 |(6)
where
x (𝑙+1) 𝑖→𝑢 and x (𝑙+1)
𝑢→𝑖 represents the message propagated from item 𝑖 toward user 𝑢 and from user 𝑢 toward item 𝑖 through the l-th layer to l+1-th layer, respectively. 𝑁 𝑢 and 𝑁 𝑖 denote the set of items interacted by user 𝑢 and the set of users interacted with item 𝑖.
e (𝑙+1) 𝑢 = ∑︁ 𝑖 ∈ N 𝑢 x (𝑙+1)
𝑖→𝑢 , e
(𝑙+1) 𝑖 = ∑︁ 𝑢 ∈ N 𝑖 x (𝑙+1) 𝑢→𝑖(7)
3.4.2 Layer Combination. After computing 𝐿 layers of propagation by Equation 6 and 7, we would like to aggregate different high-order relationships extracted by disparate graph convolutional layers. We simply follow a similar strategy as LightGCN [12] does, using mean pooling to combine the embeddings distilled at each layer to form the final user and item representations. 
where e 𝑢 and e 𝑖 are the final representations of user 𝑢 and item 𝑖.<h2>publication_ref</h2>['b11', 'b35', 'b11']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Interaction Modeling Layer.<h2>text</h2>For the final layer of the proposed model, the naive inner product operation is applied to estimate a user's preference toward a target item, which can be formulated as:
ŷ𝑢𝑖 = e 𝑢 • e 𝑖(9)
where ŷ𝑢𝑖 denotes the matching score of the user 𝑢 and item 𝑖.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Learning and Optimization<h2>text</h2>To optimize printf, we introduce the loss functions used by each component in the framework, including alignment loss, Auto-Encoder loss, and Bayesian Personalized Ranking (BPR) loss.
3.5.1 Alignment Loss. The alignment loss is from our crossmodality feature alignment encoder in CMIM. It has three subobjectives: image-text contrastive loss and masked language modeling in uni-modality encoders, as well as image-text matching applied to the multi-modal encoder.
• Image-text Contrastive Learning Loss. Image-text contrastive learning is applied to the uni-modality image encoder and text encoder respectively. It aligns image and textual features, and also trains the uni-modality encoders to better understand the semantic meaning of images and texts. Specifically, a learn-able similarity function s is utilized.
s = 𝑊 𝑖𝑚𝑔 e 𝑖𝑚𝑔 𝑐𝑙𝑠 • 𝑊 𝑡𝑥𝑡 e 𝑡𝑥𝑡 𝑐𝑙𝑠 (10
)
where 𝑊 𝑖𝑚𝑔 and 𝑊 𝑡𝑥𝑡 denotes the image projection layer and text projection layer as the pretraining phase respectively. For each (image,text) pair, we compute its image-to-text and text-to-image similarity with instances via the Softmax function:
ŷ𝑖2𝑡 (I) = Softmax( s(𝐼,𝑇 ) 𝜋 ), ŷ𝑡2𝑖 (T) = Softmax( s(𝑇 , 𝐼 ) 𝜋 )(11)
where 𝜋 is a trainable parameter for controlling temperature. Therefore, the image-text contrastive loss from each (image,text) can be written as follows:
L 𝑖𝑡𝑐 = 1 2 * CrossEntropyLoss(𝑦 𝑖2𝑡 (I), ŷ𝑖2𝑡 (I))(12)
+ 1 2 * CrossEntropyLoss(𝑦 𝑡 2𝑖 (T), ŷ𝑡2𝑖 (T))
where 𝑦 𝑖2𝑡 (I) and 𝑦 𝑡 2𝑖 (T) denote the ground-truth one-hot similarity, where negative pairs have a probability of 0 and the positive pair has a probability of 1, separately. • Masked Language Modeling Loss. Second, following the same pretraining objective in BERT [7], masked language modeling (MLM) is applied to the multi-modal encoder. The model randomly masks out the input text tokens with a probability of 15% and replaces them with [MASK] tokens, and predicts them using both the image and the masked text.
L 𝑚𝑙𝑚 = CrossEntropyLoss(𝑦 𝑚𝑙𝑚 , ŷ𝑚𝑙𝑚 (I, T masked ))
where 𝑦 𝑚𝑙𝑚 is a one-hot encoded distribution where the groundtruth token has a value of 1. ŷ𝑚𝑙𝑚 (I, T masked ) is the model's predicted probability distribution for a masked token with given the image representation I and the masked token T masked . • Image-Text Matching Loss. Third, image-text matching is employed in the multi-modal encoder, which predicts whether a (image, text) pair is positive or negative, i.e., matched or not matched. The output embedding of the [CLS] token from the multi-modal encoder is treated as the joint representation of the image-text pair, and is passed into a fully-connected network ensuing with Softmax to predict a binary classification problem.
L 𝑖𝑡𝑚 = CrossEntropyLoss(𝑦 𝑖𝑡𝑚 , ŷ𝑖𝑡𝑚 (I, T)) (14
)
where the 𝑦 𝑖𝑡𝑚 is a 2d one-hot encoded ground truth, and ŷ𝑖𝑡𝑚 (I, T) represents the prediction output probabilities with the given (image,text) pair.
To conclude, the overall alignment loss can be concisely formulated as:
L Align = L 𝑖𝑡𝑐 + L 𝑚𝑙𝑚 + L 𝑖𝑡𝑚(15)
3.5.2 Auto-Encoder Loss. In our proposed model, the AEs are used to condense user review embeddings, item image embeddings, and item text embeddings. All of these AEs are applied with mean square error (MSE) loss,
L AE = MSELoss(e, ê) + 𝜆 * |Θ AE | 2(16)
where e and ê denote original input embeddings and reconstructed embeddings. Note that we also add a regularization term on trainable parameters Θ AE in the AE with the 𝐿 2 regularization coefficient 𝜆. We have 𝜆 𝑢𝑟 , 𝜆 𝑖𝑖 , 𝜆 𝑖𝑡 of three embedding compressors for user review, item image, and item text representations, correspondingly.<h2>publication_ref</h2>['b6']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>BPR Loss.<h2>text</h2>For the user-item interaction graph in EPIM, we employ Bayesian Personalized Ranking (BPR) [29] loss, a pairwise loss assuming the observed interactions reflect a user's preference more than the unobserved ones. The prediction score of an observed interaction should be higher than an unobserved one on it.
L BPR = - ∑︁ 𝑢 ∈ U ∑︁ 𝑖 ∈ N 𝑢 ∑︁ 𝑗∉N 𝑢 ln 𝜎 ( ŷ𝑢𝑖 -ŷ𝑢 𝑗 )+𝜆 𝑏𝑝𝑟 * (|e 𝑢 | 2 +|e 𝑖 | 2 ) (17
)
where {(𝑖, 𝑗)|𝑖 ∈ N 𝑢 , 𝑗 ∉ N 𝑢 } is the pair-wise training data for user 𝑢, 𝜎 (•) is the Sigmoid function, adn 𝜆 𝑏𝑝𝑟 is the 𝐿 2 regularization coefficient.<h2>publication_ref</h2>['b28']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>EXPERIMENTS 4.1 Experimental Datasets<h2>text</h2>We apply printf model on three real-world Amazon datasets: Grocery, Tools, and Electronics, which are all publicly accessible 1 . All of the user-item interactions are thoroughly collected in the dataset file, it also contains review comments, which can be perfectly employed in the scenario of our work. Furthermore, we take images and titles from the metadata file as input features of items. In our task scenario, we sample the interactions user by user, and split the datasets 75% for train, 5% for validation, and 20% for testing. By adopting the approach that samples the interactions user by user, we can ensure that all users receive training during the training process, eliminating the possibility of a user being included in the testing set without being part of the training set. The detailed data statistics are summarized in Table 1. Cumulative Gain (NDCG). We set all the metric rankings at the top-{5, 10} to evaluate and prove efficiency. For those items that have no interaction with the user are treated as negative ones, and the interacted items in the test set which is not seen in the train set are treated as positive ones. In this paper, we abbreviate R for Recall, and N for NDCG, respectively.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_0']<h2>heading</h2>Dataset<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>4.2.2<h2>text</h2>Baseline Methods. We consider five baseline models for comparisons in the experiments, including (1) BPRMF [29] as a pure CF model, (2) LightGCN [12] as the state-of-the-art GCNbased recommendation model, (3) RGCL [34] as the state-of-the-art review-based rating prediction model, (4) TPR [6] incorporating item titles and descriptions at the token-level with a knowledge graph learning, and (5) AGTM [14] leveraging LightGCN and models both item textual contents at the sentence level and high-order connectivity in the user-item graph for the top-N recommendation.
For TPR 2 and RGCL 3 , we directly use their official codes which are publicly accessible on GitHub. Here, we would like to explain the reasons why we tend not to showcase the review-based baselines for model comparison. If we directly transform the rating prediction tasks, such as DeepCoNN [50] and NARRE [2], into top-N recommendation tasks, it empirically leads to terrible performance. Even though we perform the top-N task on the state-of-the-art review-based rating prediction model, i.e., RGCL [34], the Recall and NDCG metric are less than 0.015, which are still incredibly low. Therefore, we neglect the review-based recommendation part except for RGCL.<h2>publication_ref</h2>['b28', 'b11', 'b33', 'b5', 'b13', 'b49', 'b1', 'b33']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Implementation Details.<h2>text</h2>The printf is mainly implemented in PyTorch. In CMIM, the 12-layer visual transformer (ViT-B/16 4 ) is adopted for the image encoder and initialized with the pre-trained weights well-trained via ImageNet-1k from DEIT [35]. For the text encoder and the multi-modal encoder, the first 6-layer and the last 6-layer of the BERT [7] model (bert-base-uncased 5 ) with additional cross-attention layers applied respectively. In preactical, the cross-modality feature alignment encoder is initialized with the pre-trained weights 6 from ALBEF [18]. We fine-tune the crossmodality feature alignment encoder with item (image,text) pairs extracted from our dataset for 3 epochs using a batch size of 32 on 1 NVIDIA RTX3090 GPU. We use the AdamW optimizer with a learning rate of 10 -4 and a weight decay of 10 -2 . In RAUM, we use the pre-trained weights 7 of Sentence-Transformer for review embedding generation. For each representation compressor, i.e., AE, in CMIM and RAUM, the AdamW [23] optimizer with a learning rate of 10 -3 and a weight decay of 10 -2 is applied. For the graph in EPIM, we train it using a batch size of 4096 on 1 NVIDIA RTX3090 GPU via the AdamW optimizer with a learning rate of 10 -3 and a weight decay of 10 -2 .<h2>publication_ref</h2>['b34', 'b6', 'b17', 'b22']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Overall Performance<h2>text</h2>The complete main results among three Amazon datasets are summarized in Table 2. According to the table, we can obtain the following observations: BPRMF is a naive approach to learn low-dimensional latent representations of users and items. Its simplicity lies in the fact that it solely relies on collaborative filtering signals without considering item contents or user profiles. As a result, BPRMF generally yields sub-optimal performance in user preference modeling, highlighting the insufficiency of solely modeling direct user-item interactions. For LightGCN, as we expected, it consistently outperforms BPRMF, verifying that stacking GCN layers is an effective way to model high-order relationships which is vital to capture user preference. Moreover, in some of the metrics, LightGCN can reach better performance than TPR, this again highlights the importance of high-order connectivity modeling. Nevertheless, it still relies on the information without any user profile and item contents. RGCL, the state-of-the-art review-based recommendation model in rating prediction, utilizes the user reviews and GCN structure to model high-order connectivity for users and items; however, it performs terribly worse in top-N tasks, probably not only because of the intrinsic gap between rating predictions and top-N tasks, but also the exclusion of content information such as titles and images.
TPR generally has competitive results with the LightGCN. In the Grocery dataset with Recall@5 and Recall@10, TPR performs significantly better than LightGCN. We suggest this could be that the item textual contents in Grocery data tend to contain much more information, which assists the model in capturing additional collaborative filtering signals. This again proves the importance of modeling item textual contents for representation learning. However, their tremendous KG structure, i.e., treating word tokens, items, and users as different nodes in the KG, makes them unable to learn information effectively when the token-level information is not enough.
AGTM surpasses the preceding models due to its utilization of item modeling that incorporates textual contents at the sentencelevel, resulting in improved performance compared to TPR. AGTM<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Amazon-Grocery<h2>text</h2>Amazon-Tools Amazon-Electronics R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 Table 2: Top-N recommendation performance of different methods. N@5 and R@5 denote the metrics of Recall@5 and NDCG@5. Note that the best results are highlighted in bold while the improvement indicates relative gains over the best baseline method.
BPRMF
Amazon-Grocery Amazon-Tools Amazon-Electronics R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 printf 0. effectively captures high-order connectivity through the use of a GCN-based model, further highlighting the potency of GCN in recommendation tasks. However, in their model design, AGTM does not directly train user embeddings. Instead, the user is modeled using an aspect attentive network. As a consequence, the user embedding does not undergo direct training during the backward propagation of the model, resulting in information loss and a deficiency in learning representations. Lastly, printf model consistently achieves exceptional performance across all datasets, providing strong evidence for the effectiveness of our model design. The observed improvements are statistically significant, with all 𝑝-values being less than 0.01. These improvements can be attributed to three key factors:
• The cross-modality feature alignment encoder in CMIM enables us to extract more potent and robust image and text features with global semantics. • Using dimension-based attention via extracted review representations for user embeddings enables us to effectively determine the prominence relationship between user preferences and item contents. Moreover, the user embeddings are directly trained during the EPIM stage to avoid information loss. • The bipartite graph structure in EPIM allows printf model to successfully distill higher-order relationships and precisely model user preferences.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Ablation Study<h2>text</h2>In our printf model, we have incorporated two crucial components: CMIM and RAUM. We aim to demonstrate the effectiveness of each component individually; a comprehensive ablation study experiment is conducted. The results presented in Table 3 showcase the performance of our model when each component is excluded, allowing us to evaluate the impact and efficiency of CMIM and RAUM in isolation. The constructed variants are as follows: Based on Table 3, it's evident that every component of our model design contributes positively to the recommendation. Removing any of these components would result in a performance decrease. Our experimental findings provide validation for the effectiveness of our design, proving that incorporating both image and textual contents as initial item embeddings is a great starting point. This integration allows CMIM to proficiently capture and align the meaning of item images and titles, enhancing item representation. It's worth noting that eliminating either textual content or image content leads to a significant decline in performance, underscoring the importance of modeling both modalities. Additionally, RAUM excels in capturing user preferences. Removing this component results in a substantial drop in performance, indicating the dimension-based attention in RAUM to differentiate the significance of interacted items is an excellent approach for user preference modeling. To conclude, all of the components play a crucial role in printf. Note that in our ablation test, we did not include the variant of removing Amazon-Grocery Amazon-Tools Amazon-Electronics R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 
• w/o CMIM (<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_2', 'tab_2']<h2>heading</h2>Analysis and Discussions<h2>text</h2>4.5.1 The Effect of GCN Layers. We investigate the impact of varying the number of graph convolution layers on the performance of our model. The comprehensive experimental results can be found in Figure 2. The model performance generally improves as the number of layers is increased from 1 to 7, except for the metrics@5 on Tools and Electronics, which perform better with 5 gcn layers. In most cases, the printf model with 7 GCN layers demonstrates competitive and satisfactory results. This finding highlights that our proposed model requires a higher number of GCN layers compared to other GCN-based models, such as LightGCN [12] and AGTM [14], which typically employ 3 GCN layers. The need for more layers in printf may be attributed to the sparsity and diversity of textual and image features, necessitating additional layers to effectively extract user preferences and capture high-order connectivity. However, it's important to note that excessive propagation, i.e., an excessive number of GCN layers such as 8 or 9, can result in the over-smoothing of information and have a negative impact on predictions.  this leads to negative effects and instead creates noise that interferes with the originally generated representation, since the image embedding we obtained did not participate in the training of the recommendation task. Therefore, we design a method that uses the item image representation and user image representation we just get, as the initial embedding of BPRMF, and conducts model training with the current recommendation task, then takes the user and item embeddings which have been fully trained. We concatenate the embeddings individually behind the user and item representation to predict the top-N items, and compare the performance. From the Table 4, all the above models have positive effects when image contents are taken into consideration.<h2>publication_ref</h2>['b11', 'b13']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Dimension-Based Attention in RAUM.<h2>text</h2>Here we would like to validate and evaluate the assumptions and designs we made in RAUM. The assumption posits that each dimension in the representation corresponds to specific topics and aspects that can be easily differentiated. To investigate this hypothesis, we leveraged the attention scores calculated for each dimension to observe if such a phenomenon exists as shown in Figure 3, in which we identified the top frequently attended words for each dimension. It conclusively demonstrated that indeed, each dimension possessed words that were distinctly associated with it, as evident through their diagonal co-cluster effect. Furthermore, these words encompassed diverse types and themes. For instance, within the word cluster positioned in the middle (bound with a green rectangle and arrows pointed), the words are "light, day, night, bulb, room," it can be easily inferred that these particular dimensions pertain to items related to indoor electric lights, encompassing discussions about the brightness and color characteristics of the products. We also explored the direct relationships between different dimensions of the review representations and item representations since we hypothesized that the text within a review would have a certain connection with the item's title or picture. To apply the concept to the embedding representation within the latent space, each dimension of the review embedding should have a specific relationship with the corresponding dimension in the item embedding. By employing the formula developed in Section 3.3, we obtained matrix 𝐷 successfully. Subsequently, through spectral clustering analysis, we were able to confirm the presence of a specific correlation between the review and the embedding dimensions of the item. Referring to Figure 4, we can initially observe the successful differentiation between review embedding dimensions and item embedding dimensions. This differentiation enables us to trace back the importance of the current review embedding dimension and its corresponding dimensions in item embeddings. By doing so, we gain insights into the frequent transmission of specific information between these two latent spaces and understand the type of information that is transparent in-between.
To conclude, for RAUM, we can treat the matrix 𝐷 not only as a projection for the review from its space to the item space, but also as an attention matrix to further select the significance of users' reviews toward different interacted items. The design of RAUM not only effectively assists the user preference modeling, but also increases the interpretability of attention.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>CONCLUSION<h2>text</h2>In this work, we proposed a new method to effectively model the common scenario in the recommendation. In the scenario, users click the item based on the image and text information provided and write down review comments. The printf jointly models the item's image and textual features and determines user preference based on review comments via a bipartite user-item interaction graph. The crucial keys in our model include not only the utilization of a cross-modality feature alignment encoder in CMIM to extract global semantics from item information but also the effectiveness of dimension-based attention in RAUM to make up a deficiency of the semantic gap, making the information have a better starting position of propagation in EPIM stage. Extensive experiments on three real-world datasets demonstrate the effectiveness, power, and significant improvement of our printf.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Yixin Cao; Xiang Wang; Xiangnan He; Zikun Hu; Tat-Seng Chua<h2>ref_id</h2>b1<h2>title</h2>Neural Attentional Rating Regression with Review-Level Explanations<h2>journal</h2>WWW<h2>year</h2>2018<h2>authors</h2>Chong Chen; Min Zhang; Yiqun Liu; Shaoping Ma<h2>ref_id</h2>b2<h2>title</h2>Convolutional neural network for sentence classification<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Yahui Chen<h2>ref_id</h2>b3<h2>title</h2>UNITER: UNiversal Image-TExt Representation Learning<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Yen-Chun Chen; Linjie Li; Licheng Yu; Ahmed El Kholy; Faisal Ahmed; Zhe Gan; Yu Cheng; Jingjing Liu<h2>ref_id</h2>b4<h2>title</h2>ANR: Aspect-based neural recommender<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Jin Yao Chin; Kaiqi Zhao; Shafiq Joty; Gao Cong<h2>ref_id</h2>b5<h2>title</h2>TPR: Text-Aware Preference Ranking for Recommender Systems<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Yu-Neng Chuang; Chih-Ming Chen; Chuan-Ju Wang; Ming-Feng Tsai; Yuan Fang; Ee-Peng Lim<h2>ref_id</h2>b6<h2>title</h2>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova<h2>ref_id</h2>b7<h2>title</h2>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Alexey Dosovitskiy; Lucas Beyer; Alexander Kolesnikov; Dirk Weissenborn; Xiaohua Zhai; Thomas Unterthiner; Mostafa Dehghani; Matthias Minderer; Georg Heigold; Sylvain Gelly; Jakob Uszkoreit; Neil Houlsby<h2>ref_id</h2>b8<h2>title</h2>Graph neural networks for social recommendation<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Wenqi Fan; Yao Ma; Qing Li; Yuan He; Eric Zhao; Jiliang Tang; Dawei Yin<h2>ref_id</h2>b9<h2>title</h2>Attentionbased Graph Convolutional Network for Recommendation System<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Chenyuan Feng; Zuozhu Liu; Shaowei Lin; Tony Q S Quek<h2>ref_id</h2>b10<h2>title</h2>Attentive Aspect Modeling for Review-Aware Recommendation<h2>journal</h2>TOIS, Article<h2>year</h2>2019<h2>authors</h2>Zhiyong Xinyu Guan; Xiangnan Cheng; Yongfeng He; Zhibo Zhang; Qinke Zhu; Tat-Seng Peng;  Chua<h2>ref_id</h2>b11<h2>title</h2>LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Xiangnan He; Kuan Deng; Xiang Wang; Yan Li; Yongdong Zhang; Meng Wang<h2>ref_id</h2>b12<h2>title</h2>Neural Collaborative Filtering<h2>journal</h2>WWW<h2>year</h2>2017<h2>authors</h2>Xiangnan He; Lizi Liao; Hanwang Zhang; Liqiang Nie; Xia Hu; Tat-Seng Chua<h2>ref_id</h2>b13<h2>title</h2>Attentive Graph-based Text-aware Preference Modeling for Top-N Recommendation<h2>journal</h2><h2>year</h2>2023<h2>authors</h2>Ming-Hao Juan; Pu-Jen Cheng; Hui-Neng Hsu; Pin-Hsin Hsiao<h2>ref_id</h2>b14<h2>title</h2>Supervised Contrastive Learning<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Prannay Khosla; Piotr Teterwak; Chen Wang; Aaron Sarna; Yonglong Tian; Phillip Isola; Aaron Maschinot; Ce Liu; Dilip Krishnan<h2>ref_id</h2>b15<h2>title</h2>Imagenet classification with deep convolutional neural networks<h2>journal</h2>Commun. ACM<h2>year</h2>2017<h2>authors</h2>Alex Krizhevsky; Ilya Sutskever; Geoffrey E Hinton<h2>ref_id</h2>b16<h2>title</h2>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation<h2>journal</h2>PMLR<h2>year</h2>2022<h2>authors</h2>Junnan Li; Dongxu Li; Caiming Xiong; Steven Hoi<h2>ref_id</h2>b17<h2>title</h2>Align before fuse: Vision and language representation learning with momentum distillation<h2>journal</h2>Advances in neural information processing systems<h2>year</h2>2021<h2>authors</h2>Junnan Li; Ramprasaath Selvaraju; Akhilesh Gotmare; Shafiq Joty; Caiming Xiong; Steven Chu; Hong Hoi<h2>ref_id</h2>b18<h2>title</h2>VisualBERT: A Simple and Performant Baseline for Vision and Language<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Liunian Harold; Li ; Mark Yatskar; Cho-Jui Da Yin; Kai-Wei Hsieh;  Chang<h2>ref_id</h2>b19<h2>title</h2>Aspect-based fashion recommendation with attention mechanism<h2>journal</h2>IEEE Access<h2>year</h2>2020<h2>authors</h2>Weiqian Li; Bugao Xu<h2>ref_id</h2>b20<h2>title</h2>Amazon. com recommendations: Item-to-item collaborative filtering<h2>journal</h2>IEEE Internet computing<h2>year</h2>2003<h2>authors</h2>Greg Linden; Brent Smith; Jeremy York<h2>ref_id</h2>b21<h2>title</h2>Neural Unified Review Recommendation with Cross Attention<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Hongtao Liu; Wenjun Wang; Hongyan Xu; Qiyao Peng; Pengfei Jiao<h2>ref_id</h2>b22<h2>title</h2>Decoupled Weight Decay Regularization<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Ilya Loshchilov; Frank Hutter<h2>ref_id</h2>b23<h2>title</h2>ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Jiasen Lu; Dhruv Batra; Devi Parikh; Stefan Lee<h2>ref_id</h2>b24<h2>title</h2>Deep contextualized word representations<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Matthew E Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer<h2>ref_id</h2>b25<h2>title</h2>Rethinking the item order in session-based recommendation with graph neural networks<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Ruihong Qiu; Jingjing Li; Zi Huang; Hongzhi Yin<h2>ref_id</h2>b26<h2>title</h2>Learning Transferable Visual Models From Natural Language Supervision<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Alec Radford; Jong Wook Kim; Chris Hallacy; Aditya Ramesh; Gabriel Goh; Sandhini Agarwal; Girish Sastry; Amanda Askell; Pamela Mishkin; Jack Clark; Gretchen Krueger; Ilya Sutskever<h2>ref_id</h2>b27<h2>title</h2>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Nils Reimers; Iryna Gurevych<h2>ref_id</h2>b28<h2>title</h2>BPR: Bayesian Personalized Ranking from Implicit Feedback<h2>journal</h2><h2>year</h2>2009<h2>authors</h2>Steffen Rendle; Christoph Freudenthaler; Zeno Gantner; Lars Schmidt-Thieme<h2>ref_id</h2>b29<h2>title</h2>Learning Internal Representations by Error Propagation<h2>journal</h2><h2>year</h2>1986<h2>authors</h2>D E Rumelhart; G E Hinton; R J Williams<h2>ref_id</h2>b30<h2>title</h2>Item-based collaborative filtering recommendation algorithms<h2>journal</h2><h2>year</h2>2001<h2>authors</h2>Badrul Sarwar; George Karypis; Joseph Konstan; John Riedl<h2>ref_id</h2>b31<h2>title</h2>Collaborative filtering recommender systems<h2>journal</h2><h2>year</h2>2007<h2>authors</h2>Ben Schafer; Dan Frankowski; Jon Herlocker; Shilad Sen<h2>ref_id</h2>b32<h2>title</h2>Deep collaborative filtering with multi-aspect information in heterogeneous networks<h2>journal</h2>IEEE transactions on knowledge and data engineering<h2>year</h2>2019<h2>authors</h2>Chuan Shi; Xiaotian Han; Li Song; Xiao Wang; Senzhang Wang; Junping Du; S Yu; Philip <h2>ref_id</h2>b33<h2>title</h2>A Review-aware Graph Contrastive Learning Framework for Recommendation<h2>journal</h2>ACM<h2>year</h2>2022<h2>authors</h2>Jie Shuai; Kun Zhang; Le Wu; Peijie Sun; Richang Hong; Meng Wang; Yong Li<h2>ref_id</h2>b34<h2>title</h2>Training data-efficient image transformers distillation through attention<h2>journal</h2><h2>year</h2>2021<h2>authors</h2>Hugo Touvron; Matthieu Cord; Matthijs Douze; Francisco Massa; Alexandre Sablayrolles; Herve Jegou<h2>ref_id</h2>b35<h2>title</h2>Graph Convolutional Matrix Completion<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Rianne Van Den; Thomas N Berg; Max Kipf;  Welling<h2>ref_id</h2>b36<h2>title</h2>Attention is all you need<h2>journal</h2>Advances in neural information processing systems<h2>year</h2>2017<h2>authors</h2>Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Łukasz Kaiser; Illia Polosukhin<h2>ref_id</h2>b37<h2>title</h2>Knowledge graph convolutional networks for recommender systems<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Hongwei Wang; Miao Zhao; Xing Xie; Wenjie Li; Minyi Guo<h2>ref_id</h2>b38<h2>title</h2>KGAT: Knowledge Graph Attention Network for Recommendation<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Xiang Wang; Xiangnan He; Yixin Cao; Meng Liu; Tat-Seng Chua<h2>ref_id</h2>b39<h2>title</h2>Neural Graph Collaborative Filtering<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Xiang Wang; Xiangnan He; Meng Wang; Fuli Feng; Tat-Seng Chua<h2>ref_id</h2>b40<h2>title</h2>Learning Intents behind Interactions with Knowledge Graph for Recommendation<h2>journal</h2>WWW<h2>year</h2>2021<h2>authors</h2>Xiang Wang; Tinglin Huang; Dingxian Wang; Yancheng Yuan; Zhenguang Liu; Xiangnan He; Tat-Seng Chua<h2>ref_id</h2>b41<h2>title</h2>CKAN: Collaborative Knowledge-Aware Attentive Network for Recommender Systems<h2>journal</h2><h2>year</h2>2020<h2>authors</h2>Ze Wang; Guangyan Lin; Huobin Tan; Qinghong Chen; Xiyang Liu<h2>ref_id</h2>b42<h2>title</h2>Neural News Recommendation with Attentive Multi-View Learning<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Chuhan Wu; Fangzhao Wu; Mingxiao An; Jianqiang Huang; Yongfeng Huang; Xing Xie<h2>ref_id</h2>b43<h2>title</h2>Hierarchical user and item representation with three-tier attention for recommendation<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>Chuhan Wu; Fangzhao Wu; Junxin Liu; Yongfeng Huang<h2>ref_id</h2>b44<h2>title</h2>HOP-Rec: High-Order Proximity for Implicit Recommendation<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Jheng-Hong Yang; Chih-Ming Chen; Chuan-Ju Wang; Ming-Feng Tsai<h2>ref_id</h2>b45<h2>title</h2>Graph Convolutional Neural Networks for Web-Scale Recommender Systems<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Rex Ying; Ruining He; Kaifeng Chen; Pong Eksombatchai; William L Hamilton; Jure Leskovec<h2>ref_id</h2>b46<h2>title</h2>Collaborative Knowledge Base Embedding for Recommender Systems<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>Fuzheng Zhang; Nicholas Jing Yuan; Defu Lian; Xing Xie; Wei-Ying Ma<h2>ref_id</h2>b47<h2>title</h2>Discrete collaborative filtering<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>Hanwang Zhang; Fumin Shen; Wei Liu; Xiangnan He; Huanbo Luan; Tat-Seng Chua<h2>ref_id</h2>b48<h2>title</h2>Joint deep modeling of users and items using reviews for recommendation<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Lei Zheng; Vahid Noroozi; Philip S Yu<h2>ref_id</h2>b49<h2>title</h2>Joint Deep Modeling of Users and Items Using Reviews for Recommendation<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Lei Zheng; Vahid Noroozi; Philip S Yu<h2>ref_id</h2>b50<h2>title</h2>Deep Interest Network for Click-Through Rate Prediction<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Guorui Zhou; Chengru Song; Xiaoqiang Zhu; Ying Fan; Han Zhu; Xiao Ma; Yanghui Yan; Junqi Jin; Han Li; Kun Gai<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Figure 1 :1Figure 1: The overall framework of our proposed "printf" for preference model.<h2>figure_data</h2><h2>figure_label</h2>522<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>4. 5 . 2 Figure 2 :522Figure 2: effect of different numbers of GCN layers.<h2>figure_data</h2><h2>figure_label</h2>34<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>Figure 3 :Figure 4 :34Figure 3: Bi-clusters of top frequently attended words between different review embedding dimensions derived from the Amazon-Tools dataset.original baseline model, so that the prediction score can also be obtained through the dot-product as original model settings. However,<h2>figure_data</h2><h2>figure_label</h2>1<h2>figure_type</h2>table<h2>figure_id</h2>tab_0<h2>figure_caption</h2>The statistics of the experimental datasets.<h2>figure_data</h2>Grocery Tools ElectronicsK-core101016Users12,46419,97318,119Items6,64411,5099,952Items w/text6,63911,4969,946Items w/image6,6398,7698,759Reviews (Interactions) 205,739 304,439440,672Density0.00248 0.001320.00244<h2>figure_label</h2>3<h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>The results of the ablation study about the contribution of different components in printf model.<h2>figure_data</h2>1233 0.1331 0.1536 0.1426 0.0466 0.0468 0.0631 0.0530 0.0390 0.0465 0.0574 0.0522w/o CMIM (image) 0.0970 0.1025 0.1298 0.1137 0.0291 0.0273 0.0460 0.0339 0.0263 0.0310 0.0434 0.0370w/o CMIM (title)0.0950 0.1010 0.1256 0.1114 0.0276 0.0259 0.0441 0.0324 0.0248 0.0292 0.0411 0.0348w/o RAUM0.1139 0.1250 0.1424 0.1346 0.0422 0.0427 0.0569 0.0485 0.0376 0.0452 0.0549 0.0505w/o Both0.0901 0.0956 0.1141 0.1038 0.0266 0.0254 0.0383 0.0299 0.0233 0.0250 0.0374 0.0318<h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_3<h2>figure_caption</h2>image): We only use the item textual features, i.e. The dimension-based attention in RAUM for selecting different importance of interacted items for users is removed. The user embeddings here are randomly initialized.• w/o Both: Both CAUM and RAUM are removed, and we directly randomly initialized the user and item embeddings in the graph for training, which is equal to LightGCN.<h2>figure_data</h2>item titles, to encode the initial item representations without us-ing image features, removing the part of information constructedfrom cross-modality item modeling (CMIM).• w/o CMIM (title): Only item image features are included toencode the initial item representations.• w/o RAUM:<h2>figure_label</h2>4<h2>figure_type</h2>table<h2>figure_id</h2>tab_4<h2>figure_caption</h2>Performance of different methods with and without using image contents.the entire CMIM, which entails not using item textual and image content simultaneously, while still utilizing the review-aware user modeling. The reason for this omission is that when employing dimension-based attention between review embeddings and randomly initialized item embeddings, it lacks any meaningful semantics. Consequently, when conducting ablation on CMIM, we can only eliminate either the textual contents or the visual ones, but not both simultaneously.<h2>figure_data</h2>TPR (text only)0.0923 0.0918 0.1228 0.1031 0.0269 0.0248 0.0405 0.0302 0.0222 0.0251 0.0362 0.0301TPR w/ image0.1077 0.1112 0.1390 0.1222 0.0346 0.0333 0.0497 0.0393 0.0284 0.0323 0.0438 0.0376AGTM (text only) 0.1001 0.1050 0.1337 0.1167 0.0336 0.0315 0.0519 0.0386 0.0309 0.0370 0.0498 0.0433AGTM w/ image0.1141 0.1220 0.1429 0.1312 0.0395 0.0385 0.0558 0.0448 0.0345 0.0407 0.0528 0.0466printf w/o image 0.0970 0.1025 0.1298 0.1137 0.0291 0.0273 0.0460 0.0339 0.0263 0.0310 0.0434 0.0370printf0.1233 0.1331 0.1536 0.1426 0.0466 0.0468 0.0631 0.0530 0.0390 0.0465 0.0574 0.0522<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>e 𝑣 𝑖 = F 𝑣 encoder (h 𝑣 𝑖 ), ĥ𝑣 𝑖 = F 𝑣 decoder (e 𝑣 𝑖 ), e 𝑡 𝑖 = F 𝑡 encoder (h 𝑡 𝑖 ), ĥ𝑡 𝑖 = F 𝑡 decoder (e 𝑡 𝑖 ),(1)<h2>formula_coordinates</h2>[4.0, 102.56, 264.41, 192.03, 29.28]<h2>formula_id</h2>formula_1<h2>formula_text</h2>e (0) 𝑖 = concat(e 𝑣 𝑖 , e 𝑡 𝑖 )(2)<h2>formula_coordinates</h2>[4.0, 136.21, 415.34, 158.37, 13.12]<h2>formula_id</h2>formula_2<h2>formula_text</h2>e 𝑟 𝑢,𝑖 = F 𝑟 encoder (h 𝑟 𝑢,𝑖 ), ĥ𝑟 𝑢,𝑖 = F 𝑟 decoder (e 𝑟 𝑢,𝑖 ),(3)<h2>formula_coordinates</h2>[4.0, 93.31, 668.65, 201.28, 13.28]<h2>formula_id</h2>formula_3<h2>formula_text</h2>D = R𝑇 e (0) 𝑖 (4)<h2>formula_coordinates</h2>[4.0, 416.38, 201.85, 142.36, 13.12]<h2>formula_id</h2>formula_4<h2>formula_text</h2>e (0) 𝑢 = ∑︁ 𝑖 ∈ N 𝑢 exp(e 𝑟 𝑢,𝑖 × D) 𝑗 ∈ N 𝑢 exp(e 𝑟 𝑢,𝑗 × D) ⊙ e (0) 𝑖(5)<h2>formula_coordinates</h2>[4.0, 363.56, 378.15, 195.18, 26.84]<h2>formula_id</h2>formula_5<h2>formula_text</h2>(0)<h2>formula_coordinates</h2>[4.0, 413.09, 425.25, 8.95, 6.25]<h2>formula_id</h2>formula_6<h2>formula_text</h2>(𝑙+1) 𝑖→𝑢 = e (𝑙 ) 𝑖 √︁ |N 𝑢 ||N 𝑖 | , x (𝑙+1) 𝑢→𝑖 = e (𝑙 ) 𝑢 √︁ |N 𝑢 ||N 𝑖 |(6)<h2>formula_coordinates</h2>[4.0, 363.63, 582.67, 195.11, 26.79]<h2>formula_id</h2>formula_7<h2>formula_text</h2>x (𝑙+1) 𝑖→𝑢 and x (𝑙+1)<h2>formula_coordinates</h2>[4.0, 343.1, 620.71, 61.08, 13.12]<h2>formula_id</h2>formula_8<h2>formula_text</h2>e (𝑙+1) 𝑢 = ∑︁ 𝑖 ∈ N 𝑢 x (𝑙+1)<h2>formula_coordinates</h2>[4.0, 359.42, 675.51, 71.33, 21.86]<h2>formula_id</h2>formula_9<h2>formula_text</h2>(𝑙+1) 𝑖 = ∑︁ 𝑢 ∈ N 𝑖 x (𝑙+1) 𝑢→𝑖(7)<h2>formula_coordinates</h2>[4.0, 447.93, 675.51, 110.81, 21.86]<h2>formula_id</h2>formula_11<h2>formula_text</h2>ŷ𝑢𝑖 = e 𝑢 • e 𝑖(9)<h2>formula_coordinates</h2>[5.0, 153.14, 251.78, 141.45, 8.43]<h2>formula_id</h2>formula_12<h2>formula_text</h2>s = 𝑊 𝑖𝑚𝑔 e 𝑖𝑚𝑔 𝑐𝑙𝑠 • 𝑊 𝑡𝑥𝑡 e 𝑡𝑥𝑡 𝑐𝑙𝑠 (10<h2>formula_coordinates</h2>[5.0, 129.18, 466.6, 161.99, 12.08]<h2>formula_id</h2>formula_13<h2>formula_text</h2>)<h2>formula_coordinates</h2>[5.0, 291.16, 468.77, 3.42, 7.94]<h2>formula_id</h2>formula_14<h2>formula_text</h2>ŷ𝑖2𝑡 (I) = Softmax( s(𝐼,𝑇 ) 𝜋 ), ŷ𝑡2𝑖 (T) = Softmax( s(𝑇 , 𝐼 ) 𝜋 )(11)<h2>formula_coordinates</h2>[5.0, 64.37, 530.97, 230.22, 18.66]<h2>formula_id</h2>formula_15<h2>formula_text</h2>L 𝑖𝑡𝑐 = 1 2 * CrossEntropyLoss(𝑦 𝑖2𝑡 (I), ŷ𝑖2𝑡 (I))(12)<h2>formula_coordinates</h2>[5.0, 89.04, 588.67, 205.54, 20.17]<h2>formula_id</h2>formula_16<h2>formula_text</h2>+ 1 2 * CrossEntropyLoss(𝑦 𝑡 2𝑖 (T), ŷ𝑡2𝑖 (T))<h2>formula_coordinates</h2>[5.0, 107.26, 610.17, 151.27, 20.17]<h2>formula_id</h2>formula_18<h2>formula_text</h2>L 𝑖𝑡𝑚 = CrossEntropyLoss(𝑦 𝑖𝑡𝑚 , ŷ𝑖𝑡𝑚 (I, T)) (14<h2>formula_coordinates</h2>[5.0, 359.19, 255.42, 196.13, 8.96]<h2>formula_id</h2>formula_19<h2>formula_text</h2>)<h2>formula_coordinates</h2>[5.0, 555.32, 256.43, 3.42, 7.94]<h2>formula_id</h2>formula_20<h2>formula_text</h2>L Align = L 𝑖𝑡𝑐 + L 𝑚𝑙𝑚 + L 𝑖𝑡𝑚(15)<h2>formula_coordinates</h2>[5.0, 383.13, 328.57, 175.61, 9.87]<h2>formula_id</h2>formula_21<h2>formula_text</h2>L AE = MSELoss(e, ê) + 𝜆 * |Θ AE | 2(16)<h2>formula_coordinates</h2>[5.0, 375.25, 393.94, 183.49, 11.75]<h2>formula_id</h2>formula_22<h2>formula_text</h2>L BPR = - ∑︁ 𝑢 ∈ U ∑︁ 𝑖 ∈ N 𝑢 ∑︁ 𝑗∉N 𝑢 ln 𝜎 ( ŷ𝑢𝑖 -ŷ𝑢 𝑗 )+𝜆 𝑏𝑝𝑟 * (|e 𝑢 | 2 +|e 𝑖 | 2 ) (17<h2>formula_coordinates</h2>[5.0, 322.62, 528.86, 232.7, 21.99]<h2>formula_id</h2>formula_23<h2>formula_text</h2>)<h2>formula_coordinates</h2>[5.0, 555.32, 532.68, 3.42, 7.94]<h2>formula_id</h2>formula_24<h2>formula_text</h2>BPRMF<h2>formula_coordinates</h2>[7.0, 60.38, 112.39, 24.84, 7.24]<h2>formula_id</h2>formula_25<h2>formula_text</h2>• w/o CMIM (<h2>formula_coordinates</h2>[7.0, 317.96, 387.91, 57.76, 8.41]<h1>doi</h1>10.1145/3583780.3615012<h1>title</h1>pyRecLab: A So ware Library for ick Prototyping of Recommender Systems<h1>authors</h1>Gabriel Sepulveda; Vicente Dominguez; Denis Parra<h1>pub_date</h1><h1>abstract</h1>is paper introduces pyRecLab, a so ware library wri en in C++ with Python bindings which allows to quickly train, test and develop recommender systems. Although there are several so ware libraries for this purpose, only a few let developers to get quickly started with the most traditional methods, permi ing them to try di erent parameters and approach several tasks without a significant loss of performance. Among the few libraries that have all these features, they are available in languages such as Java, Scala or C#, what is a disadvantage for less experienced programmers more used to the popular Python programming language. In this article we introduce details of pyRecLab, showing as well performance analysis in terms of error metrics (MAE and RMSE) and train/test time. We benchmark it against the popular Java-based library LibRec, showing similar results. We expect programmers with li le experience and people interested in quickly prototyping recommender systems to be bene ted from pyRecLab.<h1>sections</h1><h2>heading</h2>INTRODUCTION<h2>text</h2>When so ware developers face the challenge of learning about recommender systems (RecSys), developing a RecSys for the rst time, or quickly prototyping a recommender to test available data, a reasonable option to get started is using an existent so ware library. Nowadays, it is possible to nd several libraries in di erent programming languages, being among of the most popular ones MyMedialite [3], LensKit [2], LibRec [4], lightfm [6] and rrecsys [1].
While the aforementioned tools have documentation, implement several methods, and present most of the common functionality required to develop and evaluate a recommendation system, all Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).  of them miss some type of functionality or algorithm which hinder specially newcomers. In particular, while teaching for three years a graduate course on Recommender Systems during the Fall Semester (2014-2016) at a Department of Computer Science, most students have found recurrent di culties in using existent tools to nish an introductory assignment. e assignment is related to tasks such as rating prediction and item recommendation to speci c users, using well-known collaborative ltering methods such as User K-NN, Item K-NN, Slope One and FunkSVD [7]. Some of the problems found were: (a) the lack of implementation of certain methods in some libraries, (b) poor train/test time performance under medium-sized datasets (such as Rrecsys which does not implement sparse matrices), (c) lack of functionality which is typical in a recommendation se ing, such us suggesting a list of items given a speci c user ID, (d) di culties to change parameters in certain models, and (e) students' lack of familiarity with certain programming languages such as Java or C#. While Java is the most popular language based on several rankings, it is also the case that Python is the most popular introductory teaching language in the U.S. since 2004 [5] as well as the one with largest growth in the latest 5 years based on the PYPL ranking 1 .
For these reasons, we developed pyRecLab 2 . We wrote it in C++ with Python bindings, in order to facilitate its adoption among new programmers familiar with Python, but also o ering an appropriate performance when dealing with larger datasets. We implemented most of the foundational recommendation methods for rating prediction and recommendation. Moreover, users can easily change <h2>publication_ref</h2>['b2', 'b1', 'b3', 'b5', 'b0', 'b6', 'b4']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>OTHER RECOMMENDATION LIBRARIES<h2>text</h2>MyMediaLite [3]: It implements several recommendation algorithms, supporting explicit and implicit feedback, as well as contextaware methods. It also allows evaluation with metrics such as MAE, RMSE, prec@N, and nDCG [7]. Many of it functionalities are available from command line; however, to integrate it with other soware it is necessary to program in languages like C# or F#, which is di cult for many newcomer Python developers.
Lenskit [2]: A popular library which provides all basic collaborative ltering methods for predicting ratings (User/Item KNN, Slope One and FunkSVD). It is developed in Java, which could be an entry barrier for new programmers who are mostly familiar with Python.
LibRec [4]: Just like MyMediaLite and Lenskit, a well developed library in terms of algorithms implemented and the metrics available for evaluation. However, documentation is not as good as Lenskit and since it is implemented in Java, it also raises the barrier for new programmers.
Lightfm [6]: is library implements several matrix factorization algorithms for both implicit and explicit feedback. It also has an interface for Python, facilitating its use to several developers. However, it does not implement basic traditional recommender algorithms (User/Item KNN, slope One), so it is not advisable for introductory teaching purposes.
Rrecsys [1]: is tool gets the closest to pyRecLab in terms of easy-of-use, quick prototyping and educational purposes. It is wri en in R language. However, it has two main weaknesses: it misses some traditional algorithms (like Slope One) and it is limited in terms of the amount of data it can process, since it does not support sparse matrices. input le formats (csv, tsv) as well as allowing the user to specify what to le columns represent. • Data handlers. is module implements several data structures, which allow a homogeneous access to the ratings. It grants a good level of independence from the original format from which data were read, with a high level of abstraction. ese data structures will be directly used by the recommendation algorithms for the processing, storage and generation of output data. is module represents the interface between the recommendation algorithms and the Python interpreter. It was developed in C++, and since we aimed at maintaining an appropriate level of code readability, we decided to use the Python/C API rather than Cython for implementation. is allows us to de ne low-level structures in C++ language with a direct mapping with objects handled by the Python interpreter. In this way, we have de ned a data type for each of the recommendation algorithms, which can be instantiated directly from the Python interpreter.<h2>publication_ref</h2>['b2', 'b6', 'b1', 'b3', 'b5', 'b0']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>DESIGN AND IMPLEMENTATION<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>RESULTS & CONCLUSION<h2>text</h2>To check the performance of pyRecLab, we tested it against the popular library LibRec [4] in terms of error and train/test time.
Prediction Results. MAE and RMSE results of rating prediction over Movielens 100K dataset are shown in Table 1. Di erences are very small to LibRec, showing that pyRecLab can reproduce results of a mature recommender library. Time Performance. Although the results vary depending on the method, Figure 2 shows train/test performance using FunkSVD. While both libraries perform similarly in training phase, pyRecLab performs faster in testing time at di erent number of latent factors.
Summarizing, we have introduced PyRecLab, a library for recommender systems which combines the performance of C++ in its implementation with the versatility of Python for easy-of-use. We expect to add algorithms and recommendations metrics, as well as new code samples to facilitate its widespread adoption.<h2>publication_ref</h2>['b3']<h2>figure_ref</h2>[]<h2>table_ref</h2>['tab_1']<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>rrecsys: an R-package for prototyping recommendation algorithms<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>C Ludovik; Markus Zanker<h2>ref_id</h2>b1<h2>title</h2>LensKit: a modular recommender framework<h2>journal</h2>ACM<h2>year</h2>2011<h2>authors</h2>Michael Michael D Ekstrand; Jack Ludwig; John T Kolb;  Riedl<h2>ref_id</h2>b2<h2>title</h2>MyMediaLite: A free recommender system library<h2>journal</h2>ACM<h2>year</h2>2011<h2>authors</h2>Zeno Gantner; Christoph Ste En Rendle; Lars Freudenthaler;  Schmidtieme<h2>ref_id</h2>b3<h2>title</h2>LibRec: A Java Library for Recommender Systems<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Guibing Guo; Jie Zhang; Zhu Sun; Neil Yorke-Smith<h2>ref_id</h2>b4<h2>title</h2>Python is now the most popular introductory teaching language at top us universities<h2>journal</h2>BLOG@ CACM<h2>year</h2>2014-07<h2>authors</h2>Philip Guo<h2>ref_id</h2>b5<h2>title</h2>Metadata Embeddings for User and Item Cold-start Recommendations<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>Maciej Kula<h2>ref_id</h2>b6<h2>title</h2>Recommender systems: Sources of knowledge and evaluation metrics<h2>journal</h2>Springer<h2>year</h2>2013<h2>authors</h2>Denis Parra; Shaghayegh Sahebi<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Figure 1 :1Figure 1: pyRecLab architecture.<h2>figure_data</h2><h2>figure_label</h2>12<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Figure 1 ,Figure 2 :12Figure 1, shows the main modules of pyRecLab. At the bo om, the blue block represents the Python interpreter, which loads the methods and data structures when importing the PyRecLab module. At the top, in orange, all the sub-modules of the library: • File IO. is component allows data input/output by means of reading from text les, as well as writing output recommendations in txt and json formats. It allows great exibility in terms of<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_0<h2>figure_caption</h2><h2>figure_data</h2>Data Handlers( Rating MatrixPython Interface<h2>figure_label</h2>1<h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>pyRecLab vs. LibRec on MovieLens 100K data.<h2>figure_data</h2>MAERMSEpyRecLab LibRec pyRecLab LibRecUserAvg0.8501910.850191 1.0629951.062995ItemAvg0.8275680.827568 1.0334111.033411SlopeOne 0.7485520.748299 0.9527950.952460User KNN 0.7548160.755361 0.9623550.966395Item KNN 0.7493160.748354 0.9536370.953433Funk SVD 0.7328200.731986 0.9253900.923978parameters to understand their e ect and they can also producerecommendations given a speci c user ID.<h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_2<h2>figure_caption</h2>• Recommendation Algorithms. Under the Data handlers block, there are a number of contiguous blocks representing the recommendation algorithms. Algorithms for rating prediction and recommendation are: Item Average, Slope One, User KNN, Item KNN and Funk SVD. On the other hand, Most Popular is only used to generate recommendations. • Python Interface.<h2>figure_data</h2><h1>formulas</h1>[]<h1>doi</h1>10.1145/nnnnnnn.nnnnnnn<h1>title</h1>ragamAI: A Network Based Recommender System to Arrange a Indian Classical Music Concert<h1>authors</h1>Arunkumar Bagavathi; Siddharth Krishnan; Sanjay Subrahmanyan; S L Narasimhan<h1>pub_date</h1>2019-12-08<h1>abstract</h1>South Indian classical music (Carnatic music) is best consumed through live concerts. A carnatic recital requires meticulous planning accounting for several parameters like the performers' repertoire, composition variety, musical versatility, thematic structure, the recital's arrangement, etc. to ensure that the audience have a comprehensive listening experience. In this work, we present ragamAI a novel machine learning framework that utilizes the tonic nuances and musical structures in the carnatic music to generate a concert recital that melodically captures the entire range in an octave. Utilizing the underlying idea of playlist and session-based recommender models, the proposed model studies the mathematical structure present in past concerts and recommends relevant items for the playlist/concert. ragamAI ensembles recommendations given by multiple models to learn user idea and past preference of sequences in concerts to extract recommendations. Our experiments on a vast collection of concert show that our model performs 25%-50% better than baseline models. ragamAI's applications are two-fold. 1) it will assist musicians to customize their performance with the necessary variety required to sustain the interest of the audience for the entirety of the concert 2) it will generate carefully curated lists of south Indian classical music so that the listener can discover the wide range of melody that the musical system can offer.<h1>sections</h1><h2>heading</h2>I. INTRODUCTION<h2>text</h2>The analysis of classical music has become a mainstay in the field of music information retrieval (MIR) over the last decade. Recent research has investigated several nuances of classical music, be it in rhythm cycles, tempo estimation, beat tracking, instrument classification, and melodic analysis, using data-driven techniques. However, there is a paucity of work in bridging these information retrieval techniques to applications like playlist recommendations and automated concert planners. In this work, using the lens of south Indian classical music, also known as Carnatic music, we exploit the grammatical structure and mathematical underpinnings of the music system to develop a machine learning model that can be applied to build a concert plan. While there are several recent works that model different aspects of Indian classical music, to the best of our knowledge we are the first to use the melodic structure, defined as a ragam, present in the music system to design a recommender system. Carnatic musicians have been relying on their repertoire and memory to design song lists in the past. An automated system will enhance the experience and make it more versatile in the development of a concert song list. The proposed model can also be used as an ideal tool to improve a repertoire given that it can draw on source material from different types of databases that suit the case. Moreover, the model can also be used to generate music playlists for streaming applications like Spotify, Pandora, last.fm, etc. to provide an enriching experience that encompasses the musical range offered by the carnatic system in a song sequence.
The efficacy of a session-based recommendation model is to provide desired items for a user in their current session based on their past preferences. For example, a video streaming service like YouTube predicting the user's preference to watch next. In the research literature, there are several frameworks and methodologies for music and video playlist recommendations. Given the mathematical details in south Indian classical music and the task of arranging songs in a concert for a musician, we formulate this problem as session (concert)based playlist (song) recommendation and propose a network based recommender model. We give a network/graph representation for sequences in Indian classical music concerts with our proposed Raaga network. Along with features extracted from the network, the proposed model capture co-occurrences, melodic patterns, and musician's preference of flow in concerts as features. With experiments we show that inclusion of such features improve the performance of recommender model in constructing Indian classical music concerts.
In this work, we aim to solve following research questions by providing a music recommender framework for Indian classical music concerts:
• How the concerts can be structured to understand its organization? We propose a network structure called Raaga Network that captures order, co-occurrence, and context of each item(song/raaga) in concerts. • Can historical data help musicians to build new repertoire? We postulate a network-based machine learning model to recommend items(songs/raagas) based on their underlying structure and mathematical constructs in Raaga Network. We frame this model to extract a set of recommendations for a given sequence of songs. • How useful are item(songs/raagas) recommendations given by the models? We evaluate the usefulness of recommendations from the proposed framework using offline methods like precision and discounted cumulative gain.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>II. RELATED WORK<h2>text</h2>Innovations in machine learning helped the scientific community to contribute to large scale recommendation systems like Pinterest [1] and Spotify [2]. All recommender systems fall in one of the three categories: content-based (domain dependent), collaborative (domain independent), and hybrid (multi-model frameworks) [3]. These algorithmically constructed systems are evaluated using either offline measures like precision, recall, mean average precision(MAP), Normalized Discounted Cumulative Gain(nDCG) or online measures like A/B test and p-value [4].
Advancements in recommendation systems have been applied to music domain as well, Spotify and Pandora for example. The most popular problem in music recommendations is to select a set of songs as a playlist for a user based on their mood and preferred genre and artist [5]. This problem has been answered using multiple methods like: frequent pattern mining [6], collaborative filtering [7], and hybrid models [8]. IN addition to the playlist recommendation models are the session-based prediction models (i.e.), [9]- [11]. Novelty and diversity are considered to be important evaluation measures for music recommendation models [12] to provide the user interesting and unexplored suggestions. In this paper, we present a session-based recommendation model that utilizes deep learning algorithms to give recommendations.
Machine learning has been used in the Indian classical music over the past decade to study multitude of concepts. For example, classifying recordings from YouTube based on the swara using random forest algorithm [13], using pitch information in music signals [14], and a vector based classification model [15], similar to text classification model. Identifying tonic of a music from multi-pitch analysis of the given audio was also given as a classification problem [16]. Unlike the existing methods, we describe methods through the lens of recommender systems. We incorporate the idea of embedding possible features from historical concerts and frequent pattern mining methods to make recommendations. Moreover, our model is designed as a human computer interaction system to communicate recommendations.<h2>publication_ref</h2>['b0', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b10', 'b11', 'b12', 'b13', 'b14', 'b15']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>III. BACKGROUND<h2>text</h2>Carnatic music is the classical music system of Southern India. The Carnatic music tradition is built on the melodic foundation of the ragam or scale that encompasses a collection of swaras (notes) in an octave. The scale follows a 12 note per octave system. This has been expanded into a 16 note system through the 72 melakarta scheme. The ragam system over the last 3 or more centuries has evolved into being categorized according to the mela janya scheme where by ragams are either parent scales or derivatives (mela or janya).
A melakarta by definition will have the same notes in both its ascending and descending scales. The math in the organization of the melakarta scheme will give an idea of the variety of ragams available for musicians to learn, practice, and perform. A typical octave will have the 7 basic notes: S R G M P D N. The S and the P are fixed and static notes. R, G, M, D, N will have different variations lie R1, R2, R3, G1, G2, G3, M1, M2, D1, D2, D3, , N1, N2, N3. If one looks at the frequency values of these notes, R3 corresponds to G1 and D3 corresponds to N1. The melakarta scheme by definition will have a set of 7 notes with S and P constant. So the first ragam will be S R1, G1, M1, P, D1, N1, . The scheme also follows the rule that there will only be one variety of each note that is permissible. So there can be no occurrence of R1 and R2 in a melakarta. Another constraint is that since the frequency values of R3 and G1 are the same they will never occur together. So typically the scheme follows the system of the ragams 1 -36 having S, M1 & P as constants and 37 -72 having S, M2 & P as constants.
Subdividing the ragams further the first 6 will have S R1, G1, M1, P as constants and the six variations will add the combinations D1/N1, D1/N2, D1/N3, D2/N2, D2/N3 & D3/N3. The next six will have R1/G2 with the six varities of D/N and so on to arrive at a total of 72 ragams.
The janya ragams are derivative scales from the parent. Janya ragams have no rules. One can miss few notes in the scale and they can even have different ascending and descending scales.
A typical Carnatic music concert consists of pieces that are performed over a 60 -240 minute period depending on the artist and location. Each composition in a concert can be of varying lengths and usually revolving around a central piece (main ragam). Performers therefore have to train a huge repertoire of compositions set to perform in a variety of ragams. These ragams offer melodic variety in terms of sound, color, and aesthetics. This variety comes through a process of inherent differences because of the specific notes occurring as well as musical and aesthetic differences based on how they are handled by the musician.
The challenge therefore to the performer is to come up with a list of pieces that can offer variety musically, keep the listener engaged and remain fresh so as to avoid being stale and monotonous. A primary constraint of any song list would be non repetition of ragams. The Carnatic music system technically has innumerable ragams, however the numbers in vogue and practiced on a regular basis would probably be in the region of less than 1000. The second aspect of choosing pieces will be compositions. The process involved is coming up with an ideal list of pieces for a concert is taking into consideration both compositions and ragas. All compositions are set to a particular ragam and the entire rendition of the same will conform to that scale of notes only. We plan to exploit composition based recommendations in our follow-up work.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>IV. METHODOLOGIES<h2>text</h2>The proposed ragamAI framework relies on the influence of two separate models: 1. a deep attention model to capture the importance of sequence of ragams, and 2. an embedding model to capture the importance of hand picked features to train. Unlike other methods, which predict next event or item in a given sequence [6], [9], [17]- [19], the proposed model(s)  All our models use both 1-to-1 mapping of features and feature similarity to measure relevancy of recommendations. An overview of the proposed model is represented in Figure 1 A. Raaga network Networks or graphs have been considered as a promising framework to study variety of applications like influence modeling [20], community detection [21], and recommender systems [22]. Their organization of nodes and edges help to study the structural organization and positional values of entities (nodes and communities). We propose the Raaga network as a systematic representation for concerts, where each concert is an ordered transaction of items (songs/ragams). Formally, we define the Raaga network as G = (R, E, w), where R is a set of items/ragams, E is a set of edges representing the flow from one ragam(R m ) to the next(R n ) ∀R m , R n ∈ R, w is edge weight representing the overall frequency of the sequence, and |R| = Total number of unique items/ragams. Example raaga networks are given in Figures 1 and2. We capture the order of ragams in concerts by maintaining a set of attributes
A i = a 1 , a 2 . . . a n , where i = R 1 , R 2 . . . R |R| .
Example attributes include concert id, ragam type, and tonic information of a ragam. The proposed network gives intuition on the flow of each concert and gives an intuition of a ragam's context throughout all concerts. We use this network as an underlying framework to produce recommendations in the proposed model.<h2>publication_ref</h2>['b5', 'b8', 'b16', 'b18', 'b19', 'b20', 'b21']<h2>figure_ref</h2>['fig_0', 'fig_0', 'fig_1']<h2>table_ref</h2>[]<h2>heading</h2>B. Ragam representation learning using node2vec<h2>text</h2>The structural knowledge from the ragam network can help the recommender model to give better results to the user based on their co-occurrence. We use a network embedding model: node2vec [23] to extract features of nodes in the raaga network. This model capture the node context with k-iterations of random walks of length l along with the optimization function given in Equation 1max
f v∈R logP (N (v)|f (v))(1)
where N (v) is the neighbor nodes of node v and the likelihood of the neighborhood of a node is modeled as a softmax function given in Equation 2
P (N (v)|f (v)) = m∈N (v) exp(f (m).f (v)) w∈Rexp(f (w).f (v)) (2)
Thus with node2vec, we collect feature representation of each ragam from all the concerts. Since each data instance is given as a set of ragams in a concert, we aggregate the ragam vectors. Like many other session based applications, the number of songs in every music concerts also vary. Our model handles such variable length inputs by padding zero vector(s). Since order in the song sequence given in the feature vector may have different priorities, we use an attention model to get the concert vector (V c1 ) (given in Equation 3)
V c1 = k i=1 σ(W 1 s i-1 + W 2 s i + c)s i (3
)
where
W 1 , W 2 ∈ R d * d
, k is the number of songs in the concert, and s is the vector representation of the given ragam i.<h2>publication_ref</h2>['b22']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C. Representing the hand picked features<h2>text</h2>Instead of optimizing the learning model (like node2vec), we give a simple strategy to embed a concert (or a set of songs/raaga) into a vector space. In other words, we represent a concert a distribution of concert features (audava,shadava, and sampoorna for example). All features considered for this study is given in Table I. For a given concert, we create one-hot encoded vectors for each ragam based on these hand picked features. We aggregate the features with element-wise average on all the vectors. Thus we represent entire concert into a vector space(V c2 ).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>D. RagamAI model<h2>text</h2>With concert vectors from the attention model (V c1 ) and the concert embedding model (V c2 ), we perform a linear transformation after concatenating the vectors to obtain the aggregate vector using the Equation 4
V g = W 3 (V c1 ⊕ V c2 )(4)
We apply softmax function to get the |R| -dimensional output vector (y ), where each element in y vector represent a score for each ragam for recommendations and |R| is total number of nodes in the raaga network G.
We use cross-entropy, given in Equation 5, as a loss function for training the proposed model.
L(y ) = - |R| i=1 y i log(y i ) + (1 -y i )log(1 -y i )(5)<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>V. EXPERIMENTS AND RESULTS<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>A. Dataset<h2>text</h2>We use the real world data consisting of concerts performed by multiple Indian classical musicians. The data comprises of 1664 concerts performed between 1984 and 2019 with more than 800 unique songs and 280 unique ragams. Each concert is a set of songs, where each song can map to its corresponding ragam. A performer's creativity, impromptu improvisations, and different song durations during the concert, the number of songs in each concert is a dynamic value. Also, since all these concerts are performed in several locations around the world, the songs and the concert style (organization of songs and ragams) are unique. In Figure 3, we give basic data distribution available in our data. As implied in Figure 3a, most concerts have in the range of 9-16 songs. Figure 3b,3c, and3d give distribution of other features in concerts and we give proportion of songs matching these features in concerts. For example, Figure 3b gives the ratio of number of songs in a concert is janya or melakartha ragam, and Figure 3d gives the ratio of types of ragams in concerts(audava,sampoorna,vakra, and other ragams). Due to space constraints, we neglected the distribution of rarely occurring type of ragam(shadava ragam) from this figure. Our data also consists of metadata of all 280 ragams. We use concert structure(organization of ragams in each concert) to build the raaga network and we populate metadata as node features in the network. We further use this network of ragam patterns in experiments for the proposed model to recommend ragams. TABLE I: Features of ragams in the RagaNetwork. Each node in the raga network depicts a ragam and thus each node holds the below features. Although position of ragams in a concert is important, it can be identified by traversing before and after nodes matching the concert id. <h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_2', 'fig_2', 'fig_2', 'fig_2', 'fig_2', 'fig_2']<h2>table_ref</h2>[]<h2>heading</h2>B. Baseline Models<h2>text</h2>Below are some of the popular baseline models that we used to compare our methods
• Item-kNN [24]: Item-kNN is similar to extracting the nearby recommendations. In this method, we find cooccurring items normalized by popularity of all items in the list.
• FPMC [25]: Factorized Personalized Markov Chain is one of the popular models to recommend sequential item in a list. FPMC models the user preference on a list and the preference transitions to recommend next set of items.
• SWIWO [17]: Session-based Wide In Wide Out(SWIWO) is a deep learning based recommender model that uses a both user and item based feature set to predict the next item. To adapt to our problem, we eliminate the user feature section and add a softmax layer to predict the score of items for a given sequence.
We compare proposed model against existing methods in producing recommendations for south Indian classical music concerts. To make fair comparison, we use offline evaluation methods for all algorithms, similar to comparisons made in existing works. In particular, we use following evaluation methods:
• Precision@k: We evaluate on number of recommendations (k) to make (i.e.) We evaluate from k = 1 (Recommending just one item) to k = 15 (Recommending 15 items). • Normalized Discounted Cumulative Gain(nDCG): It is a measure to evaluate the rank of relevant recommendations, calculated using the formula given in Equation 6nDCG
= n i=1 1 log2(i) DCG id(6)
, where n is the number of items in the list to predicted correctly, i is the position of the relevant item in the recommendation, DCG id is an ideal score for the given test sample. Similar to Precision@k, we give results of nDCG@k, where k is the number of items to recommend.  <h2>publication_ref</h2>['b23', 'b24', 'b16']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>C. Results<h2>text</h2>For all our experiments we split the available concerts into training (80%) and test (20%) data, where the training data is used as historical data and the test data is considered as new concerts. Since the goal is to recommend ragams for incomplete concert list and number of ragams in a concert is inconsistent, we conduct our experiments with an assumption Fig. 5: nDCG@k evaluation for different models that the user starts with one ragam (first ragam from the test concert), the system gives prediction and evaluate obtain results, then it considers first two ragams, and the cycle continues. For training the model, we split each concert into multiple sequences in such a way that all instances have at least one user input ragam and one prediction. All results that we report here is an average score of all measures. Figure 4 gives precision@k scores and the Figure 5 gives nDCG@k scores for all baseline and proposed models, where k is the required number of recommendations. We use the average number of songs in overall concerts as the upper bound(15 recommendations), while maintaining a simple lower bound(1 recommendation). From these plots, we can evidence that the baseline models give poor performance in giving recommendations for south Indian classical music concerts. Importantly, we can study from Figure 5 that primarily the proposed ragamAI model give more relevant predictions with even shorter list of recommendations. It is also evident from these plots that structural organization of ragams in Indian classical music concerts play a vital role in building recommendations.
Table II gives the ablation study with multiple combinations in the proposed model. It is evident from these results that each model gives only average performance compared to the proposed model when used alone for the recommendation task. When both features combined as given in the proposed model, we get improved performance.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_3']<h2>table_ref</h2>['tab_1']<h2>heading</h2>VI. CONCLUSION AND DISCUSSION<h2>text</h2>Session-based item recommendation systems have gained many innovations by incorporating deep learning models for variety of applications. Session-based systems can give a precise set of recommendations based on a user's current session activity and their past preferences. In this work we proposed such a model to recommend a set of ragam(music scale in Indian classical music) to perform in a concert, given a set of preferred ragams. Particularly, we proposed a networkbased deep learning approach to utilize both hand picked features and structural features of ragam sequences in concerts for the recommendation task. With the experiments, we show that our model can outperform state-of-the-art methods in recommending songs for Indian classical music concerts.
Although this is the first step to merge recommender system with classical music concerts, the model can be extended in several ways in the future: 1. Even though a ragam is an integral part of a song, adding the composer and composition level features to recommend songs benefit and save time for musicians to arrange the concert, 2. Generalize the architecture to support multiple forms of classical music like Western classical music, and 3. Such an extensive model can be adopted in music streaming services like Pandora and Spotify to match the listener preference in the classical music.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>Hierarchical temporal convolutional networks for dynamic recommender systems<h2>journal</h2>ser. WWW '<h2>year</h2>2019<h2>authors</h2>J You; Y Wang; A Pal; P Eksombatchai; C Rosenburg; J Leskovec<h2>ref_id</h2>b1<h2>title</h2>Understanding user-curated playlists on spotify: A machine learning approach<h2>journal</h2>Int. J. Multimed. Data Eng. Manag<h2>year</h2>2017-10<h2>authors</h2>M Pichl; E Zangerle; G Specht<h2>ref_id</h2>b2<h2>title</h2>Recommendation systems: Principles, methods and evaluation<h2>journal</h2>Egyptian Informatics Journal<h2>year</h2>2015<h2>authors</h2>F Isinkaye; Y Folajimi; B Ojokoh<h2>ref_id</h2>b3<h2>title</h2>Evaluating recommender systems<h2>journal</h2>Springer<h2>year</h2>2015<h2>authors</h2>A Gunawardana; G Shani<h2>ref_id</h2>b4<h2>title</h2>Automated generation of music playlists: Survey and experiments<h2>journal</h2>ACM Comput. Surv<h2>year</h2>2014-11<h2>authors</h2>G Bonnin; D Jannach<h2>ref_id</h2>b5<h2>title</h2>Playlist prediction via metric embedding<h2>journal</h2><h2>year</h2>2012<h2>authors</h2>S Chen; J L Moore; D Turnbull; T Joachims<h2>ref_id</h2>b6<h2>title</h2>Exploring user-specific information in music retrieval<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>Z Cheng; J Shen; L Nie; T.-S Chua; M Kankanhalli<h2>ref_id</h2>b7<h2>title</h2>Evaluating hybrid music recommender systems<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>T Hornung; C.-N Ziegler; S Franz; M Przyjaciel-Zablocki; A Schätzle; G Lausen<h2>ref_id</h2>b8<h2>title</h2>Sessionbased recommendations with recurrent neural networks<h2>journal</h2>CoRR<h2>year</h2>2016<h2>authors</h2>B Hidasi; A Karatzoglou; L Baltrunas; D Tikk<h2>ref_id</h2>b9<h2>title</h2>When recurrent neural networks meet the neighborhood for session-based recommendation<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>D Jannach; M Ludewig<h2>ref_id</h2>b10<h2>title</h2>Recurrent neural networks with topk gains for session-based recommendations<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>B Hidasi; A Karatzoglou<h2>ref_id</h2>b11<h2>title</h2>Tailoring music recommendations to users by considering diversity, mainstreaminess, and novelty<h2>journal</h2><h2>year</h2>2015<h2>authors</h2>M Schedl; D Hauger<h2>ref_id</h2>b12<h2>title</h2>Swara histogram based structural analysis and identification of indian classical ragas<h2>journal</h2><h2>year</h2>2013<h2>authors</h2>P Dighe; H Karnick; B Raj<h2>ref_id</h2>b13<h2>title</h2>Identifying ragas in indian music<h2>journal</h2>IEEE<h2>year</h2>2014<h2>authors</h2>V Kumar; H Pandya; C Jawahar<h2>ref_id</h2>b14<h2>title</h2>Phrase-based rāga recognition using vector space modeling<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>S Gulati; J Serra; V Ishwar; S Sentürk; X Serra<h2>ref_id</h2>b15<h2>title</h2>A multipitch approach to tonic identification in indian classical music<h2>journal</h2><h2>year</h2>2012<h2>authors</h2>J Salamon; S Gulati; X Serra<h2>ref_id</h2>b16<h2>title</h2>Diversifying personalized recommendation with user-session context<h2>journal</h2><h2>year</h2>2017<h2>authors</h2>L Hu; L Cao; S Wang; G Xu; J Cao; Z Gu<h2>ref_id</h2>b17<h2>title</h2>Attentionbased transactional context embedding for next-item recommendation<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>S Wang; L Hu; L Cao; X Huang; D Lian; W Liu<h2>ref_id</h2>b18<h2>title</h2>Session-based recommendation with graph neural networks<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>S Wu; Y Tang; Y Zhu; L Wang; X Xie; T Tan<h2>ref_id</h2>b19<h2>title</h2>Influence maximization in online social networks<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>C Aslay; L V Lakshmanan; W Lu; X Xiao<h2>ref_id</h2>b20<h2>title</h2>Community detection in hypergraphs: Optimal statistical limit and efficient algorithms<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>I Chien; C.-Y Lin; I.-H Wang<h2>ref_id</h2>b21<h2>title</h2>Graph convolutional neural networks for web-scale recommender systems<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>R Ying; R He; K Chen; P Eksombatchai; W L Hamilton; J Leskovec<h2>ref_id</h2>b22<h2>title</h2>node2vec: Scalable feature learning for networks<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>A Grover; J Leskovec<h2>ref_id</h2>b23<h2>title</h2>Amazon. com recommendations: Item-to-item collaborative filtering<h2>journal</h2>IEEE Internet computing<h2>year</h2>2003<h2>authors</h2>G Linden; B Smith; J York<h2>ref_id</h2>b24<h2>title</h2>Factorizing personalized markov chains for next-basket recommendation<h2>journal</h2>ACM WWW<h2>year</h2>2010<h2>authors</h2>S Rendle; C Freudenthaler; L Schmidt-Thieme<h1>figures</h1><h2>figure_label</h2>1<h2>figure_type</h2>figure<h2>figure_id</h2>fig_0<h2>figure_caption</h2>Fig. 1 :1Fig. 1: Architecture of the ragamAI. The model composes 3 layers: 1. Network embedding(skip-gram model) and Concert embedding(embed hand picked features), 2. Vector aggregation from two embedding, and 3. Recommendation<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>figure<h2>figure_id</h2>fig_1<h2>figure_caption</h2>Fig. 2 :2Fig. 2: Sample raaga network constructed from concerts. Concerts C1,C2,C3, and C4 are represented as an ordered list of ragams. Colors in the figure are replaced with edge weights in all experiments. NOTE: Each song in a concert consists of only one ragam<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>Fig. 3 :3Fig. 3: Distribution of various aspects(features) of south Indian classical music performed by one musician.<h2>figure_data</h2><h2>figure_label</h2>4<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>Fig. 4 :4Fig. 4: Precision@k evaluation for different models<h2>figure_data</h2><h2>figure_label</h2>II<h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2>Ablation study<h2>figure_data</h2>ModelPrecision@15nDCG@15Attention W/O node2vec0.15240.0189Attention0.21410.2167Concert Embedding0.25940.5398ragamAI0.48750.7561<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>A i = a 1 , a 2 . . . a n , where i = R 1 , R 2 . . . R |R| .<h2>formula_coordinates</h2>[3.0, 365.2, 147.75, 197.84, 9.96]<h2>formula_id</h2>formula_1<h2>formula_text</h2>f v∈R logP (N (v)|f (v))(1)<h2>formula_coordinates</h2>[3.0, 389.96, 351.06, 173.07, 20.06]<h2>formula_id</h2>formula_2<h2>formula_text</h2>P (N (v)|f (v)) = m∈N (v) exp(f (m).f (v)) w∈Rexp(f (w).f (v)) (2)<h2>formula_coordinates</h2>[3.0, 343.18, 423.38, 219.85, 27.27]<h2>formula_id</h2>formula_3<h2>formula_text</h2>V c1 = k i=1 σ(W 1 s i-1 + W 2 s i + c)s i (3<h2>formula_coordinates</h2>[3.0, 363.77, 575.58, 195.39, 30.32]<h2>formula_id</h2>formula_4<h2>formula_text</h2>)<h2>formula_coordinates</h2>[3.0, 559.16, 586.31, 3.87, 8.64]<h2>formula_id</h2>formula_5<h2>formula_text</h2>W 1 , W 2 ∈ R d * d<h2>formula_coordinates</h2>[3.0, 350.77, 613.03, 67.66, 11.23]<h2>formula_id</h2>formula_6<h2>formula_text</h2>V g = W 3 (V c1 ⊕ V c2 )(4)<h2>formula_coordinates</h2>[4.0, 131.97, 222.43, 168.05, 9.65]<h2>formula_id</h2>formula_7<h2>formula_text</h2>L(y ) = - |R| i=1 y i log(y i ) + (1 -y i )log(1 -y i )(5)<h2>formula_coordinates</h2>[4.0, 79.41, 328.64, 220.61, 31.18]<h2>formula_id</h2>formula_8<h2>formula_text</h2>= n i=1 1 log2(i) DCG id(6)<h2>formula_coordinates</h2>[4.0, 431.89, 611.39, 131.14, 28.52]<h1>doi</h1><h1>title</h1>recommenderlab: An R Framework for Developing and Testing Recommendation Algorithms<h1>authors</h1>Michael Hahsler<h1>pub_date</h1>2022-05-24<h1>abstract</h1>Algorithms that create recommendations based on observed data have significant commercial value for online retailers and many other industries. Recommender systems has a significant research community and studying such systems is part of most modern data science curricula. While there is an abundance of software that implements recommendation algorithms, there is little in terms of supporting recommender system research and education. This paper describes the open-source software recommenderlab which was created with supporting research and education in mind. The package can be directly installed in R or downloaded from https://github.com/mhahsler/recommenderlab.<h1>sections</h1><h2>heading</h2>Introduction<h2>text</h2>Recommender systems apply statistical and knowledge discovery techniques to the problem of making product recommendations based on previously recorded usage data (Sarwar, Karypis, Konstan, and Riedl 2000). Creating such automatically generated personalized recommendations for products including books, songs, TV shows and movies using collaborative filtering have come a long way since Information Lense, the first system using social filtering was created more than 30 years ago (Malone, Grant, Turbak, Brobst, and Cohen 1987). Today, recommender systems are a successful technology used by market leaders in several industries (e.g., by Amazon, Netflix, and Pandora). In retail, such recommendations can improve conversion rates by helping the customer to find products she/he wants to buy faster, promote cross-selling by suggesting additional products and can improve customer loyalty through creating a value-added relationship (Schafer, Konstan, and Riedl 2001).
Even after 30 years, recommender systems still have a very active research community. It is often not clear which of the many available algorithms is appropriate for a particular application and new approaches are constantly proposed. Many commercially available software applications implement recommender algorithms, however, this paper focuses on software support for recommender systems research which includes rapid prototyping algorithms and thorough evaluation and comparison of algorithms. For this purpose, access to the source code is paramount. Many open-source projects implementing recommender algorithms have been initiated over the years. Table 1 provides links to several popular open source implementations which provide code which can be used by researchers. The extent of (currently available) functionality as well as the target usage of the available software packages vary greatly and many projects have been abandoned over the years. A comprehensive list of package was employed by several researchers to develop and test their own algorithms (e.g., Chen, Chao, and Shah 2013;Buhl, Famulare, Glazier, Harris, McDowell, Waldrip, Barnes, and Gerber 2016;Beel, Breitinger, Langer, Lommatzsch, and Gipp 2016;Lombardi and Vernero 2017).
Package recommenderlab focuses on collaborative filtering which is based on the idea that given rating data by many users for many items (e.g., 1 to 5 stars), one can predict a user's rating for an item not known to her or him (see, e.g., Goldberg, Nichols, Oki, and Terry 1992) or create for each user a so called top-N lists of recommended items (see, e.g., Sarwar, Karypis, Konstan, and Riedl 2001;Deshpande and Karypis 2004). The premise is that users who agreed on the rating for some items typically also tend to agree on the rating for other items.
recommenderlab provides implementations of many popular algorithms, including the following.
• User-based collaborative filtering (UBCF) predicts ratings by aggregating the ratings of users who have a similar rating history to the active user (Goldberg et al. 1992;Resnick, Iacovou, Suchak, Bergstrom, and Riedl 1994;Shardanand and Maes 1995).
• Item-based collaborative filtering (IBCF) uses item-to-item similarity based on user ratings to find items that are similar to the items the active user likes (Kitts, Freed, and Vrieze 2000;Sarwar et al. 2001;Linden, Smith, and York 2003;Deshpande and Karypis 2004).
• Latent factor models use singular value decomposition (SVD) to estimate missing ratings using methods like SVD with column-mean imputation, Funk SVD or alternating least squares (Hu, Koren, and Volinsky 2008;Koren, Bell, and Volinsky 2009).
• Association rule-based recommender (AR) uses association rules to find recommended items (Fu, Budzik, and Hammond 2000;Mobasher, Dai, Luo, and Nakagawa 2001;Geyer-Schulz, Hahsler, and Jahn 2002;Lin, Alvarez, and Ruiz 2002;Demiriz 2004).
• Popular items (POPULAR) is a non-personalized algorithm which recommends to all users the most popular items they have not rated yet.
• Randomly chosen items (RANDOM) creates random recommendations which can be used as a baseline for recommender algorithm evaluation.
• Re-recommend liked items (RERECOMMEND) recommends items which the user has rated highly in the past. These recommendations can be useful for items that are typically consumed more than once (e.g., listening to songs or buying groceries).
• Hybrid recommendations (HybridRecommender) aggregates the recommendations of several algorithms (Çano and Morisio 2017).
We will discuss some of these algorithms in the rest of the paper. Detailed information can be found in the survey book by Desrosiers and Karypis (2011).
This rest of this paper is structured as follows. Section 2 introduces collaborative filtering and some of its popular algorithms. In section 3 we discuss the evaluation of recommender algorithms. We introduce the infrastructure provided by recommenderlab in section 4. In section 5 we illustrate the capabilities on the package to create and evaluate recommender algorithms. We conclude with section 6.<h2>publication_ref</h2>['b36', 'b29', 'b38', 'b5', 'b3', 'b1', 'b28', 'b12', 'b37', 'b7', 'b12', 'b34', 'b39', 'b20', 'b37', 'b27', 'b7', 'b18', 'b23', 'b10', 'b31', 'b11', 'b26', 'b6', 'b4', 'b8']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Collaborative Filtering<h2>text</h2>To understand the use of the software, a few formal definitions are necessary. We will often give examples for a movie recommender, but the examples generalize to other types of items as well. Let U = {u 1 , u 2 , . . . , u m } be the set of users and I = {i 1 , i 2 , . . . , i n } the set of items.
Ratings are stored in a m × n user-item rating matrix R = (r jk ), where r jk represents the rating of user u j for item i k . Typically, only a small fraction of ratings are known and for many cells in R, the values are missing. Missing values represent movies that the user has not rated and potentially also not seen yet.
Collaborative filtering aims to create recommendations for a user called the active user u a ∈ U.
We define the set of items unknown to user u a as I a = I \ {i l ∈ I|r al is not missing}. The two typical tasks are to predict ratings for all items in I a or to create a list containing the best N recommended items from I a (i.e., a top-N recommendation list) for u a . Predicting all missing ratings means completing the row of the rating matrix where the missing values for items in I a are replaced by ratings estimated from other data in R. From this point of view, recommender systems are related to matrix completion problem. Creating a top-N list can be seen as a second step after predicting ratings for all unknown items in I a and then taking the N items with the highest predicted ratings. Some algorithms skip predicting ratings first and are able to find the top N items directly. A list of top-N recommendations for a user u a is an partially ordered set T N = (X , ≥), where X ⊂ I a and |X | ≤ N (| • | denotes the cardinality of the set). Note that there may exist cases where top-N lists contain less than N items. This can happen if |I a | < N or if the CF algorithm is unable to identify N items to recommend. The binary relation ≥ is defined as x ≥ y if and only if rax ≥ ray for all x, y ∈ X . Furthermore we require that ∀ x∈X ∀ y∈Ia rax ≥ ray to ensure that the top-N list contains only the items with the highest estimated rating.
Typically we deal with a very large number of items with unknown ratings which makes first predicting rating values for all of them computationally expensive. Some approaches (e.g., rule based approaches) can predict the top-N list directly without considering all unknown items first.
Collaborative filtering algorithms are typically divided into two groups, memory-based CF and model-based CF algorithms (Breese, Heckerman, and Kadie 1998). Memory-based CF use the whole (or at least a large sample of the) user database to create recommendations. The most prominent algorithm is user-based collaborative filtering. The disadvantages of this approach is scalability since the whole user database has to be processed online for creating recommendations. Model-based algorithms use the user database to learn a more compact model (e.g, clusters with users of similar preferences) that is later used to create recommendations.
In the following we will present the basics of well known memory and model-based collaborative filtering algorithms. Further information about these algorithms can be found in the recent survey book chapter by Desrosiers and Karypis (2011).<h2>publication_ref</h2>['b2', 'b8']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>User-based Collaborative Filtering<h2>text</h2>User-based CF (Goldberg et al. 1992;Resnick et al. 1994;Shardanand and Maes 1995) is a memory-based algorithm which tries to mimics word-of-mouth by analyzing rating data from many individuals. The assumption is that users with similar preferences will rate items similarly. Thus missing ratings for a user can be predicted by first finding a neighborhood of similar users and then aggregate the ratings of these users to form a prediction.
The neighborhood is defined in terms of similarity between users, either by taking a given number of most similar users (k nearest neighbors) or all users within a given similarity threshold. Popular similarity measures for CF are the Pearson correlation coefficient and the Cosine similarity. These similarity measures are defined between two users u x and u y as
sim Pearson ( x, y) = i∈I ( x i ¯ x)( y i ¯ y) (|I| -1) sd( x) sd( y) (1) and sim Cosine ( x, y) = x • y x y ,(2)
where x = r x and y = r y represent the row vectors in R with the two users' profile vectors. sd(•) is the standard deviation and • is the l 2 -norm of a vector. For calculating similarity using rating data only the dimensions (items) are used which were rated by both users.
Now the neighborhood for the active user N (a) ⊂ U can be selected by either a threshold on the similarity or by taking the k nearest neighbors. Once the users in the neighborhood are found, their ratings are aggregated to form the predicted rating for the active user. The easiest form is to just average the ratings in the neighborhood.
raj = 1 |N (a)| i∈N (a) r ij (3)
An example of the process of creating recommendations by user-based CF is shown in Figure 1.
To the left is the rating matrix R with 6 users and 8 items and ratings in the range 1 to 5 (stars). We want to create recommendations for the active user u a shown at the bottom of the matrix. To find the k-neighborhood (i.e., the k nearest neighbors) we calculate the similarity between the active user and all other users based on their ratings in the database and then select the k users with the highest similarity. To the right in Figure 1 we see a 2-dimensional representation of the similarities (users with higher similarity are displayed closer) with the active user in the center. The k = 3 nearest neighbors (u 1 , u 2 and u 3 ) are selected and marked in the database to the left. To generate an aggregated estimated rating, we compute the average ratings in the neighborhood for each item not rated by the active user. To create a top-N recommendation list, the items are ordered by predicted rating. In the small example in Figure 1 the order in the top-N list (with N ≥ 4) is i 2 , i 1 , i 7 and i 5 . However, for a real application we probably would not recommend items i 7 and i 5 because of their low ratings. ? 4.0 4.0 2.0 1.0 2.0 ? ? 3.0 ? ? ? 5.0 1.0 ? ? 3.0 ? ? 3.0 2.0 2.0 ? 3.0 4.0 ? ? 2.0 1.0 1.0 2.0 4.0 1.0 1.0 ? ? ? ? ? 1.0 ? 1.0 ? ? 1.0 1.0 ? 1.0 ? ? 4.0 3.0 ? 1.0 ? 5.0 3.5 4.0 2.3 2.0
i 1 i 2 i 3 i 4 i 5 i 6 i 7 i 8 u 1 u 2 u 3 u 4 u 5 u 6 u a r̂a (a) (b) 0.3 1.0 0.2 0.3 0.1 0.1 u a u 1 u 2 u 3 u 4 u 5 u 6 0.3 1.0 0.2 0.3 0.1 0.1 u a u 1 u 2 u 3 u 4 u 5 u 6 (c)<h2>publication_ref</h2>['b12', 'b34', 'b39']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R s a<h2>text</h2>Figure 1: User-based collaborative filtering example with (a) rating matrix R and estimated ratings for the active user, (b), similarites between the active user and the other users s a (Euclidean distance converted to similarities), and (b) the user neighborhood formation.
The fact that some users in the neighborhood are more similar to the active user than others (see Figure 1 (b)) can be incorporated as weights into Equation (3).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>raj = 1<h2>text</h2>i∈N (a) s ai i∈N (a)
s ai r ij (4)
s ai is the similarity between the active user u a and user u i in the neighborhood.
For some types of data the performance of the recommender algorithm can be improved by removing user rating bias. This can be done by normalizing the rating data before applying the recommender algorithm. Any normalization function h : R n×m → R n×m can be used for preprocessing. Ideally, this function is reversible to map the predicted rating on the normalized scale back to the original rating scale. Normalization is used to remove individual rating bias by users who consistently always use lower or higher ratings than other users. A popular method is to center the rows of the user-item rating matrix by
h(r ui ) = r ui -ru ,
where ru is the mean of all available ratings in row u of the user-item rating matrix R.
Other methods like Z-score normalization which also takes rating variance into account can be found in the literature (see, e.g., Desrosiers and Karypis 2011).
The two main problems of user-based CF are that the whole user database has to be kept in memory and that expensive similarity computation between the active user and all other users in the database has to be performed.<h2>publication_ref</h2>['b8']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Item-based Collaborative Filtering<h2>text</h2>Item-based CF (Kitts et al. 2000;Sarwar et al. 2001;Linden et al. 2003;Deshpande and Karypis 2004) is a model-based approach which produces recommendations based on the relationship between items inferred from the rating matrix. The assumption behind this approach is that users will prefer items that are similar to other items they like.  To reduce the model size to n × k with k n, for each item only a list of the k most similar items and their similarity values are stored. The k items which are most similar to item i is denoted by the set S(i) which can be seen as the neighborhood of size k of the item. Retaining only k similarities per item improves the space and time complexity significantly but potentially sacrifices some recommendation quality (Sarwar et al. 2001).
To make a recommendation based on the model we use the similarities to calculate a weighted sum of the user's ratings for related items.<h2>publication_ref</h2>['b20', 'b37', 'b27', 'b7', 'b37']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>rai = 1<h2>text</h2>j∈S(i)∩{l ; r al =?} s ij j∈S(i)∩{l ; r al =?} s ij r aj (5) Figure 2 shows an example for n = 8 items with k = 3. For the similarity matrix S only the k = 3 largest entries are stored per row (these entries are marked using bold face). For the example we assume that we have ratings for the active user for items i 1 , i 5 and i 8 . The rows corresponding to these items are highlighted in the item similarity matrix. We can now compute the weighted sum using the similarities (only the reduced matrix with the k = 3 highest ratings is used) and the user's ratings. The result (below the matrix) shows that i 3 has the highest estimated rating for the active user.
Similar to user-based recommender algorithms, user-bias can be reduced by first normalizing the user-item rating matrix before computing the item-to-item similarity matrix.
Item-based CF is more efficient than user-based CF since the model (reduced similarity matrix) is relatively small (N × k) and can be fully precomputed. Item-based CF is known to only produce slightly inferior results compared to user-based CF and higher order models which take the joint distribution of sets of items into account are possible (Deshpande and Karypis 2004). Furthermore, item-based CF is successfully applied in large scale recommender systems (e.g., by Amazon.com).<h2>publication_ref</h2>['b7']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>User and Item-Based CF using 0-1 Data<h2>text</h2>Less research is available for situations where no large amount of detailed directly elicited rating data is available. However, this is a common situation and occurs when users do not want to directly reveal their preferences by rating an item (e.g., because it is to time consuming). In this case preferences can only be inferred by analyzing usage behavior. For example, we can easily record in a supermarket setting what items a customer purchases. However, we do not know why other products were not purchased. The reason might be one of the following.
• The customer does not need the product right now.
• The customer does not know about the product. Such a product is a good candidate for recommendation.
• The customer does not like the product. Such a product should obviously not be recommended. Mild and Reutterer (2003) and Lee, Jun, Lee, and Kim (2005) present and evaluate recommender algorithms for this setting. The same reasoning is true for recommending pages of a web site given click-stream data. Here we only have information about which pages were viewed but not why some pages were not viewed. This situation leads to binary data or more exactly to 0-1 data where 1 means that we inferred that the user has a preference for an item and 0 means that either the user does not like the item or does not know about it. Pan, Zhou, Cao, Liu, Lukose, Scholz, and Yang (2008)  In the 0-1 case with r jk ∈ 0, 1 where we define:
r jk = 1 user u j is known to have a preference for item i k 0 otherwise.(6)
Two strategies to deal with one-class data is to assume all missing ratings (zeros) are negative examples or to assume that all missing ratings are unknown. In addition, Pan et al. (2008) propose strategies which represent a trade-off between the two extreme strategies based on wighted low rank approximations of the rating matrix and on negative example sampling which might improve results across all recommender algorithms.
If we assume that users typically favor only a small fraction of the items and thus most items with no rating will be indeed negative examples. then we have no missing values and can use the approaches described above for real valued rating data. However, if we assume all zeroes are missing values, then this lead to the problem that we cannot compute similarities using Pearson correlation or Cosine similarity since the not missing parts of the vectors only contains ones. A similarity measure which only focuses on matching ones and thus prevents the problem with zeroes is the Jaccard index:
sim Jaccard (X , Y) = |X ∩ Y| |X ∪ Y| , (7
)
where X and Y are the sets of the items with a 1 in user profiles u a and u b , respectively. The Jaccard index can be used between users for user-based filtering and between items for item-based filtering as described above.<h2>publication_ref</h2>['b30', 'b24', 'b32', 'b32']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Recommendations for 0-1 Data Based on Association Rules<h2>text</h2>Recommender systems using association rules produce recommendations based on a dependency model for items given by a set of association rules (Fu et al. 2000;Mobasher et al. 2001;Geyer-Schulz et al. 2002;Lin et al. 2002;Demiriz 2004). The binary profile matrix R is seen as a database where each user is treated as a transaction that contains the subset of items in I with a rating of 1. Hence transaction k is defined as T k = {i j ∈ I|r jk = 1} and the whole transaction data base is D = {T 1 , T 2 , . . . , T U } where U is the number of users. To build the dependency model, a set of association rules R is mined from R. Association rules are rules of the form X → Y where X , Y ⊆ I and X ∩ Y = ∅. For the model we only use association rules with a single item in the right-hand-side of the rule (|Y| = 1). To select a set of useful association rules, thresholds on measures of significance and interestingness are used. Two widely applied measures are:
support(X → Y) = support(X ∪ Y) = Freq(X ∪ Y)/|D| = P (E X ∩ E Y ) confidence(X → Y) = support(X ∪ Y)/support(X ) = P (E Y |E X )
Freq(I) gives the number of transactions in the data base D that contains all items in I. E I is the event that the itemset I is contained in a transaction.
We now require support(X → Y) > s and confidence(X → Y) > c and also include a length constraint |X ∪ Y| ≤ l. The set of rules R that satisfy these constraints form the dependency model. Although finding all association rules given thresholds on support and confidence is a hard problem (the model grows in the worse case exponential with the number of items), algorithms that efficiently find all rules in most cases are available (e.g., Agrawal and Srikant 1994;Zaki 2000;Han, Pei, Yin, and Mao 2004). Also model size can be controlled by l, s and c.
To make a recommendation for an active user u a given the set of items T a the user likes and the set of association rules R (dependency model), the following steps are necessary:
1. Find all matching rules X → Y for which X ⊆ T a in R.
2. Recommend N unique right-hand-sides (Y) of the matching rules with the highest confidence (or another measure of interestingness).
The dependency model is very similar to item-based CF with conditional probability-based similarity (Deshpande and Karypis 2004). It can be fully precomputed and rules with more than one items in the left-hand-side (X ), it incorporates higher order effects between more than two items.<h2>publication_ref</h2>['b10', 'b31', 'b11', 'b26', 'b6', 'b0', 'b41', 'b16', 'b7']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Other collaborative filtering methods<h2>text</h2>Over time several other model-based approaches have been developed. A popular simple item-based approach is the Slope One algorithm (Lemire and Maclachlan 2005). Another family of algorithms is based on latent factors approach using matrix decomposition (Koren et al. 2009). More recently, deep learning has become a very popular method for flexible matrix completion, matrix factorization and collaborative ranking. A comprehensive survey is presented by Zhang, Yao, Sun, and Tay (2019).
These algorithms are outside the scope of this introductory paper.<h2>publication_ref</h2>['b25', 'b23', 'b42']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Evaluation of Recommender Algorithms<h2>text</h2>Evaluation of recommender systems is an important topic and reviews were presented by Herlocker, Konstan, Terveen, and Riedl ( 2004) and Gunawardana and Shani (2009). Typically, given a rating matrix R, recommender algorithms are evaluated by first partitioning the users (rows) in R into two sets U train ∪ U test = U. The rows of R corresponding to the training users U train are used to learn the recommender model. Then each user u a ∈ U test is seen as an active user. Before creating recommendations some items are withheld from the profile r ua• and it measured either how well the predicted rating matches the withheld value or, for top-N algorithms, if the items in the recommended list are rated highly by the user. Finally, the evaluation measures calculated for all test users are averaged.
To determine how to split U into U train and U test we can use several approaches (Kohavi 1995).
• Splitting: We can randomly assign a predefined proportion of the users to the training set and all others to the test set.
• Bootstrap sampling: We can sample from U test with replacement to create the training set and then use the users not in the training set as the test set. This procedure has the advantage that for smaller data sets we can create larger training sets and still have users left for testing.
• k-fold cross-validation: Here we split U into k sets (called folds) of approximately the same size. Then we evaluate k times, always using one fold for testing and all other folds for leaning. The k results can be averaged. This approach makes sure that each user is at least once in the test set and the averaging produces more robust results and error estimates.
The items withheld in the test data are randomly chosen. Breese et al. (1998) introduced the four experimental protocols called Given 2, Given 5, Given 10 and All-but-1. For the Given x protocols for each user x randomly chosen items are given to the recommender algorithm and the remaining items are withheld for evaluation. For All but x the algorithm gets all but x withheld items.
In the following we discuss the evaluation of predicted ratings and then of top-N recommendation lists.<h2>publication_ref</h2>['b15', 'b21', 'b2']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Evaluation of predicted ratings<h2>text</h2>A typical way to evaluate a prediction is to compute the deviation of the prediction from the true value. This is the basis for the Mean Average Error (MAE) where K is the set of all user-item pairings (i, j) for which we have a predicted rating rij and a known rating r ij which was not used to learn the recommendation model.
MAE = 1 |K| (i,j)∈K |r ij -rij |,(8)
Another popular measure is the Root Mean Square Error (RMSE).
RMSE = (i,j)∈K (r ij -rij ) 2 |K| (9)
RMSE penalizes larger errors stronger than MAE and thus is suitable for situations where small prediction errors are not very important.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Evaluation Top-N recommendations<h2>text</h2>The items in the predicted top-N lists and the withheld items liked by the user (typically determined by a simple threshold on the actual rating) for all test users U test can be aggregated into a so called confusion matrix depicted in table 2 (see Kohavi and Provost (1998)) which corresponds exactly to the outcomes of a classical statistical experiment. The confusion matrix shows how many of the items recommended in the top-N lists (column predicted positive; d + b) were withheld items and thus correct recommendations (cell d) and how many where potentially incorrect (cell b). The matrix also shows how many of the not recommended items (column predicted negative; a + c) should have actually been recommended since they represent withheld items (cell c).
From the confusion matrix several performance measures can be derived. For the data mining task of a recommender system the performance of an algorithm depends on its ability to learn significant patterns in the data set. Performance measures used to evaluate these algorithms have their root in machine learning. A commonly used measure is accuracy, the fraction of correct recommendations to total possible recommendations.<h2>publication_ref</h2>['b22']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Accuracy = correct recommendations total possible recommendations<h2>text</h2>= a + d a + b + c + d (10)
A common error measure is the mean absolute error (MAE, also called mean absolute deviation or MAD).
MAE = 1 N N i=1 | i | = b + c a + b + c + d , (11
)
where N = a + b + c + d is the total number of items which can be recommended and | i | is the absolute error of each item. Since we deal with 0-1 data, | i | can only be zero (in cells a and d in the confusion matrix) or one (in cells b and c). For evaluation recommender algorithms for rating data, the root mean square error is often used. For 0-1 data it reduces to the square root of MAE.
Recommender systems help to find items of interest from the set of all available items. This can be seen as a retrieval task known from information retrieval. Therefore, standard information retrieval performance measures are frequently used to evaluate recommender performance. Precision and recall are the best known measures used in information retrieval (Salton and McGill 1983;van Rijsbergen 1979).
Precision = correctly recommended items total recommended items = d b + d (12) Recall = correctly recommended items total useful recommendations = d c + d (13)
Often the number of total useful recommendations needed for recall is unknown since the whole collection would have to be inspected. However, instead of the actual total useful recommendations often the total number of known useful recommendations is used. Precision and recall are conflicting properties, high precision means low recall and vice versa. To find an optimal trade-off between precision and recall a single-valued measure like the E-measure (van Rijsbergen 1979) can be used. The parameter α controls the trade-off between precision and recall.
E-measure = 1 α(1/Precision) + (1 -α)(1/Recall)(14)
A popular single-valued measure is the F-measure. It is defined as the harmonic mean of precision and recall.
F-measure = 2 Precision Recall Precision + Recall = 2 1/Precision + 1/Recall (15)
It is a special case of the E-measure with α = .5 which places the same weight on both, precision and recall. In the recommender evaluation literature the F-measure is often referred to as the measure F1.
Another method used in the literature to compare two classifiers at different parameter settings is the Receiver Operating Characteristic (ROC). The method was developed for signal detection and goes back to the Swets model (van Rijsbergen 1979). The ROC-curve is a plot of the system's probability of detection (also called sensitivity or true positive rate TPR which is equivalent to recall as defined in formula 13) by the probability of false alarm (also called false positive rate FPR or 1 -specificity, where specificity = a a+b ) with regard to model parameters. A possible way to compare the efficiency of two systems is by comparing the size of the area under the ROC-curve, where a bigger area indicates better performance.<h2>publication_ref</h2>['b35', 'b40']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Recommenderlab Infrastructure<h2>text</h2>recommenderlab is implemented using formal classes in the S4 class system. Figure 3 shows the main classes and their relationships.
The package uses the abstract ratingMatrix to provide a common interface for rating data. ratingMatrix implements many methods typically available for matrix-like objects. For example, dim(), dimnames(), colCounts(), rowCounts(), colMeans(), rowMeans(), colSums() and rowSums(). Additionally sample() can be used to sample from users (rows) and image() produces an image plot.
For ratingMatrix we provide two concrete implementations realRatingMatrix and binaryRatingMatrix to represent different types of rating matrices R. realRatingMatrix implements a rating matrix with real valued ratings stored in sparse format defined in package Matrix. Sparse matrices in Matrix typically do not store 0s explicitly, however for realRatingMatrix we use these sparse matrices such that instead of 0s, NAs are not explicitly stored.
binaryRatingMatrix implements a 0-1 rating matrix using the implementation of itemMatrix defined in package arules. itemMatrix stores only the ones and internally uses a sparse representation from package Matrix. With this class structure recommenderlab can be easily extended to other forms of rating matrices with different concepts for efficient storage in the future.
Class Recommender implements the data structure to store recommendation models. The creator method
Recommender(data, method, parameter = NULL)
takes data as a ratingMatrix, a method name and some optional parameters for the method and returns a Recommender object. Once we have a recommender object, we can predict top-N recommendations for active users using predict(object, newdata, n=10, type=c("topNList", "ratings", "ratingMatrix"), ...).
Predict can return either top-N lists (default setting) or predicted ratings. object is the recommender object, newdata is the data for the active users. For top-N lists n is the maximal number of recommended items in each list and predict() will return an objects of class topNList which contains one top-N list for each active user. For "ratings" and "ratingMatrix", n is ignored and an object of realRatingMatrix is returned. Each row contains the predicted ratings for one active user. The difference is, that for "ratings", the items for which a rating exists in newdata have a NA instead of a predicted/actual ratings.
The actual implementations for the recommendation algorithms are managed using the registry mechanism provided by package registry. The registry called recommenderRegistry and stores recommendation method names and a short description. Generally, the registry mechanism is hidden from the user and the creator function Recommender() uses it in the background to map a recommender method name to its implementation. However, the registry can be directly queried by recommenderRegistry$get_entries() and new recommender algorithms can be added by the user. We will give and example for this feature in the examples section of this paper.
To evaluate recommender algorithms package recommenderlab provides the infrastructure to create and maintain evaluation schemes stored as an object of class evaluationScheme from rating data. The creator function evaluationScheme(data, method="split", train=0.9, k=10, given=3)
creates the evaluation scheme from a data set using a method (e.g., simple split, bootstrap sampling, k-fold cross validation). Testing is perfomed by withholding items (parameter given). Breese et al. (1998) introduced the four experimental witholding protocols called Given 2, Given 5, Given 10 and All-but-1. During testing, the Given x protocol presents the algorithm with only x randomly chosen items for the test user, and the algorithm is evaluated by how well it is able to predict the withheld items. For All-but-x, a generalization of All-but-1, the algorithm sees all but x withheld ratings for the test user. given controls x in the evaluations scheme. Positive integers result in a Given x protocol, while negative values produce a All-but-x protocol.
The function evaluate() is then used to evaluate several recommender algorithms using an evaluation scheme resulting in a evaluation result list (class evaluationResultList) with one entry (class evaluationResult) per algorithm. Each object of evaluationResult contains one or several object of confusionMatrix depending on the number of evaluations specified in the evaluationScheme (e.g., k for k-fold cross validation). With this infrastructure several recommender algorithms can be compared on a data set with a single line of code.
In the following, we will illustrate the usage of recommenderlab with several examples.<h2>publication_ref</h2>['b2']<h2>figure_ref</h2>['fig_3']<h2>table_ref</h2>[]<h2>heading</h2>Examples<h2>text</h2>This fist few example shows how to manage data in recommender lab and then we create and evaluate recommenders. First, we load the package.
R> library("recommenderlab")<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Coercion to and from rating matrices<h2>text</h2>For this example we create a small artificial data set as a matrix.
R> m <-matrix(sample(c(as.numeric(0:5), NA), 50, + replace=TRUE, prob=c(rep(.4/6,6),.6)), ncol=10, + dimnames=list(user=paste("u", 1:5, sep= ), + item=paste("i", 1:10, sep= ))) R> m With coercion, the matrix can be easily converted into a realRatingMatrix object which stores the data in sparse format (only non-NA values are stored explicitly; NA values are represented by a dot).
R> r <-as(m, "realRatingMatrix") R> r 5 x 10 rating matrix of class ???realRatingMatrix??? with 19 ratings.
R> getRatingMatrix(r) 5 x 10 sparse Matrix of class "dgCMatrix" u1 . 2 3 5 . 5 . 4.000e+00 . . u2 2 . . . . . . . 2 3 u3 2 . . . . 1 . . . . u4 2 2 1 . . 5 . 2.225e-308 2 . u5 5 . . . . . . 5.000e+00 . 4
The realRatingMatrix can be coerced back into a matrix which is identical to the original matrix.
R> identical(as(r, "matrix"),m)
[1] TRUE It can also be coerced into a list of users with their ratings for closer inspection or into a data.frame with user/item/rating tuples. The data.frame version is especially suited for writing rating data to a file (e.g., by write.csv()). Coercion from data.frame (user/item/rating tuples) and list into a sparse rating matrix is also provided. This way, external rating data can easily be imported into recommenderlab.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> as(r, "list")<h2>text</h2>$ 0 i2 i3 i4 i6 i8 2 3 5 5 4 $ 1 i1 i9 i10 2 2 3 $ 2 i1 i6 2 1 $ 3 i1 i2 i3 i6 i8 i9 2.000e+00 2.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Normalization<h2>text</h2>An important operation for rating matrices is to normalize the entries to, e.g., centering to remove rating bias by subtracting the row mean from all ratings in the row. This is can be easily done using normalize().
R> r_m <-normalize(r) R> r_m 5 x 10 rating matrix of class ???realRatingMatrix??? with 19 ratings. Normalized using center on rows. R> getRatingMatrix(r_m) Normalization can be reversed using denormalize().
5
R> denormalize(r_m) 5 x 10 rating matrix of class ???realRatingMatrix??? with 19 ratings.
Small portions of rating matrices can be visually inspected using image().
R> image(r, main = "Raw Ratings") R> image(r_m, main = "Normalized Ratings")
Figure 4 shows the resulting plots.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_5']<h2>table_ref</h2>[]<h2>heading</h2>Binarization of data<h2>text</h2>A matrix with real valued ratings can be transformed into a 0-1 matrix with binarize() and a user specified threshold (min_ratings) on the raw or normalized ratings. In the following only items with a rating of 4 or higher will become a positive rating in the new binary rating matrix.
R> r_b <-binarize(r, minRating=4) R> r_b 5 x 10 rating matrix of class ???binaryRatingMatrix??? with 7 ratings.
R> as(r_b, "matrix") 
i1 i2 i3 i4 i5 i6 i7 i8 i9 i10<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Inspection of data set properties<h2>text</h2>We will use the data set Jester5k for the rest of this section. This data set comes with recommenderlab and contains a sample of 5000 users from the anonymous ratings data from the Jester Online Joke Recommender System collected between April 1999and May 2003(Goldberg, Roeder, Gupta, and Perkins 2001). The data set contains ratings for 100 jokes on a scale from -10 to +10. All users in the data set have rated 36 or more jokes.
R> data(Jester5k) R> Jester5k 5000 x 100 rating matrix of class ???realRatingMatrix??? with 363209 ratings.
Jester5k contains 363209 ratings. For the following examples we use only a subset of the data containing a sample of 1000 users (we set the random number generator seed for reproducibility). For random sampling sample() is provided for rating matrices.
R> set.seed(1234) R> r <-sample(Jester5k, 1000) R> r 1000 x 100 rating matrix of class ???realRatingMatrix??? with 74323 ratings.
This subset still contains 74323 ratings. Next, we inspect the ratings for the first user. We can select an individual user with the extraction operator.<h2>publication_ref</h2>['b13']<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> rowCounts(r[1,])<h2>text</h2>u20648 74
R> as(r[1,], "list") $ 0 j1 j2 j3 j4 j5 j6 j7 j8 j9 j10 j11 j12 -2.86 1. 75 -4.03 -5.78 2.23 -5.44 -3.40 8.74 -4.51 3.74  The user has rated 74 jokes, the list shows the ratings and the user's rating average is -0.423243243243243 .
Next, we look at several distributions to understand the data better. getRatings() extracts a vector with all non-missing ratings from a rating matrix.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> hist(getRatings(r), breaks=100)<h2>text</h2>In the histogram in Figure 5 shoes an interesting distribution where all negative values occur with a almost identical frequency and the positive ratings more frequent with a steady decline towards the rating 10. Since this distribution can be the result of users with strong rating bias, we look next at the rating distribution after normalization.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_6']<h2>table_ref</h2>[]<h2>heading</h2>R> hist(getRatings(normalize(r)), breaks=100)<h2>text</h2>R> hist(getRatings(normalize(r, method="Z-score")), breaks=100)
Figure 6 shows that the distribution of ratings ins closer to a normal distribution after row centering and Z-score normalization additionally reduces the variance to a range of roughly -3 to +3 standard deviations. It is interesting to see that there is a pronounced peak of ratings between zero and two.  R> hist(colMeans(r), breaks=20)
Figure 7 shows that there are unusually many users with ratings around 70 and users who have rated all jokes. The average ratings per joke look closer to a normal distribution with a mean above 0.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Creating a recommender<h2>text</h2>A recommender is created using the creator function Recommender(). Available recommendation methods are stored in a registry. The registry can be queried. Here we are only interested in methods for real-valued rating data.
R> recommenderRegistry$get_entries(dataType = "realRatingMatrix") Next, we create a recommender which generates recommendations solely on the popularity of items (the number of users who have the item in their profile). We create a recommender from the first 1000 users in the Jester5k data set.
$
R> r <-Recommender(Jester5k[1:1000], method = "POPULAR") R> r
Recommender of type ???POPULAR??? for ???realRatingMatrix??? learned using 1000 users.
The model can be obtained from a recommender using getModel().<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> names(getModel(r))<h2>text</h2>[1] "topN" "ratings" "normalize" [4] "aggregationRatings" "aggregationPopularity" "verbose"<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> getModel(r)$topN<h2>text</h2>Recommendations as ???topNList??? with n = 100 for 1 users.
In this case the model has a top-N list to store the popularity order and further elements (average ratings, if it used normalization and the used aggregation function).
Recommendations are generated by predict() (consistent with its use for other types of models in R). The result are recommendations in the form of an object of class TopNList.
Here we create top-5 recommendation lists for two users who were not used to learn the model.
R> recom <-predict(r, Jester5k[1001:1002], n=5) R> recom
Recommendations as ???topNList??? with n = 5 for 2 users.
The result contains two ordered top-N recommendation lists, one for each user. The recommended items can be inspected as a list.
R> as(recom, "list")<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>$u15553<h2>text</h2>[1] "j89" "j72" "j93" "j76" "j87" $u7886
[1] "j89" "j72" "j93" "j76" "j1"
Since the top-N lists are ordered, we can extract sublists of the best items in the top-N . For example, we can get the best 3 recommendations for each list using bestN().
R> recom3 <-bestN(recom, n = 3) R> recom3
Recommendations as ???topNList??? with n = 3 for 2 users.
R> as(recom3, "list")<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>$u15553<h2>text</h2>[1] "j89" "j72" "j93" $u7886
[1] "j89" "j72" "j93"
Many recommender algorithms can also predict ratings. This is also implemented using predict() with the parameter type set to "ratings". Predicted ratings are returned as an object of realRatingMatrix. The prediction contains NA for the items rated by the active users. In the example we show the predicted ratings for the first 10 items for the two active users.
Alternatively, we can also request the complete rating matrix which includes the original ratings by the user.
R> recom <-predict (r, Jester5k[1001(r, Jester5k[ :1002]], type="ratingMatrix") R> recom 2 x 100 rating matrix of class ???realRatingMatrix??? with 200 ratings. R> as (recom, "matrix") <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Evaluation of predicted ratings<h2>text</h2>Next, we will look at the evaluation of recommender algorithms. recommenderlab implements several standard evaluation methods for recommender systems. Evaluation starts with creating an evaluation scheme that determines what and how data is used for training and testing. Here we create an evaluation scheme which splits the first 1000 users in Jester5k into a training set (90%) and a test set (10%). For the test set 15 items will be given to the recommender algorithm and the other items will be held out for computing the error.
R> e <-evaluationScheme(Jester5k[1:1000], method="split", train=0.9, + given=15, goodRating=5) R> e Evaluation scheme with 15 items given Method: ???split??? with 1 run(s). Training set proportion: 0.900 Good ratings: >=5.000000 Data set: 1000 x 100 rating matrix of class ???realRatingMatrix??? with 74164 ratings.
We create two recommenders (user-based and item-based collaborative filtering) using the training data. R> r1 <-Recommender(getData(e, "train"), "UBCF") R> r1
Recommender of type ???UBCF??? for ???realRatingMatrix??? learned using 900 users. R> r2 <-Recommender(getData(e, "train"), "IBCF") R> r2
Recommender of type ???IBCF??? for ???realRatingMatrix??? learned using 900 users.
Next, we compute predicted ratings for the known part of the test data (15 items for each user) using the two algorithms.
R> p1 <-predict(r1, getData(e, "known"), type="ratings") R> p1 100 x 100 rating matrix of class ???realRatingMatrix??? with 8357 ratings. R> p2 <-predict(r2, getData(e, "known"), type="ratings") R> p2 100 x 100 rating matrix of class ???realRatingMatrix??? with 8417 ratings.
Finally, we can calculate the error between the prediction and the unknown part of the test data.
R> error <-rbind( + UBCF = calcPredictionAccuracy(p1, getData(e, "unknown")), + IBCF = calcPredictionAccuracy(p2, getData(e, "unknown")) + ) R> error 4.571 20.89 3.585 IBCF 4.538 20.60 3.440 In this example user-based collaborative filtering produces a smaller prediction error.
RMSE MSE MAE UBCF<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Evaluation of a top-N recommender algorithm<h2>text</h2>For this example we create a 4-fold cross validation scheme with the the Given-3 protocol, i.e., for the test users all but three randomly selected items are withheld for evaluation.
R> scheme <-evaluationScheme(Jester5k[1:1000], method="cross", k=4, given=3, + goodRating=5) R> scheme Evaluation scheme with 3 items given Method: ???cross-validation??? with 4 run(s). Good ratings: >=5.000000 Data set: 1000 x 100 rating matrix of class ???realRatingMatrix??? with 74164 ratings.
Next we use the created evaluation scheme to evaluate the recommender method popular. We evaluate top-1, top-3, top-5, top-10, top-15 and top-20 recommendation lists.
R> results <-evaluate(scheme, method="POPULAR", type = "topNList", + n=c (1,3,5,10,15,20)) 
POPULAR<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> results<h2>text</h2>Evaluation results for 4 folds/samples using method ???POPULAR???.
The result is an object of class EvaluationResult which contains several confusion matrices. getConfusionMatrix() will return the confusion matrices for the 4 runs (we used 4-fold cross evaluation) as a list. In the following we look at the first element of the list which represents the first of the 4 runs. Evaluation results can be plotted using plot(). The default plot is the ROC curve which plots the true positive rate (TPR) against the false positive rate (FPR).<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> getConfusionMatrix(results)[[1]]<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> plot(results, annotate=TRUE)<h2>text</h2>For the plot where we annotated the curve with the size of the top-N list is shown in Figure 8. By using "prec/rec" as the second argument, a precision-recall plot is produced (see Figure 9). R> plot(results, "prec/rec", annotate=TRUE)<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_8']<h2>table_ref</h2>[]<h2>heading</h2>Comparing recommender algorithms<h2>text</h2><h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Comparing top-N recommendations<h2>text</h2>The comparison of several recommender algorithms is one of the main functions of recommenderlab. For comparison also evaluate() is used. The only change is to use evaluate() with a list of algorithms together with their parameters instead of a single method name. In the following we use the evaluation scheme created above to compare the five recommender algorithms: random items, popular items, user-based CF, item-based CF, and SVD approximation. Note that when running the following code, the CF based algorithms are very slow.
For the evaluation we use a "all-but-5" scheme. This is indicated by a negative number for given.
R> set.seed(2016) R> scheme <-evaluationScheme(Jester5k[1:1000], method="split", train = .9, + k=1, given=-5, goodRating=5) R> scheme Evaluation scheme using all-but-5 items Method: ???split??? with 1 run(s). Training set proportion: 0.900 Good ratings: >=5.000000 Data set: 1000 x 100 rating matrix of class ???realRatingMatrix??? with 74164 ratings.
R> algorithms <-list( + "random items" = list(name="RANDOM", param=NULL), + "popular items" = list(name="POPULAR", param=NULL), + "user-based CF" = list(name="UBCF", param=list(nn=50)), + "item-based CF" = list(name="IBCF", param=list(k=50)), + "SVD approximation" = list(name="SVD", param=list(k = 50)) + ) R> ## run algorithms R> results <-evaluate(scheme, algorithms, type = "topNList", + n=c (1,3,5,10,15,20)) The result is an object of class evaluationResultList for the five recommender algorithms.
RANDOM<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> results<h2>text</h2>List of evaluation results for 5 recommenders:
$ random items Evaluation results for 1 folds/samples using method ???RANDOM???.
$ popular items Evaluation results for 1 folds/samples using method ???POPULAR???.
$ user-based CF Evaluation results for 1 folds/samples using method ???UBCF???.
$ item-based CF Evaluation results for 1 folds/samples using method ???IBCF???.
$ SVD approximation Evaluation results for 1 folds/samples using method ???SVD???.
Individual results can be accessed by list subsetting using an index or the name specified when calling evaluate().<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> names(results)<h2>text</h2>[1] "random items" "popular items" "user-based CF" [4] "item-based CF" "SVD approximation"<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> results[["user-based CF"]]<h2>text</h2>Evaluation results for 1 folds/samples using method ???UBCF???.
Again plot() can be used to create ROC and precision-recall plots (see Figures 10 and11). Plot accepts most of the usual graphical parameters like pch, type, lty, etc. In addition annotate can be used to annotate the points on selected curves with the list length.
R> plot(results, annotate=c(1,3), legend="bottomright") R> plot(results, "prec/rec", annotate=3, legend="topleft")
For this data set and the given evaluation scheme popular items and the user-based CF methods clearly outperform the other methods. In Figure 10 we see that they dominate (almost completely) the other method since for each length of top-N list they provide a better combination of TPR and FPR.<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_9', 'fig_10', 'fig_9']<h2>table_ref</h2>[]<h2>heading</h2>Comparing ratings<h2>text</h2>Next, we evaluate not top-N recommendations, but how well the algorithms can predict ratings.
R> ## run algorithms R> results <-evaluate(scheme, algorithms, type = "ratings")   The result is again an object of class evaluationResultList for the five recommender algorithms.
RANDOM<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>R> results<h2>text</h2>List of evaluation results for 5 recommenders:
$ random items Evaluation results for 1 folds/samples using method ???RANDOM???.
$ popular items Evaluation results for 1 folds/samples using method ???POPULAR???.
$ user-based CF Evaluation results for 1 folds/samples using method ???UBCF???.
$ item-based CF Evaluation results for 1 folds/samples using method ???IBCF???.
$ SVD approximation Evaluation results for 1 folds/samples using method ???SVD???.
R> plot(results, ylim = c(0,100))
Plotting the results shows a barplot with the root mean square error, the mean square error and the mean absolute error (see Figures 12).<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_11']<h2>table_ref</h2>[]<h2>heading</h2>Using a 0-1 data set<h2>text</h2>For comparison we will check how the algorithms compare given less information. We convert the data set into 0-1 data and instead of a all-but-5 we use the given-3 scheme.
R> Jester_binary <-binarize(Jester5k, minRating=5) R> Jester_binary <-Jester_binary[rowCounts(Jester_binary)>20] R> Jester_binary 1840 x 100 rating matrix of class ???binaryRatingMatrix??? with 67728 ratings.
R> scheme_binary <-evaluationScheme(Jester_binary[1:1000], + method="split", train=.9, k=1, given=3) R> scheme_binary Evaluation scheme with 3 items given Method: ???split??? with 1 run(s). Training set proportion: 0.900 Good ratings: NA Data set: 1000 x 100 rating matrix of class ???binaryRatingMatrix??? with 36619 ratings. R> results_binary <-evaluate(scheme_binary, algorithms, + type = "topNList", n=c (1,3,5,10,15,20)) Note that SVD does not implement a method for binary data and is thus skipped.
RANDOM
R> plot(results_binary, annotate=c(1,3), legend="topright")
From Figure 13 we see that given less information, the performance of user-based CF suffers the most and the simple popularity based recommender performs almost a well as item-based CF.
Similar to the examples presented here, it is easy to compare different recommender algorithms for different data sets or to compare different algorithm settings (e.g., the influence of neighborhood formation using different distance measures or different neighborhood sizes).<h2>publication_ref</h2>[]<h2>figure_ref</h2>['fig_12']<h2>table_ref</h2>[]<h2>heading</h2>Implementing a new recommender algorithm<h2>text</h2>Adding a new recommender algorithm to recommenderlab is straight forward since it uses a registry mechanism to manage the algorithms. To implement the actual recommender algorithm we need to implement a creator function which takes a training data set, trains a model and provides a predict function which uses the model to create recommendations for new data. The model and the predict function are both encapsulated in an object of class Recommender.
For examples look at the files starting with RECOM in the packages R directory. A good examples is in RECOM_POPULAR.R.<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Conclusion<h2>text</h2>In this paper we described the R extension package recommenderlab which is especially geared towards developing and testing recommender algorithms. The package allows to create evaluation schemes following accepted methods and then use them to evaluate and compare recommender algorithms. recommenderlab currently includes several standard algorithms and adding new recommender algorithms to the package is facilitated by the built in registry mechanism to manage algorithms. In the future we will add more and more of these algorithms to the package and we hope that some algorithms will also be contributed by other researchers. <h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2><h2>text</h2>filtering Java http://mahout.apache.org/ Crab Components to create recommender systems Python https://github.com/muricoca/crab LensKit Collaborative filtering algorithms from GroupLens Research Python http://lenskit.grouplens.org/ MyMediaLite factorization C++ https://www.jmlr.org/papers/v13/chen12a.html Vogoo PHP LIB //sourceforge.net/projects://github.com/mhahsler/recommenderlab<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h2>heading</h2>Affiliation:<h2>text</h2>Michael Hahsler Computer Science Lyle School of Engineering Southern Methodist University P.O. Box 750122 Dallas, TX 75275-0122 E-mail: mhahsler@lyle.smu.edu URL: michael@hahsler.net<h2>publication_ref</h2>[]<h2>figure_ref</h2>[]<h2>table_ref</h2>[]<h1>references</h1><h2>ref_id</h2>b0<h2>title</h2>Fast Algorithms for Mining Association Rules in Large Databases<h2>journal</h2>VLDB<h2>year</h2>1994<h2>authors</h2>R Agrawal; R Srikant<h2>ref_id</h2>b1<h2>title</h2>Towards reproducibility in recommender-systems research User Model<h2>journal</h2>User Modeling and User-Adapted Interaction<h2>year</h2>2016<h2>authors</h2>J Beel; C Breitinger; S Langer; A Lommatzsch; B Gipp<h2>ref_id</h2>b2<h2>title</h2>Empirical Analysis of Predictive Algorithms for Collaborative Filtering<h2>journal</h2><h2>year</h2>1998<h2>authors</h2>J S Breese; D Heckerman; C Kadie<h2>ref_id</h2>b3<h2>title</h2>Optimizing multi-channel health information delivery for behavioral change<h2>journal</h2><h2>year</h2>2016<h2>authors</h2>M Buhl; J Famulare; C Glazier; J Harris; A Mcdowell; G Waldrip; L Barnes; M Gerber<h2>ref_id</h2>b4<h2>title</h2>Hybrid recommender systems: A systematic literature review<h2>journal</h2>Intell. Data Anal<h2>year</h2>2017<h2>authors</h2>E Çano; M Morisio<h2>ref_id</h2>b5<h2>title</h2>Hybrid recommendation system for tourism<h2>journal</h2>IEEE Computer Society<h2>year</h2>2013<h2>authors</h2>J Chen; K Chao; N Shah<h2>ref_id</h2>b6<h2>title</h2>Enhancing Product Recommender Systems on Sparse Binary Data<h2>journal</h2>Data Minining and Knowledge Discovery<h2>year</h2>2004<h2>authors</h2>A Demiriz<h2>ref_id</h2>b7<h2>title</h2>Item-based top-N recommendation algorithms<h2>journal</h2>ACM Transations on Information Systems<h2>year</h2>2004<h2>authors</h2>M Deshpande; G Karypis<h2>ref_id</h2>b8<h2>title</h2>A Comprehensive Survey of Neighborhood-based Recommendation Methods<h2>journal</h2>Springer US<h2>year</h2>2011<h2>authors</h2>C Desrosiers; G Karypis<h2>ref_id</h2>b9<h2>title</h2>UML Distilled: A Brief Guide to the Standard Object Modeling Language<h2>journal</h2>Addison-Wesley Professional<h2>year</h2>2004<h2>authors</h2>M Fowler<h2>ref_id</h2>b10<h2>title</h2>Mining navigation history for recommendation<h2>journal</h2>ACM<h2>year</h2>2000<h2>authors</h2>X Fu; J Budzik; K J Hammond<h2>ref_id</h2>b11<h2>title</h2>A Customer Purchase Incidence Model Applied to Recommender Systems<h2>journal</h2>Springer-Verlag<h2>year</h2>2001<h2>authors</h2>A Geyer-Schulz; M Hahsler; M Jahn<h2>ref_id</h2>b12<h2>title</h2>Using collaborative filtering to weave an information tapestry<h2>journal</h2>Communications of the ACM<h2>year</h2>1992<h2>authors</h2>D Goldberg; D Nichols; B M Oki; D Terry<h2>ref_id</h2>b13<h2>title</h2>Eigentaste: A Constant Time Collaborative Filtering Algorithm<h2>journal</h2>Information Retrieval<h2>year</h2>2001<h2>authors</h2>K Goldberg; T Roeder; D Gupta; C Perkins<h2>ref_id</h2>b14<h2>title</h2>Building a Recommendation System with R<h2>journal</h2>Packt Publishing<h2>year</h2>2015<h2>authors</h2>S K Gorakala; M Usuelli<h2>ref_id</h2>b15<h2>title</h2>A Survey of Accuracy Evaluation Metrics of Recommendation Tasks<h2>journal</h2>Journal of Machine Learning Research<h2>year</h2>2009<h2>authors</h2>A Gunawardana; G Shani<h2>ref_id</h2>b16<h2>title</h2>Mining frequent patterns without candidate generation<h2>journal</h2>Data Mining and Knowledge Discovery<h2>year</h2>2004<h2>authors</h2>J Han; J Pei; Y Yin; R Mao<h2>ref_id</h2>b17<h2>title</h2>Evaluating collaborative filtering recommender systems<h2>journal</h2>ACM Transactions on Information Systems<h2>year</h2>2004<h2>authors</h2>J L Herlocker; J A Konstan; L G Terveen; J T Riedl<h2>ref_id</h2>b18<h2>title</h2>Collaborative Filtering for Implicit Feedback Datasets<h2>journal</h2>IEEE Computer Society<h2>year</h2>2008<h2>authors</h2>Y Hu; Y Koren; C Volinsky<h2>ref_id</h2>b19<h2>title</h2>A List of Recommender Systems and Resources<h2>journal</h2><h2>year</h2>2019<h2>authors</h2>G Jenson<h2>ref_id</h2>b20<h2>title</h2>Cross-sell: a fast promotion-tunable customer-item recommendation method based on conditionally independent probabilities<h2>journal</h2>ACM<h2>year</h2>2000<h2>authors</h2>B Kitts; D Freed; M Vrieze<h2>ref_id</h2>b21<h2>title</h2>A study of cross-validation and bootstrap for accuracy estimation and model selection<h2>journal</h2><h2>year</h2>1995<h2>authors</h2>R Kohavi<h2>ref_id</h2>b22<h2>title</h2>Glossary of Terms<h2>journal</h2>Machine Learning<h2>year</h2>1998<h2>authors</h2>R Kohavi; F Provost<h2>ref_id</h2>b23<h2>title</h2>Matrix Factorization Techniques for Recommender Systems<h2>journal</h2>Computer<h2>year</h2>2009<h2>authors</h2>Y Koren; R Bell; C Volinsky<h2>ref_id</h2>b24<h2>title</h2>Classification-based collaborative filtering using market basket data<h2>journal</h2>Expert Systems with Applications<h2>year</h2>2005<h2>authors</h2>J S Lee; C H Jun; J Lee; S Kim<h2>ref_id</h2>b25<h2>title</h2>Slope One Predictors for Online Rating-Based Collaborative Filtering<h2>journal</h2><h2>year</h2>2005<h2>authors</h2>D Lemire; A Maclachlan<h2>ref_id</h2>b26<h2>title</h2>Efficient Adaptive-Support Association Rule Mining for Recommender Systems<h2>journal</h2>Data Mining and Knowledge Discovery<h2>year</h2>2002<h2>authors</h2>W Lin; S A Alvarez; C Ruiz<h2>ref_id</h2>b27<h2>title</h2>Amazon.com Recommendations: Item-to-Item Collaborative Filtering<h2>journal</h2>IEEE Internet Computing<h2>year</h2>2003<h2>authors</h2>G Linden; B Smith; J York<h2>ref_id</h2>b28<h2>title</h2>What and who with: A social approach to double-sided recommendation<h2>journal</h2>International Journal of Human-Computer Studies<h2>year</h2>2017<h2>authors</h2>I Lombardi; F Vernero<h2>ref_id</h2>b29<h2>title</h2>Intelligent informationsharing systems<h2>journal</h2>Communications of the ACM<h2>year</h2>1987<h2>authors</h2>T W Malone; K R Grant; F A Turbak; S A Brobst; M D Cohen<h2>ref_id</h2>b30<h2>title</h2>An improved collaborative filtering approach for predicting cross-category purchases based on binary market basket data<h2>journal</h2>Journal of Retailing and Consumer Services<h2>year</h2>2003<h2>authors</h2>A Mild; T Reutterer<h2>ref_id</h2>b31<h2>title</h2>Effective Personalization Based on Association Rule Discovery from Web Usage Data<h2>journal</h2><h2>year</h2>2001<h2>authors</h2>B Mobasher; H Dai; T Luo; M Nakagawa<h2>ref_id</h2>b32<h2>title</h2>One-Class Collaborative Filtering<h2>journal</h2>IEEE Computer Society<h2>year</h2>2008<h2>authors</h2>R Pan; Y Zhou; B Cao; N N Liu; R Lukose; M Scholz; Q Yang<h2>ref_id</h2>b33<h2>title</h2>R: A Language and Environment for Statistical Computing<h2>journal</h2><h2>year</h2>2018<h2>authors</h2>Team Core<h2>ref_id</h2>b34<h2>title</h2>GroupLens: an open architecture for collaborative filtering of netnews<h2>journal</h2>ACM<h2>year</h2>1994<h2>authors</h2>P Resnick; N Iacovou; M Suchak; P Bergstrom; J Riedl<h2>ref_id</h2>b35<h2>title</h2>Introduction to Modern Information Retrieval<h2>journal</h2>McGraw-Hill<h2>year</h2>1983<h2>authors</h2>G Salton; M Mcgill<h2>ref_id</h2>b36<h2>title</h2>Analysis of recommendation algorithms for e-commerce<h2>journal</h2>ACM<h2>year</h2>2000<h2>authors</h2>B Sarwar; G Karypis; J Konstan; J Riedl<h2>ref_id</h2>b37<h2>title</h2>Item-based collaborative filtering recommendation algorithms<h2>journal</h2>ACM<h2>year</h2>2001<h2>authors</h2>B Sarwar; G Karypis; J Konstan; J Riedl<h2>ref_id</h2>b38<h2>title</h2>E-Commerce Recommendation Applications<h2>journal</h2>Data Mining and Knowledge Discovery<h2>year</h2>2001<h2>authors</h2>J B Schafer; J A Konstan; J Riedl<h2>ref_id</h2>b39<h2>title</h2>Social Information Filtering: Algorithms for Automating 'Word of Mouth<h2>journal</h2>ACM Press/Addison-Wesley Publishing Co<h2>year</h2>1995<h2>authors</h2>U Shardanand; P Maes<h2>ref_id</h2>b40<h2>title</h2>Information retrieval<h2>journal</h2><h2>year</h2>1979<h2>authors</h2>C Van Rijsbergen<h2>ref_id</h2>b41<h2>title</h2>Scalable Algorithms for Association Mining<h2>journal</h2>IEEE Transactions on Knowledge and Data Engineering<h2>year</h2>2000<h2>authors</h2>M J Zaki<h2>ref_id</h2>b42<h2>title</h2>Deep learning based recommender system: A survey and new perspectives<h2>journal</h2>ACM Computing Surveys (CSUR)<h2>year</h2>2019<h2>authors</h2>S Zhang; L Yao; A Sun; Y Tay<h1>figures</h1><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_2<h2>figure_caption</h2>call this type of data in the context of collaborative filtering analogous to similar situations for classifiers one-class data since only the 1-class is pure and contains only positive examples. The 0-class is a mixture of positive and negative examples.<h2>figure_data</h2><h2>figure_label</h2>3<h2>figure_type</h2>figure<h2>figure_id</h2>fig_3<h2>figure_caption</h2>Figure 3 :3Figure3: UML class diagram for package recommenderlab(Fowler 2004).<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>figure<h2>figure_id</h2>fig_4<h2>figure_caption</h2>NA NA NA 1 NA NA NA NA u4 2 2 1 NA NA 5 NA 0 2 NA u5 5 NA NA NA NA NA NA 5 NA 4<h2>figure_data</h2><h2>figure_label</h2>4<h2>figure_type</h2>figure<h2>figure_id</h2>fig_5<h2>figure_caption</h2>Figure 4 :4Figure 4: Image plot the artificial rating data before and after normalization.<h2>figure_data</h2><h2>figure_label</h2>5<h2>figure_type</h2>figure<h2>figure_id</h2>fig_6<h2>figure_caption</h2>FinallyFigure 5 :5Figure 5: Raw rating distribution for as sample of Jester.<h2>figure_data</h2><h2>figure_label</h2>67<h2>figure_type</h2>figure<h2>figure_id</h2>fig_7<h2>figure_caption</h2>Figure 6 :Figure 7 :67Figure 6: Histogram of normalized ratings using row centering (left) and Z-score normalization (right).<h2>figure_data</h2><h2>figure_label</h2>9<h2>figure_type</h2>figure<h2>figure_id</h2>fig_8<h2>figure_caption</h2>Figure 9 :9Figure 9: Precision-recall plot for method POPULAR.<h2>figure_data</h2><h2>figure_label</h2>10<h2>figure_type</h2>figure<h2>figure_id</h2>fig_9<h2>figure_caption</h2>Figure 10 :10Figure 10: Comparison of ROC curves for several recommender methods for the given-3 evaluation scheme.<h2>figure_data</h2><h2>figure_label</h2>11<h2>figure_type</h2>figure<h2>figure_id</h2>fig_10<h2>figure_caption</h2>Figure 11 :11Figure 11: Comparison of precision-recall curves for several recommender methods for the given-3 evaluation scheme.<h2>figure_data</h2><h2>figure_label</h2>12<h2>figure_type</h2>figure<h2>figure_id</h2>fig_11<h2>figure_caption</h2>Figure 12 :12Figure 12: Comparison of RMSE, MSE, and MAE for recommender methods for the given-3 evaluation scheme.<h2>figure_data</h2><h2>figure_label</h2>13<h2>figure_type</h2>figure<h2>figure_id</h2>fig_12<h2>figure_caption</h2>Figure 13 :13Figure 13: Comparison of ROC curves for several recommender methods for the given-3 evaluation scheme.<h2>figure_data</h2><h2>figure_label</h2>2<h2>figure_type</h2>table<h2>figure_id</h2>tab_1<h2>figure_caption</h2><h2>figure_data</h2>: 2x2 confusion matrixactual / predicted negative positivenegativeabpositivecd<h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_4<h2>figure_caption</h2>u1 FALSE FALSE FALSE TRUE FALSE TRUE FALSE TRUE FALSE FALSE u2 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE u3 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE u4 FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE u5 TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE<h2>figure_data</h2><h2>figure_label</h2><h2>figure_type</h2>table<h2>figure_id</h2>tab_10<h2>figure_caption</h2>For the first run we have 6 confusion matrices represented by rows, one for each of the six different top-N lists we used for evaluation. n is the number of recommendations per list. TP, FP, FN and TN are the entries for true positives, false positives, false negatives and true negatives in the confusion matrix. The remaining columns contain precomputed performance measures. The average for all runs can be obtained from the evaluation results directly using avg().<h2>figure_data</h2>200.4150.310TPR0.250.1310.050.100.15FPRFigure 8: ROC curve for recommender method POPULAR.R> avg(results)TPFPFNTN N precision recallTPRFPR nTP [1,] 0.453 0.547 18.12 77.88 97 FP FN TN N precision recall 0.4530 0.03428 0.03428 0.006685 1 TPR FPR n [1,] 0.464 0.536 19.25 76.75 97 [2,] 1.299 1.701 17.28 76.72 97 0.4330 0.09435 0.09435 0.020704 3 0.4640 0.03458 0.03458 0.00678 1 [2,] 1.372 1.628 18.34 75.66 97 [3,] 2.119 2.881 16.46 75.54 97 0.4238 0.14437 0.14437 0.034955 5 0.4573 0.08912 0.08912 0.01988 3 [3,] 2.228 2.772 17.49 74.51 97 [4,] 4.127 5.873 14.45 72.55 97 0.4127 0.27452 0.27452 0.071829 10 0.4456 0.13867 0.13867 0.03379 5 [4,] 4.352 5.648 15.36 71.64 97 [5,] 5.922 9.078 12.65 69.35 97 0.3948 0.38732 0.38732 0.111440 15 0.4352 0.28256 0.28256 0.06890 10 [5,] 6.272 8.728 13.44 68.56 97 0.4181 0.40248 0.40248 0.10828 15 [6,] 7.347 12.653 11.23 65.77 97 0.3673 0.46375 0.46375 0.155834 20[6,] 7.600 12.400 12.12 64.88 970.3800 0.45762 0.45762 0.15522 20<h1>formulas</h1><h2>formula_id</h2>formula_0<h2>formula_text</h2>sim Pearson ( x, y) = i∈I ( x i ¯ x)( y i ¯ y) (|I| -1) sd( x) sd( y) (1) and sim Cosine ( x, y) = x • y x y ,(2)<h2>formula_coordinates</h2>[5.0, 81.0, 304.22, 441.0, 81.23]<h2>formula_id</h2>formula_1<h2>formula_text</h2>raj = 1 |N (a)| i∈N (a) r ij (3)<h2>formula_coordinates</h2>[5.0, 249.41, 512.75, 272.59, 29.32]<h2>formula_id</h2>formula_2<h2>formula_text</h2>i 1 i 2 i 3 i 4 i 5 i 6 i 7 i 8 u 1 u 2 u 3 u 4 u 5 u 6 u a r̂a (a) (b) 0.3 1.0 0.2 0.3 0.1 0.1 u a u 1 u 2 u 3 u 4 u 5 u 6 0.3 1.0 0.2 0.3 0.1 0.1 u a u 1 u 2 u 3 u 4 u 5 u 6 (c)<h2>formula_coordinates</h2>[6.0, 127.11, 123.41, 274.32, 145.51]<h2>formula_id</h2>formula_3<h2>formula_text</h2>s ai r ij (4)<h2>formula_coordinates</h2>[6.0, 346.74, 397.99, 175.26, 10.7]<h2>formula_id</h2>formula_4<h2>formula_text</h2>h(r ui ) = r ui -ru ,<h2>formula_coordinates</h2>[6.0, 260.29, 547.94, 82.41, 10.7]<h2>formula_id</h2>formula_5<h2>formula_text</h2>r jk = 1 user u j is known to have a preference for item i k 0 otherwise.(6)<h2>formula_coordinates</h2>[8.0, 158.08, 504.14, 363.92, 25.89]<h2>formula_id</h2>formula_6<h2>formula_text</h2>sim Jaccard (X , Y) = |X ∩ Y| |X ∪ Y| , (7<h2>formula_coordinates</h2>[8.0, 235.56, 723.52, 281.8, 24.43]<h2>formula_id</h2>formula_7<h2>formula_text</h2>)<h2>formula_coordinates</h2>[8.0, 517.35, 730.84, 4.65, 9.63]<h2>formula_id</h2>formula_8<h2>formula_text</h2>support(X → Y) = support(X ∪ Y) = Freq(X ∪ Y)/|D| = P (E X ∩ E Y ) confidence(X → Y) = support(X ∪ Y)/support(X ) = P (E Y |E X )<h2>formula_coordinates</h2>[9.0, 132.58, 348.24, 337.85, 38.41]<h2>formula_id</h2>formula_9<h2>formula_text</h2>MAE = 1 |K| (i,j)∈K |r ij -rij |,(8)<h2>formula_coordinates</h2>[10.0, 233.83, 717.8, 288.17, 29.32]<h2>formula_id</h2>formula_10<h2>formula_text</h2>RMSE = (i,j)∈K (r ij -rij ) 2 |K| (9)<h2>formula_coordinates</h2>[11.0, 227.4, 262.97, 294.6, 27.11]<h2>formula_id</h2>formula_11<h2>formula_text</h2>= a + d a + b + c + d (10)<h2>formula_coordinates</h2>[11.0, 369.65, 577.91, 152.35, 24.5]<h2>formula_id</h2>formula_12<h2>formula_text</h2>MAE = 1 N N i=1 | i | = b + c a + b + c + d , (11<h2>formula_coordinates</h2>[11.0, 218.55, 641.16, 298.6, 31.85]<h2>formula_id</h2>formula_13<h2>formula_text</h2>)<h2>formula_coordinates</h2>[11.0, 517.15, 651.78, 4.85, 9.63]<h2>formula_id</h2>formula_14<h2>formula_text</h2>Precision = correctly recommended items total recommended items = d b + d (12) Recall = correctly recommended items total useful recommendations = d c + d (13)<h2>formula_coordinates</h2>[12.0, 181.77, 193.48, 340.23, 66.92]<h2>formula_id</h2>formula_15<h2>formula_text</h2>E-measure = 1 α(1/Precision) + (1 -α)(1/Recall)(14)<h2>formula_coordinates</h2>[12.0, 184.54, 363.48, 337.46, 24.5]<h2>formula_id</h2>formula_16<h2>formula_text</h2>F-measure = 2 Precision Recall Precision + Recall = 2 1/Precision + 1/Recall (15)<h2>formula_coordinates</h2>[12.0, 161.93, 437.64, 360.07, 24.5]<h2>formula_id</h2>formula_17<h2>formula_text</h2>$ 0 i2 i3 i4 i6 i8 2 3 5 5 4 $ 1 i1 i9 i10 2 2 3 $ 2 i1 i6 2 1 $ 3 i1 i2 i3 i6 i8 i9 2.000e+00 2.<h2>formula_coordinates</h2>[16.0, 81.0, 111.64, 372.27, 200.6]<h2>formula_id</h2>formula_18<h2>formula_text</h2>5<h2>formula_coordinates</h2>[17.0, 81.0, 393.61, 5.73, 10.91]<h2>formula_id</h2>formula_19<h2>formula_text</h2>i1 i2 i3 i4 i5 i6 i7 i8 i9 i10<h2>formula_coordinates</h2>[18.0, 115.36, 207.96, 320.73, 10.91]<h2>formula_id</h2>formula_20<h2>formula_text</h2>$<h2>formula_coordinates</h2>[21.0, 81.0, 572.31, 5.73, 10.91]<h2>formula_id</h2>formula_21<h2>formula_text</h2>RMSE MSE MAE UBCF<h2>formula_coordinates</h2>[26.0, 81.0, 615.94, 126.0, 24.46]<h2>formula_id</h2>formula_22<h2>formula_text</h2>POPULAR<h2>formula_coordinates</h2>[27.0, 81.0, 320.3, 40.09, 10.91]<h2>formula_id</h2>formula_23<h2>formula_text</h2>RANDOM<h2>formula_coordinates</h2>[30.0, 81.0, 384.02, 34.36, 10.91]<h2>formula_id</h2>formula_24<h2>formula_text</h2>RANDOM<h2>formula_coordinates</h2>[31.0, 81.0, 612.96, 34.36, 10.91]<h2>formula_id</h2>formula_25<h2>formula_text</h2>RANDOM<h2>formula_coordinates</h2>[36.0, 81.0, 155.75, 34.36, 10.91]<h1>doi</h1>10.1145/138859.138867