<h1>abstract</h1> <p>The prevalence of tree-like structures, encompassing hierarchical structures and power law distributions, exists extensively in realworld applications, including recommendation systems, ecosystems, financial networks, social networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness modeling has garnered considerable attention owing to its exponential growth volume. Compared to the flat Euclidean space, the curved hyperbolic space provides a more amenable and embeddable room, especially for datasets exhibiting implicit tree-like architectures. However, the intricate nature of real-world tree-like data presents a considerable challenge, as it frequently displays a heterogeneous composition of tree-like, flat, and circular regions. The direct embedding of such heterogeneous structures into a homogeneous embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions. To mitigate the aforementioned shortage, this study endeavors to explore the curvature between discrete structure and continuous learning space, aiming at encoding the message conveyed by the network topology in the learning process, thereby improving tree-likeness modeling. To the end, a curvature-aware hyperbolic graph convolutional neural network, 𝜅HGCN, is proposed, which utilizes the curvature to guide message passing and improve long-range propagation. Extensive experiments on node classification and link prediction tasks verify the superiority of the proposal as it consistently outperforms various competitive models by a large margin. </p><h1>sections</h1><h2>heading</h2> <p>INTRODUCTION </p><h2>text</h2> <p>Tree-like structures refer to networks, systems, or data organizations that resemble a tree in their architecture, with nodes branching out from the root into multiple levels. They are widely observed in various real-world domains [1,3,44,53,64,101], such as recommendation systems, financial networks, and social networks.
Recently, the utilization of hyperbolic space for modeling treelike structures has garnered substantial attention [11,24,45,56,57,61,65,90,96]. Compared with the zero curvature Euclidean space, one key property of negative curvature hyperbolic space is that it expand exponentially, making it can be considered as a continuous tree and vice versa (as shown in Figure 1). In other words, hyperbolic space allows for the efficient representation of a tree-like structure, as it enables nodes to be spread apart as they move away from the root, preventing crowding and overlapping of nodes as is commonly observed in Euclidean space. Additionally, hyperbolic space allows for exponential growth in the number of nodes in a given area, which is well-suited for modeling the exponential growth of trees.
However, the real-world dataset often deviates from a pure tree configuration and manifests in a labyrinthine complexity, posing large challenges for tree-likeness modeling within hyperbolic space [82,103]. For instance, biological taxonomies, which depict the hierarchical relationships between different species from a for 𝑇 2 . By incorporating curvature 𝜅 into the neighboring aggregation, the detection of local structures is improved. This is achieved by assigning asymmetric weights (𝜅 1 𝑎,𝑜 vs 𝜅 1  𝑎,𝑏 ) to nodes at different levels and results in larger values for nodes in a circular shape (𝜅 1  𝑎,𝑜 vs 𝜅 2 𝑎,𝑜 and 𝜅 1 𝑎,𝑐 vs 𝜅 2 𝑎,𝑐 ).
global view, often exhibit both extensive local flat regions where multiple species are closely related and local circular regions where species connections are less defined. The local structure of a treelike graph exhibits a heterogeneous blend of tree-like, flat, and circular patterns, resulting in difficulties in uniformly embedding the data into a homogeneous embedding space and thereby engendering structural biases and distortions.
To mitigate the limitations, this study seeks to examine the intersection between the discrete structure of the data and the continuous learning space. The aim is to encode the information inherent in the network topology in a manner that is both effective and imbued with structural inductive bias, thereby enhancing the performance of downstream tasks. From a geometric perspective, the quality of the embedding in geometric learning depends on the compatibility between the intrinsic graph structure and the embedding space [26]. In light of this principle, we employs the concept of curvature to guide tree-likeness modeling in hyperbolic space. As shown in Figure 2, the incorporation of curvature information offers a more comprehensive grasp of the local shape characteristics, facilitating the representation of the shape and contours of diverse regions within the learning space.
In Riemannian geometry, curvature measures the deviation of a geometric object from being flat, originally defined on continuous smooth manifolds [40]. Smooth manifolds with positive, zero, or negative curvature are spherical, Euclidean, and hyperbolic spaces, respectively. This concept has been extended to discrete objects such as graphs, where curvature describes the deviation of a local pair of neighborhoods from a "flat" case.
Graph curvature, analogous to curvature in the realm of continuous geometry, consists of Gaussian curvature, Ricci curvature, and mean curvature. These components have unique roles: Gaussian curvature quantifies the local curvature at vertices, Ricci curvature allocates curvature to the edges, and mean curvature provides an overall metric for the entire graph. In this work, we focus on Ricci curvature for graph convolution and edge-based filtering. Several definitions have been proposed about Ricci curvature, including Ollivier Ricci curvature [58], Forman Ricci curvature [21], Balanced Forman curvature [74]. Ricci curvature controls the overlap between two distance balls by considering the radii of the balls and the distance between their centers [33]. Furthermore, the lower bound of Ricci curvature can reveal valuable global geometric and topological information [4]. It is also an effective indicator of treelike, flat, and cyclic areas, making it well-suited for integration into hyperbolic space to capture asymmetries, biases, and hierarchies.
Overall, we put forward a novel framework: curvature-aware hyperbolic graph convolutional neural network (𝜅HGCN) for effectively modeling tree-like datasets with complex structures. Specifically, 𝜅HGCN leverages the discrete Ricci curvature to guide message passing and dynamically adapts the global continuous hyperbolic curvature. Through empirical evaluations on diverse datasets and tasks, we confirm the superiority of the 𝜅HGCN, as it consistently outperforms existing baselines by substantial margins. The major contributions of this work are summarized as follows:
• We design a novel hyperbolic geometric learning framework that encapsulates the graph Ricci curvature into the continuous embedding space, producing less distortion, powerful expressiveness, and topology-aware embeddings; • We present a new message technique for hyperbolic graph embedding, and we further prove that it produces a smaller (larger) embedding distance when larger (smaller) curvature is involved, which well handles the inconsistency between the local structure and global curvature of embedding space; • Extensive experiments demonstrate that the proposed model 𝜅HGCN achieves significant improvements over various baselines on link prediction and node classification tasks. </p><h2>publication_ref</h2> <p>['b0', 'b2', 'b43', 'b52', 'b63', 'b100', 'b10', 'b23', 'b44', 'b55', 'b56', 'b60', 'b64', 'b89', 'b95', 'b81', 'b102', 'b25', 'b39', 'b57', 'b20', 'b73', 'b32', 'b3'] </p><h2>figure_ref</h2> <p>['fig_0', 'fig_1'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>RELATED WORK </p><h2>text</h2> <p>For tree-likeness modeling, we mainly review the latest research techniques including graph neural networks and hyperbolic geometry. In addition to this, we also review the recent advancements in curvature and curvature-based learning. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Graph Neural Networks </p><h2>text</h2> <p>Tree-structured data can often be represented as graphs. In recent years, graph neural networks (GNNs) have gained significant attention within the graph learning community. The main concept behind GNNs is a message-passing mechanism that aggregates information from neighboring nodes. GNNs have demonstrated remarkable performance in various tasks, including node classification, link prediction, graph classifications, and graph reconstruction [23,25,28,35,41,42,48,52,67,69,78,88,95,[98][99][100]. They have also found wide applications in recommender systems, anomaly detection, social networks analysis, and more [12-16, 20, 35, 49, 66, 68, 69, 92]. The majority of GNNs learn graphs in Euclidean space due to their computational advantages and intuitiveness. However, Euclidean models are limited in their ability to represent complex patterns in graph [9]. </p><h2>publication_ref</h2> <p>['b22', 'b24', 'b27', 'b34', 'b40', 'b41', 'b47', 'b51', 'b66', 'b68', 'b77', 'b87', 'b94', 'b97', 'b98', 'b99', 'b8'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Hyperbolic Geometry </p><h2>text</h2> <p>Hyperbolic representation learning has recently garnered considerable attention [17,102]. Hyperbolic geometry has been recognized as a continuous tree [37], exhibiting properties such as low distortion and small generalization errors when modeling treelike structured data [63,72,73]. Its applications span various domains [50,61,90], including computer vision [5,30,34], natural language processing [8,10,36,51,56,57,62], recommender systems [14,70,80,81,87,91], graph learning [7,11,27,43,45,89,96] and more [84]. In the graph learning domain, recent works [11,43,45,47,89,92,96,97] have generalized graph neural networks to hyperbolic space and demonstrated impressive performance, particularly on tree-like data. Some studies [60,71,82,103] propose learning graph in different embedding spaces or product spaces. Furthermore, researchers have also explored the use of ultrahyperbolic geometry for graph learning [38,39,85,86]. However, many existing methods fail to consider the local heterogeneous structure of graphs, resulting in significant distortion and low-quality embeddings. </p><h2>publication_ref</h2> <p>['b16', 'b101', 'b36', 'b62', 'b71', 'b72', 'b49', 'b60', 'b89', 'b4', 'b29', 'b33', 'b7', 'b9', 'b35', 'b50', 'b55', 'b56', 'b61', 'b13', 'b69', 'b79', 'b80', 'b86', 'b90', 'b6', 'b10', 'b26', 'b42', 'b44', 'b88', 'b95', 'b83', 'b10', 'b42', 'b44', 'b46', 'b88', 'b91', 'b95', 'b96', 'b59', 'b70', 'b81', 'b102', 'b37', 'b38', 'b84', 'b85'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Graph Curvature </p><h2>text</h2> <p>Graph curvature, resembling curvature in continuous geometry, includes Gaussian curvature, Ricci curvature, and average curvature. Each of these elements serves a distinct purpose: Gaussian curvature measures local curvature at vertices, Ricci curvature assigns curvature to edges, and average curvature offers a global measure for the entire graph. Applications of graph curvatures span various domains in network alignment, congestion and vulnerability detection, community detection, and robustness analysis [32,54,55].
The recent work of curvature graph neural network (CurvGN) [93] introduced the notion of Ricci curvature into the field of graph learning. The study in [74] showed that edges with negative curvature can contribute to the over-squashing problem in graph embeddings. Coinciding with the announcement of the accepted papers for WWW 2023, we noted a parallel work by Fu et al. [22] that introduces the idea of class-aware Ricci curvature for addressing hierarchy-imbalance in node classification, while in our work, we aim to explore the integration of more generalized ricci curvature with hyperbolic graph convolution and curvature-based filtering mechanism to enhance the performance of HGCN for a more range of tasks, including node classification and link prediction. </p><h2>publication_ref</h2> <p>['b31', 'b53', 'b54', 'b92', 'b73', 'b21'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>BACKGROUND </p><h2>text</h2> <p>In this part, we first briefly review the necessary definitions of differential geometry, primarily concentrating on hyperbolic geometry. A thorough and in-depth explanation can be found in [40]. We also give a short introduction about Ollivier Ricci curvature (ORC), which is a generalized Ricci curvature tailored for discrete objects (e.g., graphs) 1 . The readers may refer to [58] for more details. </p><h2>publication_ref</h2> <p>['b39', 'b57'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Riemannian Geometry </p><h2>text</h2> <p>Manifold and Tangent Space. Riemannian geometry, a subfield of differential geometry, denoted as M with a Riemannian metric 𝑔. An 𝑛-dimensional manifold (M, 𝑔) represents a smooth and real space, essentially an extension of a 2-D surface to higher dimensions, that can be locally approximated by R 𝑛 . For any point x on M, we define a tangent space T x M, acting as the first-order approximation of M in the vicinity of x. This tangent space is an 𝑛-dimensional vector space that is isomorphic to R 𝑛 . The metric 𝑔 on the Riemannian 
𝐿(𝛾) = ∫ 𝛽 𝛼 ∥𝛾 ′ (𝑡)∥ 𝑔 𝑑𝑡. Then the distance of u, v ∈ M, 𝑑 M (u, v) = inf 𝐿(𝛾) where 𝛾 is a curve that 𝛾 (𝑎) = u, 𝛾 (𝑏) = v.
Maps and Parallel Transport. The maps define the relationship between the hyperbolic space and the corresponding tangent space. Given a point x ∈ M and a vector v ∈ T x M, a unique geodesic 𝛾 : [0, 1] → M exists, satisfying 𝛾 (0) = x, 𝛾 ′ (0) = v. The exponential map, symbolized as exp x : T x M → M, is defined such that exp x (v) = 𝛾 (1). Conversely, the logarithmic map, denoted as log x , acts as the inverse of exp x . Furthermore, the parallel transport 𝑃𝑇 x→y : T xM → T yM achieves the transportation from point x to point y, while ensuring the preservation of the metric tensors.
Hyperbolic Models. Hyperbolic geometry describes a curved space with negative curvature. There are several mathematically equivalent ways to model hyperbolic geometry that emphasize different properties, but our methods apply to hyperbolic geometry in general and are not limited to any particular model. Formulas for concepts such as distance, maps, and parallel transport are summarized in Appendix B. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Graph Curvature </p><h2>text</h2> <p>Curvature is a fundamental concept in smooth spaces that has also generalized to discrete objects (e.g., graphs). There are several distinct notions of discrete Ricci curvature for graphs or networks, such as the Forman-Ricci curvature [21] and Ollivier-Ricci curvature [58]. Here we mainly focus on ORC since it is more geometrical [33,58]. Another reason is ORC builds a bridge between continuous geometry and discrete structures [2,76]. Definition 3.1 (Ollivier-Ricci Curvature). Let 𝐺 = (𝑉 , 𝐸) be a locally finite, connected, and simple graph (i.e., 𝐺 contains no multiple edges or self-loops), for any two distinct vertices 𝑣 1 , 𝑣 2 , the ORC of 𝑣 1 and 𝑣 2 is defined as
𝜅 (𝑣 1 , 𝑣 2 ) = 1 - 𝑊 (𝑚 𝑣 1 , 𝑚 𝑣 2 ) 𝑑 (𝑣 1 , 𝑣 2 ) ∈ (-2, 1),(1)
where 𝑑 (𝑣 
𝑊 (𝑚 1 , 𝑚 2 ) = inf 𝜋 𝑖,𝑗 ∈Π ∑︁ 𝑣 𝑖 ,𝑣 𝑗 ∈𝑉 𝜋 𝑖,𝑗 (𝑣 𝑖 , 𝑣 𝑗 )𝑑 (𝑣 𝑖 , 𝑣 𝑗 ),(2)
where 𝜋 𝑖,𝑗 : 𝑉 × 𝑉 → [0, 1] is a transport plan, i.e., the probability measure of the amount of mass transferred from 𝑣 𝑖 to 𝑣 𝑗 . Then to seek an optimal transference plan (𝜋) that is to minimize the total cost of moving from 𝑣 𝑖 to 𝑣 𝑗 such that for every 𝑣 𝑖 , 𝑣 𝑗 in 𝑉 satisfying
∑︁ 𝑣 𝑖 ∈𝑉 𝜋 𝑖,𝑗 (𝑣 𝑖 , 𝑣 𝑗 ) = 𝑚 1 ; ∑︁ 𝑣 𝑗 ∈𝑉 𝜋 𝑖,𝑗 (𝑣 𝑖 , 𝑣 𝑗 ) = 𝑚 2 . (3
)
𝑇 ℎ ′ ℍ 𝑑 ′ ,𝐾 ℍ 𝑑,𝐾 x 𝐻 ∈ ℝ 𝑛×𝑑 h 𝐻 ∈ ℝ 𝑛×𝑑 ′ ℍ 𝑑 ′ ,𝐾 x 𝐸 ∈ ℝ 𝑛×𝑑 𝒛 ∈ 𝐻 𝑛x𝑐 x 𝐻 𝑇 𝑜 ℍ 𝑑,𝐾 AGG 𝜅 (h 𝑖 𝐻 ) h 𝑖 𝐻 log h ′ 𝐾 (⋅) exp h ′ 𝐾 (⋅) ෨ h 𝑖 𝐻 ℍ 𝑑 ′ ,𝐾 𝑑 ′ 𝑑 ′ 𝑑 ′ 2𝑑 ′ , 1 𝜅 𝑖𝑗 𝛼 𝑖𝑗 h 𝑖 𝐻 h 𝑗 𝐻 h 𝑖 𝐻 h 𝑗 𝐻 ℍ Figure 3: Schematic of 𝜅HGCN. (1)
The simplified algorithm flow of our method: consists of hyperbolic projection, feature transform, and aggregation. After that, a readout layer is applied to the embeddings for either a node classification or link prediction task. ( 2) The visualization of neighborhood aggregation procedure: first project information to hyperbolic space for transformation, then map messages to the tangent space, perform the aggregation in the tangent space with the guide of discrete curvature (and attention), and then map back to the hyperbolic space. (3) The details of Ricci Curvature-aware aggregation and its combination with feature-based attention.
Definition 3.3 (Probability Measure). Given 𝐺 = (𝑉 , 𝐸), for a vertex 𝑣 𝑖 in 𝑉 , denote 𝑑 𝑣 𝑖 the degree of 𝑣 𝑖 and 𝑁 (𝑣) the neighbors of 𝑣, for any 𝑝 ∈ [0, 1], the probability measure 𝑚 𝑣 𝑖 on 𝑉 is defined as:
𝑚 𝑣 𝑖 =        𝑝, if 𝑣 = 𝑣 𝑖 1-𝑝 𝑑 𝑣 , if (𝑣 𝑖 ) ∈ 𝑁 (𝑣). 0, otherwise(4)
Geometric Intuition. ORC seeks the most efficient transportation plan that preserves mass between two probability measures, which may be solved using linear programming. Intuitively, transporting messages between two nodes whose neighbors are highly overlapping, such as two nodes in the same cluster, is costless. On the other hand, if two nodes are situated in distinct groups or clusters, information flow between them is difficult. </p><h2>publication_ref</h2> <p>['b20', 'b57', 'b32', 'b57', 'b1', 'b75'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>METHODOLOGY </p><h2>text</h2> <p>The proposed method, 𝜅HGCN, combines discrete and continuous curvatures to improve tree-like modeling in hyperbolic space. Our approach emphasizes the strengthening of message passing in nodes with high local graph curvature and the weakening of message propagation in nodes with low local curvature. This curvatureguided approach enhances the formation of hierarchies and reduces the impact of structural incompatibility on the modeling process. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>𝜅HGCN </p><h2>text</h2> <p>Our approach, named 𝜅HGCN, presents a novel curvature-aware hyperbolic graph network model, as depicted in Figure 3. Building upon the foundation of HGCN [11], we implement graph convolution operations via the tangential method [11,45] space. However, it is worth mentioning that 𝜅HGCN is flexible and can be applied to non-tangential methods as well [97]. Similar to other GNN and HGNN models, 𝜅HGCN also comprises three fundamental modules: hyperbolic feature transformation, curvature-aware neighbor aggregation, and non-linear activation. </p><h2>publication_ref</h2> <p>['b10', 'b10', 'b44', 'b96'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Hyperbolic Feature Transformation. Hyperbolic feature transformation is formulated as: </p><h2>text</h2> <p>h ℓ,H 𝑖 = W ℓ ⊗ 𝜅 ℓ -1 x ℓ -1,H 𝑖 ⊕ 𝜅 ℓ -1 b ℓ ,(5)
where ℓ denotes the ℓ-th layer, W is the trainable matrix and b is the bias.
W ⊗ 𝜅 x := exp 𝜅 o (W log 𝜅 o (x)) and x ⊕ 𝜅 b := exp 𝜅
x (𝑃𝑇 𝜅 o→x (b)) are matrix-vector multiplication and bias translation operations in hyperbolic space, respectively. The superscript H denotes the hyperbolic feature. space of the origin:
hℓ,H 𝑖 = exp 𝜅 𝑙 -1 o ∑︁ 𝑗 ∈ N 𝑖 κ𝑖,𝑗 • log 𝜅 𝑙 -1 o (h ℓ,H 𝑗 ) .(6)
Here κ𝑖,𝑗 denotes the curvature for capturing the local structure, computed by softmax operation within the neighbors N 𝑖 (N 𝑖 contains the node self):
κ𝑖,𝑗 = softmax 𝑗 ∈ N (𝑖 ) MLP(𝜅 𝑖,𝑗 ) ,(7)
where 𝜅 𝑖,𝑗 is the raw ORC value, and MLP (Multilayer Perceptron) is employed to make the curvature more adaptive to the overall negative curvature. This approach is referred to as Curv. When it comes to the case where the topology information and node features are inconsistent to a certain degree, e.g., the network is quite sparse or depends more on the node features, inspired by [94], we propose a feature-attention enhanced aggregation (CurvAtt), which encodes node state into the curvature:
κ′ 𝑖,𝑗 = 𝑤 𝜅 κ𝑖,𝑗 + 𝑤 𝛼 𝛼 𝑖,𝑗 𝑤 𝜅 + 𝑤 𝛼 ,(8)
where 𝛼 𝑖,𝑗 is hyperbolic feature attention, 𝑤 𝜅 and 𝑤 𝛼 are the trainable parameters that adjust structure information and feature correlation with the initial value 1.0. The hyperbolic feature attention 𝛼 𝑖,𝑗 is defined as:
𝛼 𝑖,𝑗 = softmax 𝑗 ∈ N 𝑖 MLP(log 𝜅 o (h H 𝑖 )∥ log 𝜅 o (h H 𝑗 )) .(9)
4.1.3 Hyperbolic Non-linear Activation. After that, we apply a nonlinear activation:
x ℓ,H 𝑖 = 𝜎 𝜅 ℓ -1 ,𝜅 ℓ ( hℓ,H ) = exp 𝜅 ℓ o 𝜎 (log 𝜅 ℓ -1 o ( hℓ,H )) . (10
)
Geometric Intuition. The real-world tree-like graphs with heterogeneous local structures are inevitably distorted if we directly embed them into a homogeneous manifold. For instance, the embedding of quasi-cycle graphs such as 𝑛 × 𝑛 square lattices (zero curvature) and 𝑛-node cycles (positive curvature) incur at least a multiplicative distortion of 𝑂 (𝑛/log 𝑛) in hyperbolic space [79]. Graph Ricci curvature is able to mitigate this distortion. The geometric intuition is that the more positive the curvature is, the more two distance balls centered at nearby points overlap, and therefore, the cheaper it is to transport the mass from one to the other. Theoretical results show that with the increasing number of triangles involved in the linked pair (𝑖, 𝑗), the lower bound of curvature will be increased [33], as stated in Theorem 4.1. It is easy to understand because when the two vertices share many triangles, then the transportation distance should be smaller, and the curvature, therefore, is correspondingly larger. Theorem 4.1 (Lower bound of ORC [33]). On a locally finite graph, for any pair of neighboring vertices i, j, let #(𝑖, 𝑗) := number of triangles which include 𝑖, 𝑗 as vertices for 𝑖 ∼ 𝑗. Then, we have the inequality, saying that
𝜅 𝑖,𝑗 ≥ -1 - 1 𝑑 𝑖 - 1 𝑑 𝑗 - #(𝑖, 𝑗) 𝑑 𝑖 ∧ 𝑑 𝑗 + -1 - 1 𝑑 𝑖 - 1 𝑑 𝑗 - #(𝑖, 𝑗) 𝑑 𝑖 ∨ 𝑑 𝑗 + + #(𝑖, 𝑗) 𝑑 𝑖 ∨ 𝑑 𝑗 , (11
)
where 𝑠 + := max(𝑠, 0), 𝑠 ∨ 𝑡 := max(𝑠, 𝑡), 𝑎𝑛𝑑 𝑠 ∧ 𝑡 := min(𝑠, 𝑡).
In this study, we make a theoretical analysis in Theorem 4.2, which further demonstrates the relations of ORC and embedding distance, i.e., when a large curvature is involved within the linked node, the closer of their embedding distance, which thus mitigates the distortion. Theorem 4.2 (Embedding Distance w.r.t ORC). Let (𝑖, 𝑗) ∈ 𝐸 be the linked pair, h 𝑖 , h 𝑗 ∈ R 𝑑 be the node state in the tangent space, 𝑑 𝑖 be the degree of node 𝑖, and 𝐷 be the distance of node 𝑖 and node 𝑗 in the tangent space, that is
𝐷 = ∥h 𝑖 -h 𝑗 ∥,(12)
where ∥ • ∥ is the Euclidean norm. Define a large ORC as κ𝑖,𝑗 > max1/𝑑 𝑖 , 1/𝑑 𝑗  and a small ORC as κ𝑖,𝑗 < min1/𝑑 𝑖 , 1/𝑑 𝑗 . Then, when the large ORC is involved, their embedding distance will get smaller if using 𝜅HGCN. On the contrary, when the small ORC is involved, their embedding distance will get larger.
Proof. In the following, we use 𝐷 𝑙 and 𝐷 𝑠 to denote the distance when large and small curvature κ are involved, respectively. The main idea is that when there is a large curvature involved, the node distance will be decreased compared with the original case (degreebased aggregation), that is 𝐷 𝑙 < 𝐷. At the same time, when there is a small curvature involved, the node distance will increase, that is 𝐷 𝑠 > 𝐷.
(1) When a large curvature (i.e., 𝜅 𝑖,𝑗 > max(1/𝑑 𝑖 , 1/𝑑 𝑗 )) is involved, more messages will be transferred, and we decompose the embedding, taking h 𝑖 as an example, into two components: one is from original h 𝑖 and another is the incremental parts from h 𝑗 , then
𝐷 𝑙 = ∥(h 𝑖 + 𝛼 𝑖 h 𝑗 ) -(h 𝑗 + 𝛼 𝑗 h 𝑖 )∥ = ∥(h 𝑖 -𝛼 𝑗 h 𝑖 ) -(h 𝑗 -𝛼 𝑖 h 𝑗 )∥,(13)
where 𝛼 𝑖 (𝛼 𝑗 ) is the difference between κ𝑖,𝑗 and 1/𝑑 𝑖 ( 1/𝑑 𝑗 ), i.e., 𝛼 𝑖 = κ𝑖,𝑗 -1/𝑑 𝑖 , 𝛼 𝑗 = κ𝑖,𝑗 -1/𝑑 𝑗 . Since κ𝑖,𝑗 > max(1/𝑑 𝑖 , 1/𝑑 𝑗 ), 𝛼 𝑖 and 𝛼 𝑗 are both positive. Let 𝛼 𝑖 𝑗 = 𝛼 𝑖 ≈ 𝛼 𝑗 , then
𝐷 𝑙 ≈ ∥(h 𝑖 -𝛼 𝑖 𝑗 h 𝑖 ) -(h 𝑗 -𝛼 𝑖 𝑗 h 𝑗 )∥ = (1 -𝛼 𝑖 𝑗 )∥h 𝑖 -h 𝑗 ∥ < 𝐷.(14)
Then, we easily know that when large curvature is involved, the distance will be reduced and two nodes will be closer to each other. What's more, the larger the curvature, the closer the nodes are.
(2) Similarly, when a small curvature (𝜅 𝑖,𝑗 < min(1/𝑑 𝑖 , 1/𝑑 𝑗 )) is involved, fewer messages will be transferred, and we decompose the embedding, taking h 𝑖 as an example, into two components: one is from original h 𝑖 and another is the reduction parts of h 𝑗 , that is
𝐷 𝑠 = ∥(h 𝑖 -𝛽 𝑖 h 𝑗 ) -(h 𝑗 -𝛽 𝑗 h 𝑖 )∥ ≈ (1 + 𝛽 𝑖 𝑗 )∥h 𝑖 -h 𝑗 ∥ > 𝐷,(15)
where 𝛽 𝑖 is the difference between 1/𝑑 𝑖 and κ𝑖,𝑗 , i.e., 𝛽 𝑖 = 1/𝑑 𝑖 -κ𝑖,𝑗 , 𝛽 𝑗 = 1/𝑑 𝑗 -κ𝑖,𝑗 . Both 𝛽 𝑖 and 𝛽 𝑗 are positive in that 𝜅 𝑖,𝑗 < min(1/𝑑 𝑖 , 1/𝑑 𝑗 ). Let 𝛽 𝑖 𝑗 = 𝛽 𝑖 ≈ 𝛽 𝑗 , then, we easily know that when small curvature is involved, the node pair will become more distant in the embedding space. What's more, the smaller the curvature, the more distant the nodes are. □ </p><h2>publication_ref</h2> <p>['b93', 'b78', 'b32', 'b32'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Curvature-based Homophily Constraint </p><h2>text</h2> <p>In the degree-based learning paradigm, like GCN [35], the influence of a node on another node decays exponentially as their graph distance increases as shown by [31]. The analysis in [31] is limited degree-based aggregation and Euclidean space. The hyperbolic message passing learning paradigm of 𝜅HGCN also shows a similar phenomenon, which causes too much influence loss in long-term propagation. Especially, if the paths consist of numerous connections to other nodes, the node influence is minimal. For clarity, we term the hyperbolic message passing learning paradigm in 𝜅HGCN or original HGNNs [11,45,96] as HMP. The HMP is a local aggregation method, in which the influence of nodes decreases with increasing distance, as demonstrated in Theorem 4.3. </p><h2>publication_ref</h2> <p>['b34', 'b30', 'b30', 'b10', 'b44', 'b95'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Theorem 4.3 (Decaying property of HMP). </p><h2>text</h2> <p>Let 𝑝 be a path between node 𝑢 and node 𝑣, 𝑑 * 𝑔 be the shortest distance between 𝑢 and 𝑣, let 𝐶 be a constant and z be the embedding on the tangent space. Consider the node influence 𝐼 𝑢,𝑣 (𝐼 𝑢,𝑣 = ∥𝜕z 𝑢 /𝜕z 𝑣 ∥) from 𝑣 to 𝑢 using HMP, 𝐼 𝑢,𝑣 ≤ 𝐶𝛾 𝑑 * 𝑔 (0 < 𝛾 <= 1). The condition for equality is 𝑑 * 𝑔 = 1, and 𝑣 is the unique neighbors of node 𝑢, correspondingly 𝐶 = 1, 𝛾 = 1.
Proof. Recall the aggregation rule in Equations ( 6) and ( 7) (similar to that in origin HGNNs), we focus on the aggregation in the tangent space and ignore the previous logarithmic map and the later exponential map since they are applied before and after the whole aggregation process, respectively. Then for any node 𝑢 and 𝑣, the update rule in the tangent space can be formulated as:
z 𝑢 = ∑︁ 𝑗 ∈ N (𝑢 ) κ𝑢,𝑗 z 𝑗 = 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑖 ) exp( κ𝑢,𝑗 )z 𝑗 ,(16)
where 𝐾 𝑢𝑢 = 𝑗 ∈ N (𝑢 ) exp( κ𝑢,𝑗 ). 2 By an expansion of node in the neighbor N ( 𝑗), we have:
z 𝑢 = 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑢 ) exp( κ𝑢,𝑗 ) * 1 𝐾 𝑗 𝑗 ∑︁ 𝑘 ∈ N ( 𝑗 ) exp( κ𝑗,𝑘 )z 𝑘 . (17
)
2 For original HGNNs, the 𝜅 can be replaced with degree-based weight or attentionbased weight.
We completely expand it:
z 𝑢 = 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑢 ) exp( κ𝑖,𝑗 ) * • • • * 1 𝐾 𝑜𝑜 ∑︁ 𝑝 ∈ N (𝑜 )
exp( κ𝑜,𝑝 )z 𝑝 .
(18) Node influences 𝐼 𝑢,𝑣 of 𝑣 on 𝑢 in the message passing output is 𝐼 𝑢,𝑣 = ∥𝜕z 𝑢 /𝜕z 𝑣 ∥, where the norm is any subordinate norm and the node influence measures how a change in 𝑣 passes to a change in 𝑢. By equation (18), the node influence can be computed as:
𝐼 𝑢,𝑣 = ∥ 𝜕z 𝑢 𝜕z 𝑣 ∥ = ∥ 𝜕 𝜕z 𝑣 ( 1 𝐾 𝑢𝑢 ∑︁ 𝑗 ∈ N (𝑢 ) exp( κ𝑖,𝑗 ) * • • • * 1 𝐾 𝑜𝑜 ∑︁ 𝑝 ∈ N (𝑜 ) exp( κ𝑜,𝑝 )z 𝑝 ) ∥. (19)
The partial derivative of the nodes in Equation ( 19) is zero if they are not on the path between node 𝑢 and 𝑣, and then the feature influence can be decomposed into the sum influence of all related paths. Suppose there are 𝑛 paths between 𝑢 and 𝑣, then
𝐼 𝑢,𝑣 = 𝜕 𝜕z 𝑣 𝐼 𝑝 1 + • • • + 𝐼 𝑝 𝑖 • • • + 𝐼 𝑝 𝑛 ,(20)
where
𝐼 𝑝 𝑖 = 1 𝐾 𝑢𝑢 exp( κ𝑢,𝑝 𝑖 𝑗 ) • • • 1 𝐾 𝑝 𝑖 𝑛 𝑖 𝑝 𝑖 𝑛 𝑖 exp( κ𝑝 𝑖 𝑛 𝑖 ,𝑣 ) 𝑆 (𝐼 𝑝 𝑖 ) z 𝑣 .
Note that, in Equation ( 20), the scalar term 𝑆 (𝐼 𝑝 𝑖 ) ranges from (0, 1] and all 𝐼 𝑝 𝑖 (1 ≤ 𝑖 ≤ 𝑛) have the term 𝑚 𝑣 , thus we separate 𝑆 (𝐼 𝑝 𝑖 ) and the rest derivative term and then uses the absolute homogeneous property, i.e., ∥𝛼𝑀 ∥ = |𝛼 |∥𝑀 ∥
𝐼 𝑢,𝑣 = 𝑆 (𝐼 𝑝 1 ) + • • • + 𝑆 (𝐼 𝑝 𝑖 ) • • • + 𝑆 (𝐼 𝑝 𝑛 ) ∥ 𝜕𝑧 𝑣 𝜕𝑧 𝑣 ∥ = 𝑆 (𝐼 𝑝 1 ) + • • • + 𝑆 (𝐼 𝑝 𝑖 ) • • • + 𝑆 (𝐼 𝑝 𝑛 ) ≤ |𝑛 * max(𝑆 (𝐼 𝑝 𝑖 ))| = |𝑛 * 𝛾 𝑛 𝑖 | ≤ |𝑛 * 𝛾 𝑑 * 𝑔 | = 𝐶𝛾 𝑑 * 𝑔 ,(21)
where 𝑑 𝑔 is the shortest path between 𝑢 and 𝑣, 𝑑 * ≤ 𝑛 𝑖 and 0 < 𝛾 ≤ 1, thus the second inequality holds on in Equation (21). For more generality, we use constant 𝐶 to denote the 𝑛. The condition for equality is if and only if 𝑑 * 𝑔 = 1 and the 𝑣 is the unique neighbor of node 𝑢, i.e., 𝛾 = 1 and 𝐶 = 1. □ Theorem 4.3 shows that the node influence using HMP exponentially decays as the shortest graph distance 𝑑 * 𝑔 between two nodes increases. In other words, distant nodes in dense areas will have less interaction, even if they are in a dense connected area. To alleviate the phenomenon, we propose a Curvature-based Homophily Constraint (𝜅HC) to enhance the connection within linked pairs. The basic idea is to push the embeddings of linked nodes closer if their ORC value is larger than a threshold. In this way, we can enforce disjoint node pairs in dense areas or clusters to have more Table 1: Comparisons of the abilities of models in terms of global tree-likeness modeling (Global), local heterogeneous structure learning (Local), and neighbor interaction (Neighbor) are indicated by ✓ for the presence of the ability and × for its absence. </p><h2>publication_ref</h2> <p>['b17', 'b20'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Model Type Models Global Local Neighbor </p><h2>text</h2> <p>Shallow models
EUC × × × HYP ✓ × × Euclidean GNN models GCN × × ✓ GAT × × ✓ SAGE × × ✓ SGC × × ✓ Curvature GNN models CurvGN × ✓ ✓ 𝜅GCN ✓ × ✓ Hyperbolic GNN models HGCN ✓ × ✓ LGCN ✓ × ✓ Curvature-aware HGNN model 𝜅HGCN ✓ ✓ ✓
influence on each other through their mutual neighbors, which is given by:
L 𝜅ℎ𝑐 + = - 1 |𝐸 𝜅 | ∑︁ (𝑖,𝑗 ) ∈𝐸 𝜅 log 𝑝 (x ℓ,H 𝑖 , x ℓ,H 𝑗 ),(22)
where 𝐸 𝜅 is the filtered edge set based on ORC threshold 𝜏 3 ; 𝑝 (•) is the Fermi-Dirac function, indicating the probability of two hyperbolic nodes (𝑢, 𝑣) link or not, which is given by:
𝑝 (x 𝑢 , x 𝑣 ) = exp (𝑑 2 H (x 𝑢 , x 𝑣 ) -𝑟 )/𝑡 + 1 -1 ,(23)
and 𝑑 H (x 𝑢 , x 𝑣 ) is the hyperbolic distance from 𝑢 to 𝑣, 𝑟 and 𝑡 is hyper parameters and we set it as previous work [11]. We also sample the same number of negative link pairs that they have no connections or the curvature is very small based on the results in [74]. Totally,
L 𝜅ℎ𝑐 = L 𝜅ℎ𝑐 + + L 𝜅ℎ𝑐 -.(24)
Geometric Intuition. HMP helps build the connection between the graph topology and the embedding space, adjust the curvature of the hyperbolic geometry, and guide the information flow. It also shortens the distance of two linked nodes in an area with many triangles, helping mitigate the distortion caused by hyperbolic space. Nonetheless, HMP is local inherently, and the proposed 𝜅HC further enhances the interactions of unconnected nodes, which is non-local. </p><h2>publication_ref</h2> <p>['b10', 'b73'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>𝜅HGCN Architecture </p><h2>text</h2> <p>Given the Euclidean feature x 𝐸 , we first project it into the hyperbolic manifold by the exponential map. 𝜅HGCN architecture takes layers of HMP as the encoder. Following the literature, the Fermi-Dirac function is used as a decoder in the link prediction task. For the node classification task, the final hyperbolic vector is mapped back to tangent space and decoded with MLP, which is the same with work [11]. 3 We select edges if their ORC value is larger than a threshold where edges can be constructed by multiple hop neighbors and the weight is added their curvature together based on their shortest distance.  </p><h2>publication_ref</h2> <p>['b10'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>EXPERIMENTS 5.1 Experimental Setup </p><h2>text</h2> <p>Datasets. The evaluation of our work utilizes several datasets, including Disease, Airport, and three benchmark citation networks, namely PubMed and Cora. While Disease and Airport exhibit a more hierarchical structure, the citation networks are less so, making them suitable for demonstrating the generalization capability of our proposal. In Table 2, we provide the data statistics and hyperbolicity metric that measures the tree-likeness of each graph. For further details, please refer to Appendix A.
Baselines. We compare our proposed model with various baselines. ( 1) Shallow Euclidean and hyperbolic models, including Euclidean embeddings (EUC) and Poincaré embeddings (HYP) [56];
(2) Euclidean GNN models, i.e., GCN [35], GraphSAGE (SAGE) [28], Graph Attention Networks (GAT) [78], Simplified Graph Convolution, (SGC) [83]; (3) Curvature GNN models, including Curvature Graph Network (CurvGN) [93] which applies the discrete curvature in Euclidean model and ProdGCN which deploys GNNs to products of constant curvature spaces, both of them are close to our work; Hyperbolic GNNs, including HGCN [11], HGNN [45], HGAT [96],
LGCN [97]. Table 1 presents the different features of the aforementioned models regarding their capabilities for perceiving both global tree-likeness modeling and local heterogeneous structure, as well as their interactional aptitude with regard to neighboring information.
Experimental Details Data split. We evaluate 𝜅HGCN on both node classification and link prediction tasks. The data split is the same with the previous works [11]. More specifically, in link prediction, we randomly split edges into 85%, 5%, 10% for training, validation, and test sets, respectively. For node classification, we split nodes into 70%, 15%, 15% for Airport, 30%, 10%, 60% for Disease, and we use 20 labeled train examples per class for Cora, and PubMed. Implementation details. We closely follow the parameter settings as HGCN [11], fix the number of embedding dimensions to 16 and then perform hyper-parameter search on a validation set over learning rate, weight decay, dropout, and the number of layers. We also adopt the early stopping strategies based on the validation set as [11]. For baselines, we mainly refer to the reported results in the literature, and for the inconsistent cases (such as different embedding dimensions in H2H-GCN), we re-implement their official code in similar experimental settings. Evaluation metric. Following the literature, we report the F1-score for Disease and Airport datasets, and accuracy for the others in the node classification tasks. For the link predictions task, the Area Under Curve (AUC) is calculated.  </p><h2>publication_ref</h2> <p>['b55', 'b34', 'b27', 'b77', 'b82', 'b92', 'b10', 'b44', 'b95', 'b96', 'b10', 'b10', 'b10'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>['tab_2'] </p><h2>heading</h2> <p>Experimental Results </p><h2>text</h2> <p>We report the results of 10 random experiments 4 , including standard deviations in TABLE 3 and4, where the Δ 𝐸 , Δ 𝜅 , Δ 𝐻 is the improvement of the proposed model 𝜅HGCN over the Euclidean GNNs, Curvature-related GNNs and Hyperbolic GNNs, respectively.
Node Classification. The experimental results of node classification are summarized in TABLE 3, where a lower hyperbolicity value corresponds to a more tree-like structure. The key findings are: (1) Overall, the proposed model performs impressively, surpassing previous models on four out of five datasets. Specifically, hyperbolic models (e.g., HGCN, LGCN) perform substantially better on the more hyperbolic dataset (e.g., Disease) than on the less hyperbolic dataset; Euclidean models (e.g., GCN, GAT) find more success on the less hyperbolic datasets (e.g., Cora) than on the more hyperbolic dataset; whereas our model performs better on both datasets, which is consistent with our motivation, namely, that the graph can be better learned under the guidance of curvature. In addition, from the improvements of Δ 𝜅 , we discovered that CurvGN with discrete curvature and 𝜅GCN with continuous curvature both perform worse than our method which validates the power of hyperbolic geometry and the curvature-aware learning. (3) When it comes to Cora, both hyperbolic models and the proposed 𝜅HGCN fail to outperform Euclidean GAT [78], indicating Euclidean geometry is more suitable for modeling data with scarcely hierarchical structures. Nevertheless, it is noted that 𝜅HGCN still outperforms well-known Euclidean GCN models, e.g., GCN [35], SGC [83], SAGE [28]. It is observed that the proposed method 𝜅HGCN also helps to narrow down the gap between hyperbolic models and Euclidean GAT.
Link Prediction. The experimental results of link prediction tasks are summarized in TABLE 4. In the link prediction task, we further have the following observations: (1) Compared with Euclidean 4 The results on Lorentz model are similar. counterparts, Our proposed 𝜅HGCN, and other hyperbolic models have achieved better performance. It is because hyperbolic space owns a larger embedding space, where the structural dependencies could be well preserved by the link prediction loss, providing more space or boundary for nodes to be well arranged; (2) In comparison with the advanced hyperbolic models, our model also obtains remarkable gains and refresh the records. According to the above extensive experiments, we are safe to conclude that equipping ORC with hyperbolic geometry further improves its generalization ability, obtaining high-quality representations for both tree-like and non-tree-like structured data. This confirms our primary motivation that the curvature carries rich information which is beneficial for graph representation learning in the embedding manifold. Specifically, incorporating the structure information featured by ORC helps the models developed in a continuous manifold with negative curvature to perceive the role of each node, accelerating the learning procedure, and reducing the distortion for graph embedding of less hierarchical networks. </p><h2>publication_ref</h2> <p>['b0', 'b77', 'b34', 'b82', 'b27', 'b1'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>['tab_3', 'tab_4', 'tab_3'] </p><h2>heading</h2> <p>Effectiveness of Aggregations </p><h2>text</h2> <p>In Section 4.1, we introduce two tangential aggregation strategies: the curvature-based approach (denoted as Curv) and the featureaugmented method (denoted as CurvAtt). The performance of  these two aggregation strategies is evaluated and reported in Table 5. Our results show that the feature-augmented approach outperforms the curvature-based one in most cases, which can be attributed to the complex nature of real-world systems and the incongruities between node features and topology. The feature-augmented method offers a more flexible and adaptive way to synthesize information from various sources, thus resulting in improved performance. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>['tab_5'] </p><h2>heading</h2> <p>Effectiveness of 𝜅HC </p><h2>text</h2> <p>Figure 5 displays the results of adding 𝜅HC or not on 𝜅HGCN. As it observed, the performance degenerates substantially on DISEASE when applying 𝜅HC, while there are significant improvements on the three citation networks, i.e., PubMed, and Cora. This phenomenon can be understood as follows. Adding 𝜅HC as in node classification will force linked nodes to obtain more similar representations. For the pure tree-like dataset, i.e., DISEASE (without any triangle and circle), these node pairs belong to different levels, and adding 𝜅HC impairs the learning of asymmetric dependencies, which further affects the establishment of hierarchical awareness. When it comes to the citation networks instead, 𝜅HC helps to reduce the distortion caused by hyperbolic geometry and thus boost the learning. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>['fig_3'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>Case Study of Ricci Curvature Weights </p><h2>text</h2> <p>In this section, we demonstrate the effectiveness of our method through a case study. We extract a subtree centered on a randomly sampled node (node 10, in this case), from the Disease dataset, where node 2 is from a higher level and the remaining nodes (42,43,44,45) belong to a lower level. The edges depict disease propagation paths. As shown in the upper sub-figures of Figure 6, we display the corresponding edge weights assigned by both 𝜅HGCN and HGCN during the node classification task. The comparison between the two reveals that 𝜅HGCN (upper right) effectively distinguishes node levels, as it assigns greater importance to the parent node (node 2) and equally emphasizes the child nodes from the same level, whereas HGCN (upper left) fails to make such distinctions. This substantiates the importance of ORC in facilitating hierarchical learning.
Furthermore, we illustrate a subgraph with triangles selected from the citation network, Cora, as depicted in the lower two sub-figures of Figure 6. This subgraph comprises nodes 𝑜, 𝑒, 𝑑 that form a triangle. We examine the edge weights around node 𝑜 and present the results in the lower two sub-figures of Figure 6. Observing these results, it is evident that 𝜅HGCN (as shown in the lower right) effectively identifies the local triangle structure and assigns larger weights to promote inter-node message exchange. In contrast, HGCN (as depicted in the lower left) fails to grasp the intricate topology in this area. These observations further attest to the efficacy of our proposed method in uncovering local clusters and mitigating the distortions imposed by hyperbolic geometry. </p><h2>publication_ref</h2> <p>['b41', 'b42', 'b43', 'b44'] </p><h2>figure_ref</h2> <p>['fig_4', 'fig_4', 'fig_4'] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>CONCLUSION </p><h2>text</h2> <p>For modeling tree-like structures, the hyperbolic space has demonstrated its ability to capture hierarchical relationships. However, approximating a discrete tree-like graph with a hyperbolic manifold can result in inevitable distortions, as real-world tree-like graphs are inherently complex. In this work, we integrate the intrinsic graph structure into the continuous hyperbolic embedding space via the discrete Ricci curvature. As expected, the graph curvature facilitates the node to perceive the role and the hierarchy it belongs to, helping accelerate the hierarchical formation as well as alleviate the distortion in local clusters or cliques. The superiority of the proposal is demonstrated by extensive experiments. Curvature is a geometric notion with appealing and descriptive properties for both network and continuous space. Via the interaction of curvatures, we can build proper connections for a graph and the embedding space to obtain high-quality representations, which is a promising direction to advance geometric learning. In the future, we will consider the use of Ricci flow, a more sophisticated geometric concept derived from Ricci curvature, to further enhance graph embedding and graph machine learning in tree-likeness modeling. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>APPENDIX A DATASETS </p><h2>text</h2> <p>In this section, we provide details about the datasets used in our study. The Disease dataset contains nodes that are labeled as infected or not infected with a disease, with features indicating their susceptibility to the disease. The disease-spreading network in this dataset displays a clear hierarchical structure, which makes it ideal for testing the effectiveness of hyperbolic embedding models. We use this dataset to validate our proposal. The Airport dataset consists of nodes representing airports, with edges indicating the existence of routes between two airports and labels reflecting the population of the respective country that the airport belongs to. In contrast, the PubMed and Cora datasets represent scientific papers as nodes, with edges indicating citations and labels corresponding to academic subfields. Additionally, the Disease and Airport datasets have imbalanced node labels, rendering accuracy an inadequate measure of model performance. Therefore, we use the F1 score as a more suitable measure for imbalanced datasets. Conversely, for the remaining datasets with balanced node classes, we use accuracy to evaluate the models. Furthermore, note that according to the official code of HGCN 5 , the data statistics for Disease in node classification and link prediction differ slightly, and we list them separately in Table 2 for clarity.
Hyperbolicity 𝛿 is a metric that quantifies how closely a graph resembles a tree structure, with lower values of 𝛿 indicating greater tree-like characteristics. A 𝛿 value of 0 corresponds to a tree, while higher hyperbolicity values indicate a less tree-like structure. The relationship between hyperbolicity and tree-like characteristics has been established in various studies, including [3,32,53]. </p><h2>publication_ref</h2> <p>['b2', 'b31', 'b52'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>['tab_2'] </p><h2>heading</h2> <p>B HYPERBOLIC GEOMETRY </p><h2>text</h2> <p>The geometry of a Riemannian manifold is defined by its curvature: elliptic geometry for positive curvature, Euclidean geometry for zero curvature, and hyperbolic geometry for negative curvature. In this study, we concentrate on the latter, i.e. hyperbolic geometry. There exist several equivalent hyperbolic models that exhibit diverse characteristics, yet are mathematically isometric. Our main focus will be on two extensively researched hyperbolic models.: the Poincaré ball model [56] and the Lorentz model (also known as the hyperboloid model) [57]. Let ∥.∥ be the Euclidean norm and ⟨., .⟩ L denote the Minkowski inner product, respectively. The two models are denoted by Definition B.1 and Definition B.2. A compilation of the formulas and operations associated with these models, such as distance, mapping, and parallel transport, is presented in Table 6. These operations include Möbius addition [75] denoted by ⊕ 𝜅 and the gyration operator [75]  </p><h2>publication_ref</h2> <p>['b55', 'b56', 'b74', 'b74'] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>['tab_7'] </p><h2>heading</h2> <p>C MORE ANALYSIS C.1 Computational Complexity of ORC </p><h2>text</h2> <p>The computation of ORC is formulated as linear programming problems [18,46], and its computational complexity is 𝑂 (|𝐸|𝑑 3 max ), where 𝑑 max is the maximum degree of the graph. To illustrate the computational cost, we present the actual run-time on a machine with the environment Intel(R) Xeon(R) Gold 6132 CPU @ 2.60GHZ in TABLE 7. The results indicate that the time required for computing ORC is proportional to the size of the graph, and the cost for smaller graphs is correspondingly lower. Importantly, it is noteworthy that ORC only needs to be computed once prior to the training process, with the same computational complexity as HGCN [11] during both training and inference. To handle extremely large-scale graphs, approximation methods such as Sinkhorn [19] or Jaccard proxy [59] may be utilized. The efficacy of 𝜅HGCN and HGCN in learning representations for node classification is demonstrated through the visualization of their performance on the Disease and Cora datasets. To accomplish this, we employ the t-distributed Stochastic Neighbor Embedding (t-SNE) technique [77] to reduce the high-dimensional embeddings produced by the final layer of each model to a two-dimensional plane for visual examination. The results, shown in Figure 7, depict nodes as individual points, where each point is assigned a color that corresponds to its class. The visualization indicates that    </p><h2>publication_ref</h2> <p>['b17', 'b45', 'b10', 'b18', 'b58', 'b76'] </p><h2>figure_ref</h2> <p>['fig_5'] </p><h2>table_ref</h2> <p>['tab_8'] </p><h2>heading</h2> <p>C.2 Embedding Visualization </p><h2>text</h2> <p>= x ∈ R 𝑛 : ⟨x, x⟩ 2 < -1 𝜅 L 𝑛 𝜅 = x ∈ R 𝑛+1 : ⟨x, x⟩ L = 1 𝜅 Metric 𝑔 B𝜅 x = 𝜆 𝜅 </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p><h2>heading</h2> <p>C.3 Curvature-wise Performance </p><h2>text</h2> <p>The objective of the study is to perceive the local structure around nodes in tree-like graphs, encompassing local tree-like, zero-density, and densely connected structures. To demonstrate the effectiveness of the proposed method, we conducted further analysis by classifying nodes into defined local substructures. As an example, consider the following, we set the nodes to the following three types: tree-like (𝜅 𝑖 ≥ -0.01); zero-like (-0.01 < 𝜅 𝑖 ≤ 0.01); positive-like (𝜅 𝑖 > 0.01). The curvature of each node is defined as the sum of curvatures over its edges, that is:
𝜅 𝑖 = 1 |𝑁 𝑖 |
𝑗 ∈𝑁 𝑖 𝜅 𝑖 𝑗 , where |𝑁 𝑖 | is the number of neighbors of node 𝑖, and 𝜅 𝑖 𝑗 is the curvature of the edge between nodes 𝑖 and 𝑗. This curvature measure was used to determine the local substructure of each node. In the following, we evaluated the performance of three models (GCN, HGCN, and the proposed method) on both the Cora and Airport datasets. Specifically, we calculated the accuracy/F1-score on the test set for nodes in each local substructure.
The results are shown in Table 8 and Table 9. The "GT-Prop" row in the Tables show the proportion of nodes belonging to each Overall, the study found that the models achieved comparable accuracy to the best achievable, but performance varied across different substructures. Specifically, Euclidean GCN outperformed HGCN on zero-density and densely connected areas, but underperformed on tree-like nodes. In contrast, HGCN showed improved accuracy for tree-like nodes, but lower accuracy on other substructures. The proposed model achieved a balance of performance across all substructures, with high accuracy for both tree-like and non-tree-like nodes. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>['tab_9', 'tab_10'] </p><h2>heading</h2> <p>C.4 Connections of the Discrete and Continuous Curvatures </p><h2>text</h2> <p>The discrete curvature 𝜅 is computed in advance, which can be regarded as an edge weight. The continuous curvature 𝑐 of the predefined hyperbolic space is learnable and differentiable. For simplicity, we denote the learnable parameters of our model 𝜅HGCN as 𝜃 . Let us consider node classification as an example. If the groundtruth label of node x is y and the predicted label is ȳ, then we have ȳ = 𝜅HGCN(x, 𝜅, 𝜃, 𝑐) and the loss is given by 𝐿(y, ȳ). We can take the derivative of 𝑐 with respect to the loss, i.e., 𝜕𝐿 (y,ȳ) 𝜕𝑐 = 𝜕 (y,𝜅HGCN(x,𝜅,𝜃,𝑐 ) ) 𝜕𝑐
. It is easy to see that the update of 𝑐 is constrained by 𝜅. In other words, we learn a good embedding space equipped with curvature 𝑐 that matches the graph structure through discrete Ricci curvature 𝜅. </p><h2>publication_ref</h2> <p>[] </p><h2>figure_ref</h2> <p>[] </p><h2>table_ref</h2> <p>[] </p>